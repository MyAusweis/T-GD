{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 250\n",
    "test_batch = 250\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_style2/1000shot' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.2\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.2\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.01\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.5, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "#         auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "#         arc.update(auroc, inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 16/16 | Loss:1.0529 | MainLoss:0.9226 | SPLoss:1.1501 | CLSLoss:1.5266 | top1:51.3867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8152 | MainLoss:0.6924 | SPLoss:1.0818 | CLSLoss:1.4672 | top1:53.5897 | AUROC:0.5697\n",
      "Test | 129/16 | Loss:0.7885 | MainLoss:0.6657 | SPLoss:1.0818 | CLSLoss:1.4672 | top1:92.0156 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 16/16 | Loss:0.7988 | MainLoss:0.6939 | SPLoss:0.9045 | CLSLoss:1.4402 | top1:50.9067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7782 | MainLoss:0.6914 | SPLoss:0.7268 | CLSLoss:1.4083 | top1:55.3462 | AUROC:0.5661\n",
      "Test | 129/16 | Loss:0.7041 | MainLoss:0.6173 | SPLoss:0.7268 | CLSLoss:1.4083 | top1:99.0872 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 16/16 | Loss:0.7637 | MainLoss:0.6916 | SPLoss:0.5830 | CLSLoss:1.3774 | top1:52.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7462 | MainLoss:0.6887 | SPLoss:0.4408 | CLSLoss:1.3435 | top1:54.8590 | AUROC:0.5690\n",
      "Test | 129/16 | Loss:0.5312 | MainLoss:0.4737 | SPLoss:0.4408 | CLSLoss:1.3435 | top1:99.7041 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 16/16 | Loss:0.7328 | MainLoss:0.6851 | SPLoss:0.3450 | CLSLoss:1.3109 | top1:54.6933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7166 | MainLoss:0.6775 | SPLoss:0.2631 | CLSLoss:1.2747 | top1:57.1282 | AUROC:0.6027\n",
      "Test | 129/16 | Loss:0.2324 | MainLoss:0.1934 | SPLoss:0.2631 | CLSLoss:1.2747 | top1:99.8505 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 16/16 | Loss:0.7129 | MainLoss:0.6737 | SPLoss:0.2695 | CLSLoss:1.2278 | top1:58.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7401 | MainLoss:0.7006 | SPLoss:0.2772 | CLSLoss:1.1726 | top1:59.8077 | AUROC:0.6869\n",
      "Test | 129/16 | Loss:0.0688 | MainLoss:0.0294 | SPLoss:0.2772 | CLSLoss:1.1726 | top1:99.9315 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 16/16 | Loss:0.7165 | MainLoss:0.6692 | SPLoss:0.3644 | CLSLoss:1.0878 | top1:60.3467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6719 | MainLoss:0.6210 | SPLoss:0.4056 | CLSLoss:1.0364 | top1:65.1538 | AUROC:0.7352\n",
      "Test | 129/16 | Loss:0.1476 | MainLoss:0.0966 | SPLoss:0.4056 | CLSLoss:1.0364 | top1:99.8162 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 16/16 | Loss:0.6979 | MainLoss:0.6317 | SPLoss:0.5652 | CLSLoss:0.9620 | top1:65.2000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6866 | MainLoss:0.6097 | SPLoss:0.6762 | CLSLoss:0.9330 | top1:68.4872 | AUROC:0.8087\n",
      "Test | 129/16 | Loss:0.1002 | MainLoss:0.0232 | SPLoss:0.6762 | CLSLoss:0.9330 | top1:99.7788 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 16/16 | Loss:0.7679 | MainLoss:0.6707 | SPLoss:0.8928 | CLSLoss:0.7937 | top1:62.3200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8392 | MainLoss:0.7188 | SPLoss:1.1363 | CLSLoss:0.6819 | top1:50.0000 | AUROC:0.7956\n",
      "Test | 129/16 | Loss:0.6802 | MainLoss:0.5597 | SPLoss:1.1363 | CLSLoss:0.6819 | top1:50.2368 | AUROC:0.9998\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 16/16 | Loss:6.9515 | MainLoss:0.6438 | SPLoss:63.0224 | CLSLoss:0.5531 | top1:63.1200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.8196 | MainLoss:0.5329 | SPLoss:62.8168 | CLSLoss:0.5068 | top1:72.1667 | AUROC:0.8635\n",
      "Test | 129/16 | Loss:6.7401 | MainLoss:0.4534 | SPLoss:62.8169 | CLSLoss:0.5068 | top1:80.2897 | AUROC:0.9277\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 16/16 | Loss:5.2930 | MainLoss:0.6194 | SPLoss:46.7010 | CLSLoss:0.3486 | top1:66.6133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.8039 | MainLoss:0.4285 | SPLoss:33.7080 | CLSLoss:0.4690 | top1:81.0128 | AUROC:0.8915\n",
      "Test | 129/16 | Loss:4.0782 | MainLoss:0.7027 | SPLoss:33.7080 | CLSLoss:0.4690 | top1:67.5545 | AUROC:0.7397\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:5.0236 | MainLoss:0.6540 | SPLoss:43.6686 | CLSLoss:0.2725 | top1:65.4933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:11.9750 | MainLoss:0.6035 | SPLoss:113.6977 | CLSLoss:0.1758 | top1:72.4359 | AUROC:0.7972\n",
      "Test | 129/16 | Loss:12.0085 | MainLoss:0.6369 | SPLoss:113.6979 | CLSLoss:0.1758 | top1:64.8006 | AUROC:0.7342\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:7.7327 | MainLoss:0.6212 | SPLoss:71.0950 | CLSLoss:0.1984 | top1:66.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.0203 | MainLoss:0.5599 | SPLoss:54.5801 | CLSLoss:0.2400 | top1:71.8077 | AUROC:0.7853\n",
      "Test | 129/16 | Loss:6.1317 | MainLoss:0.6713 | SPLoss:54.5801 | CLSLoss:0.2400 | top1:61.0623 | AUROC:0.6505\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 16/16 | Loss:4.0476 | MainLoss:0.5280 | SPLoss:35.1706 | CLSLoss:0.2617 | top1:74.7200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:9.4427 | MainLoss:0.6230 | SPLoss:88.1708 | CLSLoss:0.2614 | top1:64.9487 | AUROC:0.7268\n",
      "Test | 129/16 | Loss:9.5405 | MainLoss:0.7208 | SPLoss:88.1709 | CLSLoss:0.2614 | top1:54.2991 | AUROC:0.5741\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 16/16 | Loss:13.9972 | MainLoss:0.4854 | SPLoss:135.0867 | CLSLoss:0.3137 | top1:77.2000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:10.5463 | MainLoss:0.4503 | SPLoss:100.9206 | CLSLoss:0.3965 | top1:80.6795 | AUROC:0.9193\n",
      "Test | 129/16 | Loss:11.0581 | MainLoss:0.9620 | SPLoss:100.9207 | CLSLoss:0.3965 | top1:55.5950 | AUROC:0.5773\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 16/16 | Loss:6.6647 | MainLoss:0.3838 | SPLoss:62.7713 | CLSLoss:0.3826 | top1:83.6800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.6076 | MainLoss:0.2691 | SPLoss:33.3451 | CLSLoss:0.3932 | top1:89.2564 | AUROC:0.9618\n",
      "Test | 129/16 | Loss:4.5817 | MainLoss:1.2432 | SPLoss:33.3452 | CLSLoss:0.3932 | top1:50.6978 | AUROC:0.5037\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 16/16 | Loss:2.9736 | MainLoss:0.3479 | SPLoss:26.2148 | CLSLoss:0.4279 | top1:85.7333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.9565 | MainLoss:0.3942 | SPLoss:25.5868 | CLSLoss:0.3616 | top1:84.1538 | AUROC:0.9269\n",
      "Test | 129/16 | Loss:3.5142 | MainLoss:0.9519 | SPLoss:25.5868 | CLSLoss:0.3616 | top1:49.8816 | AUROC:0.4663\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 16/16 | Loss:2.0783 | MainLoss:0.3037 | SPLoss:17.7009 | CLSLoss:0.4565 | top1:87.5467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.0521 | MainLoss:0.4588 | SPLoss:15.8837 | CLSLoss:0.4895 | top1:80.2179 | AUROC:0.9701\n",
      "Test | 129/16 | Loss:2.5040 | MainLoss:0.9107 | SPLoss:15.8837 | CLSLoss:0.4895 | top1:56.2773 | AUROC:0.5903\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 16/16 | Loss:2.6500 | MainLoss:0.3641 | SPLoss:22.8149 | CLSLoss:0.4416 | top1:85.2533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.0879 | MainLoss:0.2317 | SPLoss:28.5137 | CLSLoss:0.4903 | top1:90.9231 | AUROC:0.9720\n",
      "Test | 129/16 | Loss:4.1195 | MainLoss:1.2632 | SPLoss:28.5137 | CLSLoss:0.4903 | top1:53.4112 | AUROC:0.6175\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 16/16 | Loss:180.4609 | MainLoss:0.3507 | SPLoss:1801.0560 | CLSLoss:0.4552 | top1:84.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:165.2278 | MainLoss:0.4192 | SPLoss:1648.0492 | CLSLoss:0.3642 | top1:82.3462 | AUROC:0.9236\n",
      "Test | 129/16 | Loss:165.8335 | MainLoss:1.0249 | SPLoss:1648.0485 | CLSLoss:0.3642 | top1:48.5047 | AUROC:0.4154\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 16/16 | Loss:100.8113 | MainLoss:0.3123 | SPLoss:1004.9449 | CLSLoss:0.4527 | top1:87.0133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:52.6528 | MainLoss:0.1744 | SPLoss:524.7350 | CLSLoss:0.4895 | top1:93.5128 | AUROC:0.9819\n",
      "Test | 129/16 | Loss:54.0084 | MainLoss:1.5300 | SPLoss:524.7358 | CLSLoss:0.4895 | top1:51.1184 | AUROC:0.5788\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 16/16 | Loss:43.8083 | MainLoss:0.3306 | SPLoss:434.7304 | CLSLoss:0.4588 | top1:85.9467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:34.7063 | MainLoss:0.2623 | SPLoss:344.3929 | CLSLoss:0.4742 | top1:89.6667 | AUROC:0.9702\n",
      "Test | 129/16 | Loss:35.6222 | MainLoss:1.1782 | SPLoss:344.3924 | CLSLoss:0.4742 | top1:49.1807 | AUROC:0.4616\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 16/16 | Loss:159.0835 | MainLoss:0.3478 | SPLoss:1587.3134 | CLSLoss:0.4412 | top1:85.9467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:109.8945 | MainLoss:0.3997 | SPLoss:1094.9038 | CLSLoss:0.4481 | top1:82.3718 | AUROC:0.9731\n",
      "Test | 129/16 | Loss:110.4606 | MainLoss:0.9658 | SPLoss:1094.9037 | CLSLoss:0.4481 | top1:50.2336 | AUROC:0.5103\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 16/16 | Loss:94.5028 | MainLoss:0.3434 | SPLoss:941.5527 | CLSLoss:0.4182 | top1:85.3067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:50.5732 | MainLoss:0.3799 | SPLoss:501.8940 | CLSLoss:0.3894 | top1:82.5513 | AUROC:0.9514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 129/16 | Loss:51.0522 | MainLoss:0.8590 | SPLoss:501.8930 | CLSLoss:0.3894 | top1:51.5016 | AUROC:0.5263\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 16/16 | Loss:31.7946 | MainLoss:0.3068 | SPLoss:314.8351 | CLSLoss:0.4281 | top1:88.1867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:18.0294 | MainLoss:0.4889 | SPLoss:175.3637 | CLSLoss:0.4162 | top1:77.6410 | AUROC:0.9645\n",
      "Test | 129/16 | Loss:18.4293 | MainLoss:0.8887 | SPLoss:175.3639 | CLSLoss:0.4162 | top1:54.0467 | AUROC:0.5600\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 16/16 | Loss:14.7331 | MainLoss:0.3734 | SPLoss:143.5577 | CLSLoss:0.3910 | top1:84.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:8.6165 | MainLoss:0.3180 | SPLoss:82.9413 | CLSLoss:0.4367 | top1:87.0000 | AUROC:0.9691\n",
      "Test | 129/16 | Loss:9.2550 | MainLoss:0.9565 | SPLoss:82.9413 | CLSLoss:0.4367 | top1:56.0343 | AUROC:0.5964\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 16/16 | Loss:7.8735 | MainLoss:0.3401 | SPLoss:75.2932 | CLSLoss:0.4051 | top1:86.1067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.2236 | MainLoss:0.3836 | SPLoss:58.3582 | CLSLoss:0.4239 | top1:83.4359 | AUROC:0.9635\n",
      "Test | 129/16 | Loss:6.8170 | MainLoss:0.9769 | SPLoss:58.3581 | CLSLoss:0.4239 | top1:53.3458 | AUROC:0.5491\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 16/16 | Loss:16.5376 | MainLoss:0.3653 | SPLoss:161.6822 | CLSLoss:0.4084 | top1:84.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:85.5403 | MainLoss:0.4737 | SPLoss:850.6295 | CLSLoss:0.3653 | top1:77.2949 | AUROC:0.8878\n",
      "Test | 129/16 | Loss:86.1309 | MainLoss:1.0643 | SPLoss:850.6301 | CLSLoss:0.3653 | top1:50.1931 | AUROC:0.4751\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 16/16 | Loss:52.7253 | MainLoss:0.4313 | SPLoss:522.9040 | CLSLoss:0.3614 | top1:80.3733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:26.7172 | MainLoss:0.5646 | SPLoss:261.4900 | CLSLoss:0.3524 | top1:73.8077 | AUROC:0.9439\n",
      "Test | 129/16 | Loss:26.9327 | MainLoss:0.7802 | SPLoss:261.4900 | CLSLoss:0.3524 | top1:58.2586 | AUROC:0.6140\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 16/16 | Loss:17.6309 | MainLoss:0.3480 | SPLoss:172.7901 | CLSLoss:0.3881 | top1:85.6533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:9.1513 | MainLoss:0.2665 | SPLoss:88.8025 | CLSLoss:0.4505 | top1:89.1923 | AUROC:0.9566\n",
      "Test | 129/16 | Loss:10.3206 | MainLoss:1.4358 | SPLoss:88.8024 | CLSLoss:0.4505 | top1:49.1620 | AUROC:0.4732\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 16/16 | Loss:6.3168 | MainLoss:0.3365 | SPLoss:59.7607 | CLSLoss:0.4156 | top1:86.1333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.6425 | MainLoss:0.3682 | SPLoss:32.7010 | CLSLoss:0.4155 | top1:85.2051 | AUROC:0.9329\n",
      "Test | 129/16 | Loss:4.5523 | MainLoss:1.2780 | SPLoss:32.7010 | CLSLoss:0.4155 | top1:49.6854 | AUROC:0.4767\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 16/16 | Loss:4.9540 | MainLoss:0.3790 | SPLoss:45.7104 | CLSLoss:0.3989 | top1:83.4933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.3644 | MainLoss:0.3017 | SPLoss:50.5883 | CLSLoss:0.3836 | top1:88.7436 | AUROC:0.9511\n",
      "Test | 129/16 | Loss:6.1128 | MainLoss:1.0501 | SPLoss:50.5884 | CLSLoss:0.3836 | top1:49.7041 | AUROC:0.4777\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 16/16 | Loss:301.4436 | MainLoss:0.3747 | SPLoss:3010.6482 | CLSLoss:0.4035 | top1:83.7067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:282.4023 | MainLoss:0.5351 | SPLoss:2818.6396 | CLSLoss:0.3237 | top1:72.7821 | AUROC:0.9182\n",
      "Test | 129/16 | Loss:282.8625 | MainLoss:0.9952 | SPLoss:2818.6360 | CLSLoss:0.3237 | top1:49.7227 | AUROC:0.4572\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 16/16 | Loss:169.9589 | MainLoss:0.3251 | SPLoss:1696.2963 | CLSLoss:0.4136 | top1:86.0267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:84.2432 | MainLoss:0.5170 | SPLoss:837.2214 | CLSLoss:0.4068 | top1:76.8333 | AUROC:0.9527\n",
      "Test | 129/16 | Loss:85.3880 | MainLoss:1.6618 | SPLoss:837.2220 | CLSLoss:0.4068 | top1:49.5078 | AUROC:0.3658\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 16/16 | Loss:53.9245 | MainLoss:0.3788 | SPLoss:535.4164 | CLSLoss:0.4060 | top1:84.0533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:35.7045 | MainLoss:0.3227 | SPLoss:353.7750 | CLSLoss:0.4297 | top1:86.8205 | AUROC:0.9380\n",
      "Test | 129/16 | Loss:36.6112 | MainLoss:1.2294 | SPLoss:353.7757 | CLSLoss:0.4297 | top1:50.0872 | AUROC:0.4802\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 16/16 | Loss:23.3640 | MainLoss:0.3872 | SPLoss:229.7272 | CLSLoss:0.4078 | top1:82.0533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:12.5273 | MainLoss:0.4193 | SPLoss:121.0389 | CLSLoss:0.4072 | top1:80.3974 | AUROC:0.9465\n",
      "Test | 129/16 | Loss:13.3323 | MainLoss:1.2243 | SPLoss:121.0390 | CLSLoss:0.4072 | top1:49.6854 | AUROC:0.4687\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 16/16 | Loss:9.0973 | MainLoss:0.3411 | SPLoss:87.5210 | CLSLoss:0.4128 | top1:85.5733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.0141 | MainLoss:0.4295 | SPLoss:55.8034 | CLSLoss:0.4283 | top1:82.4103 | AUROC:0.9675\n",
      "Test | 129/16 | Loss:6.6215 | MainLoss:1.0369 | SPLoss:55.8035 | CLSLoss:0.4283 | top1:51.2897 | AUROC:0.5121\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 16/16 | Loss:4.1843 | MainLoss:0.3232 | SPLoss:38.5694 | CLSLoss:0.4166 | top1:86.8267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.7744 | MainLoss:0.2259 | SPLoss:25.4428 | CLSLoss:0.4226 | top1:91.3205 | AUROC:0.9699\n",
      "Test | 129/16 | Loss:3.8420 | MainLoss:1.2935 | SPLoss:25.4428 | CLSLoss:0.4226 | top1:50.1931 | AUROC:0.5133\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 16/16 | Loss:2.3491 | MainLoss:0.3001 | SPLoss:20.4464 | CLSLoss:0.4406 | top1:87.9200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.1668 | MainLoss:0.5641 | SPLoss:45.9811 | CLSLoss:0.4589 | top1:76.4872 | AUROC:0.9226\n",
      "Test | 129/16 | Loss:5.5022 | MainLoss:0.8995 | SPLoss:45.9811 | CLSLoss:0.4589 | top1:58.1153 | AUROC:0.6257\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 16/16 | Loss:58.5228 | MainLoss:0.4363 | SPLoss:580.8292 | CLSLoss:0.3653 | top1:81.6533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:45.5737 | MainLoss:0.2550 | SPLoss:453.1507 | CLSLoss:0.3647 | top1:90.9103 | AUROC:0.9677\n",
      "Test | 129/16 | Loss:46.3598 | MainLoss:1.0411 | SPLoss:453.1511 | CLSLoss:0.3647 | top1:50.5047 | AUROC:0.5280\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 16/16 | Loss:28.1106 | MainLoss:0.3464 | SPLoss:277.6035 | CLSLoss:0.3842 | top1:85.8400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:14.0886 | MainLoss:0.2693 | SPLoss:138.1505 | CLSLoss:0.4189 | top1:89.4359 | AUROC:0.9710\n",
      "Test | 129/16 | Loss:15.3780 | MainLoss:1.5587 | SPLoss:138.1507 | CLSLoss:0.4189 | top1:49.8037 | AUROC:0.5025\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 16/16 | Loss:18.1783 | MainLoss:0.3184 | SPLoss:178.5560 | CLSLoss:0.4299 | top1:86.0533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:33.1129 | MainLoss:0.2377 | SPLoss:328.7063 | CLSLoss:0.4539 | top1:90.5897 | AUROC:0.9683\n",
      "Test | 129/16 | Loss:34.3570 | MainLoss:1.4818 | SPLoss:328.7065 | CLSLoss:0.4539 | top1:50.9346 | AUROC:0.5775\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 16/16 | Loss:20.5460 | MainLoss:0.3063 | SPLoss:202.3538 | CLSLoss:0.4406 | top1:87.7067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:11.8512 | MainLoss:0.4235 | SPLoss:114.2330 | CLSLoss:0.4313 | top1:81.2821 | AUROC:0.9424\n",
      "Test | 129/16 | Loss:12.8243 | MainLoss:1.3967 | SPLoss:114.2330 | CLSLoss:0.4313 | top1:50.1963 | AUROC:0.5494\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 16/16 | Loss:7.4080 | MainLoss:0.3222 | SPLoss:70.8147 | CLSLoss:0.4340 | top1:86.8533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.9330 | MainLoss:0.2665 | SPLoss:36.6134 | CLSLoss:0.5132 | top1:89.5897 | AUROC:0.9695\n",
      "Test | 129/16 | Loss:5.1879 | MainLoss:1.5215 | SPLoss:36.6134 | CLSLoss:0.5132 | top1:50.8411 | AUROC:0.5608\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 16/16 | Loss:49.1507 | MainLoss:0.3190 | SPLoss:488.2693 | CLSLoss:0.4697 | top1:86.6667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:31.0777 | MainLoss:0.2582 | SPLoss:308.1443 | CLSLoss:0.5038 | top1:89.3462 | AUROC:0.9755\n",
      "Test | 129/16 | Loss:32.0006 | MainLoss:1.1811 | SPLoss:308.1441 | CLSLoss:0.5038 | top1:52.6355 | AUROC:0.5751\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 16/16 | Loss:19.4399 | MainLoss:0.3265 | SPLoss:191.0873 | CLSLoss:0.4645 | top1:86.6133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:10.1711 | MainLoss:0.2314 | SPLoss:99.3514 | CLSLoss:0.4563 | top1:90.8077 | AUROC:0.9710\n",
      "Test | 129/16 | Loss:11.4171 | MainLoss:1.4773 | SPLoss:99.3513 | CLSLoss:0.4563 | top1:50.7570 | AUROC:0.5296\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:9.6322 | MainLoss:0.3692 | SPLoss:92.5881 | CLSLoss:0.4257 | top1:84.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:11.3284 | MainLoss:0.3019 | SPLoss:110.2292 | CLSLoss:0.3582 | top1:89.3718 | AUROC:0.9578\n",
      "Test | 129/16 | Loss:11.8985 | MainLoss:0.8720 | SPLoss:110.2291 | CLSLoss:0.3582 | top1:51.8536 | AUROC:0.6187\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 16/16 | Loss:9.9858 | MainLoss:0.3772 | SPLoss:96.0442 | CLSLoss:0.4132 | top1:84.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.4715 | MainLoss:0.3807 | SPLoss:50.8730 | CLSLoss:0.3526 | top1:87.0000 | AUROC:0.9476\n",
      "Test | 129/16 | Loss:6.1096 | MainLoss:1.0188 | SPLoss:50.8730 | CLSLoss:0.3526 | top1:48.9439 | AUROC:0.4818\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 16/16 | Loss:3.4841 | MainLoss:0.3032 | SPLoss:31.7641 | CLSLoss:0.4483 | top1:88.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.0690 | MainLoss:0.2714 | SPLoss:17.9306 | CLSLoss:0.4574 | top1:89.5513 | AUROC:0.9629\n",
      "Test | 129/16 | Loss:2.8579 | MainLoss:1.0602 | SPLoss:17.9306 | CLSLoss:0.4574 | top1:51.8474 | AUROC:0.5325\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 16/16 | Loss:4.6661 | MainLoss:0.3236 | SPLoss:43.3780 | CLSLoss:0.4731 | top1:86.5867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.7666 | MainLoss:0.3445 | SPLoss:54.1789 | CLSLoss:0.4246 | top1:86.3333 | AUROC:0.9570\n",
      "Test | 129/16 | Loss:6.3669 | MainLoss:0.9448 | SPLoss:54.1790 | CLSLoss:0.4246 | top1:53.1838 | AUROC:0.5780\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 16/16 | Loss:3.6787 | MainLoss:0.2822 | SPLoss:33.9186 | CLSLoss:0.4611 | top1:89.1200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.0883 | MainLoss:0.2406 | SPLoss:18.4315 | CLSLoss:0.4607 | top1:90.9872 | AUROC:0.9805\n",
      "Test | 129/16 | Loss:2.8481 | MainLoss:1.0003 | SPLoss:18.4315 | CLSLoss:0.4607 | top1:54.5109 | AUROC:0.6334\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 16/16 | Loss:12.9860 | MainLoss:0.3251 | SPLoss:126.5632 | CLSLoss:0.4564 | top1:86.7200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:21.3098 | MainLoss:0.3313 | SPLoss:209.7419 | CLSLoss:0.4314 | top1:85.9231 | AUROC:0.9414\n",
      "Test | 129/16 | Loss:22.0939 | MainLoss:1.1154 | SPLoss:209.7420 | CLSLoss:0.4314 | top1:50.7508 | AUROC:0.5884\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 16/16 | Loss:20.1425 | MainLoss:0.2829 | SPLoss:198.5516 | CLSLoss:0.4418 | top1:88.4533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:18.8543 | MainLoss:0.2276 | SPLoss:186.2208 | CLSLoss:0.4571 | top1:90.8718 | AUROC:0.9704\n",
      "Test | 129/16 | Loss:19.7831 | MainLoss:1.1565 | SPLoss:186.2207 | CLSLoss:0.4571 | top1:51.9688 | AUROC:0.5737\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 16/16 | Loss:17.8578 | MainLoss:0.2257 | SPLoss:176.2745 | CLSLoss:0.4672 | top1:90.7467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:16.7426 | MainLoss:0.2030 | SPLoss:165.3483 | CLSLoss:0.4782 | top1:92.0000 | AUROC:0.9753\n",
      "Test | 129/16 | Loss:17.7912 | MainLoss:1.2516 | SPLoss:165.3482 | CLSLoss:0.4782 | top1:52.5078 | AUROC:0.5943\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 16/16 | Loss:15.8604 | MainLoss:0.2024 | SPLoss:156.5310 | CLSLoss:0.4877 | top1:92.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:14.9011 | MainLoss:0.2107 | SPLoss:146.8541 | CLSLoss:0.4975 | top1:91.8718 | AUROC:0.9772\n",
      "Test | 129/16 | Loss:15.9145 | MainLoss:1.2241 | SPLoss:146.8540 | CLSLoss:0.4975 | top1:54.2399 | AUROC:0.6229\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 16/16 | Loss:14.0946 | MainLoss:0.1873 | SPLoss:139.0224 | CLSLoss:0.5048 | top1:92.8533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:13.2374 | MainLoss:0.1891 | SPLoss:130.4315 | CLSLoss:0.5134 | top1:92.7436 | AUROC:0.9791\n",
      "Test | 129/16 | Loss:14.4329 | MainLoss:1.3846 | SPLoss:130.4313 | CLSLoss:0.5134 | top1:53.2773 | AUROC:0.6243\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 16/16 | Loss:12.5421 | MainLoss:0.1873 | SPLoss:123.4960 | CLSLoss:0.5178 | top1:92.9333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:11.7884 | MainLoss:0.1939 | SPLoss:115.8927 | CLSLoss:0.5188 | top1:92.5128 | AUROC:0.9784\n",
      "Test | 129/16 | Loss:12.9131 | MainLoss:1.3186 | SPLoss:115.8928 | CLSLoss:0.5188 | top1:54.8006 | AUROC:0.6658\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 16/16 | Loss:11.1378 | MainLoss:0.1593 | SPLoss:109.7328 | CLSLoss:0.5262 | top1:94.1867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:10.5098 | MainLoss:0.2054 | SPLoss:102.9912 | CLSLoss:0.5338 | top1:91.9103 | AUROC:0.9798\n",
      "Test | 129/16 | Loss:11.6226 | MainLoss:1.3182 | SPLoss:102.9911 | CLSLoss:0.5338 | top1:55.9065 | AUROC:0.6743\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 16/16 | Loss:9.9245 | MainLoss:0.1651 | SPLoss:97.5400 | CLSLoss:0.5360 | top1:93.7333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:9.3476 | MainLoss:0.1871 | SPLoss:91.5503 | CLSLoss:0.5402 | top1:93.0897 | AUROC:0.9790\n",
      "Test | 129/16 | Loss:10.6662 | MainLoss:1.5057 | SPLoss:91.5503 | CLSLoss:0.5402 | top1:53.4486 | AUROC:0.6515\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 16/16 | Loss:8.8452 | MainLoss:0.1674 | SPLoss:86.7243 | CLSLoss:0.5414 | top1:93.5200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:8.3395 | MainLoss:0.1908 | SPLoss:81.4322 | CLSLoss:0.5452 | top1:92.6410 | AUROC:0.9798\n",
      "Test | 129/16 | Loss:9.5889 | MainLoss:1.4402 | SPLoss:81.4321 | CLSLoss:0.5452 | top1:54.1433 | AUROC:0.6538\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 16/16 | Loss:7.8621 | MainLoss:0.1418 | SPLoss:77.1472 | CLSLoss:0.5524 | top1:94.9067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:7.4313 | MainLoss:0.1815 | SPLoss:72.4425 | CLSLoss:0.5566 | top1:93.1538 | AUROC:0.9811\n",
      "Test | 129/16 | Loss:8.7876 | MainLoss:1.5378 | SPLoss:72.4426 | CLSLoss:0.5566 | top1:53.8131 | AUROC:0.6536\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 16/16 | Loss:7.0172 | MainLoss:0.1465 | SPLoss:68.6511 | CLSLoss:0.5604 | top1:94.1600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.6414 | MainLoss:0.1867 | SPLoss:64.4905 | CLSLoss:0.5648 | top1:92.8077 | AUROC:0.9807\n",
      "Test | 129/16 | Loss:8.0129 | MainLoss:1.5582 | SPLoss:64.4905 | CLSLoss:0.5648 | top1:54.0249 | AUROC:0.6686\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 16/16 | Loss:6.2488 | MainLoss:0.1308 | SPLoss:61.1234 | CLSLoss:0.5693 | top1:95.1200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.9382 | MainLoss:0.1894 | SPLoss:57.4302 | CLSLoss:0.5726 | top1:92.6282 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:7.2576 | MainLoss:1.5089 | SPLoss:57.4301 | CLSLoss:0.5726 | top1:54.8598 | AUROC:0.6748\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 16/16 | Loss:5.5807 | MainLoss:0.1291 | SPLoss:54.4581 | CLSLoss:0.5774 | top1:95.1200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.3109 | MainLoss:0.1844 | SPLoss:51.2071 | CLSLoss:0.5807 | top1:92.8462 | AUROC:0.9804\n",
      "Test | 129/16 | Loss:6.8120 | MainLoss:1.6855 | SPLoss:51.2070 | CLSLoss:0.5807 | top1:53.6916 | AUROC:0.6746\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 16/16 | Loss:5.0157 | MainLoss:0.1399 | SPLoss:48.7002 | CLSLoss:0.5811 | top1:95.0400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.7790 | MainLoss:0.1899 | SPLoss:45.8332 | CLSLoss:0.5833 | top1:92.8077 | AUROC:0.9812\n",
      "Test | 129/16 | Loss:6.1118 | MainLoss:1.5227 | SPLoss:45.8331 | CLSLoss:0.5833 | top1:55.6168 | AUROC:0.7225\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 16/16 | Loss:4.4847 | MainLoss:0.1305 | SPLoss:43.4824 | CLSLoss:0.5863 | top1:94.7200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.2881 | MainLoss:0.1927 | SPLoss:40.8955 | CLSLoss:0.5888 | top1:92.6026 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:5.6821 | MainLoss:1.5866 | SPLoss:40.8955 | CLSLoss:0.5888 | top1:54.7882 | AUROC:0.6826\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 16/16 | Loss:4.0302 | MainLoss:0.1453 | SPLoss:38.7901 | CLSLoss:0.5887 | top1:94.4267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.8351 | MainLoss:0.1809 | SPLoss:36.4830 | CLSLoss:0.5873 | top1:92.8718 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:5.2191 | MainLoss:1.5649 | SPLoss:36.4830 | CLSLoss:0.5873 | top1:54.2897 | AUROC:0.6680\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 16/16 | Loss:3.6051 | MainLoss:0.1345 | SPLoss:34.6470 | CLSLoss:0.5908 | top1:94.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.5537 | MainLoss:0.2379 | SPLoss:33.0993 | CLSLoss:0.5903 | top1:90.8333 | AUROC:0.9680\n",
      "Test | 129/16 | Loss:5.0919 | MainLoss:1.7761 | SPLoss:33.0993 | CLSLoss:0.5903 | top1:50.8224 | AUROC:0.5679\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 16/16 | Loss:3.3683 | MainLoss:0.1791 | SPLoss:31.8339 | CLSLoss:0.5851 | top1:92.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.2088 | MainLoss:0.1914 | SPLoss:30.1154 | CLSLoss:0.5858 | top1:92.2692 | AUROC:0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 129/16 | Loss:4.3726 | MainLoss:1.3552 | SPLoss:30.1153 | CLSLoss:0.5858 | top1:56.3956 | AUROC:0.7199\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 16/16 | Loss:3.0073 | MainLoss:0.1422 | SPLoss:28.5919 | CLSLoss:0.5900 | top1:94.8533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.8614 | MainLoss:0.1633 | SPLoss:26.9218 | CLSLoss:0.5911 | top1:93.3846 | AUROC:0.9841\n",
      "Test | 129/16 | Loss:4.3260 | MainLoss:1.6279 | SPLoss:26.9218 | CLSLoss:0.5911 | top1:54.0779 | AUROC:0.7228\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 16/16 | Loss:2.6926 | MainLoss:0.1298 | SPLoss:25.5677 | CLSLoss:0.5940 | top1:95.3067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.5869 | MainLoss:0.1710 | SPLoss:24.0986 | CLSLoss:0.5983 | top1:93.0769 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:4.0505 | MainLoss:1.6347 | SPLoss:24.0986 | CLSLoss:0.5983 | top1:54.7414 | AUROC:0.7295\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 16/16 | Loss:2.4164 | MainLoss:0.1200 | SPLoss:22.9045 | CLSLoss:0.6022 | top1:95.5200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.3390 | MainLoss:0.1748 | SPLoss:21.5817 | CLSLoss:0.6033 | top1:93.3718 | AUROC:0.9822\n",
      "Test | 129/16 | Loss:3.9253 | MainLoss:1.7611 | SPLoss:21.5817 | CLSLoss:0.6033 | top1:53.3302 | AUROC:0.6898\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 16/16 | Loss:2.1695 | MainLoss:0.1123 | SPLoss:20.5115 | CLSLoss:0.6062 | top1:95.8400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.1445 | MainLoss:0.2053 | SPLoss:19.3313 | CLSLoss:0.6107 | top1:92.1026 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:3.5732 | MainLoss:1.6339 | SPLoss:19.3313 | CLSLoss:0.6107 | top1:54.8224 | AUROC:0.6888\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 16/16 | Loss:1.9685 | MainLoss:0.1231 | SPLoss:18.3932 | CLSLoss:0.6104 | top1:95.6533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.9323 | MainLoss:0.1906 | SPLoss:17.3559 | CLSLoss:0.6111 | top1:92.8077 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:3.3350 | MainLoss:1.5933 | SPLoss:17.3559 | CLSLoss:0.6111 | top1:55.1838 | AUROC:0.6991\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 16/16 | Loss:1.7749 | MainLoss:0.1177 | SPLoss:16.5109 | CLSLoss:0.6137 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.7403 | MainLoss:0.1755 | SPLoss:15.5866 | CLSLoss:0.6144 | top1:93.2692 | AUROC:0.9826\n",
      "Test | 129/16 | Loss:3.2644 | MainLoss:1.6996 | SPLoss:15.5866 | CLSLoss:0.6144 | top1:54.1215 | AUROC:0.7090\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 16/16 | Loss:1.6085 | MainLoss:0.1169 | SPLoss:14.8546 | CLSLoss:0.6165 | top1:95.5200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.5889 | MainLoss:0.1789 | SPLoss:14.0380 | CLSLoss:0.6209 | top1:93.0385 | AUROC:0.9843\n",
      "Test | 129/16 | Loss:3.0523 | MainLoss:1.6423 | SPLoss:14.0380 | CLSLoss:0.6209 | top1:55.2866 | AUROC:0.7242\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 16/16 | Loss:1.4626 | MainLoss:0.1193 | SPLoss:13.3706 | CLSLoss:0.6221 | top1:95.8667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4519 | MainLoss:0.1794 | SPLoss:12.6622 | CLSLoss:0.6220 | top1:93.0256 | AUROC:0.9824\n",
      "Test | 129/16 | Loss:2.8720 | MainLoss:1.5995 | SPLoss:12.6622 | CLSLoss:0.6220 | top1:55.6355 | AUROC:0.7424\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 16/16 | Loss:1.3121 | MainLoss:0.0992 | SPLoss:12.0667 | CLSLoss:0.6261 | top1:96.8267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.3239 | MainLoss:0.1758 | SPLoss:11.4184 | CLSLoss:0.6306 | top1:93.4103 | AUROC:0.9836\n",
      "Test | 129/16 | Loss:2.9138 | MainLoss:1.7657 | SPLoss:11.4185 | CLSLoss:0.6306 | top1:54.7632 | AUROC:0.7234\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 16/16 | Loss:1.2279 | MainLoss:0.1309 | SPLoss:10.9079 | CLSLoss:0.6283 | top1:95.3867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.2312 | MainLoss:0.1887 | SPLoss:10.3620 | CLSLoss:0.6238 | top1:92.5769 | AUROC:0.9796\n",
      "Test | 129/16 | Loss:2.7346 | MainLoss:1.6921 | SPLoss:10.3620 | CLSLoss:0.6238 | top1:54.0685 | AUROC:0.7279\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 16/16 | Loss:1.1228 | MainLoss:0.1248 | SPLoss:9.9181 | CLSLoss:0.6234 | top1:95.3333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.1370 | MainLoss:0.1839 | SPLoss:9.4681 | CLSLoss:0.6244 | top1:93.0385 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:2.8422 | MainLoss:1.8892 | SPLoss:9.4681 | CLSLoss:0.6244 | top1:52.8474 | AUROC:0.6657\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 16/16 | Loss:1.0287 | MainLoss:0.1173 | SPLoss:9.0519 | CLSLoss:0.6222 | top1:95.8933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.0528 | MainLoss:0.1881 | SPLoss:8.5838 | CLSLoss:0.6253 | top1:93.0641 | AUROC:0.9833\n",
      "Test | 129/16 | Loss:2.5152 | MainLoss:1.6506 | SPLoss:8.5838 | CLSLoss:0.6253 | top1:54.7165 | AUROC:0.6962\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 16/16 | Loss:0.9385 | MainLoss:0.1109 | SPLoss:8.2131 | CLSLoss:0.6281 | top1:95.9733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9633 | MainLoss:0.1758 | SPLoss:7.8120 | CLSLoss:0.6275 | top1:93.2051 | AUROC:0.9847\n",
      "Test | 129/16 | Loss:2.5017 | MainLoss:1.7142 | SPLoss:7.8120 | CLSLoss:0.6275 | top1:54.4611 | AUROC:0.6789\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 16/16 | Loss:0.8931 | MainLoss:0.1086 | SPLoss:7.7816 | CLSLoss:0.6305 | top1:95.9733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.0225 | MainLoss:0.1951 | SPLoss:8.2105 | CLSLoss:0.6312 | top1:92.5513 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:2.4498 | MainLoss:1.6224 | SPLoss:8.2104 | CLSLoss:0.6312 | top1:55.1558 | AUROC:0.7004\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 16/16 | Loss:0.9149 | MainLoss:0.1222 | SPLoss:7.8640 | CLSLoss:0.6305 | top1:95.1733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9238 | MainLoss:0.1693 | SPLoss:7.4822 | CLSLoss:0.6273 | top1:93.3590 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:2.5371 | MainLoss:1.7826 | SPLoss:7.4822 | CLSLoss:0.6273 | top1:52.9346 | AUROC:0.6813\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 16/16 | Loss:0.8229 | MainLoss:0.0987 | SPLoss:7.1787 | CLSLoss:0.6336 | top1:96.1333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8582 | MainLoss:0.1689 | SPLoss:6.8295 | CLSLoss:0.6366 | top1:93.6282 | AUROC:0.9842\n",
      "Test | 129/16 | Loss:2.5958 | MainLoss:1.9065 | SPLoss:6.8295 | CLSLoss:0.6366 | top1:52.9097 | AUROC:0.6699\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 16/16 | Loss:0.7901 | MainLoss:0.1092 | SPLoss:6.7456 | CLSLoss:0.6385 | top1:95.6533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8556 | MainLoss:0.1682 | SPLoss:6.8102 | CLSLoss:0.6419 | top1:93.5769 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:2.5310 | MainLoss:1.8436 | SPLoss:6.8102 | CLSLoss:0.6419 | top1:53.4424 | AUROC:0.6755\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 16/16 | Loss:0.7527 | MainLoss:0.0909 | SPLoss:6.5535 | CLSLoss:0.6464 | top1:96.6667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8054 | MainLoss:0.1744 | SPLoss:6.2446 | CLSLoss:0.6506 | top1:93.6923 | AUROC:0.9846\n",
      "Test | 129/16 | Loss:2.5604 | MainLoss:1.9294 | SPLoss:6.2446 | CLSLoss:0.6506 | top1:53.7352 | AUROC:0.6761\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 16/16 | Loss:0.7211 | MainLoss:0.1132 | SPLoss:6.0144 | CLSLoss:0.6485 | top1:95.7600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7698 | MainLoss:0.1893 | SPLoss:5.7412 | CLSLoss:0.6469 | top1:92.7564 | AUROC:0.9836\n",
      "Test | 129/16 | Loss:2.2589 | MainLoss:1.6783 | SPLoss:5.7412 | CLSLoss:0.6469 | top1:54.4798 | AUROC:0.6531\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 16/16 | Loss:0.6733 | MainLoss:0.1098 | SPLoss:5.5702 | CLSLoss:0.6480 | top1:95.7333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7235 | MainLoss:0.1791 | SPLoss:5.3794 | CLSLoss:0.6500 | top1:93.6795 | AUROC:0.9839\n",
      "Test | 129/16 | Loss:2.3229 | MainLoss:1.7784 | SPLoss:5.3794 | CLSLoss:0.6500 | top1:54.1994 | AUROC:0.6808\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n",
      "Train | 16/16 | Loss:0.6108 | MainLoss:0.0850 | SPLoss:5.1927 | CLSLoss:0.6549 | top1:97.1733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6698 | MainLoss:0.1648 | SPLoss:4.9843 | CLSLoss:0.6585 | top1:94.1154 | AUROC:0.9856\n",
      "Test | 129/16 | Loss:2.5516 | MainLoss:2.0466 | SPLoss:4.9843 | CLSLoss:0.6585 | top1:53.6480 | AUROC:0.6751\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 16/16 | Loss:0.5844 | MainLoss:0.0986 | SPLoss:4.7926 | CLSLoss:0.6581 | top1:96.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7322 | MainLoss:0.2652 | SPLoss:4.6045 | CLSLoss:0.6552 | top1:89.9872 | AUROC:0.9842\n",
      "Test | 129/16 | Loss:1.8769 | MainLoss:1.4099 | SPLoss:4.6044 | CLSLoss:0.6552 | top1:57.8442 | AUROC:0.6852\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 16/16 | Loss:1.2291 | MainLoss:0.1219 | SPLoss:11.0064 | CLSLoss:0.6529 | top1:95.5200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.7090 | MainLoss:0.2141 | SPLoss:64.8840 | CLSLoss:0.6496 | top1:92.0256 | AUROC:0.9810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 129/16 | Loss:8.1745 | MainLoss:1.6796 | SPLoss:64.8840 | CLSLoss:0.6496 | top1:54.3769 | AUROC:0.6421\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 16/16 | Loss:6.2970 | MainLoss:0.1382 | SPLoss:61.5238 | CLSLoss:0.6472 | top1:94.8533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.9806 | MainLoss:0.1896 | SPLoss:57.8452 | CLSLoss:0.6426 | top1:92.6410 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:7.3840 | MainLoss:1.5930 | SPLoss:57.8451 | CLSLoss:0.6426 | top1:55.4050 | AUROC:0.7428\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n",
      "Train | 16/16 | Loss:5.6166 | MainLoss:0.1162 | SPLoss:54.9391 | CLSLoss:0.6449 | top1:95.5733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.3761 | MainLoss:0.1782 | SPLoss:51.9143 | CLSLoss:0.6476 | top1:93.3462 | AUROC:0.9841\n",
      "Test | 129/16 | Loss:6.9717 | MainLoss:1.7738 | SPLoss:51.9144 | CLSLoss:0.6476 | top1:54.8536 | AUROC:0.7473\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 16/16 | Loss:5.0310 | MainLoss:0.1023 | SPLoss:49.2226 | CLSLoss:0.6492 | top1:96.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.8228 | MainLoss:0.1886 | SPLoss:46.2768 | CLSLoss:0.6520 | top1:93.2051 | AUROC:0.9833\n",
      "Test | 129/16 | Loss:6.4196 | MainLoss:1.7854 | SPLoss:46.2768 | CLSLoss:0.6520 | top1:55.0872 | AUROC:0.7370\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 16/16 | Loss:4.5188 | MainLoss:0.1198 | SPLoss:43.9245 | CLSLoss:0.6508 | top1:95.7067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.3198 | MainLoss:0.1781 | SPLoss:41.3520 | CLSLoss:0.6477 | top1:93.1282 | AUROC:0.9825\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : teacher_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
