{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 250\n",
    "test_batch = 250\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_style2/1000shot' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.2\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.2\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.01\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "#         auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "#         arc.update(auroc, inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 16/16 | Loss:1.0880 | MainLoss:0.9324 | SPLoss:1.4076 | CLSLoss:1.4923 | top1:50.1600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8361 | MainLoss:0.6915 | SPLoss:1.3033 | CLSLoss:1.4289 | top1:55.0000 | AUROC:0.5710\n",
      "Test | 129/16 | Loss:0.7838 | MainLoss:0.6392 | SPLoss:1.3033 | CLSLoss:1.4289 | top1:97.9595 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 16/16 | Loss:0.8118 | MainLoss:0.6908 | SPLoss:1.0695 | CLSLoss:1.4007 | top1:52.2133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7882 | MainLoss:0.6920 | SPLoss:0.8241 | CLSLoss:1.3715 | top1:50.6026 | AUROC:0.5737\n",
      "Test | 129/16 | Loss:0.6097 | MainLoss:0.5136 | SPLoss:0.8241 | CLSLoss:1.3715 | top1:99.8847 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 16/16 | Loss:0.7667 | MainLoss:0.6874 | SPLoss:0.6599 | CLSLoss:1.3382 | top1:54.3467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7452 | MainLoss:0.6829 | SPLoss:0.4934 | CLSLoss:1.3026 | top1:56.2179 | AUROC:0.5946\n",
      "Test | 129/16 | Loss:0.3393 | MainLoss:0.2769 | SPLoss:0.4934 | CLSLoss:1.3026 | top1:99.6822 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 16/16 | Loss:0.7633 | MainLoss:0.7009 | SPLoss:0.5018 | CLSLoss:1.2179 | top1:52.1333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7408 | MainLoss:0.6889 | SPLoss:0.4043 | CLSLoss:1.1486 | top1:56.7692 | AUROC:0.5958\n",
      "Test | 129/16 | Loss:0.5970 | MainLoss:0.5451 | SPLoss:0.4043 | CLSLoss:1.1486 | top1:99.8536 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 16/16 | Loss:0.7224 | MainLoss:0.6823 | SPLoss:0.2894 | CLSLoss:1.1188 | top1:57.0933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7031 | MainLoss:0.6700 | SPLoss:0.2232 | CLSLoss:1.0849 | top1:58.8974 | AUROC:0.6323\n",
      "Test | 129/16 | Loss:0.1684 | MainLoss:0.1353 | SPLoss:0.2232 | CLSLoss:1.0849 | top1:99.7072 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 16/16 | Loss:0.7410 | MainLoss:0.6878 | SPLoss:0.4347 | CLSLoss:0.9700 | top1:57.3067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7314 | MainLoss:0.6848 | SPLoss:0.3782 | CLSLoss:0.8839 | top1:52.8205 | AUROC:0.6401\n",
      "Test | 129/16 | Loss:0.5245 | MainLoss:0.4779 | SPLoss:0.3782 | CLSLoss:0.8839 | top1:97.4642 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 16/16 | Loss:0.6989 | MainLoss:0.6544 | SPLoss:0.3592 | CLSLoss:0.8586 | top1:61.1467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9231 | MainLoss:0.8472 | SPLoss:0.6798 | CLSLoss:0.7935 | top1:50.2949 | AUROC:0.7402\n",
      "Test | 129/16 | Loss:0.1903 | MainLoss:0.1144 | SPLoss:0.6798 | CLSLoss:0.7935 | top1:99.1059 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 16/16 | Loss:0.7532 | MainLoss:0.6767 | SPLoss:0.6978 | CLSLoss:0.6733 | top1:57.2800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6501 | MainLoss:0.5942 | SPLoss:0.4906 | CLSLoss:0.6878 | top1:68.4615 | AUROC:0.7539\n",
      "Test | 129/16 | Loss:0.1843 | MainLoss:0.1284 | SPLoss:0.4906 | CLSLoss:0.6878 | top1:99.7196 | AUROC:1.0000\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 16/16 | Loss:0.8010 | MainLoss:0.6842 | SPLoss:1.1145 | CLSLoss:0.5310 | top1:57.3333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8414 | MainLoss:0.7456 | SPLoss:0.9121 | CLSLoss:0.4646 | top1:50.0000 | AUROC:0.7450\n",
      "Test | 129/16 | Loss:0.6871 | MainLoss:0.5912 | SPLoss:0.9121 | CLSLoss:0.4646 | top1:50.0467 | AUROC:1.0000\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 16/16 | Loss:0.8026 | MainLoss:0.6931 | SPLoss:1.0577 | CLSLoss:0.3716 | top1:55.7600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8087 | MainLoss:0.6888 | SPLoss:1.1736 | CLSLoss:0.2588 | top1:50.0000 | AUROC:0.7306\n",
      "Test | 129/16 | Loss:0.7668 | MainLoss:0.6469 | SPLoss:1.1736 | CLSLoss:0.2588 | top1:89.3551 | AUROC:1.0000\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:0.7616 | MainLoss:0.6783 | SPLoss:0.8080 | CLSLoss:0.2560 | top1:59.6000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7266 | MainLoss:0.6604 | SPLoss:0.6389 | CLSLoss:0.2254 | top1:62.6026 | AUROC:0.7106\n",
      "Test | 129/16 | Loss:0.3981 | MainLoss:0.3319 | SPLoss:0.6389 | CLSLoss:0.2254 | top1:99.4050 | AUROC:1.0000\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:0.7755 | MainLoss:0.6817 | SPLoss:0.9224 | CLSLoss:0.1528 | top1:58.5600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7515 | MainLoss:0.6840 | SPLoss:0.6656 | CLSLoss:0.0922 | top1:65.4615 | AUROC:0.7398\n",
      "Test | 129/16 | Loss:0.6774 | MainLoss:0.6099 | SPLoss:0.6656 | CLSLoss:0.0922 | top1:99.8318 | AUROC:1.0000\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 16/16 | Loss:120.3060 | MainLoss:0.6624 | SPLoss:1196.4252 | CLSLoss:0.1158 | top1:63.3867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:226.0841 | MainLoss:0.6230 | SPLoss:2254.5989 | CLSLoss:0.1232 | top1:68.3718 | AUROC:0.7653\n",
      "Test | 129/16 | Loss:225.7906 | MainLoss:0.3295 | SPLoss:2254.5955 | CLSLoss:0.1232 | top1:98.6480 | AUROC:0.9994\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 16/16 | Loss:132.4768 | MainLoss:0.6380 | SPLoss:1318.3782 | CLSLoss:0.1061 | top1:63.6800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:62.1888 | MainLoss:0.5198 | SPLoss:616.6768 | CLSLoss:0.1296 | top1:76.3205 | AUROC:0.8470\n",
      "Test | 129/16 | Loss:62.1249 | MainLoss:0.4559 | SPLoss:616.6768 | CLSLoss:0.1296 | top1:80.1651 | AUROC:0.9373\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 16/16 | Loss:35.8889 | MainLoss:0.5834 | SPLoss:353.0392 | CLSLoss:0.1650 | top1:72.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:16.4181 | MainLoss:0.4177 | SPLoss:159.9760 | CLSLoss:0.2786 | top1:81.0769 | AUROC:0.9151\n",
      "Test | 129/16 | Loss:16.7408 | MainLoss:0.7404 | SPLoss:159.9760 | CLSLoss:0.2786 | top1:59.9221 | AUROC:0.7645\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 16/16 | Loss:9.9322 | MainLoss:0.4860 | SPLoss:94.4364 | CLSLoss:0.2583 | top1:77.4933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.8443 | MainLoss:0.4543 | SPLoss:43.8600 | CLSLoss:0.4096 | top1:80.6923 | AUROC:0.9519\n",
      "Test | 129/16 | Loss:5.1145 | MainLoss:0.7244 | SPLoss:43.8600 | CLSLoss:0.4096 | top1:64.4143 | AUROC:0.6959\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 16/16 | Loss:34.1626 | MainLoss:0.4842 | SPLoss:336.7473 | CLSLoss:0.3633 | top1:76.9333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:76.2697 | MainLoss:0.6422 | SPLoss:756.2606 | CLSLoss:0.1517 | top1:67.9744 | AUROC:0.8577\n",
      "Test | 129/16 | Loss:76.3397 | MainLoss:0.7121 | SPLoss:756.2614 | CLSLoss:0.1517 | top1:49.6916 | AUROC:0.4711\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 16/16 | Loss:48.7227 | MainLoss:0.5042 | SPLoss:482.1509 | CLSLoss:0.3409 | top1:77.1200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:32.2022 | MainLoss:0.5203 | SPLoss:316.7740 | CLSLoss:0.4497 | top1:74.3333 | AUROC:0.9244\n",
      "Test | 129/16 | Loss:32.4451 | MainLoss:0.7632 | SPLoss:316.7736 | CLSLoss:0.4497 | top1:55.5888 | AUROC:0.5745\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 16/16 | Loss:41.9634 | MainLoss:0.4641 | SPLoss:414.9459 | CLSLoss:0.4656 | top1:78.8800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:32.5034 | MainLoss:0.4210 | SPLoss:320.7733 | CLSLoss:0.5090 | top1:82.6667 | AUROC:0.9406\n",
      "Test | 129/16 | Loss:32.9702 | MainLoss:0.8878 | SPLoss:320.7733 | CLSLoss:0.5090 | top1:54.3302 | AUROC:0.5617\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 16/16 | Loss:19.0358 | MainLoss:0.4307 | SPLoss:186.0041 | CLSLoss:0.4720 | top1:81.4400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:8.9057 | MainLoss:0.3931 | SPLoss:85.0856 | CLSLoss:0.4108 | top1:84.3077 | AUROC:0.9318\n",
      "Test | 129/16 | Loss:9.4844 | MainLoss:0.9717 | SPLoss:85.0856 | CLSLoss:0.4108 | top1:49.5358 | AUROC:0.4836\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 16/16 | Loss:5.8550 | MainLoss:0.3701 | SPLoss:54.8015 | CLSLoss:0.4811 | top1:84.3200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.3644 | MainLoss:0.2561 | SPLoss:31.0299 | CLSLoss:0.5314 | top1:90.2051 | AUROC:0.9610\n",
      "Test | 129/16 | Loss:4.2124 | MainLoss:1.1041 | SPLoss:31.0300 | CLSLoss:0.5314 | top1:53.2648 | AUROC:0.5938\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 16/16 | Loss:3.9037 | MainLoss:0.3304 | SPLoss:35.6809 | CLSLoss:0.5275 | top1:86.1333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.5316 | MainLoss:0.3003 | SPLoss:22.2591 | CLSLoss:0.5378 | top1:87.7179 | AUROC:0.9543\n",
      "Test | 129/16 | Loss:3.5653 | MainLoss:1.3340 | SPLoss:22.2592 | CLSLoss:0.5378 | top1:50.8692 | AUROC:0.5303\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 16/16 | Loss:33.9486 | MainLoss:0.4552 | SPLoss:334.8872 | CLSLoss:0.4586 | top1:78.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:130.3940 | MainLoss:0.3952 | SPLoss:1299.9517 | CLSLoss:0.3581 | top1:84.7051 | AUROC:0.9325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 129/16 | Loss:130.7483 | MainLoss:0.7495 | SPLoss:1299.9519 | CLSLoss:0.3581 | top1:54.9221 | AUROC:0.5841\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 16/16 | Loss:74.9576 | MainLoss:0.3852 | SPLoss:745.6807 | CLSLoss:0.4358 | top1:83.7333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:34.8661 | MainLoss:0.3695 | SPLoss:344.9193 | CLSLoss:0.4733 | top1:83.7436 | AUROC:0.9474\n",
      "Test | 129/16 | Loss:35.8477 | MainLoss:1.3511 | SPLoss:344.9197 | CLSLoss:0.4733 | top1:50.2866 | AUROC:0.4839\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 16/16 | Loss:22.4154 | MainLoss:0.3699 | SPLoss:220.4109 | CLSLoss:0.4448 | top1:84.1333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:22.0054 | MainLoss:0.3250 | SPLoss:216.7620 | CLSLoss:0.4152 | top1:87.1923 | AUROC:0.9509\n",
      "Test | 129/16 | Loss:22.5113 | MainLoss:0.8309 | SPLoss:216.7620 | CLSLoss:0.4152 | top1:56.4953 | AUROC:0.6460\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 16/16 | Loss:19.7422 | MainLoss:0.3672 | SPLoss:193.7091 | CLSLoss:0.4166 | top1:83.6533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:13.3733 | MainLoss:0.3105 | SPLoss:130.5808 | CLSLoss:0.4696 | top1:86.9103 | AUROC:0.9607\n",
      "Test | 129/16 | Loss:14.0507 | MainLoss:0.9879 | SPLoss:130.5809 | CLSLoss:0.4696 | top1:55.3645 | AUROC:0.6017\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 16/16 | Loss:8.7866 | MainLoss:0.3553 | SPLoss:84.2692 | CLSLoss:0.4381 | top1:84.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.4059 | MainLoss:0.2441 | SPLoss:51.5711 | CLSLoss:0.4691 | top1:90.1923 | AUROC:0.9636\n",
      "Test | 129/16 | Loss:6.5064 | MainLoss:1.3446 | SPLoss:51.5711 | CLSLoss:0.4691 | top1:50.4237 | AUROC:0.5018\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 16/16 | Loss:5.7285 | MainLoss:0.4029 | SPLoss:53.2141 | CLSLoss:0.4129 | top1:82.0267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.2655 | MainLoss:0.3390 | SPLoss:49.2302 | CLSLoss:0.3484 | top1:87.5385 | AUROC:0.9464\n",
      "Test | 129/16 | Loss:5.7964 | MainLoss:0.8699 | SPLoss:49.2301 | CLSLoss:0.3484 | top1:50.9844 | AUROC:0.5529\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 16/16 | Loss:7.1199 | MainLoss:0.3727 | SPLoss:67.4309 | CLSLoss:0.4168 | top1:84.4267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:57.2200 | MainLoss:0.8069 | SPLoss:564.1011 | CLSLoss:0.3017 | top1:51.8590 | AUROC:0.5169\n",
      "Test | 129/16 | Loss:57.2061 | MainLoss:0.7930 | SPLoss:564.1010 | CLSLoss:0.3017 | top1:50.5358 | AUROC:0.6038\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 16/16 | Loss:36.2896 | MainLoss:0.5705 | SPLoss:357.1656 | CLSLoss:0.2543 | top1:70.6667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:17.3340 | MainLoss:0.4276 | SPLoss:169.0352 | CLSLoss:0.2857 | top1:80.7436 | AUROC:0.8879\n",
      "Test | 129/16 | Loss:17.7227 | MainLoss:0.8163 | SPLoss:169.0353 | CLSLoss:0.2857 | top1:55.0312 | AUROC:0.6007\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 16/16 | Loss:31.7991 | MainLoss:0.4888 | SPLoss:313.0752 | CLSLoss:0.2757 | top1:77.4400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:41.3414 | MainLoss:0.4126 | SPLoss:409.2630 | CLSLoss:0.2479 | top1:82.1538 | AUROC:0.9078\n",
      "Test | 129/16 | Loss:41.8679 | MainLoss:0.9391 | SPLoss:409.2630 | CLSLoss:0.2479 | top1:50.6916 | AUROC:0.5522\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 16/16 | Loss:24.1229 | MainLoss:0.4205 | SPLoss:236.9944 | CLSLoss:0.3040 | top1:81.3600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:12.1918 | MainLoss:0.3376 | SPLoss:118.5101 | CLSLoss:0.3164 | top1:86.0128 | AUROC:0.9482\n",
      "Test | 129/16 | Loss:12.7630 | MainLoss:0.9088 | SPLoss:118.5100 | CLSLoss:0.3164 | top1:51.3707 | AUROC:0.5311\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 16/16 | Loss:7.5473 | MainLoss:0.3990 | SPLoss:71.4487 | CLSLoss:0.3433 | top1:82.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.9928 | MainLoss:0.7710 | SPLoss:42.1764 | CLSLoss:0.4126 | top1:69.0897 | AUROC:0.9445\n",
      "Test | 129/16 | Loss:5.0039 | MainLoss:0.7822 | SPLoss:42.1765 | CLSLoss:0.4126 | top1:62.9564 | AUROC:0.6778\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 16/16 | Loss:4.0819 | MainLoss:0.4690 | SPLoss:36.0962 | CLSLoss:0.3292 | top1:78.5600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.7378 | MainLoss:0.4229 | SPLoss:33.1118 | CLSLoss:0.3760 | top1:80.1410 | AUROC:0.9382\n",
      "Test | 129/16 | Loss:4.5479 | MainLoss:1.2329 | SPLoss:33.1118 | CLSLoss:0.3760 | top1:50.4143 | AUROC:0.5474\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 16/16 | Loss:6.8138 | MainLoss:0.3857 | SPLoss:64.2428 | CLSLoss:0.3820 | top1:83.2800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.9690 | MainLoss:0.4985 | SPLoss:64.6703 | CLSLoss:0.3463 | top1:74.8718 | AUROC:0.9475\n",
      "Test | 129/16 | Loss:7.4549 | MainLoss:0.9844 | SPLoss:64.6704 | CLSLoss:0.3463 | top1:50.0498 | AUROC:0.4882\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 16/16 | Loss:5.3397 | MainLoss:0.3817 | SPLoss:49.5438 | CLSLoss:0.3627 | top1:84.0267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.8942 | MainLoss:0.2479 | SPLoss:46.4197 | CLSLoss:0.4313 | top1:90.3718 | AUROC:0.9642\n",
      "Test | 129/16 | Loss:5.9191 | MainLoss:1.2728 | SPLoss:46.4197 | CLSLoss:0.4313 | top1:51.2056 | AUROC:0.5597\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 16/16 | Loss:3.6830 | MainLoss:0.4201 | SPLoss:32.5902 | CLSLoss:0.3903 | top1:81.6800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:12.7505 | MainLoss:0.5019 | SPLoss:122.4663 | CLSLoss:0.1981 | top1:80.5769 | AUROC:0.8879\n",
      "Test | 129/16 | Loss:12.9764 | MainLoss:0.7278 | SPLoss:122.4662 | CLSLoss:0.1981 | top1:50.7041 | AUROC:0.5275\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 16/16 | Loss:18.6967 | MainLoss:0.4364 | SPLoss:182.5693 | CLSLoss:0.3352 | top1:81.2267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:17.5058 | MainLoss:0.3215 | SPLoss:171.8057 | CLSLoss:0.3732 | top1:86.8974 | AUROC:0.9428\n",
      "Test | 129/16 | Loss:18.1708 | MainLoss:0.9865 | SPLoss:171.8058 | CLSLoss:0.3732 | top1:50.7882 | AUROC:0.5344\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 16/16 | Loss:10.3025 | MainLoss:0.3743 | SPLoss:99.2414 | CLSLoss:0.4064 | top1:83.8933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.0834 | MainLoss:0.3636 | SPLoss:47.1635 | CLSLoss:0.3430 | top1:87.4103 | AUROC:0.9388\n",
      "Test | 129/16 | Loss:5.6449 | MainLoss:0.9251 | SPLoss:47.1635 | CLSLoss:0.3430 | top1:50.4891 | AUROC:0.5024\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 16/16 | Loss:3.9083 | MainLoss:0.3679 | SPLoss:35.3641 | CLSLoss:0.3993 | top1:84.1067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.8390 | MainLoss:0.2300 | SPLoss:26.0447 | CLSLoss:0.4551 | top1:90.8974 | AUROC:0.9680\n",
      "Test | 129/16 | Loss:3.9192 | MainLoss:1.3102 | SPLoss:26.0448 | CLSLoss:0.4551 | top1:51.8069 | AUROC:0.5629\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 16/16 | Loss:2.2800 | MainLoss:0.3710 | SPLoss:19.0487 | CLSLoss:0.4110 | top1:83.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4499 | MainLoss:0.2476 | SPLoss:11.9784 | CLSLoss:0.4454 | top1:90.1026 | AUROC:0.9693\n",
      "Test | 129/16 | Loss:2.2782 | MainLoss:1.0759 | SPLoss:11.9784 | CLSLoss:0.4454 | top1:53.7913 | AUROC:0.6003\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 16/16 | Loss:1.6496 | MainLoss:0.3984 | SPLoss:12.4695 | CLSLoss:0.4246 | top1:82.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.9919 | MainLoss:0.4409 | SPLoss:35.4857 | CLSLoss:0.2465 | top1:86.5513 | AUROC:0.9417\n",
      "Test | 129/16 | Loss:4.2857 | MainLoss:0.7347 | SPLoss:35.4857 | CLSLoss:0.2465 | top1:52.3551 | AUROC:0.5457\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 16/16 | Loss:6.7094 | MainLoss:0.3369 | SPLoss:63.6837 | CLSLoss:0.4154 | top1:86.3733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:7.4460 | MainLoss:0.2238 | SPLoss:72.1766 | CLSLoss:0.4546 | top1:91.0128 | AUROC:0.9753\n",
      "Test | 129/16 | Loss:8.5384 | MainLoss:1.3162 | SPLoss:72.1766 | CLSLoss:0.4546 | top1:50.8598 | AUROC:0.5215\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 16/16 | Loss:43.6197 | MainLoss:0.3922 | SPLoss:432.2341 | CLSLoss:0.4153 | top1:82.9600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:34.0375 | MainLoss:0.3015 | SPLoss:337.3261 | CLSLoss:0.3349 | top1:89.1154 | AUROC:0.9565\n",
      "Test | 129/16 | Loss:34.7055 | MainLoss:0.9695 | SPLoss:337.3265 | CLSLoss:0.3349 | top1:50.9097 | AUROC:0.5102\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 16/16 | Loss:68.3659 | MainLoss:0.3410 | SPLoss:680.2102 | CLSLoss:0.3901 | top1:85.4400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:62.3843 | MainLoss:0.3055 | SPLoss:620.7418 | CLSLoss:0.4669 | top1:87.4359 | AUROC:0.9627\n",
      "Test | 129/16 | Loss:63.5706 | MainLoss:1.4917 | SPLoss:620.7415 | CLSLoss:0.4669 | top1:51.2586 | AUROC:0.5448\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 16/16 | Loss:37.4931 | MainLoss:0.3737 | SPLoss:371.1558 | CLSLoss:0.3892 | top1:85.0400 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 32/16 | Loss:18.6560 | MainLoss:0.2602 | SPLoss:183.9115 | CLSLoss:0.4691 | top1:89.4231 | AUROC:0.9627\n",
      "Test | 129/16 | Loss:19.6209 | MainLoss:1.2251 | SPLoss:183.9118 | CLSLoss:0.4691 | top1:54.2118 | AUROC:0.6114\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 16/16 | Loss:11.5977 | MainLoss:0.3713 | SPLoss:112.2231 | CLSLoss:0.4109 | top1:84.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:8.8656 | MainLoss:0.2612 | SPLoss:85.9993 | CLSLoss:0.4449 | top1:90.7692 | AUROC:0.9677\n",
      "Test | 129/16 | Loss:9.7300 | MainLoss:1.1256 | SPLoss:85.9993 | CLSLoss:0.4449 | top1:50.3863 | AUROC:0.4824\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 16/16 | Loss:8.4806 | MainLoss:0.3792 | SPLoss:80.9690 | CLSLoss:0.4498 | top1:84.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:8.1159 | MainLoss:0.8016 | SPLoss:73.1143 | CLSLoss:0.2849 | top1:50.2821 | AUROC:0.7830\n",
      "Test | 129/16 | Loss:8.2837 | MainLoss:0.9694 | SPLoss:73.1144 | CLSLoss:0.2849 | top1:49.9782 | AUROC:0.3827\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 16/16 | Loss:4.8507 | MainLoss:0.4404 | SPLoss:44.0673 | CLSLoss:0.3570 | top1:79.7600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.6299 | MainLoss:0.2760 | SPLoss:23.4937 | CLSLoss:0.4507 | top1:89.3333 | AUROC:0.9561\n",
      "Test | 129/16 | Loss:3.3946 | MainLoss:1.0407 | SPLoss:23.4937 | CLSLoss:0.4507 | top1:53.2555 | AUROC:0.6390\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 16/16 | Loss:3917.1501 | MainLoss:0.3867 | SPLoss:39167.5938 | CLSLoss:0.4134 | top1:83.5733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2324.0753 | MainLoss:0.2308 | SPLoss:23238.3965 | CLSLoss:0.4701 | top1:90.8718 | AUROC:0.9701\n",
      "Test | 129/16 | Loss:2325.0184 | MainLoss:1.1739 | SPLoss:23238.3984 | CLSLoss:0.4701 | top1:53.4206 | AUROC:0.6030\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 16/16 | Loss:1326.4808 | MainLoss:0.3046 | SPLoss:13261.7158 | CLSLoss:0.4564 | top1:87.1200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:593.8813 | MainLoss:0.3973 | SPLoss:5934.7896 | CLSLoss:0.4898 | top1:83.6667 | AUROC:0.9551\n",
      "Test | 129/16 | Loss:595.1119 | MainLoss:1.6278 | SPLoss:5934.7856 | CLSLoss:0.4898 | top1:50.3240 | AUROC:0.4827\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 16/16 | Loss:558.3660 | MainLoss:0.2432 | SPLoss:5581.1792 | CLSLoss:0.4903 | top1:90.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:519.6751 | MainLoss:0.1942 | SPLoss:5194.7578 | CLSLoss:0.5016 | top1:92.3846 | AUROC:0.9788\n",
      "Test | 129/16 | Loss:520.7661 | MainLoss:1.2852 | SPLoss:5194.7534 | CLSLoss:0.5016 | top1:52.7414 | AUROC:0.5657\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 16/16 | Loss:488.7402 | MainLoss:0.1883 | SPLoss:4885.4678 | CLSLoss:0.5114 | top1:92.7467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:454.9149 | MainLoss:0.1775 | SPLoss:4547.3208 | CLSLoss:0.5189 | top1:93.1923 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:456.0969 | MainLoss:1.3595 | SPLoss:4547.3262 | CLSLoss:0.5189 | top1:52.8131 | AUROC:0.5725\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 16/16 | Loss:427.8520 | MainLoss:0.1809 | SPLoss:4276.6587 | CLSLoss:0.5265 | top1:93.0933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:398.2452 | MainLoss:0.1671 | SPLoss:3980.7273 | CLSLoss:0.5336 | top1:93.4744 | AUROC:0.9839\n",
      "Test | 129/16 | Loss:399.4682 | MainLoss:1.3902 | SPLoss:3980.7297 | CLSLoss:0.5336 | top1:53.3458 | AUROC:0.5937\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 16/16 | Loss:374.5482 | MainLoss:0.1577 | SPLoss:3743.8501 | CLSLoss:0.5409 | top1:94.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:348.6501 | MainLoss:0.1592 | SPLoss:3484.8538 | CLSLoss:0.5478 | top1:93.7821 | AUROC:0.9846\n",
      "Test | 129/16 | Loss:350.0201 | MainLoss:1.5292 | SPLoss:3484.8564 | CLSLoss:0.5478 | top1:52.3302 | AUROC:0.5756\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 16/16 | Loss:327.9184 | MainLoss:0.1585 | SPLoss:3277.5435 | CLSLoss:0.5533 | top1:93.8933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:305.2635 | MainLoss:0.1691 | SPLoss:3050.8884 | CLSLoss:0.5562 | top1:93.4872 | AUROC:0.9849\n",
      "Test | 129/16 | Loss:306.5302 | MainLoss:1.4357 | SPLoss:3050.8884 | CLSLoss:0.5562 | top1:53.3209 | AUROC:0.5791\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 16/16 | Loss:287.0940 | MainLoss:0.1446 | SPLoss:2869.4380 | CLSLoss:0.5610 | top1:94.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:267.2787 | MainLoss:0.1679 | SPLoss:2671.0510 | CLSLoss:0.5685 | top1:93.5513 | AUROC:0.9846\n",
      "Test | 129/16 | Loss:268.6091 | MainLoss:1.4983 | SPLoss:2671.0503 | CLSLoss:0.5685 | top1:53.4642 | AUROC:0.5966\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 16/16 | Loss:251.3704 | MainLoss:0.1379 | SPLoss:2512.2678 | CLSLoss:0.5734 | top1:94.9333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:234.0385 | MainLoss:0.1681 | SPLoss:2338.6467 | CLSLoss:0.5795 | top1:93.4487 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:235.4733 | MainLoss:1.6029 | SPLoss:2338.6450 | CLSLoss:0.5795 | top1:52.9720 | AUROC:0.5840\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 16/16 | Loss:220.0923 | MainLoss:0.1226 | SPLoss:2199.6387 | CLSLoss:0.5837 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:204.9520 | MainLoss:0.1803 | SPLoss:2047.6570 | CLSLoss:0.5890 | top1:93.1026 | AUROC:0.9832\n",
      "Test | 129/16 | Loss:206.3456 | MainLoss:1.5739 | SPLoss:2047.6602 | CLSLoss:0.5890 | top1:54.0156 | AUROC:0.6071\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 16/16 | Loss:192.7375 | MainLoss:0.1321 | SPLoss:1925.9950 | CLSLoss:0.5931 | top1:94.9333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:179.4766 | MainLoss:0.1726 | SPLoss:1792.9808 | CLSLoss:0.5915 | top1:93.3846 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:180.8747 | MainLoss:1.5706 | SPLoss:1792.9799 | CLSLoss:0.5915 | top1:53.5078 | AUROC:0.6042\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 16/16 | Loss:168.7800 | MainLoss:0.1250 | SPLoss:1686.4899 | CLSLoss:0.5981 | top1:95.2800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:157.1891 | MainLoss:0.1768 | SPLoss:1570.0638 | CLSLoss:0.6005 | top1:93.1667 | AUROC:0.9825\n",
      "Test | 129/16 | Loss:158.6245 | MainLoss:1.6122 | SPLoss:1570.0640 | CLSLoss:0.6005 | top1:53.7134 | AUROC:0.6121\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 16/16 | Loss:147.8205 | MainLoss:0.1301 | SPLoss:1476.8440 | CLSLoss:0.6020 | top1:95.0933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:137.6768 | MainLoss:0.1787 | SPLoss:1374.9198 | CLSLoss:0.6034 | top1:93.1026 | AUROC:0.9836\n",
      "Test | 129/16 | Loss:139.0604 | MainLoss:1.5624 | SPLoss:1374.9181 | CLSLoss:0.6034 | top1:54.3925 | AUROC:0.6150\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 16/16 | Loss:129.4573 | MainLoss:0.1204 | SPLoss:1293.3085 | CLSLoss:0.6082 | top1:95.6000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:120.5941 | MainLoss:0.1770 | SPLoss:1204.1096 | CLSLoss:0.6083 | top1:92.9744 | AUROC:0.9842\n",
      "Test | 129/16 | Loss:121.9124 | MainLoss:1.4954 | SPLoss:1204.1107 | CLSLoss:0.6083 | top1:55.1122 | AUROC:0.6180\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 16/16 | Loss:113.4018 | MainLoss:0.1256 | SPLoss:1132.7008 | CLSLoss:0.6112 | top1:95.3333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:105.6553 | MainLoss:0.1850 | SPLoss:1054.6421 | CLSLoss:0.6119 | top1:93.0513 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:106.9780 | MainLoss:1.5077 | SPLoss:1054.6415 | CLSLoss:0.6119 | top1:54.9844 | AUROC:0.6327\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 16/16 | Loss:99.3329 | MainLoss:0.1171 | SPLoss:992.0964 | CLSLoss:0.6167 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:92.5880 | MainLoss:0.2087 | SPLoss:923.7304 | CLSLoss:0.6198 | top1:92.1923 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:93.7548 | MainLoss:1.3755 | SPLoss:923.7299 | CLSLoss:0.6198 | top1:58.1464 | AUROC:0.6841\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 16/16 | Loss:87.0232 | MainLoss:0.1180 | SPLoss:868.9893 | CLSLoss:0.6224 | top1:95.6533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:81.0860 | MainLoss:0.1663 | SPLoss:809.1355 | CLSLoss:0.6231 | top1:93.7692 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:82.6986 | MainLoss:1.7789 | SPLoss:809.1359 | CLSLoss:0.6231 | top1:53.1651 | AUROC:0.6109\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 16/16 | Loss:76.2464 | MainLoss:0.1186 | SPLoss:761.2159 | CLSLoss:0.6255 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:71.0449 | MainLoss:0.1579 | SPLoss:708.8067 | CLSLoss:0.6262 | top1:94.0897 | AUROC:0.9847\n",
      "Test | 129/16 | Loss:72.5948 | MainLoss:1.7079 | SPLoss:708.8074 | CLSLoss:0.6262 | top1:53.7196 | AUROC:0.6266\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 16/16 | Loss:66.8075 | MainLoss:0.1172 | SPLoss:666.8403 | CLSLoss:0.6270 | top1:95.8667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:62.3022 | MainLoss:0.2013 | SPLoss:620.9468 | CLSLoss:0.6259 | top1:92.1154 | AUROC:0.9842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 129/16 | Loss:63.5483 | MainLoss:1.4474 | SPLoss:620.9468 | CLSLoss:0.6259 | top1:56.8474 | AUROC:0.6654\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 16/16 | Loss:58.5368 | MainLoss:0.1098 | SPLoss:584.2076 | CLSLoss:0.6301 | top1:96.0000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:54.5897 | MainLoss:0.1786 | SPLoss:544.0479 | CLSLoss:0.6316 | top1:93.2821 | AUROC:0.9849\n",
      "Test | 129/16 | Loss:55.9823 | MainLoss:1.5712 | SPLoss:544.0479 | CLSLoss:0.6316 | top1:55.5078 | AUROC:0.6550\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 16/16 | Loss:51.3220 | MainLoss:0.1276 | SPLoss:511.8808 | CLSLoss:0.6315 | top1:95.3867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:47.8520 | MainLoss:0.1736 | SPLoss:476.7202 | CLSLoss:0.6317 | top1:93.2821 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:49.3282 | MainLoss:1.6499 | SPLoss:476.7201 | CLSLoss:0.6317 | top1:53.0841 | AUROC:0.5899\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 16/16 | Loss:44.9737 | MainLoss:0.1108 | SPLoss:448.5657 | CLSLoss:0.6345 | top1:95.8400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:41.9573 | MainLoss:0.1739 | SPLoss:417.7701 | CLSLoss:0.6370 | top1:93.4487 | AUROC:0.9826\n",
      "Test | 129/16 | Loss:43.5132 | MainLoss:1.7298 | SPLoss:417.7708 | CLSLoss:0.6370 | top1:54.3489 | AUROC:0.6094\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 16/16 | Loss:39.4219 | MainLoss:0.1049 | SPLoss:393.1057 | CLSLoss:0.6390 | top1:96.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:36.8214 | MainLoss:0.2018 | SPLoss:366.1311 | CLSLoss:0.6434 | top1:92.5128 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:38.1585 | MainLoss:1.5389 | SPLoss:366.1317 | CLSLoss:0.6434 | top1:56.9595 | AUROC:0.6670\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 16/16 | Loss:34.5635 | MainLoss:0.1020 | SPLoss:344.5502 | CLSLoss:0.6456 | top1:96.3200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:32.2878 | MainLoss:0.1866 | SPLoss:320.9476 | CLSLoss:0.6463 | top1:92.9615 | AUROC:0.9822\n",
      "Test | 129/16 | Loss:33.7101 | MainLoss:1.6089 | SPLoss:320.9479 | CLSLoss:0.6463 | top1:55.5514 | AUROC:0.6296\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 16/16 | Loss:30.3278 | MainLoss:0.1125 | SPLoss:302.0881 | CLSLoss:0.6454 | top1:96.1600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:28.3172 | MainLoss:0.1709 | SPLoss:281.3987 | CLSLoss:0.6458 | top1:93.6154 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:29.9414 | MainLoss:1.7951 | SPLoss:281.3985 | CLSLoss:0.6458 | top1:54.3738 | AUROC:0.6496\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 16/16 | Loss:26.6231 | MainLoss:0.1290 | SPLoss:264.8771 | CLSLoss:0.6443 | top1:94.8000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:24.8668 | MainLoss:0.1724 | SPLoss:246.8804 | CLSLoss:0.6433 | top1:93.4487 | AUROC:0.9838\n",
      "Test | 129/16 | Loss:26.2263 | MainLoss:1.5318 | SPLoss:246.8801 | CLSLoss:0.6433 | top1:56.5296 | AUROC:0.6959\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 16/16 | Loss:23.3516 | MainLoss:0.1072 | SPLoss:232.3785 | CLSLoss:0.6468 | top1:95.9467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:21.8275 | MainLoss:0.1686 | SPLoss:216.5235 | CLSLoss:0.6493 | top1:93.6154 | AUROC:0.9848\n",
      "Test | 129/16 | Loss:23.3240 | MainLoss:1.6651 | SPLoss:216.5239 | CLSLoss:0.6493 | top1:55.5452 | AUROC:0.6567\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 16/16 | Loss:20.5030 | MainLoss:0.1157 | SPLoss:203.8079 | CLSLoss:0.6495 | top1:95.7600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:19.1600 | MainLoss:0.1642 | SPLoss:189.8925 | CLSLoss:0.6481 | top1:93.6282 | AUROC:0.9847\n",
      "Test | 129/16 | Loss:20.5604 | MainLoss:1.5646 | SPLoss:189.8925 | CLSLoss:0.6481 | top1:56.1526 | AUROC:0.6919\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 16/16 | Loss:17.9860 | MainLoss:0.1037 | SPLoss:178.7580 | CLSLoss:0.6513 | top1:95.9467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:16.8386 | MainLoss:0.1735 | SPLoss:166.5850 | CLSLoss:0.6531 | top1:93.5897 | AUROC:0.9852\n",
      "Test | 129/16 | Loss:18.2608 | MainLoss:1.5958 | SPLoss:166.5852 | CLSLoss:0.6531 | top1:56.4486 | AUROC:0.6807\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 16/16 | Loss:15.7849 | MainLoss:0.0949 | SPLoss:156.8342 | CLSLoss:0.6566 | top1:96.7200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:14.8158 | MainLoss:0.1882 | SPLoss:146.2101 | CLSLoss:0.6556 | top1:92.9231 | AUROC:0.9810\n",
      "Test | 129/16 | Loss:16.2447 | MainLoss:1.6171 | SPLoss:146.2100 | CLSLoss:0.6556 | top1:56.5171 | AUROC:0.6770\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 16/16 | Loss:13.8849 | MainLoss:0.1116 | SPLoss:137.6679 | CLSLoss:0.6551 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:13.0059 | MainLoss:0.1674 | SPLoss:128.3204 | CLSLoss:0.6516 | top1:93.6667 | AUROC:0.9843\n",
      "Test | 129/16 | Loss:14.4681 | MainLoss:1.6295 | SPLoss:128.3203 | CLSLoss:0.6516 | top1:55.3551 | AUROC:0.6532\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 16/16 | Loss:12.2142 | MainLoss:0.1171 | SPLoss:120.9066 | CLSLoss:0.6523 | top1:95.7600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:11.4511 | MainLoss:0.1629 | SPLoss:112.8171 | CLSLoss:0.6501 | top1:93.9231 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:12.9951 | MainLoss:1.7069 | SPLoss:112.8170 | CLSLoss:0.6501 | top1:54.5670 | AUROC:0.6413\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 16/16 | Loss:10.7483 | MainLoss:0.1166 | SPLoss:106.2513 | CLSLoss:0.6512 | top1:95.8400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:10.0913 | MainLoss:0.1775 | SPLoss:99.0727 | CLSLoss:0.6494 | top1:93.3205 | AUROC:0.9835\n",
      "Test | 129/16 | Loss:11.5278 | MainLoss:1.6140 | SPLoss:99.0728 | CLSLoss:0.6494 | top1:55.5950 | AUROC:0.6421\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 16/16 | Loss:9.4891 | MainLoss:0.1209 | SPLoss:93.6168 | CLSLoss:0.6500 | top1:95.6267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:9.0317 | MainLoss:0.1758 | SPLoss:88.4940 | CLSLoss:0.6480 | top1:93.2051 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:10.4320 | MainLoss:1.5762 | SPLoss:88.4940 | CLSLoss:0.6480 | top1:55.9782 | AUROC:0.6749\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 16/16 | Loss:8.4749 | MainLoss:0.1296 | SPLoss:83.3880 | CLSLoss:0.6470 | top1:94.6933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:7.9592 | MainLoss:0.1742 | SPLoss:77.7856 | CLSLoss:0.6453 | top1:93.1795 | AUROC:0.9846\n",
      "Test | 129/16 | Loss:9.3229 | MainLoss:1.5379 | SPLoss:77.7856 | CLSLoss:0.6453 | top1:56.3863 | AUROC:0.6983\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 16/16 | Loss:7.4407 | MainLoss:0.1048 | SPLoss:73.2933 | CLSLoss:0.6490 | top1:95.9733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:7.0211 | MainLoss:0.1764 | SPLoss:68.3820 | CLSLoss:0.6526 | top1:93.0128 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:8.5104 | MainLoss:1.6656 | SPLoss:68.3820 | CLSLoss:0.6526 | top1:55.4050 | AUROC:0.6731\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 16/16 | Loss:6.5592 | MainLoss:0.1079 | SPLoss:64.4469 | CLSLoss:0.6533 | top1:95.9733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:6.2236 | MainLoss:0.2021 | SPLoss:60.1497 | CLSLoss:0.6559 | top1:92.5897 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:7.6035 | MainLoss:1.5820 | SPLoss:60.1499 | CLSLoss:0.6560 | top1:57.2336 | AUROC:0.7126\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 16/16 | Loss:5.9021 | MainLoss:0.1122 | SPLoss:57.8329 | CLSLoss:0.6559 | top1:95.8933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.7271 | MainLoss:0.1904 | SPLoss:55.3009 | CLSLoss:0.6539 | top1:92.8462 | AUROC:0.9806\n",
      "Test | 129/16 | Loss:7.3024 | MainLoss:1.7658 | SPLoss:55.3010 | CLSLoss:0.6539 | top1:54.9626 | AUROC:0.6804\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 16/16 | Loss:5.3405 | MainLoss:0.1184 | SPLoss:52.1557 | CLSLoss:0.6534 | top1:95.6800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:5.0583 | MainLoss:0.1787 | SPLoss:48.7301 | CLSLoss:0.6536 | top1:93.1538 | AUROC:0.9816\n",
      "Test | 129/16 | Loss:6.6671 | MainLoss:1.7876 | SPLoss:48.7301 | CLSLoss:0.6536 | top1:54.4704 | AUROC:0.6814\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n",
      "Train | 16/16 | Loss:4.7110 | MainLoss:0.1082 | SPLoss:45.9630 | CLSLoss:0.6552 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.4789 | MainLoss:0.1797 | SPLoss:42.9269 | CLSLoss:0.6559 | top1:93.2564 | AUROC:0.9819\n",
      "Test | 129/16 | Loss:6.1037 | MainLoss:1.8044 | SPLoss:42.9270 | CLSLoss:0.6559 | top1:54.6106 | AUROC:0.6682\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 16/16 | Loss:4.1850 | MainLoss:0.1263 | SPLoss:40.5218 | CLSLoss:0.6526 | top1:95.2000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.9606 | MainLoss:0.1686 | SPLoss:37.8547 | CLSLoss:0.6521 | top1:93.7564 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:5.4800 | MainLoss:1.6880 | SPLoss:37.8547 | CLSLoss:0.6521 | top1:55.2336 | AUROC:0.6871\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 16/16 | Loss:3.6865 | MainLoss:0.1067 | SPLoss:35.7325 | CLSLoss:0.6548 | top1:95.7867 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 32/16 | Loss:3.5272 | MainLoss:0.1793 | SPLoss:33.4135 | CLSLoss:0.6534 | top1:93.0385 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:4.9057 | MainLoss:1.5578 | SPLoss:33.4135 | CLSLoss:0.6534 | top1:56.9097 | AUROC:0.7070\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 16/16 | Loss:3.2601 | MainLoss:0.1007 | SPLoss:31.5282 | CLSLoss:0.6566 | top1:96.3200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.1258 | MainLoss:0.1716 | SPLoss:29.4771 | CLSLoss:0.6544 | top1:93.5385 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:4.6881 | MainLoss:1.7338 | SPLoss:29.4772 | CLSLoss:0.6544 | top1:54.5763 | AUROC:0.6600\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n",
      "Train | 16/16 | Loss:2.9036 | MainLoss:0.1111 | SPLoss:27.8595 | CLSLoss:0.6539 | top1:95.8400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.7918 | MainLoss:0.1775 | SPLoss:26.0773 | CLSLoss:0.6542 | top1:93.3205 | AUROC:0.9843\n",
      "Test | 129/16 | Loss:4.1674 | MainLoss:1.5532 | SPLoss:26.0773 | CLSLoss:0.6542 | top1:56.7508 | AUROC:0.7022\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 16/16 | Loss:2.5887 | MainLoss:0.1166 | SPLoss:24.6554 | CLSLoss:0.6542 | top1:95.9200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.4919 | MainLoss:0.1735 | SPLoss:23.1181 | CLSLoss:0.6549 | top1:93.6154 | AUROC:0.9842\n",
      "Test | 129/16 | Loss:3.8819 | MainLoss:1.5636 | SPLoss:23.1180 | CLSLoss:0.6549 | top1:56.5296 | AUROC:0.6891\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 16/16 | Loss:2.2955 | MainLoss:0.1016 | SPLoss:21.8735 | CLSLoss:0.6570 | top1:96.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.2995 | MainLoss:0.2259 | SPLoss:20.6704 | CLSLoss:0.6570 | top1:91.8205 | AUROC:0.9844\n",
      "Test | 129/16 | Loss:3.4271 | MainLoss:1.3535 | SPLoss:20.6704 | CLSLoss:0.6570 | top1:59.8660 | AUROC:0.7253\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.039308\n",
      "Train | 16/16 | Loss:2.0544 | MainLoss:0.0943 | SPLoss:19.5358 | CLSLoss:0.6601 | top1:96.4533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.0241 | MainLoss:0.1859 | SPLoss:18.3153 | CLSLoss:0.6599 | top1:93.0769 | AUROC:0.9842\n",
      "Test | 129/16 | Loss:3.3372 | MainLoss:1.4990 | SPLoss:18.3153 | CLSLoss:0.6599 | top1:58.3084 | AUROC:0.7160\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.039291\n",
      "Train | 16/16 | Loss:1.8403 | MainLoss:0.0980 | SPLoss:17.3569 | CLSLoss:0.6623 | top1:96.5600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.8319 | MainLoss:0.1881 | SPLoss:16.3713 | CLSLoss:0.6615 | top1:92.9487 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:3.1984 | MainLoss:1.5547 | SPLoss:16.3713 | CLSLoss:0.6615 | top1:57.7664 | AUROC:0.7063\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.039274\n",
      "Train | 16/16 | Loss:1.6833 | MainLoss:0.0926 | SPLoss:15.8409 | CLSLoss:0.6654 | top1:96.6133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.7273 | MainLoss:0.1777 | SPLoss:15.4293 | CLSLoss:0.6627 | top1:93.2436 | AUROC:0.9826\n",
      "Test | 129/16 | Loss:3.1885 | MainLoss:1.6390 | SPLoss:15.4293 | CLSLoss:0.6627 | top1:56.3925 | AUROC:0.6944\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.039258\n",
      "Train | 16/16 | Loss:1.5744 | MainLoss:0.1047 | SPLoss:14.6298 | CLSLoss:0.6648 | top1:96.4267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.5590 | MainLoss:0.1772 | SPLoss:13.7518 | CLSLoss:0.6622 | top1:93.4231 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:2.9824 | MainLoss:1.6006 | SPLoss:13.7519 | CLSLoss:0.6622 | top1:56.7757 | AUROC:0.7123\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.039241\n",
      "Train | 16/16 | Loss:1.4055 | MainLoss:0.0952 | SPLoss:13.0357 | CLSLoss:0.6649 | top1:96.4800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4088 | MainLoss:0.1770 | SPLoss:12.2514 | CLSLoss:0.6684 | top1:93.5513 | AUROC:0.9825\n",
      "Test | 129/16 | Loss:3.0460 | MainLoss:1.8142 | SPLoss:12.2514 | CLSLoss:0.6684 | top1:54.9470 | AUROC:0.7004\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.039223\n",
      "Train | 16/16 | Loss:1.2779 | MainLoss:0.1061 | SPLoss:11.6505 | CLSLoss:0.6690 | top1:96.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.3025 | MainLoss:0.1962 | SPLoss:10.9964 | CLSLoss:0.6652 | top1:92.6667 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:2.5978 | MainLoss:1.4915 | SPLoss:10.9964 | CLSLoss:0.6652 | top1:57.8567 | AUROC:0.7154\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.039206\n",
      "Train | 16/16 | Loss:2.2513 | MainLoss:0.1449 | SPLoss:20.9978 | CLSLoss:0.6605 | top1:94.7467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.0561 | MainLoss:0.1970 | SPLoss:38.5248 | CLSLoss:0.6551 | top1:92.3718 | AUROC:0.9772\n",
      "Test | 129/16 | Loss:5.3918 | MainLoss:1.5327 | SPLoss:38.5248 | CLSLoss:0.6551 | top1:56.5452 | AUROC:0.8345\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.039188\n",
      "Train | 16/16 | Loss:3.7653 | MainLoss:0.1210 | SPLoss:36.3777 | CLSLoss:0.6583 | top1:95.2267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.5888 | MainLoss:0.1805 | SPLoss:34.0168 | CLSLoss:0.6605 | top1:93.2308 | AUROC:0.9814\n",
      "Test | 129/16 | Loss:5.0439 | MainLoss:1.6356 | SPLoss:34.0168 | CLSLoss:0.6605 | top1:55.8536 | AUROC:0.7683\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.039170\n",
      "Train | 16/16 | Loss:3.3323 | MainLoss:0.1139 | SPLoss:32.1181 | CLSLoss:0.6616 | top1:95.8933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.2009 | MainLoss:0.1895 | SPLoss:30.0484 | CLSLoss:0.6619 | top1:92.7821 | AUROC:0.9800\n",
      "Test | 129/16 | Loss:4.6826 | MainLoss:1.6712 | SPLoss:30.0483 | CLSLoss:0.6619 | top1:56.2430 | AUROC:0.8068\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.039152\n",
      "Train | 16/16 | Loss:2.9643 | MainLoss:0.1124 | SPLoss:28.4525 | CLSLoss:0.6613 | top1:95.9200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.8478 | MainLoss:0.1753 | SPLoss:26.6586 | CLSLoss:0.6625 | top1:93.3974 | AUROC:0.9828\n",
      "Test | 129/16 | Loss:4.2905 | MainLoss:1.6180 | SPLoss:26.6586 | CLSLoss:0.6625 | top1:56.9003 | AUROC:0.7639\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.039134\n",
      "Train | 16/16 | Loss:2.6385 | MainLoss:0.1124 | SPLoss:25.1939 | CLSLoss:0.6636 | top1:95.6267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.5490 | MainLoss:0.1814 | SPLoss:23.6101 | CLSLoss:0.6637 | top1:93.0897 | AUROC:0.9824\n",
      "Test | 129/16 | Loss:3.8691 | MainLoss:1.5014 | SPLoss:23.6100 | CLSLoss:0.6637 | top1:58.5545 | AUROC:0.8053\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.039116\n",
      "Train | 16/16 | Loss:2.3436 | MainLoss:0.1051 | SPLoss:22.3184 | CLSLoss:0.6637 | top1:96.2667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.3287 | MainLoss:0.2297 | SPLoss:20.9231 | CLSLoss:0.6634 | top1:91.3974 | AUROC:0.9823\n",
      "Test | 129/16 | Loss:3.2992 | MainLoss:1.2002 | SPLoss:20.9231 | CLSLoss:0.6634 | top1:63.1776 | AUROC:0.8035\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.039097\n",
      "Train | 16/16 | Loss:2.0812 | MainLoss:0.0944 | SPLoss:19.8020 | CLSLoss:0.6656 | top1:96.4267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.0469 | MainLoss:0.1838 | SPLoss:18.5645 | CLSLoss:0.6691 | top1:93.4359 | AUROC:0.9827\n",
      "Test | 129/16 | Loss:3.4149 | MainLoss:1.5517 | SPLoss:18.5645 | CLSLoss:0.6691 | top1:58.7570 | AUROC:0.8040\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.039079\n",
      "Train | 16/16 | Loss:2.0350 | MainLoss:0.1054 | SPLoss:19.2300 | CLSLoss:0.6684 | top1:96.2667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.4738 | MainLoss:0.1801 | SPLoss:32.8709 | CLSLoss:0.6674 | top1:93.0769 | AUROC:0.9826\n",
      "Test | 129/16 | Loss:4.8770 | MainLoss:1.5833 | SPLoss:32.8708 | CLSLoss:0.6674 | top1:57.6854 | AUROC:0.7668\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.039060\n",
      "Train | 16/16 | Loss:3.2295 | MainLoss:0.0939 | SPLoss:31.2887 | CLSLoss:0.6687 | top1:96.6133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.1509 | MainLoss:0.2152 | SPLoss:29.2900 | CLSLoss:0.6692 | top1:91.9359 | AUROC:0.9823\n",
      "Test | 129/16 | Loss:4.2985 | MainLoss:1.3628 | SPLoss:29.2900 | CLSLoss:0.6692 | top1:60.6885 | AUROC:0.7782\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.039040\n",
      "Train | 16/16 | Loss:2.8891 | MainLoss:0.1141 | SPLoss:27.6832 | CLSLoss:0.6668 | top1:96.0533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.7926 | MainLoss:0.1936 | SPLoss:25.9239 | CLSLoss:0.6642 | top1:92.5641 | AUROC:0.9815\n",
      "Test | 129/16 | Loss:4.0679 | MainLoss:1.4688 | SPLoss:25.9238 | CLSLoss:0.6642 | top1:58.5296 | AUROC:0.7675\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.039021\n",
      "Train | 16/16 | Loss:2.5522 | MainLoss:0.0950 | SPLoss:24.5053 | CLSLoss:0.6669 | top1:96.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.4798 | MainLoss:0.1787 | SPLoss:22.9438 | CLSLoss:0.6680 | top1:93.3077 | AUROC:0.9827\n",
      "Test | 129/16 | Loss:4.0290 | MainLoss:1.7279 | SPLoss:22.9438 | CLSLoss:0.6680 | top1:56.5109 | AUROC:0.7514\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.039002\n",
      "Train | 16/16 | Loss:2.2746 | MainLoss:0.0971 | SPLoss:21.7091 | CLSLoss:0.6682 | top1:96.3467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.2158 | MainLoss:0.1727 | SPLoss:20.3640 | CLSLoss:0.6675 | top1:93.5897 | AUROC:0.9835\n",
      "Test | 129/16 | Loss:3.7744 | MainLoss:1.7313 | SPLoss:20.3640 | CLSLoss:0.6675 | top1:56.1371 | AUROC:0.7375\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.038982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:2.0423 | MainLoss:0.1074 | SPLoss:19.2826 | CLSLoss:0.6651 | top1:96.0800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.9906 | MainLoss:0.1762 | SPLoss:18.0776 | CLSLoss:0.6672 | top1:93.3590 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:3.4584 | MainLoss:1.6440 | SPLoss:18.0776 | CLSLoss:0.6672 | top1:57.2212 | AUROC:0.7686\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.038962\n",
      "Train | 16/16 | Loss:1.8084 | MainLoss:0.0904 | SPLoss:17.1130 | CLSLoss:0.6694 | top1:96.8800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.7988 | MainLoss:0.1865 | SPLoss:16.0567 | CLSLoss:0.6685 | top1:93.1154 | AUROC:0.9812\n",
      "Test | 129/16 | Loss:3.2289 | MainLoss:1.6165 | SPLoss:16.0566 | CLSLoss:0.6685 | top1:57.7788 | AUROC:0.7822\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.038942\n",
      "Train | 16/16 | Loss:1.6430 | MainLoss:0.1051 | SPLoss:15.3122 | CLSLoss:0.6684 | top1:96.1333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.6331 | MainLoss:0.1814 | SPLoss:14.4501 | CLSLoss:0.6676 | top1:93.2308 | AUROC:0.9814\n",
      "Test | 129/16 | Loss:3.0660 | MainLoss:1.6143 | SPLoss:14.4500 | CLSLoss:0.6676 | top1:57.5265 | AUROC:0.7709\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.038922\n",
      "Train | 16/16 | Loss:1.4871 | MainLoss:0.1042 | SPLoss:13.7618 | CLSLoss:0.6690 | top1:95.9733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.6028 | MainLoss:0.2636 | SPLoss:13.3263 | CLSLoss:0.6668 | top1:89.9615 | AUROC:0.9792\n",
      "Test | 129/16 | Loss:2.5107 | MainLoss:1.1714 | SPLoss:13.3263 | CLSLoss:0.6668 | top1:63.8006 | AUROC:0.8073\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.038901\n",
      "Train | 16/16 | Loss:1.4036 | MainLoss:0.1294 | SPLoss:12.6758 | CLSLoss:0.6647 | top1:95.0933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.3894 | MainLoss:0.1862 | SPLoss:11.9661 | CLSLoss:0.6615 | top1:92.8718 | AUROC:0.9809\n",
      "Test | 129/16 | Loss:2.7252 | MainLoss:1.5219 | SPLoss:11.9661 | CLSLoss:0.6615 | top1:57.0156 | AUROC:0.7767\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.038881\n",
      "Train | 16/16 | Loss:1.2638 | MainLoss:0.1215 | SPLoss:11.3568 | CLSLoss:0.6620 | top1:95.7067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.2777 | MainLoss:0.2008 | SPLoss:10.7027 | CLSLoss:0.6633 | top1:92.3333 | AUROC:0.9814\n",
      "Test | 129/16 | Loss:2.5725 | MainLoss:1.4956 | SPLoss:10.7027 | CLSLoss:0.6633 | top1:58.0935 | AUROC:0.7737\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.038860\n",
      "Train | 16/16 | Loss:1.1246 | MainLoss:0.1007 | SPLoss:10.1725 | CLSLoss:0.6659 | top1:96.3467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.1335 | MainLoss:0.1677 | SPLoss:9.5905 | CLSLoss:0.6669 | top1:93.8333 | AUROC:0.9828\n",
      "Test | 129/16 | Loss:2.8114 | MainLoss:1.8457 | SPLoss:9.5905 | CLSLoss:0.6669 | top1:54.4206 | AUROC:0.7159\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.038839\n",
      "Train | 16/16 | Loss:1.0373 | MainLoss:0.1125 | SPLoss:9.1814 | CLSLoss:0.6668 | top1:95.7333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.1005 | MainLoss:0.1872 | SPLoss:9.0663 | CLSLoss:0.6636 | top1:92.8974 | AUROC:0.9828\n",
      "Test | 129/16 | Loss:2.5284 | MainLoss:1.6151 | SPLoss:9.0663 | CLSLoss:0.6636 | top1:56.0312 | AUROC:0.7269\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.038818\n",
      "Train | 16/16 | Loss:0.9757 | MainLoss:0.0961 | SPLoss:8.7296 | CLSLoss:0.6688 | top1:96.5333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.0808 | MainLoss:0.1980 | SPLoss:8.7612 | CLSLoss:0.6688 | top1:92.7308 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:2.5127 | MainLoss:1.6299 | SPLoss:8.7612 | CLSLoss:0.6688 | top1:57.2087 | AUROC:0.7651\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.038796\n",
      "Train | 16/16 | Loss:0.9672 | MainLoss:0.1252 | SPLoss:8.3539 | CLSLoss:0.6643 | top1:95.2800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9693 | MainLoss:0.1719 | SPLoss:7.9073 | CLSLoss:0.6653 | top1:93.2179 | AUROC:0.9843\n",
      "Test | 129/16 | Loss:2.5037 | MainLoss:1.7064 | SPLoss:7.9073 | CLSLoss:0.6653 | top1:55.9190 | AUROC:0.7652\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.038775\n",
      "Train | 16/16 | Loss:0.8715 | MainLoss:0.1101 | SPLoss:7.5472 | CLSLoss:0.6673 | top1:96.0267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9000 | MainLoss:0.1756 | SPLoss:7.1775 | CLSLoss:0.6657 | top1:93.4231 | AUROC:0.9838\n",
      "Test | 129/16 | Loss:2.4380 | MainLoss:1.7136 | SPLoss:7.1775 | CLSLoss:0.6657 | top1:55.4174 | AUROC:0.7299\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.038753\n",
      "Train | 16/16 | Loss:0.8040 | MainLoss:0.1124 | SPLoss:6.8488 | CLSLoss:0.6666 | top1:95.6800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8621 | MainLoss:0.2049 | SPLoss:6.5055 | CLSLoss:0.6659 | top1:92.2308 | AUROC:0.9849\n",
      "Test | 129/16 | Loss:2.1722 | MainLoss:1.5150 | SPLoss:6.5055 | CLSLoss:0.6659 | top1:56.9875 | AUROC:0.7227\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.038731\n",
      "Train | 16/16 | Loss:0.7242 | MainLoss:0.0945 | SPLoss:6.2297 | CLSLoss:0.6704 | top1:96.8000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7800 | MainLoss:0.1788 | SPLoss:5.9444 | CLSLoss:0.6719 | top1:93.2179 | AUROC:0.9842\n",
      "Test | 129/16 | Loss:2.3927 | MainLoss:1.7916 | SPLoss:5.9444 | CLSLoss:0.6719 | top1:55.8131 | AUROC:0.7513\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.038709\n",
      "Train | 16/16 | Loss:0.6884 | MainLoss:0.1052 | SPLoss:5.7644 | CLSLoss:0.6708 | top1:96.2400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7372 | MainLoss:0.1793 | SPLoss:5.5115 | CLSLoss:0.6704 | top1:93.2692 | AUROC:0.9833\n",
      "Test | 129/16 | Loss:2.3135 | MainLoss:1.7557 | SPLoss:5.5115 | CLSLoss:0.6704 | top1:55.6324 | AUROC:0.7277\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.038687\n",
      "Train | 16/16 | Loss:0.6244 | MainLoss:0.0891 | SPLoss:5.2849 | CLSLoss:0.6749 | top1:96.7733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6849 | MainLoss:0.1748 | SPLoss:5.0332 | CLSLoss:0.6738 | top1:93.6923 | AUROC:0.9829\n",
      "Test | 129/16 | Loss:2.4729 | MainLoss:1.9628 | SPLoss:5.0332 | CLSLoss:0.6738 | top1:54.0000 | AUROC:0.6862\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.038664\n",
      "Train | 16/16 | Loss:0.6103 | MainLoss:0.1193 | SPLoss:4.8422 | CLSLoss:0.6735 | top1:95.7600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6389 | MainLoss:0.1605 | SPLoss:4.7178 | CLSLoss:0.6614 | top1:93.7949 | AUROC:0.9844\n",
      "Test | 129/16 | Loss:2.1387 | MainLoss:1.6603 | SPLoss:4.7178 | CLSLoss:0.6614 | top1:55.2835 | AUROC:0.7212\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.038641\n",
      "Train | 16/16 | Loss:0.5509 | MainLoss:0.0907 | SPLoss:4.5353 | CLSLoss:0.6664 | top1:96.8800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6181 | MainLoss:0.1760 | SPLoss:4.3541 | CLSLoss:0.6690 | top1:93.3462 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:2.2175 | MainLoss:1.7753 | SPLoss:4.3541 | CLSLoss:0.6690 | top1:56.2555 | AUROC:0.7593\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.038619\n",
      "Train | 16/16 | Loss:0.5253 | MainLoss:0.0995 | SPLoss:4.1912 | CLSLoss:0.6678 | top1:96.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5760 | MainLoss:0.1681 | SPLoss:4.0125 | CLSLoss:0.6688 | top1:93.9359 | AUROC:0.9841\n",
      "Test | 129/16 | Loss:2.4127 | MainLoss:2.0047 | SPLoss:4.0125 | CLSLoss:0.6688 | top1:53.7041 | AUROC:0.7058\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.038596\n",
      "Train | 16/16 | Loss:0.5417 | MainLoss:0.1030 | SPLoss:4.3199 | CLSLoss:0.6679 | top1:96.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6939 | MainLoss:0.2030 | SPLoss:4.8422 | CLSLoss:0.6666 | top1:92.2821 | AUROC:0.9821\n",
      "Test | 129/16 | Loss:2.0874 | MainLoss:1.5965 | SPLoss:4.8422 | CLSLoss:0.6666 | top1:56.8318 | AUROC:0.7109\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.038572\n",
      "Train | 16/16 | Loss:0.5671 | MainLoss:0.0958 | SPLoss:4.6466 | CLSLoss:0.6680 | top1:96.7733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6277 | MainLoss:0.1759 | SPLoss:4.4514 | CLSLoss:0.6695 | top1:93.3333 | AUROC:0.9839\n",
      "Test | 129/16 | Loss:2.3562 | MainLoss:1.9044 | SPLoss:4.4514 | CLSLoss:0.6695 | top1:54.4704 | AUROC:0.6971\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.038549\n",
      "Train | 16/16 | Loss:0.5883 | MainLoss:0.1096 | SPLoss:4.7201 | CLSLoss:0.6673 | top1:96.1067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8821 | MainLoss:0.2146 | SPLoss:6.6087 | CLSLoss:0.6649 | top1:91.6282 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:2.2760 | MainLoss:1.6085 | SPLoss:6.6087 | CLSLoss:0.6649 | top1:57.5265 | AUROC:0.7587\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.038525\n",
      "Train | 16/16 | Loss:0.8056 | MainLoss:0.1425 | SPLoss:6.5653 | CLSLoss:0.6591 | top1:94.3733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8088 | MainLoss:0.1703 | SPLoss:6.3194 | CLSLoss:0.6574 | top1:93.3974 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:2.4382 | MainLoss:1.7997 | SPLoss:6.3194 | CLSLoss:0.6574 | top1:53.8100 | AUROC:0.7036\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.038502\n",
      "Train | 16/16 | Loss:0.7129 | MainLoss:0.1013 | SPLoss:6.0502 | CLSLoss:0.6610 | top1:96.4000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7633 | MainLoss:0.1790 | SPLoss:5.7771 | CLSLoss:0.6609 | top1:93.2051 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:2.2597 | MainLoss:1.6753 | SPLoss:5.7771 | CLSLoss:0.6610 | top1:55.4268 | AUROC:0.7156\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.038478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6441 | MainLoss:0.0849 | SPLoss:5.5252 | CLSLoss:0.6645 | top1:96.9333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7128 | MainLoss:0.1796 | SPLoss:5.2652 | CLSLoss:0.6677 | top1:93.4744 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:2.3459 | MainLoss:1.8127 | SPLoss:5.2652 | CLSLoss:0.6677 | top1:55.3645 | AUROC:0.7337\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.038453\n",
      "Train | 16/16 | Loss:0.6435 | MainLoss:0.1304 | SPLoss:5.0644 | CLSLoss:0.6626 | top1:95.1467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6670 | MainLoss:0.1741 | SPLoss:4.8636 | CLSLoss:0.6569 | top1:93.4103 | AUROC:0.9832\n",
      "Test | 129/16 | Loss:2.1860 | MainLoss:1.6931 | SPLoss:4.8636 | CLSLoss:0.6569 | top1:54.7726 | AUROC:0.6914\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.038429\n",
      "Train | 16/16 | Loss:0.5694 | MainLoss:0.0964 | SPLoss:4.6642 | CLSLoss:0.6604 | top1:96.6933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6297 | MainLoss:0.1782 | SPLoss:4.4490 | CLSLoss:0.6620 | top1:93.3974 | AUROC:0.9832\n",
      "Test | 129/16 | Loss:2.1689 | MainLoss:1.7173 | SPLoss:4.4490 | CLSLoss:0.6620 | top1:55.7134 | AUROC:0.7350\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.038405\n",
      "Train | 16/16 | Loss:0.5410 | MainLoss:0.1036 | SPLoss:4.3079 | CLSLoss:0.6599 | top1:96.1067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5894 | MainLoss:0.1694 | SPLoss:4.1339 | CLSLoss:0.6619 | top1:93.7436 | AUROC:0.9835\n",
      "Test | 129/16 | Loss:2.2057 | MainLoss:1.7857 | SPLoss:4.1339 | CLSLoss:0.6619 | top1:55.2368 | AUROC:0.7391\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.038380\n",
      "Train | 16/16 | Loss:0.4958 | MainLoss:0.0908 | SPLoss:3.9839 | CLSLoss:0.6642 | top1:96.5600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5890 | MainLoss:0.1992 | SPLoss:3.8310 | CLSLoss:0.6637 | top1:92.7308 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:1.9590 | MainLoss:1.5693 | SPLoss:3.8310 | CLSLoss:0.6637 | top1:57.7601 | AUROC:0.7692\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.038355\n",
      "Train | 16/16 | Loss:0.4863 | MainLoss:0.1086 | SPLoss:3.7108 | CLSLoss:0.6638 | top1:96.1067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5399 | MainLoss:0.1707 | SPLoss:3.6261 | CLSLoss:0.6617 | top1:93.3590 | AUROC:0.9833\n",
      "Test | 129/16 | Loss:2.2520 | MainLoss:1.8828 | SPLoss:3.6261 | CLSLoss:0.6617 | top1:53.9875 | AUROC:0.7309\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.038330\n",
      "Train | 16/16 | Loss:0.4612 | MainLoss:0.1010 | SPLoss:3.5352 | CLSLoss:0.6631 | top1:96.2133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5334 | MainLoss:0.1804 | SPLoss:3.4641 | CLSLoss:0.6616 | top1:93.2949 | AUROC:0.9828\n",
      "Test | 129/16 | Loss:2.0149 | MainLoss:1.6618 | SPLoss:3.4641 | CLSLoss:0.6616 | top1:56.0997 | AUROC:0.7547\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.038305\n",
      "Train | 16/16 | Loss:0.4314 | MainLoss:0.0876 | SPLoss:3.3719 | CLSLoss:0.6651 | top1:96.8800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5159 | MainLoss:0.1802 | SPLoss:3.2908 | CLSLoss:0.6663 | top1:93.3846 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:2.0857 | MainLoss:1.7500 | SPLoss:3.2908 | CLSLoss:0.6663 | top1:55.8474 | AUROC:0.7261\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.038279\n",
      "Train | 16/16 | Loss:0.4152 | MainLoss:0.0906 | SPLoss:3.1791 | CLSLoss:0.6670 | top1:96.6933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4930 | MainLoss:0.1802 | SPLoss:3.0613 | CLSLoss:0.6678 | top1:93.1667 | AUROC:0.9830\n",
      "Test | 129/16 | Loss:2.2215 | MainLoss:1.9086 | SPLoss:3.0613 | CLSLoss:0.6678 | top1:54.9221 | AUROC:0.7333\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.038254\n",
      "Train | 16/16 | Loss:0.3915 | MainLoss:0.0858 | SPLoss:2.9903 | CLSLoss:0.6702 | top1:96.8267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4813 | MainLoss:0.1833 | SPLoss:2.9133 | CLSLoss:0.6688 | top1:93.2436 | AUROC:0.9813\n",
      "Test | 129/16 | Loss:2.1894 | MainLoss:1.8914 | SPLoss:2.9133 | CLSLoss:0.6688 | top1:54.7882 | AUROC:0.7027\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.038228\n",
      "Train | 16/16 | Loss:0.3855 | MainLoss:0.0951 | SPLoss:2.8368 | CLSLoss:0.6685 | top1:96.4800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4640 | MainLoss:0.1826 | SPLoss:2.7466 | CLSLoss:0.6692 | top1:93.1923 | AUROC:0.9822\n",
      "Test | 129/16 | Loss:2.2571 | MainLoss:1.9758 | SPLoss:2.7466 | CLSLoss:0.6692 | top1:54.1246 | AUROC:0.7031\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.038202\n",
      "Train | 16/16 | Loss:0.4677 | MainLoss:0.1458 | SPLoss:3.1525 | CLSLoss:0.6640 | top1:94.2133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5166 | MainLoss:0.1733 | SPLoss:3.3676 | CLSLoss:0.6519 | top1:93.2051 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:1.9401 | MainLoss:1.5968 | SPLoss:3.3676 | CLSLoss:0.6519 | top1:54.4455 | AUROC:0.6934\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.038176\n",
      "Train | 16/16 | Loss:0.4410 | MainLoss:0.1077 | SPLoss:3.2678 | CLSLoss:0.6563 | top1:95.9467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5262 | MainLoss:0.2053 | SPLoss:3.1430 | CLSLoss:0.6598 | top1:92.3974 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:1.8577 | MainLoss:1.5368 | SPLoss:3.1430 | CLSLoss:0.6598 | top1:57.7850 | AUROC:0.7357\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.038150\n",
      "Train | 16/16 | Loss:0.4181 | MainLoss:0.1029 | SPLoss:3.0862 | CLSLoss:0.6614 | top1:96.2133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4760 | MainLoss:0.1702 | SPLoss:2.9912 | CLSLoss:0.6608 | top1:93.6026 | AUROC:0.9835\n",
      "Test | 129/16 | Loss:2.2014 | MainLoss:1.8957 | SPLoss:2.9912 | CLSLoss:0.6608 | top1:53.7414 | AUROC:0.7081\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.038123\n",
      "Train | 16/16 | Loss:0.4050 | MainLoss:0.1057 | SPLoss:2.9272 | CLSLoss:0.6623 | top1:96.0267 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4654 | MainLoss:0.1750 | SPLoss:2.8376 | CLSLoss:0.6636 | top1:93.6923 | AUROC:0.9837\n",
      "Test | 129/16 | Loss:2.3399 | MainLoss:2.0495 | SPLoss:2.8376 | CLSLoss:0.6636 | top1:53.1028 | AUROC:0.7307\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.038097\n",
      "Train | 16/16 | Loss:0.3693 | MainLoss:0.0861 | SPLoss:2.7651 | CLSLoss:0.6645 | top1:97.0933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4400 | MainLoss:0.1655 | SPLoss:2.6780 | CLSLoss:0.6664 | top1:94.1795 | AUROC:0.9847\n",
      "Test | 129/16 | Loss:2.1689 | MainLoss:1.8944 | SPLoss:2.6780 | CLSLoss:0.6664 | top1:54.8723 | AUROC:0.7442\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.038070\n",
      "Train | 16/16 | Loss:0.3447 | MainLoss:0.0769 | SPLoss:2.6109 | CLSLoss:0.6688 | top1:97.1467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4498 | MainLoss:0.1901 | SPLoss:2.5303 | CLSLoss:0.6703 | top1:93.4744 | AUROC:0.9835\n",
      "Test | 129/16 | Loss:2.4866 | MainLoss:2.2268 | SPLoss:2.5303 | CLSLoss:0.6703 | top1:52.9502 | AUROC:0.7246\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.038043\n",
      "Train | 16/16 | Loss:0.3500 | MainLoss:0.0951 | SPLoss:2.4827 | CLSLoss:0.6667 | top1:96.7467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4289 | MainLoss:0.1790 | SPLoss:2.4330 | CLSLoss:0.6673 | top1:93.7308 | AUROC:0.9827\n",
      "Test | 129/16 | Loss:2.2339 | MainLoss:1.9839 | SPLoss:2.4330 | CLSLoss:0.6673 | top1:53.9875 | AUROC:0.7206\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.038015\n",
      "Train | 16/16 | Loss:0.3197 | MainLoss:0.0760 | SPLoss:2.3696 | CLSLoss:0.6696 | top1:97.4133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4227 | MainLoss:0.1849 | SPLoss:2.3105 | CLSLoss:0.6704 | top1:93.7051 | AUROC:0.9826\n",
      "Test | 129/16 | Loss:2.2005 | MainLoss:1.9628 | SPLoss:2.3105 | CLSLoss:0.6704 | top1:54.9813 | AUROC:0.7361\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.037988\n",
      "Train | 16/16 | Loss:0.3299 | MainLoss:0.0941 | SPLoss:2.2912 | CLSLoss:0.6685 | top1:96.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4103 | MainLoss:0.1796 | SPLoss:2.2402 | CLSLoss:0.6684 | top1:93.4359 | AUROC:0.9826\n",
      "Test | 129/16 | Loss:2.2769 | MainLoss:2.0462 | SPLoss:2.2402 | CLSLoss:0.6684 | top1:53.7041 | AUROC:0.7326\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.037961\n",
      "Train | 16/16 | Loss:0.3236 | MainLoss:0.0946 | SPLoss:2.2229 | CLSLoss:0.6695 | top1:96.6933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.3891 | MainLoss:0.1624 | SPLoss:2.2013 | CLSLoss:0.6646 | top1:94.0256 | AUROC:0.9846\n",
      "Test | 129/16 | Loss:2.0535 | MainLoss:1.8267 | SPLoss:2.2013 | CLSLoss:0.6646 | top1:54.6262 | AUROC:0.7185\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.037933\n",
      "Train | 16/16 | Loss:0.3041 | MainLoss:0.0820 | SPLoss:2.1548 | CLSLoss:0.6678 | top1:96.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.3816 | MainLoss:0.1640 | SPLoss:2.1093 | CLSLoss:0.6706 | top1:94.0641 | AUROC:0.9852\n",
      "Test | 129/16 | Loss:2.1065 | MainLoss:1.8889 | SPLoss:2.1093 | CLSLoss:0.6706 | top1:55.5545 | AUROC:0.7535\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.037905\n",
      "Train | 16/16 | Loss:0.3022 | MainLoss:0.0865 | SPLoss:2.0894 | CLSLoss:0.6706 | top1:96.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.3815 | MainLoss:0.1678 | SPLoss:2.0699 | CLSLoss:0.6709 | top1:93.7821 | AUROC:0.9848\n",
      "Test | 129/16 | Loss:2.1225 | MainLoss:1.9088 | SPLoss:2.0699 | CLSLoss:0.6709 | top1:55.1589 | AUROC:0.7430\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.037877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.3047 | MainLoss:0.0945 | SPLoss:2.0343 | CLSLoss:0.6707 | top1:96.9600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.3883 | MainLoss:0.1826 | SPLoss:1.9897 | CLSLoss:0.6681 | top1:93.0897 | AUROC:0.9844\n",
      "Test | 129/16 | Loss:1.8926 | MainLoss:1.6870 | SPLoss:1.9897 | CLSLoss:0.6680 | top1:57.2523 | AUROC:0.7628\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.037849\n",
      "Train | 16/16 | Loss:0.3145 | MainLoss:0.1098 | SPLoss:1.9799 | CLSLoss:0.6678 | top1:95.8133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.3720 | MainLoss:0.1637 | SPLoss:2.0163 | CLSLoss:0.6612 | top1:93.8462 | AUROC:0.9845\n",
      "Test | 129/16 | Loss:1.9982 | MainLoss:1.7900 | SPLoss:2.0163 | CLSLoss:0.6612 | top1:54.9564 | AUROC:0.7283\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.037820\n",
      "Train | 16/16 | Loss:0.2901 | MainLoss:0.0856 | SPLoss:1.9782 | CLSLoss:0.6639 | top1:96.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.3821 | MainLoss:0.1825 | SPLoss:1.9290 | CLSLoss:0.6670 | top1:93.4487 | AUROC:0.9857\n",
      "Test | 129/16 | Loss:1.8892 | MainLoss:1.6896 | SPLoss:1.9290 | CLSLoss:0.6670 | top1:57.7321 | AUROC:0.7674\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.037792\n",
      "Train | 16/16 | Loss:0.3150 | MainLoss:0.1097 | SPLoss:1.9871 | CLSLoss:0.6646 | top1:96.2133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4282 | MainLoss:0.1651 | SPLoss:2.5656 | CLSLoss:0.6580 | top1:93.7949 | AUROC:0.9838\n",
      "Test | 129/16 | Loss:2.0772 | MainLoss:1.8141 | SPLoss:2.5656 | CLSLoss:0.6580 | top1:54.5826 | AUROC:0.7312\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.037763\n",
      "Train | 16/16 | Loss:0.5042 | MainLoss:0.1196 | SPLoss:3.7797 | CLSLoss:0.6599 | top1:95.7867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4980 | MainLoss:0.1811 | SPLoss:13.1031 | CLSLoss:0.6509 | top1:92.8077 | AUROC:0.9799\n",
      "Test | 129/16 | Loss:3.0581 | MainLoss:1.7413 | SPLoss:13.1031 | CLSLoss:0.6509 | top1:53.7757 | AUROC:0.6809\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.037734\n",
      "Train | 16/16 | Loss:1.4303 | MainLoss:0.1117 | SPLoss:13.1207 | CLSLoss:0.6544 | top1:95.6800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4271 | MainLoss:0.1781 | SPLoss:12.4248 | CLSLoss:0.6582 | top1:93.0000 | AUROC:0.9827\n",
      "Test | 129/16 | Loss:3.1097 | MainLoss:1.8607 | SPLoss:12.4248 | CLSLoss:0.6582 | top1:54.4268 | AUROC:0.6844\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.037705\n",
      "Train | 16/16 | Loss:1.2976 | MainLoss:0.1060 | SPLoss:11.8505 | CLSLoss:0.6579 | top1:95.8667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.3078 | MainLoss:0.1763 | SPLoss:11.2489 | CLSLoss:0.6602 | top1:93.4615 | AUROC:0.9834\n",
      "Test | 129/16 | Loss:2.9874 | MainLoss:1.8559 | SPLoss:11.2489 | CLSLoss:0.6602 | top1:54.9626 | AUROC:0.7192\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.037675\n",
      "Train | 16/16 | Loss:1.4250 | MainLoss:0.2886 | SPLoss:11.2995 | CLSLoss:0.6400 | top1:90.5067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4619 | MainLoss:0.2587 | SPLoss:11.9712 | CLSLoss:0.6030 | top1:89.0128 | AUROC:0.9638\n",
      "Test | 129/16 | Loss:2.1027 | MainLoss:0.8996 | SPLoss:11.9711 | CLSLoss:0.6030 | top1:60.7445 | AUROC:0.7439\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.037646\n",
      "Train | 16/16 | Loss:1.3219 | MainLoss:0.1766 | SPLoss:11.3912 | CLSLoss:0.6168 | top1:93.2533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.3033 | MainLoss:0.2211 | SPLoss:10.7592 | CLSLoss:0.6258 | top1:91.1795 | AUROC:0.9778\n",
      "Test | 129/16 | Loss:2.2380 | MainLoss:1.1558 | SPLoss:10.7592 | CLSLoss:0.6258 | top1:61.8505 | AUROC:0.8009\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.037616\n",
      "Train | 16/16 | Loss:1.1604 | MainLoss:0.1301 | SPLoss:10.2399 | CLSLoss:0.6302 | top1:94.9067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.1500 | MainLoss:0.1750 | SPLoss:9.6860 | CLSLoss:0.6327 | top1:93.2949 | AUROC:0.9814\n",
      "Test | 129/16 | Loss:2.4703 | MainLoss:1.4954 | SPLoss:9.6860 | CLSLoss:0.6327 | top1:57.4891 | AUROC:0.7758\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.037586\n",
      "Train | 16/16 | Loss:1.0500 | MainLoss:0.1194 | SPLoss:9.2424 | CLSLoss:0.6351 | top1:95.5467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.0715 | MainLoss:0.1892 | SPLoss:8.7595 | CLSLoss:0.6400 | top1:92.7692 | AUROC:0.9828\n",
      "Test | 129/16 | Loss:2.3374 | MainLoss:1.4550 | SPLoss:8.7594 | CLSLoss:0.6400 | top1:59.0997 | AUROC:0.7785\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.037556\n",
      "Train | 16/16 | Loss:2.9591 | MainLoss:0.1325 | SPLoss:28.2019 | CLSLoss:0.6393 | top1:94.9600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.9614 | MainLoss:0.2285 | SPLoss:47.2652 | CLSLoss:0.6372 | top1:91.4103 | AUROC:0.9729\n",
      "Test | 129/16 | Loss:5.8132 | MainLoss:1.0803 | SPLoss:47.2652 | CLSLoss:0.6372 | top1:65.0717 | AUROC:0.8551\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.037526\n",
      "Train | 16/16 | Loss:4.6645 | MainLoss:0.1578 | SPLoss:45.0037 | CLSLoss:0.6342 | top1:93.8667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:4.4573 | MainLoss:0.2141 | SPLoss:42.3688 | CLSLoss:0.6343 | top1:91.8974 | AUROC:0.9778\n",
      "Test | 129/16 | Loss:5.5112 | MainLoss:1.2680 | SPLoss:42.3689 | CLSLoss:0.6343 | top1:61.2399 | AUROC:0.7998\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.037496\n",
      "Train | 16/16 | Loss:4.1597 | MainLoss:0.1424 | SPLoss:40.1098 | CLSLoss:0.6336 | top1:94.4000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.9502 | MainLoss:0.1821 | SPLoss:37.6175 | CLSLoss:0.6328 | top1:92.9744 | AUROC:0.9806\n",
      "Test | 129/16 | Loss:5.2914 | MainLoss:1.5234 | SPLoss:37.6175 | CLSLoss:0.6328 | top1:57.1402 | AUROC:0.7825\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.037465\n",
      "Train | 16/16 | Loss:3.6910 | MainLoss:0.1240 | SPLoss:35.6070 | CLSLoss:0.6319 | top1:95.1733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.5855 | MainLoss:0.2400 | SPLoss:33.3913 | CLSLoss:0.6332 | top1:91.0897 | AUROC:0.9812\n",
      "Test | 129/16 | Loss:4.4700 | MainLoss:1.1246 | SPLoss:33.3913 | CLSLoss:0.6332 | top1:63.7882 | AUROC:0.8093\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.037435\n",
      "Train | 16/16 | Loss:3.2843 | MainLoss:0.1158 | SPLoss:31.6221 | CLSLoss:0.6357 | top1:95.8133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.1423 | MainLoss:0.1670 | SPLoss:29.6896 | CLSLoss:0.6346 | top1:93.6154 | AUROC:0.9836\n",
      "Test | 129/16 | Loss:4.4356 | MainLoss:1.4603 | SPLoss:29.6896 | CLSLoss:0.6346 | top1:58.8816 | AUROC:0.8147\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.037404\n",
      "Train | 16/16 | Loss:2.9314 | MainLoss:0.1116 | SPLoss:28.1345 | CLSLoss:0.6360 | top1:95.9467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.8319 | MainLoss:0.1752 | SPLoss:26.5039 | CLSLoss:0.6346 | top1:93.3462 | AUROC:0.9831\n",
      "Test | 129/16 | Loss:4.0892 | MainLoss:1.4325 | SPLoss:26.5039 | CLSLoss:0.6346 | top1:59.4860 | AUROC:0.8104\n",
      "\n",
      "Epoch: [177 | 1000] LR: 0.037373\n",
      "Train | 16/16 | Loss:2.6140 | MainLoss:0.0964 | SPLoss:25.1127 | CLSLoss:0.6384 | top1:96.4800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.5511 | MainLoss:0.1856 | SPLoss:23.5916 | CLSLoss:0.6391 | top1:93.1026 | AUROC:0.9822\n",
      "Test | 129/16 | Loss:3.8727 | MainLoss:1.5071 | SPLoss:23.5915 | CLSLoss:0.6391 | top1:59.3022 | AUROC:0.7932\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.037341\n",
      "Train | 16/16 | Loss:2.7328 | MainLoss:0.0888 | SPLoss:26.3753 | CLSLoss:0.6414 | top1:96.6133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:3.1646 | MainLoss:0.1958 | SPLoss:29.6238 | CLSLoss:0.6418 | top1:92.8846 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:4.4241 | MainLoss:1.4553 | SPLoss:29.6238 | CLSLoss:0.6418 | top1:60.7788 | AUROC:0.8585\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.037310\n",
      "Train | 16/16 | Loss:2.9225 | MainLoss:0.1088 | SPLoss:28.0729 | CLSLoss:0.6412 | top1:95.9200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.8134 | MainLoss:0.1700 | SPLoss:26.3701 | CLSLoss:0.6394 | top1:93.5513 | AUROC:0.9833\n",
      "Test | 129/16 | Loss:4.2153 | MainLoss:1.5718 | SPLoss:26.3702 | CLSLoss:0.6394 | top1:58.0561 | AUROC:0.8121\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.037278\n",
      "Train | 16/16 | Loss:2.6018 | MainLoss:0.0961 | SPLoss:24.9929 | CLSLoss:0.6407 | top1:96.5333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.5339 | MainLoss:0.1791 | SPLoss:23.4836 | CLSLoss:0.6406 | top1:93.1538 | AUROC:0.9836\n",
      "Test | 129/16 | Loss:3.8532 | MainLoss:1.4984 | SPLoss:23.4835 | CLSLoss:0.6406 | top1:59.5171 | AUROC:0.8103\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.037247\n",
      "Train | 16/16 | Loss:2.3123 | MainLoss:0.0816 | SPLoss:22.2432 | CLSLoss:0.6447 | top1:96.8800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.3073 | MainLoss:0.2109 | SPLoss:20.8989 | CLSLoss:0.6433 | top1:92.2308 | AUROC:0.9825\n",
      "Test | 129/16 | Loss:3.3660 | MainLoss:1.2697 | SPLoss:20.8989 | CLSLoss:0.6433 | top1:63.7975 | AUROC:0.8613\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.037215\n",
      "Train | 16/16 | Loss:2.0866 | MainLoss:0.0965 | SPLoss:19.8368 | CLSLoss:0.6437 | top1:96.5333 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:2.0672 | MainLoss:0.1930 | SPLoss:18.6776 | CLSLoss:0.6434 | top1:92.9359 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:3.4203 | MainLoss:1.5461 | SPLoss:18.6775 | CLSLoss:0.6434 | top1:59.6822 | AUROC:0.8066\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.037183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.8576 | MainLoss:0.0790 | SPLoss:17.7212 | CLSLoss:0.6457 | top1:97.2000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.8634 | MainLoss:0.1887 | SPLoss:16.6820 | CLSLoss:0.6497 | top1:93.1795 | AUROC:0.9816\n",
      "Test | 129/16 | Loss:3.3751 | MainLoss:1.7004 | SPLoss:16.6820 | CLSLoss:0.6497 | top1:58.6386 | AUROC:0.8277\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.037151\n",
      "Train | 16/16 | Loss:1.7046 | MainLoss:0.1131 | SPLoss:15.8503 | CLSLoss:0.6466 | top1:95.3067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.6813 | MainLoss:0.1791 | SPLoss:14.9574 | CLSLoss:0.6454 | top1:93.2949 | AUROC:0.9816\n",
      "Test | 129/16 | Loss:3.0606 | MainLoss:1.5584 | SPLoss:14.9574 | CLSLoss:0.6454 | top1:59.0000 | AUROC:0.8348\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.037118\n",
      "Train | 16/16 | Loss:1.5207 | MainLoss:0.0917 | SPLoss:14.2253 | CLSLoss:0.6490 | top1:96.9600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.5393 | MainLoss:0.1897 | SPLoss:13.4315 | CLSLoss:0.6469 | top1:93.0256 | AUROC:0.9810\n",
      "Test | 129/16 | Loss:2.8569 | MainLoss:1.5073 | SPLoss:13.4315 | CLSLoss:0.6469 | top1:60.1495 | AUROC:0.8233\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.037086\n",
      "Train | 16/16 | Loss:1.3733 | MainLoss:0.0916 | SPLoss:12.7527 | CLSLoss:0.6467 | top1:96.9067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.4034 | MainLoss:0.1945 | SPLoss:12.0242 | CLSLoss:0.6447 | top1:92.7308 | AUROC:0.9816\n",
      "Test | 129/16 | Loss:2.6535 | MainLoss:1.4446 | SPLoss:12.0242 | CLSLoss:0.6447 | top1:60.8442 | AUROC:0.8230\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.037053\n",
      "Train | 16/16 | Loss:1.2480 | MainLoss:0.0946 | SPLoss:11.4702 | CLSLoss:0.6460 | top1:96.5867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.2739 | MainLoss:0.1809 | SPLoss:10.8654 | CLSLoss:0.6449 | top1:93.3846 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:2.7565 | MainLoss:1.6635 | SPLoss:10.8654 | CLSLoss:0.6449 | top1:57.8754 | AUROC:0.8082\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.037020\n",
      "Train | 16/16 | Loss:1.1252 | MainLoss:0.0831 | SPLoss:10.3562 | CLSLoss:0.6472 | top1:96.9867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.1909 | MainLoss:0.2037 | SPLoss:9.8073 | CLSLoss:0.6485 | top1:93.0256 | AUROC:0.9814\n",
      "Test | 129/16 | Loss:2.5076 | MainLoss:1.5203 | SPLoss:9.8073 | CLSLoss:0.6485 | top1:60.6417 | AUROC:0.8225\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.036987\n",
      "Train | 16/16 | Loss:1.0264 | MainLoss:0.0863 | SPLoss:9.3356 | CLSLoss:0.6481 | top1:97.0133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:1.0853 | MainLoss:0.1956 | SPLoss:8.8328 | CLSLoss:0.6449 | top1:93.2051 | AUROC:0.9813\n",
      "Test | 129/16 | Loss:2.4043 | MainLoss:1.5145 | SPLoss:8.8328 | CLSLoss:0.6449 | top1:60.6012 | AUROC:0.8479\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.036954\n",
      "Train | 16/16 | Loss:0.9296 | MainLoss:0.0784 | SPLoss:8.4475 | CLSLoss:0.6456 | top1:97.2000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9951 | MainLoss:0.1883 | SPLoss:8.0036 | CLSLoss:0.6471 | top1:93.5513 | AUROC:0.9818\n",
      "Test | 129/16 | Loss:2.4766 | MainLoss:1.6697 | SPLoss:8.0036 | CLSLoss:0.6471 | top1:58.3240 | AUROC:0.8184\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.036920\n",
      "Train | 16/16 | Loss:0.8592 | MainLoss:0.0871 | SPLoss:7.6557 | CLSLoss:0.6473 | top1:96.9600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.9163 | MainLoss:0.1831 | SPLoss:7.2670 | CLSLoss:0.6487 | top1:93.6410 | AUROC:0.9822\n",
      "Test | 129/16 | Loss:2.4691 | MainLoss:1.7359 | SPLoss:7.2670 | CLSLoss:0.6487 | top1:57.7414 | AUROC:0.8166\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.036887\n",
      "Train | 16/16 | Loss:0.7845 | MainLoss:0.0822 | SPLoss:6.9574 | CLSLoss:0.6492 | top1:97.0133 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.8662 | MainLoss:0.1983 | SPLoss:6.6145 | CLSLoss:0.6478 | top1:93.0513 | AUROC:0.9812\n",
      "Test | 129/16 | Loss:2.2273 | MainLoss:1.5594 | SPLoss:6.6145 | CLSLoss:0.6478 | top1:59.9128 | AUROC:0.8386\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.036853\n",
      "Train | 16/16 | Loss:0.7507 | MainLoss:0.1077 | SPLoss:6.3653 | CLSLoss:0.6465 | top1:96.0000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7987 | MainLoss:0.1837 | SPLoss:6.0854 | CLSLoss:0.6437 | top1:93.2436 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:2.1422 | MainLoss:1.5272 | SPLoss:6.0854 | CLSLoss:0.6437 | top1:58.3645 | AUROC:0.8223\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.036819\n",
      "Train | 16/16 | Loss:0.6807 | MainLoss:0.0915 | SPLoss:5.8277 | CLSLoss:0.6458 | top1:96.7200 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7641 | MainLoss:0.2014 | SPLoss:5.5619 | CLSLoss:0.6469 | top1:92.8205 | AUROC:0.9810\n",
      "Test | 129/16 | Loss:2.1011 | MainLoss:1.5384 | SPLoss:5.5619 | CLSLoss:0.6469 | top1:60.0000 | AUROC:0.8424\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.036785\n",
      "Train | 16/16 | Loss:0.6347 | MainLoss:0.0945 | SPLoss:5.3381 | CLSLoss:0.6444 | top1:96.6667 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.7115 | MainLoss:0.1954 | SPLoss:5.0970 | CLSLoss:0.6445 | top1:92.8205 | AUROC:0.9819\n",
      "Test | 129/16 | Loss:2.0053 | MainLoss:1.4892 | SPLoss:5.0970 | CLSLoss:0.6445 | top1:60.7227 | AUROC:0.8402\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.036751\n",
      "Train | 16/16 | Loss:0.6088 | MainLoss:0.1104 | SPLoss:4.9200 | CLSLoss:0.6416 | top1:96.0533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6709 | MainLoss:0.1930 | SPLoss:4.7158 | CLSLoss:0.6393 | top1:92.8718 | AUROC:0.9821\n",
      "Test | 129/16 | Loss:2.3487 | MainLoss:1.8707 | SPLoss:4.7158 | CLSLoss:0.6393 | top1:55.0312 | AUROC:0.8189\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.036716\n",
      "Train | 16/16 | Loss:0.5409 | MainLoss:0.0787 | SPLoss:4.5576 | CLSLoss:0.6431 | top1:97.2533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6321 | MainLoss:0.1889 | SPLoss:4.3670 | CLSLoss:0.6475 | top1:93.2692 | AUROC:0.9824\n",
      "Test | 129/16 | Loss:2.1675 | MainLoss:1.7243 | SPLoss:4.3671 | CLSLoss:0.6475 | top1:58.3676 | AUROC:0.8186\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.036682\n",
      "Train | 16/16 | Loss:0.5146 | MainLoss:0.0861 | SPLoss:4.2200 | CLSLoss:0.6478 | top1:96.8800 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6137 | MainLoss:0.1985 | SPLoss:4.0870 | CLSLoss:0.6455 | top1:92.7308 | AUROC:0.9819\n",
      "Test | 129/16 | Loss:2.0671 | MainLoss:1.6519 | SPLoss:4.0871 | CLSLoss:0.6455 | top1:58.1745 | AUROC:0.8114\n",
      "\n",
      "Epoch: [199 | 1000] LR: 0.036647\n",
      "Train | 16/16 | Loss:0.4927 | MainLoss:0.0917 | SPLoss:3.9459 | CLSLoss:0.6459 | top1:96.6400 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6028 | MainLoss:0.2162 | SPLoss:3.8012 | CLSLoss:0.6486 | top1:92.2436 | AUROC:0.9813\n",
      "Test | 129/16 | Loss:1.8315 | MainLoss:1.4449 | SPLoss:3.8012 | CLSLoss:0.6486 | top1:61.2710 | AUROC:0.8230\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.036612\n",
      "Train | 16/16 | Loss:0.4562 | MainLoss:0.0794 | SPLoss:3.7028 | CLSLoss:0.6513 | top1:97.0933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5661 | MainLoss:0.1981 | SPLoss:3.6143 | CLSLoss:0.6516 | top1:93.1667 | AUROC:0.9809\n",
      "Test | 129/16 | Loss:2.1373 | MainLoss:1.7694 | SPLoss:3.6143 | CLSLoss:0.6516 | top1:57.9470 | AUROC:0.8100\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.036577\n",
      "Train | 16/16 | Loss:0.4434 | MainLoss:0.0851 | SPLoss:3.5186 | CLSLoss:0.6518 | top1:96.9600 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5364 | MainLoss:0.1898 | SPLoss:3.4013 | CLSLoss:0.6497 | top1:93.1538 | AUROC:0.9810\n",
      "Test | 129/16 | Loss:2.1729 | MainLoss:1.8263 | SPLoss:3.4013 | CLSLoss:0.6497 | top1:56.4517 | AUROC:0.8197\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.036542\n",
      "Train | 16/16 | Loss:0.4214 | MainLoss:0.0844 | SPLoss:3.3047 | CLSLoss:0.6491 | top1:97.1467 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5141 | MainLoss:0.1884 | SPLoss:3.1919 | CLSLoss:0.6514 | top1:93.3462 | AUROC:0.9817\n",
      "Test | 129/16 | Loss:2.1927 | MainLoss:1.8670 | SPLoss:3.1919 | CLSLoss:0.6514 | top1:56.0748 | AUROC:0.8290\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.036506\n",
      "Train | 16/16 | Loss:0.3946 | MainLoss:0.0774 | SPLoss:3.1072 | CLSLoss:0.6520 | top1:97.6000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4988 | MainLoss:0.1914 | SPLoss:3.0090 | CLSLoss:0.6511 | top1:93.3846 | AUROC:0.9822\n",
      "Test | 129/16 | Loss:1.9744 | MainLoss:1.6670 | SPLoss:3.0090 | CLSLoss:0.6511 | top1:58.9065 | AUROC:0.8421\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.036471\n",
      "Train | 16/16 | Loss:0.3875 | MainLoss:0.0867 | SPLoss:2.9433 | CLSLoss:0.6488 | top1:96.9067 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5042 | MainLoss:0.2136 | SPLoss:2.8412 | CLSLoss:0.6499 | top1:92.4615 | AUROC:0.9812\n",
      "Test | 129/16 | Loss:2.3426 | MainLoss:2.0520 | SPLoss:2.8412 | CLSLoss:0.6499 | top1:54.4143 | AUROC:0.8421\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.036435\n",
      "Train | 16/16 | Loss:0.3638 | MainLoss:0.0754 | SPLoss:2.8194 | CLSLoss:0.6516 | top1:97.2000 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.4985 | MainLoss:0.2109 | SPLoss:2.8104 | CLSLoss:0.6554 | top1:92.7051 | AUROC:0.9810\n",
      "Test | 129/16 | Loss:1.9067 | MainLoss:1.6191 | SPLoss:2.8104 | CLSLoss:0.6554 | top1:60.4050 | AUROC:0.8341\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.036399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.4561 | MainLoss:0.0946 | SPLoss:3.5490 | CLSLoss:0.6539 | top1:96.7733 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.6619 | MainLoss:0.2245 | SPLoss:4.3086 | CLSLoss:0.6530 | top1:92.0256 | AUROC:0.9761\n",
      "Test | 129/16 | Loss:1.8753 | MainLoss:1.4379 | SPLoss:4.3086 | CLSLoss:0.6530 | top1:61.3489 | AUROC:0.8716\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.036363\n",
      "Train | 16/16 | Loss:0.5646 | MainLoss:0.1392 | SPLoss:4.1893 | CLSLoss:0.6500 | top1:94.2933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5885 | MainLoss:0.1762 | SPLoss:4.0583 | CLSLoss:0.6421 | top1:93.0641 | AUROC:0.9812\n",
      "Test | 129/16 | Loss:1.8546 | MainLoss:1.4423 | SPLoss:4.0583 | CLSLoss:0.6421 | top1:57.4424 | AUROC:0.8257\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.036327\n",
      "Train | 16/16 | Loss:0.4960 | MainLoss:0.0966 | SPLoss:3.9288 | CLSLoss:0.6478 | top1:96.6933 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5710 | MainLoss:0.1830 | SPLoss:3.8152 | CLSLoss:0.6470 | top1:93.2308 | AUROC:0.9819\n",
      "Test | 129/16 | Loss:2.0762 | MainLoss:1.6882 | SPLoss:3.8152 | CLSLoss:0.6470 | top1:57.2025 | AUROC:0.8188\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.036290\n",
      "Train | 16/16 | Loss:0.4655 | MainLoss:0.0888 | SPLoss:3.7014 | CLSLoss:0.6486 | top1:96.8533 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5616 | MainLoss:0.1991 | SPLoss:3.5596 | CLSLoss:0.6519 | top1:93.1282 | AUROC:0.9816\n",
      "Test | 129/16 | Loss:2.0274 | MainLoss:1.6649 | SPLoss:3.5596 | CLSLoss:0.6519 | top1:58.9595 | AUROC:0.8295\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.036254\n",
      "Train | 16/16 | Loss:0.4471 | MainLoss:0.0837 | SPLoss:3.5696 | CLSLoss:0.6529 | top1:96.5867 | AUROC:0.0000\n",
      "Test | 32/16 | Loss:0.5530 | MainLoss:0.1899 | SPLoss:3.5659 | CLSLoss:0.6515 | top1:93.2949 | AUROC:0.9833\n",
      "Test | 129/16 | Loss:1.9092 | MainLoss:1.5461 | SPLoss:3.5659 | CLSLoss:0.6515 | top1:60.0530 | AUROC:0.8275\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.036217\n",
      "Train | 16/16 | Loss:0.4432 | MainLoss:0.0913 | SPLoss:3.4539 | CLSLoss:0.6511 | top1:96.7733 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
