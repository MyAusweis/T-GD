{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = './log/pggan/128x128/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "start_epoch = 0\n",
    "train_batch = 256\n",
    "test_batch = 200\n",
    "lr = 0.1\n",
    "schedule = [70, 150]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128x128' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 4\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes, override_params={'dropout_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=(3,64,64), device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [31 | 200] LR: 0.782317\n",
      "1/502 Data:2.028 | Batch:5.241 | Total:0:00:05 | ETA:0:43:46 | Loss:0.41410720348358154 | top1:80.46875\n",
      "11/502 Data:0.004 | Batch:0.692 | Total:0:00:10 | ETA:0:07:47 | Loss:0.4151278950951316 | top1:81.28551483154297\n",
      "21/502 Data:0.001 | Batch:0.507 | Total:0:00:14 | ETA:0:03:19 | Loss:0.4172373243740627 | top1:80.78497314453125\n",
      "31/502 Data:0.001 | Batch:0.453 | Total:0:00:19 | ETA:0:03:42 | Loss:0.41744823609628984 | top1:80.83416748046875\n",
      "41/502 Data:0.001 | Batch:0.355 | Total:0:00:23 | ETA:0:03:37 | Loss:0.4185655516822164 | top1:80.69740295410156\n",
      "51/502 Data:0.001 | Batch:0.483 | Total:0:00:28 | ETA:0:03:33 | Loss:0.41998923993578147 | top1:80.53768920898438\n",
      "61/502 Data:0.003 | Batch:0.425 | Total:0:00:33 | ETA:0:03:41 | Loss:0.42032849984090837 | top1:80.56480407714844\n",
      "71/502 Data:0.001 | Batch:0.350 | Total:0:00:37 | ETA:0:03:17 | Loss:0.4182482449941232 | top1:80.69982147216797\n",
      "81/502 Data:0.002 | Batch:0.411 | Total:0:00:42 | ETA:0:03:08 | Loss:0.4191371066334807 | top1:80.66647338867188\n",
      "91/502 Data:0.001 | Batch:0.341 | Total:0:00:47 | ETA:0:03:08 | Loss:0.4202521250143156 | top1:80.61470031738281\n",
      "101/502 Data:0.001 | Batch:0.416 | Total:0:00:51 | ETA:0:03:01 | Loss:0.4224417026680295 | top1:80.395263671875\n",
      "111/502 Data:0.003 | Batch:0.397 | Total:0:00:56 | ETA:0:03:21 | Loss:0.42239201176273933 | top1:80.42300415039062\n",
      "121/502 Data:0.001 | Batch:0.444 | Total:0:01:01 | ETA:0:03:04 | Loss:0.42393294444754104 | top1:80.26859283447266\n",
      "131/502 Data:0.001 | Batch:0.419 | Total:0:01:06 | ETA:0:03:00 | Loss:0.4232412035228642 | top1:80.32263946533203\n",
      "141/502 Data:0.018 | Batch:0.499 | Total:0:01:11 | ETA:0:03:03 | Loss:0.42348117080140624 | top1:80.33023071289062\n",
      "151/502 Data:0.001 | Batch:0.359 | Total:0:01:16 | ETA:0:03:01 | Loss:0.424303033494002 | top1:80.2669677734375\n",
      "161/502 Data:0.005 | Batch:0.656 | Total:0:01:21 | ETA:0:02:56 | Loss:0.4247264271567327 | top1:80.25524139404297\n",
      "171/502 Data:0.003 | Batch:0.389 | Total:0:01:26 | ETA:0:03:07 | Loss:0.42652810822453413 | top1:80.1260986328125\n",
      "181/502 Data:0.009 | Batch:0.456 | Total:0:01:31 | ETA:0:02:34 | Loss:0.42656746819533037 | top1:80.10618591308594\n",
      "191/502 Data:0.001 | Batch:0.320 | Total:0:01:36 | ETA:0:02:26 | Loss:0.42649822069712334 | top1:80.13743591308594\n",
      "201/502 Data:0.001 | Batch:0.448 | Total:0:01:41 | ETA:0:02:25 | Loss:0.425914327007028 | top1:80.20639038085938\n",
      "211/502 Data:0.008 | Batch:0.676 | Total:0:01:46 | ETA:0:02:36 | Loss:0.42586391373268234 | top1:80.22438049316406\n",
      "221/502 Data:0.001 | Batch:0.532 | Total:0:01:51 | ETA:0:02:19 | Loss:0.4258336496299209 | top1:80.19125366210938\n",
      "231/502 Data:0.008 | Batch:0.552 | Total:0:01:56 | ETA:0:02:17 | Loss:0.42519850235480766 | top1:80.25399017333984\n",
      "241/502 Data:0.001 | Batch:0.682 | Total:0:02:01 | ETA:0:01:59 | Loss:0.42612161638825763 | top1:80.17051696777344\n",
      "251/502 Data:0.001 | Batch:0.341 | Total:0:02:05 | ETA:0:02:02 | Loss:0.42597094439415345 | top1:80.19951629638672\n",
      "261/502 Data:0.003 | Batch:0.384 | Total:0:02:10 | ETA:0:01:53 | Loss:0.4260915007399417 | top1:80.1918716430664\n",
      "271/502 Data:0.001 | Batch:0.376 | Total:0:02:15 | ETA:0:01:52 | Loss:0.42579529162262636 | top1:80.22370910644531\n",
      "281/502 Data:0.001 | Batch:0.518 | Total:0:02:19 | ETA:0:01:37 | Loss:0.4258871116672122 | top1:80.22825622558594\n",
      "291/502 Data:0.005 | Batch:0.567 | Total:0:02:24 | ETA:0:01:32 | Loss:0.4258789550192987 | top1:80.22846984863281\n",
      "301/502 Data:0.001 | Batch:0.396 | Total:0:02:28 | ETA:0:01:28 | Loss:0.4256624825967111 | top1:80.23515319824219\n",
      "311/502 Data:0.001 | Batch:0.341 | Total:0:02:32 | ETA:0:01:19 | Loss:0.42549675003508663 | top1:80.2351303100586\n",
      "321/502 Data:0.003 | Batch:0.360 | Total:0:02:36 | ETA:0:01:11 | Loss:0.4252520446836763 | top1:80.25579071044922\n",
      "331/502 Data:0.001 | Batch:0.359 | Total:0:02:39 | ETA:0:01:08 | Loss:0.42473809506597837 | top1:80.28582763671875\n",
      "341/502 Data:0.001 | Batch:0.334 | Total:0:02:43 | ETA:0:00:57 | Loss:0.42483281267703105 | top1:80.2808837890625\n",
      "351/502 Data:0.010 | Batch:0.538 | Total:0:02:48 | ETA:0:01:15 | Loss:0.4247474458142903 | top1:80.2706527709961\n",
      "361/502 Data:0.001 | Batch:0.405 | Total:0:02:53 | ETA:0:01:16 | Loss:0.42429361704974294 | top1:80.30860137939453\n",
      "371/502 Data:0.003 | Batch:0.340 | Total:0:02:57 | ETA:0:01:03 | Loss:0.42384645519552205 | top1:80.33924102783203\n",
      "381/502 Data:0.006 | Batch:0.378 | Total:0:03:02 | ETA:0:00:55 | Loss:0.42367037747475733 | top1:80.34777069091797\n",
      "391/502 Data:0.001 | Batch:0.342 | Total:0:03:07 | ETA:0:00:53 | Loss:0.4231896327279718 | top1:80.37583923339844\n",
      "401/502 Data:0.017 | Batch:0.448 | Total:0:03:11 | ETA:0:00:46 | Loss:0.4232666938382194 | top1:80.3625717163086\n",
      "411/502 Data:0.001 | Batch:0.427 | Total:0:03:16 | ETA:0:00:43 | Loss:0.4232515749995146 | top1:80.37085723876953\n",
      "421/502 Data:0.001 | Batch:0.506 | Total:0:03:21 | ETA:0:00:42 | Loss:0.42296177099266413 | top1:80.40009307861328\n",
      "431/502 Data:0.001 | Batch:0.668 | Total:0:03:26 | ETA:0:00:36 | Loss:0.4226910062566434 | top1:80.4324951171875\n",
      "441/502 Data:0.003 | Batch:0.324 | Total:0:03:31 | ETA:0:00:31 | Loss:0.4226118969538855 | top1:80.4306640625\n",
      "451/502 Data:0.001 | Batch:0.681 | Total:0:03:35 | ETA:0:00:23 | Loss:0.4223285371607 | top1:80.44709777832031\n",
      "461/502 Data:0.001 | Batch:0.521 | Total:0:03:40 | ETA:0:00:18 | Loss:0.42217585722432993 | top1:80.45773315429688\n",
      "471/502 Data:0.001 | Batch:0.429 | Total:0:03:44 | ETA:0:00:15 | Loss:0.421958142405103 | top1:80.4695816040039\n",
      "481/502 Data:0.001 | Batch:0.415 | Total:0:03:49 | ETA:0:00:11 | Loss:0.4220635235433519 | top1:80.45088195800781\n",
      "491/502 Data:0.004 | Batch:0.674 | Total:0:03:55 | ETA:0:00:07 | Loss:0.4218015538456484 | top1:80.46318817138672\n",
      "501/502 Data:0.001 | Batch:0.335 | Total:0:04:00 | ETA:0:00:01 | Loss:0.42176793477016533 | top1:80.46250915527344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 Data:0.000 | Batch:0.221 | Total:0:00:23 | ETA:0:00:00 | Loss:0.1755084312314928 | top1:99.64486694335938\n",
      "\n",
      "Epoch: [32 | 200] LR: 0.170000\n",
      "1/502 Data:1.418 | Batch:2.065 | Total:0:00:02 | ETA:0:17:15 | Loss:0.4396880865097046 | top1:78.90625\n",
      "11/502 Data:0.001 | Batch:0.329 | Total:0:00:06 | ETA:0:05:23 | Loss:0.41881704330444336 | top1:80.71733093261719\n",
      "21/502 Data:0.000 | Batch:0.385 | Total:0:00:11 | ETA:0:03:55 | Loss:0.4172038918449765 | top1:80.72917175292969\n",
      "31/502 Data:0.001 | Batch:0.482 | Total:0:00:16 | ETA:0:03:57 | Loss:0.4149245581319255 | top1:80.78376770019531\n",
      "41/502 Data:0.001 | Batch:0.480 | Total:0:00:22 | ETA:0:04:16 | Loss:0.4143368819864785 | top1:80.72599029541016\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
