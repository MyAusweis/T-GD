{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "import cv2\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/pggan/128/b0/siamese_aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 128\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 125\n",
    "test_batch = 200\n",
    "lr = 0.04\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_style2/1000shot/siamese/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'\n",
    "\n",
    "# iterative training\n",
    "feedback = 0\n",
    "iter_time = []\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "#         img0 = img0.convert(\"L\")\n",
    "#         img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/1000_shot_only')\n",
    "# source_train_dir = os.path.join(target_dir, '100_shot_style1_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_ = SiameseNetworkDataset(datasets.ImageFolder(train_dir), transform=train_aug, should_invert=False)\n",
    "train_loader = DataLoader(train_, shuffle=True, num_workers=num_workers, batch_size=train_batch, pin_memory=True)\n",
    "val_target_ = SiameseNetworkDataset(datasets.ImageFolder(val_target_dir), transform=val_aug, should_invert=False)\n",
    "val_target_loader = DataLoader(val_target_, shuffle=True, num_workers=num_workers, batch_size=test_batch, pin_memory=True)\n",
    "val_source_ = SiameseNetworkDataset(datasets.ImageFolder(val_source_dir), transform=val_aug, should_invert=False)\n",
    "val_source_loader = DataLoader(val_source_, batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/siamese_aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                                      override_params={'dropout_rate':0.0})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                                      override_params={'dropout_rate':0.0, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.17M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in student_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += torch.norm(param, p=1)\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        sp_loss += torch.norm(param - teacher_model_weights[name], p=1)\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=1.0).cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=50, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, source_train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    student_model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs0, inputs1, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs0.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs0 = student_model(inputs0)\n",
    "        outputs1 = student_model(inputs1)\n",
    "        \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            target_index = targets[targets==0]\n",
    "            target_index = target_index.long().cuda()\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs0.size(), lam)\n",
    "            inputs0[target_index, :, bbx1:bbx2, bby1:bby2] = inputs1[target_index, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs0 = teacher_model(inputs0)\n",
    "            teacher_outputs1 = teacher_model(inputs1)\n",
    "            teacher_loss = criterion(teacher_outputs0, teacher_outputs1, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        loss_main = criterion(outputs0, outputs1, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        auroc = roc_auc_score(targets.data.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "        losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs0.size(0))\n",
    "        arc.update(auroc, inputs0.size(0))\n",
    "        cls_losses.update(loss_cls, inputs0.size(0))\n",
    "        sp_losses.update(loss_sp, inputs0.size(0))\n",
    "        top1.update(prec1[0], inputs0.size(0))\n",
    "        alpha.update(sp_alpha, inputs0.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "#         if batch_idx % 10 == 0:\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs0, inputs1, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs0 = model(inputs0)\n",
    "            outputs1 = model(inputs1)\n",
    "            outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "\n",
    "            loss_main = criterion(outputs0, outputs1, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main \n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.data.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "\n",
    "            losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "            top1.update(prec1[0], inputs0.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs0.size(0))\n",
    "            arc.update(auroc, inputs0.size(0))\n",
    "            cls_losses.update(loss_cls, inputs0.size(0))\n",
    "            sp_losses.update(loss_sp, inputs0.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.040000\n",
      "Train | 32/32 | Loss:12733.3155 | MainLoss:0.4638 | Alpha:0.3902 | SPLoss:30367.8281 | CLSLoss:2233.7715 | top1:49.8064 | AUROC:0.5066\n",
      "Test | 39/32 | Loss:0.3678 | MainLoss:0.3678 | SPLoss:7922.8198 | CLSLoss:2355.3589 | top1:50.1538 | AUROC:0.5037\n",
      "Test | 161/32 | Loss:0.0504 | MainLoss:0.0504 | SPLoss:7922.8062 | CLSLoss:2355.3599 | top1:49.6947 | AUROC:0.9993\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.042400\n",
      "Train | 32/32 | Loss:15275.0341 | MainLoss:0.4385 | Alpha:0.4058 | SPLoss:36228.2031 | CLSLoss:1593.0243 | top1:48.8258 | AUROC:0.5185\n",
      "Test | 39/32 | Loss:0.3503 | MainLoss:0.3503 | SPLoss:2914.8291 | CLSLoss:1647.8798 | top1:50.0385 | AUROC:0.5058\n",
      "Test | 161/32 | Loss:0.0065 | MainLoss:0.0065 | SPLoss:2914.8291 | CLSLoss:1647.8799 | top1:50.0187 | AUROC:0.9995\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.044800\n",
      "Train | 32/32 | Loss:16028.6107 | MainLoss:0.4109 | Alpha:0.4124 | SPLoss:36264.5117 | CLSLoss:2527.4231 | top1:49.6516 | AUROC:0.5090\n",
      "Test | 39/32 | Loss:0.3280 | MainLoss:0.3280 | SPLoss:10245.8408 | CLSLoss:2819.8662 | top1:50.7564 | AUROC:0.5008\n",
      "Test | 161/32 | Loss:0.1507 | MainLoss:0.1507 | SPLoss:10245.8555 | CLSLoss:2819.8679 | top1:49.5607 | AUROC:0.9424\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.047200\n",
      "Train | 32/32 | Loss:16330.6562 | MainLoss:0.3930 | Alpha:0.4185 | SPLoss:37589.0078 | CLSLoss:1257.2203 | top1:50.4516 | AUROC:0.4935\n",
      "Test | 39/32 | Loss:0.3015 | MainLoss:0.3015 | SPLoss:18627.5293 | CLSLoss:887.5498 | top1:48.8974 | AUROC:0.5171\n",
      "Test | 161/32 | Loss:0.1781 | MainLoss:0.1781 | SPLoss:18627.5254 | CLSLoss:887.5499 | top1:50.5358 | AUROC:0.8527\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.049600\n",
      "Train | 32/32 | Loss:18280.5929 | MainLoss:0.3791 | Alpha:0.4232 | SPLoss:40128.1562 | CLSLoss:2965.8291 | top1:48.6968 | AUROC:0.5007\n",
      "Test | 39/32 | Loss:0.2964 | MainLoss:0.2964 | SPLoss:11996.1982 | CLSLoss:2972.0010 | top1:50.3846 | AUROC:0.4988\n",
      "Test | 161/32 | Loss:0.2210 | MainLoss:0.2210 | SPLoss:11996.1748 | CLSLoss:2972.0002 | top1:49.8879 | AUROC:0.7659\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.052000\n",
      "Train | 32/32 | Loss:18735.3586 | MainLoss:0.3685 | Alpha:0.4266 | SPLoss:42530.8047 | CLSLoss:1321.5139 | top1:49.2645 | AUROC:0.4984\n",
      "Test | 39/32 | Loss:0.3174 | MainLoss:0.3174 | SPLoss:7911.3213 | CLSLoss:795.9024 | top1:50.9359 | AUROC:0.4882\n",
      "Test | 161/32 | Loss:0.2886 | MainLoss:0.2886 | SPLoss:7911.3398 | CLSLoss:795.9008 | top1:49.8474 | AUROC:0.6270\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.054400\n",
      "Train | 32/32 | Loss:23130.5317 | MainLoss:0.3803 | Alpha:0.4244 | SPLoss:51353.8516 | CLSLoss:3265.5525 | top1:49.4194 | AUROC:0.5112\n",
      "Test | 39/32 | Loss:0.3069 | MainLoss:0.3069 | SPLoss:9639.9033 | CLSLoss:2667.5496 | top1:50.1026 | AUROC:0.4947\n",
      "Test | 161/32 | Loss:0.2073 | MainLoss:0.2073 | SPLoss:9639.9062 | CLSLoss:2667.5483 | top1:50.4517 | AUROC:0.7721\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.056800\n",
      "Train | 32/32 | Loss:21349.3616 | MainLoss:0.4076 | Alpha:0.4222 | SPLoss:46972.9492 | CLSLoss:3507.8494 | top1:47.4839 | AUROC:0.5095\n",
      "Test | 39/32 | Loss:0.3005 | MainLoss:0.3005 | SPLoss:14863.8486 | CLSLoss:1886.1735 | top1:50.3462 | AUROC:0.4999\n",
      "Test | 161/32 | Loss:0.2060 | MainLoss:0.2060 | SPLoss:14863.8438 | CLSLoss:1886.1755 | top1:49.7259 | AUROC:0.7756\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.059200\n",
      "Train | 32/32 | Loss:22511.6258 | MainLoss:0.4040 | Alpha:0.4252 | SPLoss:49498.1875 | CLSLoss:3424.1682 | top1:49.0839 | AUROC:0.5049\n",
      "Test | 39/32 | Loss:0.2997 | MainLoss:0.2997 | SPLoss:6856.3213 | CLSLoss:1816.4863 | top1:49.7564 | AUROC:0.5012\n",
      "Test | 161/32 | Loss:0.2057 | MainLoss:0.2057 | SPLoss:6856.3208 | CLSLoss:1816.4847 | top1:50.0343 | AUROC:0.7804\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.061600\n",
      "Train | 32/32 | Loss:23026.2626 | MainLoss:0.3982 | Alpha:0.4250 | SPLoss:50216.2695 | CLSLoss:3833.6255 | top1:50.2194 | AUROC:0.5077\n",
      "Test | 39/32 | Loss:0.3174 | MainLoss:0.3174 | SPLoss:9935.6631 | CLSLoss:2187.8013 | top1:50.6923 | AUROC:0.4985\n",
      "Test | 161/32 | Loss:0.2867 | MainLoss:0.2867 | SPLoss:9935.6768 | CLSLoss:2187.8008 | top1:49.9128 | AUROC:0.6897\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.064000\n",
      "Train | 32/32 | Loss:23763.7423 | MainLoss:0.4082 | Alpha:0.4204 | SPLoss:53109.8047 | CLSLoss:3527.1191 | top1:49.2129 | AUROC:0.5145\n",
      "Test | 39/32 | Loss:0.3541 | MainLoss:0.3541 | SPLoss:4866.1885 | CLSLoss:1742.2986 | top1:50.0641 | AUROC:0.5056\n",
      "Test | 161/32 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:4866.1885 | CLSLoss:1742.2997 | top1:49.7041 | AUROC:0.6175\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.066400\n",
      "Train | 32/32 | Loss:23941.0961 | MainLoss:0.3905 | Alpha:0.4138 | SPLoss:53641.2109 | CLSLoss:3946.5623 | top1:49.6516 | AUROC:0.5039\n",
      "Test | 39/32 | Loss:0.3074 | MainLoss:0.3074 | SPLoss:20066.7734 | CLSLoss:4284.5405 | top1:49.3974 | AUROC:0.5108\n",
      "Test | 161/32 | Loss:0.2832 | MainLoss:0.2832 | SPLoss:20066.7734 | CLSLoss:4284.5498 | top1:50.2025 | AUROC:0.5648\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.068800\n",
      "Train | 32/32 | Loss:28234.9214 | MainLoss:0.4003 | Alpha:0.4220 | SPLoss:64755.2578 | CLSLoss:1901.0640 | top1:49.8064 | AUROC:0.5064\n",
      "Test | 39/32 | Loss:0.3371 | MainLoss:0.3371 | SPLoss:22118.5684 | CLSLoss:1935.3512 | top1:49.5128 | AUROC:0.4931\n",
      "Test | 161/32 | Loss:0.3318 | MainLoss:0.3318 | SPLoss:22118.5996 | CLSLoss:1935.3550 | top1:50.8505 | AUROC:0.5803\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.071200\n",
      "Train | 32/32 | Loss:29163.4337 | MainLoss:0.3857 | Alpha:0.4179 | SPLoss:65526.3516 | CLSLoss:4333.2725 | top1:49.8581 | AUROC:0.4996\n",
      "Test | 39/32 | Loss:0.3142 | MainLoss:0.3142 | SPLoss:6129.0405 | CLSLoss:3326.1396 | top1:50.2436 | AUROC:0.5098\n",
      "Test | 161/32 | Loss:0.3172 | MainLoss:0.3172 | SPLoss:6129.0410 | CLSLoss:3326.1328 | top1:49.9190 | AUROC:0.5474\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.073600\n",
      "Train | 32/32 | Loss:26736.7762 | MainLoss:0.3849 | Alpha:0.4211 | SPLoss:59218.9141 | CLSLoss:4119.8457 | top1:49.3419 | AUROC:0.4903\n",
      "Test | 39/32 | Loss:0.3020 | MainLoss:0.3020 | SPLoss:17225.6777 | CLSLoss:1395.7454 | top1:50.3590 | AUROC:0.5095\n",
      "Test | 161/32 | Loss:0.2999 | MainLoss:0.2999 | SPLoss:17225.6387 | CLSLoss:1395.7458 | top1:49.9564 | AUROC:0.5154\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.076000\n",
      "Train | 32/32 | Loss:29731.6461 | MainLoss:0.3884 | Alpha:0.4232 | SPLoss:65618.9531 | CLSLoss:4490.7837 | top1:50.3742 | AUROC:0.5086\n",
      "Test | 39/32 | Loss:0.2985 | MainLoss:0.2985 | SPLoss:16420.4355 | CLSLoss:3713.7227 | top1:49.9231 | AUROC:0.4956\n",
      "Test | 161/32 | Loss:0.2986 | MainLoss:0.2986 | SPLoss:16420.4492 | CLSLoss:3713.7209 | top1:50.0436 | AUROC:0.5019\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.078400\n",
      "Train | 32/32 | Loss:33951.4180 | MainLoss:0.3717 | Alpha:0.4281 | SPLoss:75347.2344 | CLSLoss:4057.5864 | top1:50.2194 | AUROC:0.5068\n",
      "Test | 39/32 | Loss:0.3002 | MainLoss:0.3002 | SPLoss:13457.2422 | CLSLoss:1037.6449 | top1:50.1795 | AUROC:0.5026\n",
      "Test | 161/32 | Loss:0.2786 | MainLoss:0.2786 | SPLoss:13457.2686 | CLSLoss:1037.6426 | top1:50.3271 | AUROC:0.5629\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.080800\n",
      "Train | 32/32 | Loss:34498.5047 | MainLoss:0.3809 | Alpha:0.4252 | SPLoss:76274.6016 | CLSLoss:4682.5625 | top1:49.1613 | AUROC:0.4947\n",
      "Test | 39/32 | Loss:0.3143 | MainLoss:0.3143 | SPLoss:17780.5879 | CLSLoss:4689.5283 | top1:49.5897 | AUROC:0.4974\n",
      "Test | 161/32 | Loss:0.2924 | MainLoss:0.2924 | SPLoss:17780.5996 | CLSLoss:4689.5229 | top1:49.5888 | AUROC:0.5534\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.083200\n",
      "Train | 32/32 | Loss:32253.5410 | MainLoss:0.3956 | Alpha:0.4221 | SPLoss:73910.1406 | CLSLoss:2543.8298 | top1:49.4452 | AUROC:0.5032\n",
      "Test | 39/32 | Loss:0.3164 | MainLoss:0.3164 | SPLoss:11491.2910 | CLSLoss:1672.5653 | top1:49.2436 | AUROC:0.4996\n",
      "Test | 161/32 | Loss:0.2944 | MainLoss:0.2944 | SPLoss:11491.3047 | CLSLoss:1672.5648 | top1:50.6667 | AUROC:0.5731\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.085600\n",
      "Train | 32/32 | Loss:33248.9909 | MainLoss:0.3644 | Alpha:0.4246 | SPLoss:73132.4375 | CLSLoss:5228.1318 | top1:51.8452 | AUROC:0.5166\n",
      "Test | 39/32 | Loss:0.2986 | MainLoss:0.2986 | SPLoss:12376.0830 | CLSLoss:4749.9111 | top1:49.9487 | AUROC:0.5027\n",
      "Test | 161/32 | Loss:0.2688 | MainLoss:0.2688 | SPLoss:12376.1074 | CLSLoss:4749.9175 | top1:50.4548 | AUROC:0.5806\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.088000\n",
      "Train | 32/32 | Loss:35795.7991 | MainLoss:0.3748 | Alpha:0.4263 | SPLoss:79665.6016 | CLSLoss:4381.8521 | top1:50.6581 | AUROC:0.4939\n",
      "Test | 39/32 | Loss:0.3387 | MainLoss:0.3387 | SPLoss:26970.7969 | CLSLoss:1058.3451 | top1:49.8205 | AUROC:0.4971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 161/32 | Loss:0.2864 | MainLoss:0.2864 | SPLoss:26970.8047 | CLSLoss:1058.3475 | top1:49.7975 | AUROC:0.6294\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.090400\n",
      "Train | 32/32 | Loss:35117.4627 | MainLoss:0.3689 | Alpha:0.4175 | SPLoss:79024.0625 | CLSLoss:5427.5371 | top1:50.8387 | AUROC:0.4911\n",
      "Test | 39/32 | Loss:0.3033 | MainLoss:0.3033 | SPLoss:34069.2812 | CLSLoss:3842.8672 | top1:49.1538 | AUROC:0.5067\n",
      "Test | 161/32 | Loss:0.2747 | MainLoss:0.2747 | SPLoss:34069.3594 | CLSLoss:3842.8713 | top1:49.4984 | AUROC:0.5872\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.092800\n",
      "Train | 32/32 | Loss:39362.9526 | MainLoss:0.3636 | Alpha:0.4237 | SPLoss:88047.1953 | CLSLoss:4956.7646 | top1:49.9613 | AUROC:0.5033\n",
      "Test | 39/32 | Loss:0.3074 | MainLoss:0.3074 | SPLoss:19708.6074 | CLSLoss:5619.4019 | top1:50.2179 | AUROC:0.4924\n",
      "Test | 161/32 | Loss:0.2995 | MainLoss:0.2995 | SPLoss:19708.6094 | CLSLoss:5619.4106 | top1:50.1807 | AUROC:0.5400\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.095200\n",
      "Train | 32/32 | Loss:36036.5742 | MainLoss:0.3706 | Alpha:0.4237 | SPLoss:79310.0703 | CLSLoss:5857.0376 | top1:50.1935 | AUROC:0.4960\n",
      "Test | 39/32 | Loss:0.2922 | MainLoss:0.2922 | SPLoss:18416.1602 | CLSLoss:4863.6685 | top1:48.7436 | AUROC:0.5051\n",
      "Test | 161/32 | Loss:0.2803 | MainLoss:0.2803 | SPLoss:18416.1484 | CLSLoss:4863.6738 | top1:49.9470 | AUROC:0.5409\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.097600\n",
      "Train | 32/32 | Loss:40412.8162 | MainLoss:0.3523 | Alpha:0.4283 | SPLoss:89499.8750 | CLSLoss:5269.1787 | top1:49.7290 | AUROC:0.5049\n",
      "Test | 39/32 | Loss:0.3204 | MainLoss:0.3204 | SPLoss:31388.6992 | CLSLoss:6827.4849 | top1:47.9231 | AUROC:0.5028\n",
      "Test | 161/32 | Loss:0.2541 | MainLoss:0.2541 | SPLoss:31388.7305 | CLSLoss:6827.4971 | top1:50.3832 | AUROC:0.6556\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.100000\n",
      "Train | 32/32 | Loss:41093.2516 | MainLoss:0.3525 | Alpha:0.4218 | SPLoss:91530.5000 | CLSLoss:5743.5620 | top1:50.2710 | AUROC:0.4892\n",
      "Test | 39/32 | Loss:0.3070 | MainLoss:0.3070 | SPLoss:24546.0527 | CLSLoss:5503.1221 | top1:49.5769 | AUROC:0.4898\n",
      "Test | 161/32 | Loss:0.2691 | MainLoss:0.2691 | SPLoss:24546.0234 | CLSLoss:5503.1211 | top1:50.2897 | AUROC:0.5903\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.102400\n",
      "Train | 32/32 | Loss:38740.7536 | MainLoss:0.3585 | Alpha:0.4259 | SPLoss:85357.3828 | CLSLoss:5336.7070 | top1:50.7871 | AUROC:0.5095\n",
      "Test | 39/32 | Loss:0.2944 | MainLoss:0.2944 | SPLoss:27962.2363 | CLSLoss:3750.2190 | top1:50.5513 | AUROC:0.5095\n",
      "Test | 161/32 | Loss:0.2596 | MainLoss:0.2596 | SPLoss:27962.2383 | CLSLoss:3750.2112 | top1:50.1308 | AUROC:0.6154\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.104800\n",
      "Train | 32/32 | Loss:45456.2181 | MainLoss:0.3547 | Alpha:0.4260 | SPLoss:100296.2812 | CLSLoss:6505.4595 | top1:49.7806 | AUROC:0.5021\n",
      "Test | 39/32 | Loss:0.3017 | MainLoss:0.3017 | SPLoss:33247.5938 | CLSLoss:3075.5090 | top1:50.9744 | AUROC:0.4939\n",
      "Test | 161/32 | Loss:0.2638 | MainLoss:0.2638 | SPLoss:33247.6172 | CLSLoss:3075.5098 | top1:50.1838 | AUROC:0.6113\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.107200\n",
      "Train | 32/32 | Loss:45979.5248 | MainLoss:0.3603 | Alpha:0.4252 | SPLoss:102083.1875 | CLSLoss:5992.4810 | top1:50.3742 | AUROC:0.4989\n",
      "Test | 39/32 | Loss:0.3142 | MainLoss:0.3142 | SPLoss:12954.2256 | CLSLoss:3142.9597 | top1:49.6282 | AUROC:0.4923\n",
      "Test | 161/32 | Loss:0.2035 | MainLoss:0.2035 | SPLoss:12954.2363 | CLSLoss:3142.9600 | top1:49.9128 | AUROC:0.7751\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.109600\n",
      "Train | 32/32 | Loss:41835.1236 | MainLoss:0.3340 | Alpha:0.4219 | SPLoss:93203.7344 | CLSLoss:6253.8306 | top1:50.1677 | AUROC:0.5006\n",
      "Test | 39/32 | Loss:0.3172 | MainLoss:0.3172 | SPLoss:13530.8789 | CLSLoss:6981.0015 | top1:48.6538 | AUROC:0.5035\n",
      "Test | 161/32 | Loss:0.2893 | MainLoss:0.2893 | SPLoss:13530.8799 | CLSLoss:6981.0166 | top1:50.3458 | AUROC:0.5724\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.112000\n",
      "Train | 32/32 | Loss:40636.8610 | MainLoss:0.3398 | Alpha:0.4192 | SPLoss:89784.6016 | CLSLoss:6894.7749 | top1:50.1419 | AUROC:0.4989\n",
      "Test | 39/32 | Loss:0.2949 | MainLoss:0.2949 | SPLoss:26621.2051 | CLSLoss:5165.7676 | top1:49.0641 | AUROC:0.5016\n",
      "Test | 161/32 | Loss:0.2905 | MainLoss:0.2905 | SPLoss:26621.1973 | CLSLoss:5165.7622 | top1:49.0530 | AUROC:0.5275\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.114400\n",
      "Train | 32/32 | Loss:43836.0554 | MainLoss:0.3507 | Alpha:0.4272 | SPLoss:96508.8047 | CLSLoss:5979.2397 | top1:50.0387 | AUROC:0.4988\n",
      "Test | 39/32 | Loss:0.3448 | MainLoss:0.3448 | SPLoss:9273.5811 | CLSLoss:2013.2098 | top1:49.1667 | AUROC:0.4981\n",
      "Test | 161/32 | Loss:0.3267 | MainLoss:0.3267 | SPLoss:9273.5957 | CLSLoss:2013.2052 | top1:50.3988 | AUROC:0.5270\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.116800\n",
      "Train | 32/32 | Loss:43988.2570 | MainLoss:0.3436 | Alpha:0.4179 | SPLoss:98220.1172 | CLSLoss:7034.4497 | top1:51.5613 | AUROC:0.4896\n",
      "Test | 39/32 | Loss:0.2985 | MainLoss:0.2985 | SPLoss:15459.3193 | CLSLoss:4992.6177 | top1:49.6923 | AUROC:0.4932\n",
      "Test | 161/32 | Loss:0.2912 | MainLoss:0.2912 | SPLoss:15459.3496 | CLSLoss:4992.6147 | top1:50.0561 | AUROC:0.5322\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.119200\n",
      "Train | 32/32 | Loss:44064.3908 | MainLoss:0.3621 | Alpha:0.4261 | SPLoss:96853.5625 | CLSLoss:6137.3301 | top1:49.2645 | AUROC:0.4843\n",
      "Test | 39/32 | Loss:0.3052 | MainLoss:0.3052 | SPLoss:43796.0469 | CLSLoss:3593.7168 | top1:50.2692 | AUROC:0.4912\n",
      "Test | 161/32 | Loss:0.3032 | MainLoss:0.3032 | SPLoss:43796.0391 | CLSLoss:3593.7192 | top1:49.9221 | AUROC:0.5030\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.121600\n",
      "Train | 32/32 | Loss:49988.5450 | MainLoss:0.3505 | Alpha:0.4242 | SPLoss:111063.1719 | CLSLoss:6885.6929 | top1:50.1935 | AUROC:0.4920\n",
      "Test | 39/32 | Loss:0.3320 | MainLoss:0.3320 | SPLoss:19406.5566 | CLSLoss:2916.3301 | top1:49.1538 | AUROC:0.4868\n",
      "Test | 161/32 | Loss:0.3237 | MainLoss:0.3237 | SPLoss:19406.5391 | CLSLoss:2916.3311 | top1:49.7009 | AUROC:0.5130\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.124000\n",
      "Train | 32/32 | Loss:51850.9459 | MainLoss:0.3695 | Alpha:0.4193 | SPLoss:116165.0781 | CLSLoss:7125.9565 | top1:49.4452 | AUROC:0.4953\n",
      "Test | 39/32 | Loss:0.3064 | MainLoss:0.3064 | SPLoss:24709.9961 | CLSLoss:6216.7983 | top1:49.4744 | AUROC:0.5033\n",
      "Test | 161/32 | Loss:0.3053 | MainLoss:0.3053 | SPLoss:24709.9980 | CLSLoss:6216.7998 | top1:50.0966 | AUROC:0.5014\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.126400\n",
      "Train | 32/32 | Loss:53286.6238 | MainLoss:0.3596 | Alpha:0.4239 | SPLoss:120198.3750 | CLSLoss:5875.1421 | top1:49.9097 | AUROC:0.5163\n",
      "Test | 39/32 | Loss:0.3196 | MainLoss:0.3196 | SPLoss:8963.5781 | CLSLoss:5681.5669 | top1:50.8462 | AUROC:0.4971\n",
      "Test | 161/32 | Loss:0.3224 | MainLoss:0.3224 | SPLoss:8963.5811 | CLSLoss:5681.5723 | top1:50.0779 | AUROC:0.5054\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.128800\n",
      "Train | 32/32 | Loss:49663.4512 | MainLoss:0.3597 | Alpha:0.4224 | SPLoss:109788.1094 | CLSLoss:7686.2754 | top1:48.7742 | AUROC:0.4980\n",
      "Test | 39/32 | Loss:0.3039 | MainLoss:0.3039 | SPLoss:28977.2734 | CLSLoss:6677.1934 | top1:49.2564 | AUROC:0.5063\n",
      "Test | 161/32 | Loss:0.3004 | MainLoss:0.3004 | SPLoss:28977.2793 | CLSLoss:6677.1987 | top1:49.6885 | AUROC:0.5114\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.131200\n",
      "Train | 32/32 | Loss:54678.6216 | MainLoss:0.3461 | Alpha:0.4251 | SPLoss:122237.7891 | CLSLoss:6224.7847 | top1:50.3742 | AUROC:0.4940\n",
      "Test | 39/32 | Loss:0.2932 | MainLoss:0.2932 | SPLoss:17701.7520 | CLSLoss:2957.4602 | top1:50.5000 | AUROC:0.4998\n",
      "Test | 161/32 | Loss:0.2927 | MainLoss:0.2927 | SPLoss:17701.7578 | CLSLoss:2957.4661 | top1:50.1776 | AUROC:0.5049\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.133600\n",
      "Train | 32/32 | Loss:55057.3177 | MainLoss:0.3433 | Alpha:0.4281 | SPLoss:120547.8750 | CLSLoss:8213.9941 | top1:50.1419 | AUROC:0.4996\n",
      "Test | 39/32 | Loss:0.3061 | MainLoss:0.3061 | SPLoss:19018.9570 | CLSLoss:4724.6221 | top1:50.7821 | AUROC:0.4843\n",
      "Test | 161/32 | Loss:0.2974 | MainLoss:0.2974 | SPLoss:19018.9160 | CLSLoss:4724.6318 | top1:50.0436 | AUROC:0.5254\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.136000\n",
      "Train | 32/32 | Loss:53724.7947 | MainLoss:0.3969 | Alpha:0.4242 | SPLoss:118757.0859 | CLSLoss:8087.0112 | top1:49.4968 | AUROC:0.4964\n",
      "Test | 39/32 | Loss:0.3117 | MainLoss:0.3117 | SPLoss:11921.5986 | CLSLoss:4885.1401 | top1:49.8462 | AUROC:0.4936\n",
      "Test | 161/32 | Loss:0.3068 | MainLoss:0.3068 | SPLoss:11921.5996 | CLSLoss:4885.1460 | top1:49.9938 | AUROC:0.5111\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.138400\n",
      "Train | 32/32 | Loss:51161.7308 | MainLoss:0.3588 | Alpha:0.4236 | SPLoss:112370.6172 | CLSLoss:8365.8018 | top1:49.0581 | AUROC:0.5097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 39/32 | Loss:0.2982 | MainLoss:0.2982 | SPLoss:15260.0762 | CLSLoss:6098.5815 | top1:50.1026 | AUROC:0.5067\n",
      "Test | 161/32 | Loss:0.2998 | MainLoss:0.2998 | SPLoss:15260.0439 | CLSLoss:6098.5811 | top1:50.3022 | AUROC:0.5087\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.140800\n",
      "Train | 32/32 | Loss:52061.3109 | MainLoss:0.3818 | Alpha:0.4249 | SPLoss:114419.6406 | CLSLoss:7768.5537 | top1:49.3677 | AUROC:0.4869\n",
      "Test | 39/32 | Loss:0.3082 | MainLoss:0.3082 | SPLoss:29035.6055 | CLSLoss:3238.8799 | top1:48.5128 | AUROC:0.4941\n",
      "Test | 161/32 | Loss:0.3020 | MainLoss:0.3020 | SPLoss:29035.5918 | CLSLoss:3238.8801 | top1:50.2150 | AUROC:0.5025\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.143200\n",
      "Train | 32/32 | Loss:58925.6059 | MainLoss:0.3665 | Alpha:0.4261 | SPLoss:129904.5938 | CLSLoss:8557.0674 | top1:51.1484 | AUROC:0.5192\n",
      "Test | 39/32 | Loss:0.2923 | MainLoss:0.2923 | SPLoss:29997.7773 | CLSLoss:3847.5325 | top1:50.5641 | AUROC:0.5057\n",
      "Test | 161/32 | Loss:0.2970 | MainLoss:0.2970 | SPLoss:29997.7676 | CLSLoss:3847.5288 | top1:50.0093 | AUROC:0.4984\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.145600\n",
      "Train | 32/32 | Loss:63038.3220 | MainLoss:0.3803 | Alpha:0.4266 | SPLoss:139560.4062 | CLSLoss:8333.4111 | top1:48.7742 | AUROC:0.5086\n",
      "Test | 39/32 | Loss:0.3301 | MainLoss:0.3301 | SPLoss:41237.4922 | CLSLoss:3491.0691 | top1:49.4615 | AUROC:0.5021\n",
      "Test | 161/32 | Loss:0.3237 | MainLoss:0.3237 | SPLoss:41237.5156 | CLSLoss:3491.0708 | top1:49.9844 | AUROC:0.5084\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.148000\n",
      "Train | 32/32 | Loss:61812.9565 | MainLoss:0.3727 | Alpha:0.4179 | SPLoss:138754.5625 | CLSLoss:9000.4990 | top1:49.9097 | AUROC:0.4980\n",
      "Test | 39/32 | Loss:0.3115 | MainLoss:0.3115 | SPLoss:19328.6406 | CLSLoss:4872.3579 | top1:49.7564 | AUROC:0.5144\n",
      "Test | 161/32 | Loss:0.3115 | MainLoss:0.3115 | SPLoss:19328.6426 | CLSLoss:4872.3540 | top1:50.5981 | AUROC:0.5055\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.150400\n",
      "Train | 32/32 | Loss:63456.8221 | MainLoss:0.3682 | Alpha:0.4213 | SPLoss:141846.6562 | CLSLoss:8619.6875 | top1:49.6000 | AUROC:0.4905\n",
      "Test | 39/32 | Loss:0.3167 | MainLoss:0.3167 | SPLoss:31670.9785 | CLSLoss:4882.9419 | top1:49.7436 | AUROC:0.4932\n",
      "Test | 161/32 | Loss:0.3185 | MainLoss:0.3185 | SPLoss:31671.0254 | CLSLoss:4882.9521 | top1:49.7632 | AUROC:0.5053\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.152800\n",
      "Train | 32/32 | Loss:65173.8435 | MainLoss:0.3708 | Alpha:0.4221 | SPLoss:144652.2344 | CLSLoss:9312.6348 | top1:49.3935 | AUROC:0.4979\n",
      "Test | 39/32 | Loss:0.3067 | MainLoss:0.3067 | SPLoss:28317.2773 | CLSLoss:4677.6509 | top1:49.9231 | AUROC:0.4961\n",
      "Test | 161/32 | Loss:0.3072 | MainLoss:0.3072 | SPLoss:28317.2148 | CLSLoss:4677.6509 | top1:49.6386 | AUROC:0.5060\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.155200\n",
      "Train | 32/32 | Loss:64984.7451 | MainLoss:0.3477 | Alpha:0.4243 | SPLoss:145155.5156 | CLSLoss:8493.2051 | top1:50.3484 | AUROC:0.5063\n",
      "Test | 39/32 | Loss:0.3119 | MainLoss:0.3119 | SPLoss:16120.5596 | CLSLoss:2779.7510 | top1:50.1282 | AUROC:0.4990\n",
      "Test | 161/32 | Loss:0.3126 | MainLoss:0.3126 | SPLoss:16120.5986 | CLSLoss:2779.7566 | top1:50.0841 | AUROC:0.5096\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.157600\n",
      "Train | 32/32 | Loss:65029.3774 | MainLoss:0.3610 | Alpha:0.4228 | SPLoss:144226.3438 | CLSLoss:9609.1953 | top1:49.4194 | AUROC:0.5087\n",
      "Test | 39/32 | Loss:0.3162 | MainLoss:0.3162 | SPLoss:16853.6074 | CLSLoss:6053.7656 | top1:50.7179 | AUROC:0.4912\n",
      "Test | 161/32 | Loss:0.3231 | MainLoss:0.3231 | SPLoss:16853.6035 | CLSLoss:6053.7617 | top1:49.7695 | AUROC:0.4985\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.160000\n",
      "Train | 32/32 | Loss:61314.8255 | MainLoss:0.3888 | Alpha:0.4196 | SPLoss:137621.1406 | CLSLoss:8932.7480 | top1:48.4645 | AUROC:0.5137\n",
      "Test | 39/32 | Loss:0.2959 | MainLoss:0.2959 | SPLoss:21381.4141 | CLSLoss:6572.3857 | top1:50.9744 | AUROC:0.5040\n",
      "Test | 161/32 | Loss:0.3046 | MainLoss:0.3046 | SPLoss:21381.4336 | CLSLoss:6572.3950 | top1:49.7601 | AUROC:0.5000\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.016000\n",
      "Train | 32/32 | Loss:8960.8196 | MainLoss:0.3112 | Alpha:0.4256 | SPLoss:15061.4844 | CLSLoss:5994.2666 | top1:50.4774 | AUROC:0.5043\n",
      "Test | 39/32 | Loss:0.2931 | MainLoss:0.2931 | SPLoss:4539.0376 | CLSLoss:5758.4927 | top1:49.4487 | AUROC:0.5062\n",
      "Test | 161/32 | Loss:0.2983 | MainLoss:0.2983 | SPLoss:4539.0396 | CLSLoss:5758.4858 | top1:49.3801 | AUROC:0.5019\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.016000\n",
      "Train | 32/32 | Loss:8278.5972 | MainLoss:0.3026 | Alpha:0.4295 | SPLoss:13599.8311 | CLSLoss:5683.5054 | top1:50.8387 | AUROC:0.5272\n",
      "Test | 39/32 | Loss:0.2981 | MainLoss:0.2981 | SPLoss:1179.1521 | CLSLoss:5381.1978 | top1:49.5256 | AUROC:0.5060\n",
      "Test | 161/32 | Loss:0.2976 | MainLoss:0.2976 | SPLoss:1179.1517 | CLSLoss:5381.1992 | top1:50.4174 | AUROC:0.5077\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.016000\n",
      "Train | 32/32 | Loss:7808.0230 | MainLoss:0.3546 | Alpha:0.4254 | SPLoss:13087.0039 | CLSLoss:5235.5508 | top1:50.9419 | AUROC:0.4784\n",
      "Test | 39/32 | Loss:0.3035 | MainLoss:0.3035 | SPLoss:3389.1829 | CLSLoss:5000.5132 | top1:50.0641 | AUROC:0.5038\n",
      "Test | 161/32 | Loss:0.3118 | MainLoss:0.3118 | SPLoss:3389.1885 | CLSLoss:5000.5122 | top1:49.4174 | AUROC:0.5024\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.016000\n",
      "Train | 32/32 | Loss:8527.6861 | MainLoss:0.3472 | Alpha:0.4237 | SPLoss:15220.0869 | CLSLoss:4861.3159 | top1:49.2645 | AUROC:0.5119\n",
      "Test | 39/32 | Loss:0.2968 | MainLoss:0.2968 | SPLoss:1662.3118 | CLSLoss:4578.6704 | top1:50.3846 | AUROC:0.5020\n",
      "Test | 161/32 | Loss:0.3026 | MainLoss:0.3026 | SPLoss:1662.3142 | CLSLoss:4578.6748 | top1:50.0125 | AUROC:0.5001\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.015999\n",
      "Train | 32/32 | Loss:8465.6918 | MainLoss:0.3200 | Alpha:0.4266 | SPLoss:15387.1523 | CLSLoss:4461.8682 | top1:48.9290 | AUROC:0.5046\n",
      "Test | 39/32 | Loss:0.3006 | MainLoss:0.3006 | SPLoss:3313.6892 | CLSLoss:4162.7495 | top1:49.7564 | AUROC:0.5019\n",
      "Test | 161/32 | Loss:0.3052 | MainLoss:0.3052 | SPLoss:3313.6831 | CLSLoss:4162.7524 | top1:50.2336 | AUROC:0.5041\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.015999\n",
      "Train | 32/32 | Loss:7383.1885 | MainLoss:0.3218 | Alpha:0.4266 | SPLoss:13133.6602 | CLSLoss:4147.8564 | top1:50.3484 | AUROC:0.5037\n",
      "Test | 39/32 | Loss:0.2940 | MainLoss:0.2940 | SPLoss:2576.5762 | CLSLoss:3936.3396 | top1:49.6795 | AUROC:0.5000\n",
      "Test | 161/32 | Loss:0.2983 | MainLoss:0.2983 | SPLoss:2576.5752 | CLSLoss:3936.3306 | top1:49.6075 | AUROC:0.5079\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.015999\n",
      "Train | 32/32 | Loss:8155.6141 | MainLoss:0.3253 | Alpha:0.4272 | SPLoss:15367.0898 | CLSLoss:3689.8022 | top1:50.1419 | AUROC:0.4936\n",
      "Test | 39/32 | Loss:0.2926 | MainLoss:0.2926 | SPLoss:3047.9827 | CLSLoss:3405.2671 | top1:50.0769 | AUROC:0.5035\n",
      "Test | 161/32 | Loss:0.2980 | MainLoss:0.2980 | SPLoss:3047.9871 | CLSLoss:3405.2703 | top1:50.2586 | AUROC:0.5052\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.015998\n",
      "Train | 32/32 | Loss:7305.3303 | MainLoss:0.3348 | Alpha:0.4276 | SPLoss:13650.5576 | CLSLoss:3444.8831 | top1:50.2452 | AUROC:0.5015\n",
      "Test | 39/32 | Loss:0.2921 | MainLoss:0.2921 | SPLoss:6437.3594 | CLSLoss:3174.1909 | top1:49.9359 | AUROC:0.5094\n",
      "Test | 161/32 | Loss:0.3009 | MainLoss:0.3009 | SPLoss:6437.3594 | CLSLoss:3174.1970 | top1:49.8287 | AUROC:0.5039\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.015997\n",
      "Train | 32/32 | Loss:7234.6121 | MainLoss:0.3546 | Alpha:0.4271 | SPLoss:13886.5156 | CLSLoss:3058.5684 | top1:49.4194 | AUROC:0.4899\n",
      "Test | 39/32 | Loss:0.2993 | MainLoss:0.2993 | SPLoss:1627.5901 | CLSLoss:2681.3062 | top1:49.5769 | AUROC:0.4955\n",
      "Test | 161/32 | Loss:0.3014 | MainLoss:0.3014 | SPLoss:1627.5938 | CLSLoss:2681.3074 | top1:50.1994 | AUROC:0.4985\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.015997\n",
      "Train | 32/32 | Loss:7717.3125 | MainLoss:0.3242 | Alpha:0.4270 | SPLoss:15430.5283 | CLSLoss:2699.0103 | top1:50.1935 | AUROC:0.5239\n",
      "Test | 39/32 | Loss:0.3037 | MainLoss:0.3037 | SPLoss:2070.8611 | CLSLoss:2350.1208 | top1:49.9103 | AUROC:0.5012\n",
      "Test | 161/32 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:2070.8606 | CLSLoss:2350.1204 | top1:49.8629 | AUROC:0.5002\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.015996\n",
      "Train | 32/32 | Loss:7475.2251 | MainLoss:0.3444 | Alpha:0.4241 | SPLoss:15240.2949 | CLSLoss:2367.4128 | top1:49.2645 | AUROC:0.4942\n",
      "Test | 39/32 | Loss:0.3012 | MainLoss:0.3012 | SPLoss:2463.8601 | CLSLoss:2191.4397 | top1:49.5385 | AUROC:0.5055\n",
      "Test | 161/32 | Loss:0.3045 | MainLoss:0.3045 | SPLoss:2463.8630 | CLSLoss:2191.4399 | top1:50.1153 | AUROC:0.5016\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.015995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:7095.7760 | MainLoss:0.3413 | Alpha:0.4268 | SPLoss:14765.5469 | CLSLoss:1890.6428 | top1:50.9161 | AUROC:0.5069\n",
      "Test | 39/32 | Loss:0.3015 | MainLoss:0.3015 | SPLoss:1207.5518 | CLSLoss:1868.1403 | top1:50.4231 | AUROC:0.4998\n",
      "Test | 161/32 | Loss:0.3069 | MainLoss:0.3069 | SPLoss:1207.5519 | CLSLoss:1868.1403 | top1:49.4704 | AUROC:0.5054\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.015994\n",
      "Train | 32/32 | Loss:7164.4795 | MainLoss:0.3405 | Alpha:0.4250 | SPLoss:15155.9141 | CLSLoss:1705.2991 | top1:50.9161 | AUROC:0.5163\n",
      "Test | 39/32 | Loss:0.2983 | MainLoss:0.2983 | SPLoss:5178.6719 | CLSLoss:1516.3296 | top1:49.8846 | AUROC:0.4985\n",
      "Test | 161/32 | Loss:0.3018 | MainLoss:0.3018 | SPLoss:5178.6709 | CLSLoss:1516.3268 | top1:49.6760 | AUROC:0.5025\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.015993\n",
      "Train | 32/32 | Loss:6179.3547 | MainLoss:0.3629 | Alpha:0.4238 | SPLoss:13397.6982 | CLSLoss:1205.0430 | top1:48.4903 | AUROC:0.4921\n",
      "Test | 39/32 | Loss:0.3066 | MainLoss:0.3066 | SPLoss:1083.3274 | CLSLoss:1029.3619 | top1:49.4487 | AUROC:0.4898\n",
      "Test | 161/32 | Loss:0.3058 | MainLoss:0.3058 | SPLoss:1083.3279 | CLSLoss:1029.3606 | top1:49.4361 | AUROC:0.5035\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.015992\n",
      "Train | 32/32 | Loss:6291.3797 | MainLoss:0.3591 | Alpha:0.4247 | SPLoss:13762.0820 | CLSLoss:1076.0276 | top1:49.2645 | AUROC:0.5071\n",
      "Test | 39/32 | Loss:0.3228 | MainLoss:0.3228 | SPLoss:1761.6686 | CLSLoss:917.3456 | top1:49.6282 | AUROC:0.5111\n",
      "Test | 161/32 | Loss:0.3301 | MainLoss:0.3301 | SPLoss:1761.6658 | CLSLoss:917.3474 | top1:49.7508 | AUROC:0.5020\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.015991\n",
      "Train | 32/32 | Loss:6045.0550 | MainLoss:0.3802 | Alpha:0.4206 | SPLoss:13725.6416 | CLSLoss:679.0808 | top1:50.8129 | AUROC:0.4935\n",
      "Test | 39/32 | Loss:0.3241 | MainLoss:0.3241 | SPLoss:6257.0054 | CLSLoss:605.4376 | top1:50.2821 | AUROC:0.4950\n",
      "Test | 161/32 | Loss:0.3302 | MainLoss:0.3302 | SPLoss:6257.0146 | CLSLoss:605.4390 | top1:50.1215 | AUROC:0.5069\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.015990\n",
      "Train | 32/32 | Loss:6365.0138 | MainLoss:0.3406 | Alpha:0.4210 | SPLoss:14152.1172 | CLSLoss:962.5187 | top1:50.2710 | AUROC:0.5204\n",
      "Test | 39/32 | Loss:0.3139 | MainLoss:0.3139 | SPLoss:2609.1658 | CLSLoss:824.3340 | top1:50.1026 | AUROC:0.4940\n",
      "Test | 161/32 | Loss:0.3177 | MainLoss:0.3177 | SPLoss:2609.1633 | CLSLoss:824.3330 | top1:50.2368 | AUROC:0.4944\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.015989\n",
      "Train | 32/32 | Loss:6695.7565 | MainLoss:0.3726 | Alpha:0.4223 | SPLoss:15145.8604 | CLSLoss:728.4294 | top1:48.9806 | AUROC:0.5042\n",
      "Test | 39/32 | Loss:0.3148 | MainLoss:0.3148 | SPLoss:1269.8468 | CLSLoss:245.4136 | top1:50.5000 | AUROC:0.5082\n",
      "Test | 161/32 | Loss:0.3276 | MainLoss:0.3276 | SPLoss:1269.8467 | CLSLoss:245.4131 | top1:49.1838 | AUROC:0.5013\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.015987\n",
      "Train | 32/32 | Loss:6730.7061 | MainLoss:0.3466 | Alpha:0.4217 | SPLoss:14979.4199 | CLSLoss:967.9514 | top1:50.4774 | AUROC:0.4982\n",
      "Test | 39/32 | Loss:0.3251 | MainLoss:0.3251 | SPLoss:3973.3892 | CLSLoss:744.9725 | top1:50.5000 | AUROC:0.4979\n",
      "Test | 161/32 | Loss:0.3295 | MainLoss:0.3295 | SPLoss:3973.3921 | CLSLoss:744.9727 | top1:50.0810 | AUROC:0.5002\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.015986\n",
      "Train | 32/32 | Loss:6452.5416 | MainLoss:0.3777 | Alpha:0.4185 | SPLoss:14627.9248 | CLSLoss:755.2347 | top1:49.4452 | AUROC:0.4997\n",
      "Test | 39/32 | Loss:0.3273 | MainLoss:0.3273 | SPLoss:2557.7097 | CLSLoss:272.9975 | top1:49.7821 | AUROC:0.4995\n",
      "Test | 161/32 | Loss:0.3319 | MainLoss:0.3319 | SPLoss:2557.7075 | CLSLoss:272.9977 | top1:49.8910 | AUROC:0.5032\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.015984\n",
      "Train | 32/32 | Loss:6716.7996 | MainLoss:0.3519 | Alpha:0.4199 | SPLoss:15012.1846 | CLSLoss:930.9372 | top1:49.9871 | AUROC:0.5040\n",
      "Test | 39/32 | Loss:0.3210 | MainLoss:0.3210 | SPLoss:5107.9604 | CLSLoss:961.6230 | top1:49.6282 | AUROC:0.5033\n",
      "Test | 161/32 | Loss:0.3286 | MainLoss:0.3286 | SPLoss:5107.9678 | CLSLoss:961.6222 | top1:50.1558 | AUROC:0.5051\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.015983\n",
      "Train | 32/32 | Loss:6051.1277 | MainLoss:0.3658 | Alpha:0.4208 | SPLoss:13832.7295 | CLSLoss:526.7908 | top1:49.7806 | AUROC:0.5060\n",
      "Test | 39/32 | Loss:0.3180 | MainLoss:0.3180 | SPLoss:2062.4438 | CLSLoss:356.2050 | top1:50.2051 | AUROC:0.5023\n",
      "Test | 161/32 | Loss:0.3244 | MainLoss:0.3244 | SPLoss:2062.4412 | CLSLoss:356.2050 | top1:49.9377 | AUROC:0.5051\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.015981\n",
      "Train | 32/32 | Loss:6211.6056 | MainLoss:0.3479 | Alpha:0.4209 | SPLoss:13830.4932 | CLSLoss:933.1819 | top1:50.5806 | AUROC:0.4997\n",
      "Test | 39/32 | Loss:0.3110 | MainLoss:0.3110 | SPLoss:3415.2390 | CLSLoss:790.8942 | top1:51.0256 | AUROC:0.5012\n",
      "Test | 161/32 | Loss:0.3216 | MainLoss:0.3216 | SPLoss:3415.2397 | CLSLoss:790.8932 | top1:50.2336 | AUROC:0.4945\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.015979\n",
      "Train | 32/32 | Loss:6325.2609 | MainLoss:0.3412 | Alpha:0.4234 | SPLoss:14196.6152 | CLSLoss:730.9073 | top1:49.9871 | AUROC:0.5024\n",
      "Test | 39/32 | Loss:0.3156 | MainLoss:0.3156 | SPLoss:2548.6602 | CLSLoss:243.2575 | top1:49.9231 | AUROC:0.4944\n",
      "Test | 161/32 | Loss:0.3167 | MainLoss:0.3167 | SPLoss:2548.6638 | CLSLoss:243.2575 | top1:50.1620 | AUROC:0.5019\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.015977\n",
      "Train | 32/32 | Loss:6459.2079 | MainLoss:0.3498 | Alpha:0.4221 | SPLoss:14413.0137 | CLSLoss:934.0671 | top1:49.5742 | AUROC:0.5009\n",
      "Test | 39/32 | Loss:0.3173 | MainLoss:0.3173 | SPLoss:3326.4097 | CLSLoss:452.5649 | top1:49.5128 | AUROC:0.5036\n",
      "Test | 161/32 | Loss:0.3242 | MainLoss:0.3242 | SPLoss:3326.4033 | CLSLoss:452.5650 | top1:50.0218 | AUROC:0.5023\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.015975\n",
      "Train | 32/32 | Loss:6833.0414 | MainLoss:0.3161 | Alpha:0.4232 | SPLoss:15132.3945 | CLSLoss:975.5276 | top1:51.1226 | AUROC:0.5104\n",
      "Test | 39/32 | Loss:0.3124 | MainLoss:0.3124 | SPLoss:5015.0679 | CLSLoss:645.0552 | top1:49.9872 | AUROC:0.5025\n",
      "Test | 161/32 | Loss:0.3193 | MainLoss:0.3193 | SPLoss:5015.0669 | CLSLoss:645.0560 | top1:50.3489 | AUROC:0.5009\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.015973\n",
      "Train | 32/32 | Loss:6020.1302 | MainLoss:0.3490 | Alpha:0.4204 | SPLoss:13489.6973 | CLSLoss:858.1406 | top1:49.2129 | AUROC:0.5047\n",
      "Test | 39/32 | Loss:0.3127 | MainLoss:0.3127 | SPLoss:2125.6401 | CLSLoss:491.3722 | top1:50.3205 | AUROC:0.4992\n",
      "Test | 161/32 | Loss:0.3204 | MainLoss:0.3204 | SPLoss:2125.6404 | CLSLoss:491.3713 | top1:49.8100 | AUROC:0.5045\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.015971\n",
      "Train | 32/32 | Loss:6260.8737 | MainLoss:0.3395 | Alpha:0.4218 | SPLoss:13876.2812 | CLSLoss:992.2383 | top1:51.1226 | AUROC:0.4868\n",
      "Test | 39/32 | Loss:0.3109 | MainLoss:0.3109 | SPLoss:1420.0648 | CLSLoss:610.0875 | top1:49.8846 | AUROC:0.5063\n",
      "Test | 161/32 | Loss:0.3202 | MainLoss:0.3202 | SPLoss:1420.0620 | CLSLoss:610.0889 | top1:49.9190 | AUROC:0.5015\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.015969\n",
      "Train | 32/32 | Loss:6695.9986 | MainLoss:0.3475 | Alpha:0.4226 | SPLoss:14996.3750 | CLSLoss:883.2471 | top1:50.2452 | AUROC:0.4925\n",
      "Test | 39/32 | Loss:0.3145 | MainLoss:0.3145 | SPLoss:1882.2749 | CLSLoss:419.5724 | top1:50.6923 | AUROC:0.5015\n",
      "Test | 161/32 | Loss:0.3256 | MainLoss:0.3256 | SPLoss:1882.2787 | CLSLoss:419.5715 | top1:50.0717 | AUROC:0.4985\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.015967\n",
      "Train | 32/32 | Loss:5832.8510 | MainLoss:0.3284 | Alpha:0.4209 | SPLoss:12839.1387 | CLSLoss:972.5694 | top1:50.0645 | AUROC:0.5058\n",
      "Test | 39/32 | Loss:0.3096 | MainLoss:0.3096 | SPLoss:4347.3677 | CLSLoss:885.2944 | top1:49.6154 | AUROC:0.5199\n",
      "Test | 161/32 | Loss:0.3141 | MainLoss:0.3141 | SPLoss:4347.3623 | CLSLoss:885.2928 | top1:50.0530 | AUROC:0.5034\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.015964\n",
      "Train | 32/32 | Loss:6141.5856 | MainLoss:0.3403 | Alpha:0.4218 | SPLoss:13798.6748 | CLSLoss:724.3951 | top1:49.5226 | AUROC:0.4908\n",
      "Test | 39/32 | Loss:0.3093 | MainLoss:0.3093 | SPLoss:3466.6707 | CLSLoss:308.1354 | top1:51.0897 | AUROC:0.5011\n",
      "Test | 161/32 | Loss:0.3106 | MainLoss:0.3106 | SPLoss:3466.6770 | CLSLoss:308.1352 | top1:50.4112 | AUROC:0.5035\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.015962\n",
      "Train | 32/32 | Loss:6750.0303 | MainLoss:0.3390 | Alpha:0.4229 | SPLoss:14994.0244 | CLSLoss:978.2192 | top1:49.6774 | AUROC:0.4988\n",
      "Test | 39/32 | Loss:0.2987 | MainLoss:0.2987 | SPLoss:1552.4399 | CLSLoss:572.9599 | top1:50.5897 | AUROC:0.5157\n",
      "Test | 161/32 | Loss:0.3055 | MainLoss:0.3055 | SPLoss:1552.4401 | CLSLoss:572.9600 | top1:49.9907 | AUROC:0.5071\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.015960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:5911.6173 | MainLoss:0.3442 | Alpha:0.4252 | SPLoss:12991.1055 | CLSLoss:894.5773 | top1:51.0968 | AUROC:0.4976\n",
      "Test | 39/32 | Loss:0.3117 | MainLoss:0.3117 | SPLoss:2315.1196 | CLSLoss:370.0217 | top1:50.1538 | AUROC:0.4914\n",
      "Test | 161/32 | Loss:0.3133 | MainLoss:0.3133 | SPLoss:2315.1199 | CLSLoss:370.0215 | top1:49.7664 | AUROC:0.5054\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.015957\n",
      "Train | 32/32 | Loss:5959.9228 | MainLoss:0.3378 | Alpha:0.4256 | SPLoss:13002.2842 | CLSLoss:980.8745 | top1:51.2774 | AUROC:0.5054\n",
      "Test | 39/32 | Loss:0.3171 | MainLoss:0.3171 | SPLoss:2388.8997 | CLSLoss:594.4349 | top1:49.6795 | AUROC:0.4988\n",
      "Test | 161/32 | Loss:0.3196 | MainLoss:0.3196 | SPLoss:2388.8975 | CLSLoss:594.4344 | top1:49.8754 | AUROC:0.4998\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.015954\n",
      "Train | 32/32 | Loss:6363.3798 | MainLoss:0.3410 | Alpha:0.4224 | SPLoss:14254.3604 | CLSLoss:809.4820 | top1:50.7355 | AUROC:0.4935\n",
      "Test | 39/32 | Loss:0.3087 | MainLoss:0.3087 | SPLoss:1939.4309 | CLSLoss:289.0087 | top1:50.2564 | AUROC:0.5059\n",
      "Test | 161/32 | Loss:0.3196 | MainLoss:0.3196 | SPLoss:1939.4349 | CLSLoss:289.0083 | top1:49.8723 | AUROC:0.5015\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.015952\n",
      "Train | 32/32 | Loss:6260.6158 | MainLoss:0.3447 | Alpha:0.4205 | SPLoss:13907.2666 | CLSLoss:972.5221 | top1:48.5935 | AUROC:0.4847\n",
      "Test | 39/32 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:4200.7207 | CLSLoss:689.3775 | top1:50.8333 | AUROC:0.5085\n",
      "Test | 161/32 | Loss:0.3143 | MainLoss:0.3143 | SPLoss:4200.7207 | CLSLoss:689.3791 | top1:49.9782 | AUROC:0.5064\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.015949\n",
      "Train | 32/32 | Loss:6378.0854 | MainLoss:0.3547 | Alpha:0.4215 | SPLoss:14406.9512 | CLSLoss:767.8088 | top1:48.3871 | AUROC:0.5082\n",
      "Test | 39/32 | Loss:0.3099 | MainLoss:0.3099 | SPLoss:1507.6696 | CLSLoss:690.8623 | top1:49.0769 | AUROC:0.5095\n",
      "Test | 161/32 | Loss:0.3162 | MainLoss:0.3162 | SPLoss:1507.6670 | CLSLoss:690.8610 | top1:50.0530 | AUROC:0.5039\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.015946\n",
      "Train | 32/32 | Loss:6827.3408 | MainLoss:0.3453 | Alpha:0.4230 | SPLoss:15094.4287 | CLSLoss:985.8087 | top1:50.7355 | AUROC:0.4868\n",
      "Test | 39/32 | Loss:0.3042 | MainLoss:0.3042 | SPLoss:3155.6392 | CLSLoss:990.6852 | top1:50.9744 | AUROC:0.5002\n",
      "Test | 161/32 | Loss:0.3109 | MainLoss:0.3109 | SPLoss:3155.6396 | CLSLoss:990.6874 | top1:50.0903 | AUROC:0.5057\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.015943\n",
      "Train | 32/32 | Loss:5687.7739 | MainLoss:0.3271 | Alpha:0.4232 | SPLoss:12913.6191 | CLSLoss:504.7321 | top1:51.2000 | AUROC:0.4769\n",
      "Test | 39/32 | Loss:0.3074 | MainLoss:0.3074 | SPLoss:2353.8142 | CLSLoss:183.0310 | top1:49.8205 | AUROC:0.5026\n",
      "Test | 161/32 | Loss:0.3119 | MainLoss:0.3119 | SPLoss:2353.8147 | CLSLoss:183.0309 | top1:50.0623 | AUROC:0.4994\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.015940\n",
      "Train | 32/32 | Loss:6436.2428 | MainLoss:0.3337 | Alpha:0.4243 | SPLoss:14196.8525 | CLSLoss:978.0359 | top1:49.5226 | AUROC:0.5232\n",
      "Test | 39/32 | Loss:0.3014 | MainLoss:0.3014 | SPLoss:3336.1206 | CLSLoss:910.5406 | top1:51.3462 | AUROC:0.5034\n",
      "Test | 161/32 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:3336.1206 | CLSLoss:910.5402 | top1:49.7414 | AUROC:0.5076\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.015937\n",
      "Train | 32/32 | Loss:5824.3099 | MainLoss:0.3240 | Alpha:0.4230 | SPLoss:13177.2393 | CLSLoss:564.3699 | top1:49.4452 | AUROC:0.4990\n",
      "Test | 39/32 | Loss:0.3274 | MainLoss:0.3274 | SPLoss:5907.6943 | CLSLoss:272.0039 | top1:50.1410 | AUROC:0.4987\n",
      "Test | 161/32 | Loss:0.3253 | MainLoss:0.3253 | SPLoss:5907.6860 | CLSLoss:272.0044 | top1:50.1308 | AUROC:0.4993\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.015934\n",
      "Train | 32/32 | Loss:6308.4101 | MainLoss:0.3424 | Alpha:0.4191 | SPLoss:14088.7031 | CLSLoss:974.6411 | top1:49.8323 | AUROC:0.4979\n",
      "Test | 39/32 | Loss:0.3051 | MainLoss:0.3051 | SPLoss:3254.8767 | CLSLoss:633.3733 | top1:51.2821 | AUROC:0.5081\n",
      "Test | 161/32 | Loss:0.3132 | MainLoss:0.3132 | SPLoss:3254.8794 | CLSLoss:633.3734 | top1:49.7726 | AUROC:0.5059\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.015930\n",
      "Train | 32/32 | Loss:6027.0778 | MainLoss:0.3299 | Alpha:0.4234 | SPLoss:13406.7207 | CLSLoss:815.1559 | top1:49.8839 | AUROC:0.5061\n",
      "Test | 39/32 | Loss:0.3278 | MainLoss:0.3278 | SPLoss:6472.1450 | CLSLoss:426.2579 | top1:50.3846 | AUROC:0.5158\n",
      "Test | 161/32 | Loss:0.3330 | MainLoss:0.3330 | SPLoss:6472.1548 | CLSLoss:426.2586 | top1:50.1495 | AUROC:0.5067\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.015927\n",
      "Train | 32/32 | Loss:6314.5896 | MainLoss:0.3509 | Alpha:0.4180 | SPLoss:14173.7754 | CLSLoss:947.8530 | top1:50.1935 | AUROC:0.5099\n",
      "Test | 39/32 | Loss:0.3184 | MainLoss:0.3184 | SPLoss:2373.9963 | CLSLoss:604.8783 | top1:50.4103 | AUROC:0.4876\n",
      "Test | 161/32 | Loss:0.3187 | MainLoss:0.3187 | SPLoss:2373.9990 | CLSLoss:604.8792 | top1:50.1246 | AUROC:0.5071\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.015924\n",
      "Train | 32/32 | Loss:6161.4284 | MainLoss:0.3324 | Alpha:0.4219 | SPLoss:13830.7373 | CLSLoss:792.3110 | top1:50.1935 | AUROC:0.4997\n",
      "Test | 39/32 | Loss:0.3251 | MainLoss:0.3251 | SPLoss:1507.3046 | CLSLoss:296.3525 | top1:50.0769 | AUROC:0.5019\n",
      "Test | 161/32 | Loss:0.3305 | MainLoss:0.3305 | SPLoss:1507.3018 | CLSLoss:296.3528 | top1:49.7882 | AUROC:0.5020\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.015920\n",
      "Train | 32/32 | Loss:5988.6173 | MainLoss:0.3373 | Alpha:0.4180 | SPLoss:13457.4580 | CLSLoss:897.1475 | top1:49.3677 | AUROC:0.5055\n",
      "Test | 39/32 | Loss:0.3145 | MainLoss:0.3145 | SPLoss:1106.9277 | CLSLoss:605.0765 | top1:50.3077 | AUROC:0.4963\n",
      "Test | 161/32 | Loss:0.3205 | MainLoss:0.3205 | SPLoss:1106.9288 | CLSLoss:605.0765 | top1:49.6417 | AUROC:0.5049\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.015917\n",
      "Train | 32/32 | Loss:6641.0751 | MainLoss:0.3267 | Alpha:0.4231 | SPLoss:14793.3916 | CLSLoss:937.4764 | top1:51.1742 | AUROC:0.4860\n",
      "Test | 39/32 | Loss:0.3055 | MainLoss:0.3055 | SPLoss:1072.6924 | CLSLoss:444.0463 | top1:49.7308 | AUROC:0.5130\n",
      "Test | 161/32 | Loss:0.3105 | MainLoss:0.3105 | SPLoss:1072.6909 | CLSLoss:444.0461 | top1:50.4548 | AUROC:0.5017\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.015913\n",
      "Train | 32/32 | Loss:6693.9851 | MainLoss:0.3351 | Alpha:0.4244 | SPLoss:14843.6914 | CLSLoss:944.3145 | top1:50.4000 | AUROC:0.4942\n",
      "Test | 39/32 | Loss:0.3067 | MainLoss:0.3067 | SPLoss:1486.4917 | CLSLoss:652.0460 | top1:50.9231 | AUROC:0.4929\n",
      "Test | 161/32 | Loss:0.3089 | MainLoss:0.3089 | SPLoss:1486.4934 | CLSLoss:652.0463 | top1:50.4642 | AUROC:0.5055\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.015909\n",
      "Train | 32/32 | Loss:6419.9452 | MainLoss:0.3282 | Alpha:0.4243 | SPLoss:14139.6572 | CLSLoss:949.0046 | top1:50.4258 | AUROC:0.5012\n",
      "Test | 39/32 | Loss:0.3110 | MainLoss:0.3110 | SPLoss:2640.9602 | CLSLoss:560.0275 | top1:50.3333 | AUROC:0.4897\n",
      "Test | 161/32 | Loss:0.3088 | MainLoss:0.3088 | SPLoss:2640.9604 | CLSLoss:560.0288 | top1:50.1558 | AUROC:0.5040\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.015905\n",
      "Train | 32/32 | Loss:6785.0191 | MainLoss:0.3263 | Alpha:0.4242 | SPLoss:15173.0469 | CLSLoss:873.1270 | top1:50.5806 | AUROC:0.4996\n",
      "Test | 39/32 | Loss:0.3164 | MainLoss:0.3164 | SPLoss:1357.4034 | CLSLoss:441.3300 | top1:49.0385 | AUROC:0.4943\n",
      "Test | 161/32 | Loss:0.3144 | MainLoss:0.3144 | SPLoss:1357.4016 | CLSLoss:441.3300 | top1:49.7445 | AUROC:0.5032\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.015902\n",
      "Train | 32/32 | Loss:6378.6709 | MainLoss:0.3293 | Alpha:0.4224 | SPLoss:14115.2783 | CLSLoss:974.3676 | top1:49.8581 | AUROC:0.4923\n",
      "Test | 39/32 | Loss:0.3077 | MainLoss:0.3077 | SPLoss:1955.7395 | CLSLoss:851.9583 | top1:49.5385 | AUROC:0.5018\n",
      "Test | 161/32 | Loss:0.3109 | MainLoss:0.3109 | SPLoss:1955.7399 | CLSLoss:851.9594 | top1:49.8910 | AUROC:0.4970\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.015898\n",
      "Train | 32/32 | Loss:5854.1405 | MainLoss:0.3050 | Alpha:0.4261 | SPLoss:12963.7109 | CLSLoss:740.6049 | top1:49.9871 | AUROC:0.5178\n",
      "Test | 39/32 | Loss:0.3093 | MainLoss:0.3093 | SPLoss:2944.7334 | CLSLoss:476.5611 | top1:49.8333 | AUROC:0.5047\n",
      "Test | 161/32 | Loss:0.3099 | MainLoss:0.3099 | SPLoss:2944.7317 | CLSLoss:476.5604 | top1:49.9065 | AUROC:0.5105\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.015893\n",
      "Train | 32/32 | Loss:6193.5738 | MainLoss:0.3105 | Alpha:0.4237 | SPLoss:13628.0410 | CLSLoss:997.2315 | top1:50.5290 | AUROC:0.5001\n",
      "Test | 39/32 | Loss:0.3000 | MainLoss:0.3000 | SPLoss:1104.3944 | CLSLoss:656.8625 | top1:50.7308 | AUROC:0.4963\n",
      "Test | 161/32 | Loss:0.3043 | MainLoss:0.3043 | SPLoss:1104.3928 | CLSLoss:656.8630 | top1:50.0125 | AUROC:0.5055\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.015889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:6793.2607 | MainLoss:0.3298 | Alpha:0.4245 | SPLoss:15108.4785 | CLSLoss:878.7673 | top1:49.6258 | AUROC:0.4996\n",
      "Test | 39/32 | Loss:0.3111 | MainLoss:0.3111 | SPLoss:5931.0391 | CLSLoss:433.8425 | top1:49.9744 | AUROC:0.4971\n",
      "Test | 161/32 | Loss:0.3122 | MainLoss:0.3122 | SPLoss:5931.0396 | CLSLoss:433.8435 | top1:49.9844 | AUROC:0.5076\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.015885\n",
      "Train | 32/32 | Loss:6055.3197 | MainLoss:0.3766 | Alpha:0.4210 | SPLoss:13407.1191 | CLSLoss:961.8887 | top1:49.2645 | AUROC:0.4848\n",
      "Test | 39/32 | Loss:0.3032 | MainLoss:0.3032 | SPLoss:2784.0310 | CLSLoss:674.0017 | top1:50.5769 | AUROC:0.5041\n",
      "Test | 161/32 | Loss:0.3074 | MainLoss:0.3074 | SPLoss:2784.0364 | CLSLoss:674.0009 | top1:50.2461 | AUROC:0.4991\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.015881\n",
      "Train | 32/32 | Loss:6747.8408 | MainLoss:0.3424 | Alpha:0.4241 | SPLoss:15148.6465 | CLSLoss:806.3983 | top1:49.7548 | AUROC:0.5167\n",
      "Test | 39/32 | Loss:0.3102 | MainLoss:0.3102 | SPLoss:2243.8286 | CLSLoss:492.8799 | top1:50.0769 | AUROC:0.4945\n",
      "Test | 161/32 | Loss:0.3137 | MainLoss:0.3137 | SPLoss:2243.8259 | CLSLoss:492.8800 | top1:49.8349 | AUROC:0.4995\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.015877\n",
      "Train | 32/32 | Loss:6639.6843 | MainLoss:0.3156 | Alpha:0.4239 | SPLoss:14702.9512 | CLSLoss:956.4578 | top1:50.8129 | AUROC:0.4881\n",
      "Test | 39/32 | Loss:0.3101 | MainLoss:0.3101 | SPLoss:5940.8613 | CLSLoss:837.2192 | top1:49.4872 | AUROC:0.5037\n",
      "Test | 161/32 | Loss:0.3108 | MainLoss:0.3108 | SPLoss:5940.8730 | CLSLoss:837.2198 | top1:50.1526 | AUROC:0.5042\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.015872\n",
      "Train | 32/32 | Loss:6184.4013 | MainLoss:0.3501 | Alpha:0.4211 | SPLoss:14048.0117 | CLSLoss:667.9077 | top1:49.2129 | AUROC:0.4851\n",
      "Test | 39/32 | Loss:0.3234 | MainLoss:0.3234 | SPLoss:1531.8905 | CLSLoss:250.7995 | top1:49.6026 | AUROC:0.4999\n",
      "Test | 161/32 | Loss:0.3272 | MainLoss:0.3272 | SPLoss:1531.8933 | CLSLoss:250.7999 | top1:50.0218 | AUROC:0.4974\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.015868\n",
      "Train | 32/32 | Loss:6287.0200 | MainLoss:0.3345 | Alpha:0.4196 | SPLoss:14087.9463 | CLSLoss:967.4888 | top1:49.4452 | AUROC:0.5038\n",
      "Test | 39/32 | Loss:0.3135 | MainLoss:0.3135 | SPLoss:2502.4033 | CLSLoss:532.0262 | top1:50.2179 | AUROC:0.4919\n",
      "Test | 161/32 | Loss:0.3170 | MainLoss:0.3170 | SPLoss:2502.4011 | CLSLoss:532.0254 | top1:50.2181 | AUROC:0.4947\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.015863\n",
      "Train | 32/32 | Loss:6230.0696 | MainLoss:0.3282 | Alpha:0.4223 | SPLoss:13815.6074 | CLSLoss:958.4713 | top1:49.6516 | AUROC:0.5087\n",
      "Test | 39/32 | Loss:0.3106 | MainLoss:0.3106 | SPLoss:1425.7932 | CLSLoss:316.2142 | top1:49.4744 | AUROC:0.4816\n",
      "Test | 161/32 | Loss:0.3033 | MainLoss:0.3033 | SPLoss:1425.7941 | CLSLoss:316.2146 | top1:50.2243 | AUROC:0.5030\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.015858\n",
      "Train | 32/32 | Loss:6321.7673 | MainLoss:0.3220 | Alpha:0.4252 | SPLoss:13902.5820 | CLSLoss:973.8011 | top1:49.4452 | AUROC:0.5093\n",
      "Test | 39/32 | Loss:0.3047 | MainLoss:0.3047 | SPLoss:1019.5430 | CLSLoss:663.8774 | top1:49.5769 | AUROC:0.5038\n",
      "Test | 161/32 | Loss:0.3123 | MainLoss:0.3123 | SPLoss:1019.5421 | CLSLoss:663.8770 | top1:50.0125 | AUROC:0.4914\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.015854\n",
      "Train | 32/32 | Loss:6529.7695 | MainLoss:0.3189 | Alpha:0.4236 | SPLoss:14599.6328 | CLSLoss:821.2117 | top1:50.1161 | AUROC:0.4870\n",
      "Test | 39/32 | Loss:0.3075 | MainLoss:0.3075 | SPLoss:5757.5854 | CLSLoss:946.5201 | top1:49.5128 | AUROC:0.4922\n",
      "Test | 161/32 | Loss:0.3056 | MainLoss:0.3056 | SPLoss:5757.5942 | CLSLoss:946.5201 | top1:50.0280 | AUROC:0.5002\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.015849\n",
      "Train | 32/32 | Loss:6822.1441 | MainLoss:0.3164 | Alpha:0.4238 | SPLoss:15085.3848 | CLSLoss:1032.7931 | top1:49.5226 | AUROC:0.4890\n",
      "Test | 39/32 | Loss:0.3030 | MainLoss:0.3030 | SPLoss:3987.9973 | CLSLoss:745.2177 | top1:50.5513 | AUROC:0.5025\n",
      "Test | 161/32 | Loss:0.3058 | MainLoss:0.3058 | SPLoss:3987.9995 | CLSLoss:745.2192 | top1:49.9252 | AUROC:0.5058\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.015844\n",
      "Train | 32/32 | Loss:6741.2006 | MainLoss:0.3233 | Alpha:0.4231 | SPLoss:15067.7461 | CLSLoss:832.0486 | top1:49.9613 | AUROC:0.4970\n",
      "Test | 39/32 | Loss:0.3106 | MainLoss:0.3106 | SPLoss:3845.6997 | CLSLoss:518.8953 | top1:50.2821 | AUROC:0.5001\n",
      "Test | 161/32 | Loss:0.3165 | MainLoss:0.3165 | SPLoss:3845.6909 | CLSLoss:518.8951 | top1:49.9315 | AUROC:0.5015\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.015839\n",
      "Train | 32/32 | Loss:6441.6364 | MainLoss:0.3248 | Alpha:0.4210 | SPLoss:14363.5703 | CLSLoss:952.0123 | top1:48.7226 | AUROC:0.4955\n",
      "Test | 39/32 | Loss:0.3078 | MainLoss:0.3078 | SPLoss:2017.9550 | CLSLoss:583.1072 | top1:50.3333 | AUROC:0.5009\n",
      "Test | 161/32 | Loss:0.3113 | MainLoss:0.3113 | SPLoss:2017.9587 | CLSLoss:583.1066 | top1:50.1620 | AUROC:0.5013\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.015834\n",
      "Train | 32/32 | Loss:6697.4206 | MainLoss:0.3113 | Alpha:0.4245 | SPLoss:14912.1191 | CLSLoss:828.7701 | top1:49.9871 | AUROC:0.4914\n",
      "Test | 39/32 | Loss:0.3011 | MainLoss:0.3011 | SPLoss:4662.6670 | CLSLoss:506.4874 | top1:50.8205 | AUROC:0.5031\n",
      "Test | 161/32 | Loss:0.3085 | MainLoss:0.3085 | SPLoss:4662.6689 | CLSLoss:506.4863 | top1:49.7259 | AUROC:0.5048\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.015829\n",
      "Train | 32/32 | Loss:6453.8417 | MainLoss:0.3123 | Alpha:0.4249 | SPLoss:14266.2402 | CLSLoss:891.9114 | top1:50.7355 | AUROC:0.5063\n",
      "Test | 39/32 | Loss:0.3036 | MainLoss:0.3036 | SPLoss:2721.7468 | CLSLoss:619.7499 | top1:50.1026 | AUROC:0.5025\n",
      "Test | 161/32 | Loss:0.3051 | MainLoss:0.3051 | SPLoss:2721.7478 | CLSLoss:619.7499 | top1:49.9315 | AUROC:0.4988\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.015823\n",
      "Train | 32/32 | Loss:6164.6939 | MainLoss:0.3207 | Alpha:0.4235 | SPLoss:13820.1338 | CLSLoss:734.5037 | top1:49.8323 | AUROC:0.4916\n",
      "Test | 39/32 | Loss:0.3054 | MainLoss:0.3054 | SPLoss:1565.9250 | CLSLoss:465.0196 | top1:50.2949 | AUROC:0.4963\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b7f0df6e1031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_target_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msource_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_source_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-6c4d3bdd3924>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(val_loader, model, criterion, epoch, use_cuda)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moutputs0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutputs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_se\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mx_squeezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mx_squeezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_se_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_se_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_squeezed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_squeezed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):    \n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    \n",
    "    test_loss, test_acc,test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss, source_loss, train_acc, test_acc, source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
