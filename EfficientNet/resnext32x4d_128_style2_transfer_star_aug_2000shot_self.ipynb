{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import resnext50_32x4d\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style2/128/32x4d/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'resnext32x4d' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 400\n",
    "test_batch = 400\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/32x4d/to_star/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = 'fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'star/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style2/128/32x4d/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "student_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 22.98M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += 0.1*sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 10/10 | Loss:1.2183 | MainLoss:1.2175 | Alpha:0.0015 | SPLoss:0.5179 | CLSLoss:4.5438 | top1:50.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7556 | MainLoss:0.7556 | SPLoss:0.7662 | CLSLoss:4.3566 | top1:50.4358 | AUROC:0.5070\n",
      "Test | 20/10 | Loss:0.2605 | MainLoss:0.2605 | SPLoss:0.7662 | CLSLoss:4.3566 | top1:89.8333 | AUROC:0.9997\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 10/10 | Loss:0.7123 | MainLoss:0.7119 | Alpha:0.0325 | SPLoss:0.0113 | CLSLoss:4.3124 | top1:52.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7140 | MainLoss:0.7140 | SPLoss:0.0236 | CLSLoss:4.2818 | top1:50.5079 | AUROC:0.5105\n",
      "Test | 20/10 | Loss:0.3025 | MainLoss:0.3025 | SPLoss:0.0236 | CLSLoss:4.2818 | top1:88.3333 | AUROC:0.9994\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 10/10 | Loss:0.6977 | MainLoss:0.6976 | Alpha:0.0331 | SPLoss:0.0038 | CLSLoss:4.2636 | top1:52.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7026 | MainLoss:0.7026 | SPLoss:0.0093 | CLSLoss:4.2469 | top1:50.6619 | AUROC:0.5130\n",
      "Test | 20/10 | Loss:0.3249 | MainLoss:0.3249 | SPLoss:0.0093 | CLSLoss:4.2469 | top1:87.6538 | AUROC:0.9993\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 10/10 | Loss:0.6949 | MainLoss:0.6948 | Alpha:0.0333 | SPLoss:0.0024 | CLSLoss:4.2342 | top1:51.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6984 | MainLoss:0.6984 | SPLoss:0.0047 | CLSLoss:4.2264 | top1:50.8847 | AUROC:0.5151\n",
      "Test | 20/10 | Loss:0.3408 | MainLoss:0.3408 | SPLoss:0.0047 | CLSLoss:4.2264 | top1:86.4103 | AUROC:0.9993\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 10/10 | Loss:0.6903 | MainLoss:0.6902 | Alpha:0.0334 | SPLoss:0.0018 | CLSLoss:4.2200 | top1:53.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6973 | MainLoss:0.6973 | SPLoss:0.0036 | CLSLoss:4.2194 | top1:51.0682 | AUROC:0.5179\n",
      "Test | 20/10 | Loss:0.3512 | MainLoss:0.3512 | SPLoss:0.0036 | CLSLoss:4.2194 | top1:84.5769 | AUROC:0.9992\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 10/10 | Loss:0.6920 | MainLoss:0.6919 | Alpha:0.0334 | SPLoss:0.0024 | CLSLoss:4.2099 | top1:53.3500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6960 | MainLoss:0.6960 | SPLoss:0.0039 | CLSLoss:4.2088 | top1:51.4613 | AUROC:0.5210\n",
      "Test | 20/10 | Loss:0.3613 | MainLoss:0.3613 | SPLoss:0.0039 | CLSLoss:4.2088 | top1:82.6282 | AUROC:0.9991\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 10/10 | Loss:0.6891 | MainLoss:0.6890 | Alpha:0.0334 | SPLoss:0.0023 | CLSLoss:4.2079 | top1:53.2750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6953 | MainLoss:0.6953 | SPLoss:0.0046 | CLSLoss:4.2049 | top1:51.5564 | AUROC:0.5244\n",
      "Test | 20/10 | Loss:0.3632 | MainLoss:0.3632 | SPLoss:0.0046 | CLSLoss:4.2049 | top1:82.7692 | AUROC:0.9990\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 10/10 | Loss:0.6888 | MainLoss:0.6887 | Alpha:0.0334 | SPLoss:0.0026 | CLSLoss:4.2049 | top1:54.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6949 | MainLoss:0.6949 | SPLoss:0.0057 | CLSLoss:4.2016 | top1:51.5203 | AUROC:0.5276\n",
      "Test | 20/10 | Loss:0.3471 | MainLoss:0.3471 | SPLoss:0.0057 | CLSLoss:4.2016 | top1:87.7051 | AUROC:0.9990\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 10/10 | Loss:0.6880 | MainLoss:0.6879 | Alpha:0.0335 | SPLoss:0.0031 | CLSLoss:4.2023 | top1:54.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6948 | MainLoss:0.6948 | SPLoss:0.0061 | CLSLoss:4.1999 | top1:51.5170 | AUROC:0.5320\n",
      "Test | 20/10 | Loss:0.3410 | MainLoss:0.3410 | SPLoss:0.0061 | CLSLoss:4.1999 | top1:89.3205 | AUROC:0.9990\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 10/10 | Loss:0.6868 | MainLoss:0.6866 | Alpha:0.0335 | SPLoss:0.0041 | CLSLoss:4.2018 | top1:53.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6950 | MainLoss:0.6950 | SPLoss:0.0104 | CLSLoss:4.2013 | top1:52.8277 | AUROC:0.5380\n",
      "Test | 20/10 | Loss:0.3764 | MainLoss:0.3764 | SPLoss:0.0104 | CLSLoss:4.2013 | top1:75.6538 | AUROC:0.9991\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.6860 | MainLoss:0.6858 | Alpha:0.0335 | SPLoss:0.0059 | CLSLoss:4.2039 | top1:55.2500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6926 | MainLoss:0.6926 | SPLoss:0.0113 | CLSLoss:4.1988 | top1:53.4699 | AUROC:0.5441\n",
      "Test | 20/10 | Loss:0.3663 | MainLoss:0.3663 | SPLoss:0.0113 | CLSLoss:4.1988 | top1:80.0513 | AUROC:0.9990\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.6822 | MainLoss:0.6820 | Alpha:0.0336 | SPLoss:0.0061 | CLSLoss:4.2066 | top1:55.3500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6913 | MainLoss:0.6913 | SPLoss:0.0130 | CLSLoss:4.2098 | top1:53.8991 | AUROC:0.5512\n",
      "Test | 20/10 | Loss:0.3534 | MainLoss:0.3534 | SPLoss:0.0130 | CLSLoss:4.2098 | top1:82.9231 | AUROC:0.9990\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 10/10 | Loss:0.6771 | MainLoss:0.6769 | Alpha:0.0337 | SPLoss:0.0064 | CLSLoss:4.2155 | top1:57.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6940 | MainLoss:0.6940 | SPLoss:0.0165 | CLSLoss:4.2325 | top1:53.2536 | AUROC:0.5560\n",
      "Test | 20/10 | Loss:0.3158 | MainLoss:0.3158 | SPLoss:0.0165 | CLSLoss:4.2325 | top1:88.9744 | AUROC:0.9989\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 10/10 | Loss:0.6776 | MainLoss:0.6773 | Alpha:0.0337 | SPLoss:0.0094 | CLSLoss:4.2322 | top1:56.8000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6897 | MainLoss:0.6897 | SPLoss:0.0195 | CLSLoss:4.2338 | top1:54.4790 | AUROC:0.5659\n",
      "Test | 20/10 | Loss:0.3326 | MainLoss:0.3326 | SPLoss:0.0195 | CLSLoss:4.2338 | top1:85.7308 | AUROC:0.9987\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 10/10 | Loss:0.6735 | MainLoss:0.6731 | Alpha:0.0338 | SPLoss:0.0109 | CLSLoss:4.2474 | top1:57.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6935 | MainLoss:0.6935 | SPLoss:0.0246 | CLSLoss:4.2437 | top1:53.9515 | AUROC:0.5742\n",
      "Test | 20/10 | Loss:0.3031 | MainLoss:0.3031 | SPLoss:0.0246 | CLSLoss:4.2437 | top1:91.0385 | AUROC:0.9985\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 10/10 | Loss:0.6704 | MainLoss:0.6700 | Alpha:0.0338 | SPLoss:0.0128 | CLSLoss:4.2490 | top1:59.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6852 | MainLoss:0.6852 | SPLoss:0.0251 | CLSLoss:4.2511 | top1:55.8748 | AUROC:0.5856\n",
      "Test | 20/10 | Loss:0.3252 | MainLoss:0.3252 | SPLoss:0.0251 | CLSLoss:4.2511 | top1:86.9103 | AUROC:0.9981\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 10/10 | Loss:0.6614 | MainLoss:0.6609 | Alpha:0.0340 | SPLoss:0.0132 | CLSLoss:4.2709 | top1:59.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6808 | MainLoss:0.6808 | SPLoss:0.0292 | CLSLoss:4.2769 | top1:57.0151 | AUROC:0.5977\n",
      "Test | 20/10 | Loss:0.3289 | MainLoss:0.3289 | SPLoss:0.0292 | CLSLoss:4.2769 | top1:85.5897 | AUROC:0.9974\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 10/10 | Loss:0.6537 | MainLoss:0.6532 | Alpha:0.0343 | SPLoss:0.0144 | CLSLoss:4.2846 | top1:60.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6898 | MainLoss:0.6898 | SPLoss:0.0340 | CLSLoss:4.3012 | top1:56.3073 | AUROC:0.6036\n",
      "Test | 20/10 | Loss:0.2814 | MainLoss:0.2814 | SPLoss:0.0340 | CLSLoss:4.3011 | top1:90.1154 | AUROC:0.9972\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 10/10 | Loss:0.6512 | MainLoss:0.6504 | Alpha:0.0344 | SPLoss:0.0228 | CLSLoss:4.3096 | top1:60.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6940 | MainLoss:0.6940 | SPLoss:0.0432 | CLSLoss:4.2860 | top1:55.6782 | AUROC:0.6103\n",
      "Test | 20/10 | Loss:0.2592 | MainLoss:0.2592 | SPLoss:0.0432 | CLSLoss:4.2860 | top1:93.2821 | AUROC:0.9968\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 10/10 | Loss:0.6382 | MainLoss:0.6374 | Alpha:0.0343 | SPLoss:0.0244 | CLSLoss:4.3075 | top1:62.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6752 | MainLoss:0.6752 | SPLoss:0.0524 | CLSLoss:4.3085 | top1:58.7746 | AUROC:0.6217\n",
      "Test | 20/10 | Loss:0.3256 | MainLoss:0.3256 | SPLoss:0.0524 | CLSLoss:4.3085 | top1:84.2308 | AUROC:0.9958\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 10/10 | Loss:0.6309 | MainLoss:0.6300 | Alpha:0.0349 | SPLoss:0.0258 | CLSLoss:4.3283 | top1:64.2750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6727 | MainLoss:0.6727 | SPLoss:0.0532 | CLSLoss:4.3132 | top1:59.2857 | AUROC:0.6302\n",
      "Test | 20/10 | Loss:0.2967 | MainLoss:0.2967 | SPLoss:0.0532 | CLSLoss:4.3132 | top1:87.5000 | AUROC:0.9953\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 10/10 | Loss:0.6208 | MainLoss:0.6198 | Alpha:0.0351 | SPLoss:0.0274 | CLSLoss:4.3226 | top1:65.7500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6707 | MainLoss:0.6707 | SPLoss:0.0573 | CLSLoss:4.3381 | top1:60.1638 | AUROC:0.6420\n",
      "Test | 20/10 | Loss:0.2979 | MainLoss:0.2979 | SPLoss:0.0573 | CLSLoss:4.3381 | top1:86.3462 | AUROC:0.9940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 10/10 | Loss:0.6073 | MainLoss:0.6062 | Alpha:0.0354 | SPLoss:0.0318 | CLSLoss:4.3517 | top1:66.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6977 | MainLoss:0.6977 | SPLoss:0.0704 | CLSLoss:4.3583 | top1:58.3322 | AUROC:0.6486\n",
      "Test | 20/10 | Loss:0.2291 | MainLoss:0.2291 | SPLoss:0.0704 | CLSLoss:4.3583 | top1:92.7564 | AUROC:0.9940\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 10/10 | Loss:0.6062 | MainLoss:0.6048 | Alpha:0.0351 | SPLoss:0.0391 | CLSLoss:4.3627 | top1:66.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6925 | MainLoss:0.6925 | SPLoss:0.0659 | CLSLoss:4.3539 | top1:58.3486 | AUROC:0.6543\n",
      "Test | 20/10 | Loss:0.2310 | MainLoss:0.2310 | SPLoss:0.0659 | CLSLoss:4.3539 | top1:92.9744 | AUROC:0.9936\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 10/10 | Loss:0.6216 | MainLoss:0.6200 | Alpha:0.0352 | SPLoss:0.0459 | CLSLoss:4.3285 | top1:64.0250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6724 | MainLoss:0.6724 | SPLoss:0.0770 | CLSLoss:4.3281 | top1:61.1730 | AUROC:0.6588\n",
      "Test | 20/10 | Loss:0.3430 | MainLoss:0.3430 | SPLoss:0.0770 | CLSLoss:4.3281 | top1:82.9744 | AUROC:0.9907\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 10/10 | Loss:0.5805 | MainLoss:0.5789 | Alpha:0.0358 | SPLoss:0.0427 | CLSLoss:4.3550 | top1:68.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6644 | MainLoss:0.6644 | SPLoss:0.0944 | CLSLoss:4.3472 | top1:62.0085 | AUROC:0.6707\n",
      "Test | 20/10 | Loss:0.3552 | MainLoss:0.3552 | SPLoss:0.0944 | CLSLoss:4.3472 | top1:81.5256 | AUROC:0.9900\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 10/10 | Loss:0.5614 | MainLoss:0.5598 | Alpha:0.0362 | SPLoss:0.0457 | CLSLoss:4.3787 | top1:69.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6539 | MainLoss:0.6539 | SPLoss:0.0911 | CLSLoss:4.3867 | top1:63.0374 | AUROC:0.6816\n",
      "Test | 20/10 | Loss:0.3185 | MainLoss:0.3185 | SPLoss:0.0911 | CLSLoss:4.3867 | top1:84.5513 | AUROC:0.9900\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 10/10 | Loss:0.5960 | MainLoss:0.5932 | Alpha:0.0366 | SPLoss:0.0759 | CLSLoss:4.3213 | top1:69.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6637 | MainLoss:0.6637 | SPLoss:0.0942 | CLSLoss:4.3504 | top1:62.8997 | AUROC:0.6801\n",
      "Test | 20/10 | Loss:0.3026 | MainLoss:0.3026 | SPLoss:0.0942 | CLSLoss:4.3504 | top1:86.4103 | AUROC:0.9881\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 10/10 | Loss:0.5512 | MainLoss:0.5492 | Alpha:0.0369 | SPLoss:0.0553 | CLSLoss:4.3731 | top1:70.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7789 | MainLoss:0.7789 | SPLoss:0.1424 | CLSLoss:4.3473 | top1:56.3663 | AUROC:0.6776\n",
      "Test | 20/10 | Loss:0.2008 | MainLoss:0.2008 | SPLoss:0.1424 | CLSLoss:4.3473 | top1:94.1667 | AUROC:0.9905\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 10/10 | Loss:0.5669 | MainLoss:0.5643 | Alpha:0.0344 | SPLoss:0.0763 | CLSLoss:4.3394 | top1:69.6250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6566 | MainLoss:0.6566 | SPLoss:0.1432 | CLSLoss:4.3347 | top1:63.5714 | AUROC:0.6861\n",
      "Test | 20/10 | Loss:0.3185 | MainLoss:0.3185 | SPLoss:0.1432 | CLSLoss:4.3347 | top1:85.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 10/10 | Loss:0.5685 | MainLoss:0.5658 | Alpha:0.0374 | SPLoss:0.0713 | CLSLoss:4.3257 | top1:69.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6572 | MainLoss:0.6572 | SPLoss:0.1077 | CLSLoss:4.2982 | top1:63.6894 | AUROC:0.6883\n",
      "Test | 20/10 | Loss:0.3501 | MainLoss:0.3501 | SPLoss:0.1077 | CLSLoss:4.2982 | top1:83.1410 | AUROC:0.9867\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 10/10 | Loss:0.5050 | MainLoss:0.5022 | Alpha:0.0375 | SPLoss:0.0758 | CLSLoss:4.3649 | top1:75.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7200 | MainLoss:0.7200 | SPLoss:0.1620 | CLSLoss:4.4138 | top1:63.4731 | AUROC:0.6990\n",
      "Test | 20/10 | Loss:0.3911 | MainLoss:0.3911 | SPLoss:0.1620 | CLSLoss:4.4138 | top1:81.1667 | AUROC:0.9879\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 10/10 | Loss:0.5846 | MainLoss:0.5786 | Alpha:0.0374 | SPLoss:0.1615 | CLSLoss:4.2876 | top1:68.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6819 | MainLoss:0.6819 | SPLoss:0.1618 | CLSLoss:4.3169 | top1:63.8434 | AUROC:0.6966\n",
      "Test | 20/10 | Loss:0.2834 | MainLoss:0.2834 | SPLoss:0.1618 | CLSLoss:4.3169 | top1:87.2564 | AUROC:0.9888\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 10/10 | Loss:0.5287 | MainLoss:0.5248 | Alpha:0.0382 | SPLoss:0.1017 | CLSLoss:4.3115 | top1:73.4250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7016 | MainLoss:0.7016 | SPLoss:0.2039 | CLSLoss:4.2672 | top1:61.2647 | AUROC:0.6963\n",
      "Test | 20/10 | Loss:0.2496 | MainLoss:0.2496 | SPLoss:0.2039 | CLSLoss:4.2672 | top1:89.8205 | AUROC:0.9877\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 10/10 | Loss:0.5233 | MainLoss:0.5198 | Alpha:0.0371 | SPLoss:0.0941 | CLSLoss:4.2896 | top1:74.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6582 | MainLoss:0.6582 | SPLoss:0.1584 | CLSLoss:4.2765 | top1:64.9410 | AUROC:0.7043\n",
      "Test | 20/10 | Loss:0.3440 | MainLoss:0.3440 | SPLoss:0.1584 | CLSLoss:4.2765 | top1:83.3718 | AUROC:0.9831\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 10/10 | Loss:0.4849 | MainLoss:0.4807 | Alpha:0.0385 | SPLoss:0.1079 | CLSLoss:4.3385 | top1:75.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0299 | MainLoss:1.0299 | SPLoss:0.3283 | CLSLoss:4.2909 | top1:52.7654 | AUROC:0.6848\n",
      "Test | 20/10 | Loss:0.6578 | MainLoss:0.6578 | SPLoss:0.3283 | CLSLoss:4.2909 | top1:63.1410 | AUROC:0.9837\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 10/10 | Loss:0.5291 | MainLoss:0.5231 | Alpha:0.0308 | SPLoss:0.1959 | CLSLoss:4.2515 | top1:74.6000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7797 | MainLoss:0.7797 | SPLoss:0.2037 | CLSLoss:4.3083 | top1:61.2156 | AUROC:0.7002\n",
      "Test | 20/10 | Loss:0.4530 | MainLoss:0.4530 | SPLoss:0.2037 | CLSLoss:4.3083 | top1:77.5769 | AUROC:0.9864\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 10/10 | Loss:0.5035 | MainLoss:0.4987 | Alpha:0.0370 | SPLoss:0.1292 | CLSLoss:4.3108 | top1:74.7500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7244 | MainLoss:0.7244 | SPLoss:0.2084 | CLSLoss:4.2503 | top1:61.5236 | AUROC:0.7044\n",
      "Test | 20/10 | Loss:0.4660 | MainLoss:0.4660 | SPLoss:0.2084 | CLSLoss:4.2503 | top1:74.9744 | AUROC:0.9818\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 10/10 | Loss:0.4270 | MainLoss:0.4222 | Alpha:0.0374 | SPLoss:0.1270 | CLSLoss:4.3345 | top1:80.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7321 | MainLoss:0.7321 | SPLoss:0.2759 | CLSLoss:4.4380 | top1:66.3696 | AUROC:0.7224\n",
      "Test | 20/10 | Loss:0.3402 | MainLoss:0.3402 | SPLoss:0.2759 | CLSLoss:4.4380 | top1:85.2179 | AUROC:0.9816\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 10/10 | Loss:0.5270 | MainLoss:0.5164 | Alpha:0.0399 | SPLoss:0.2663 | CLSLoss:4.3373 | top1:74.6000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6500 | MainLoss:0.6500 | SPLoss:0.4298 | CLSLoss:4.2281 | top1:65.3506 | AUROC:0.7105\n",
      "Test | 20/10 | Loss:0.3602 | MainLoss:0.3602 | SPLoss:0.4298 | CLSLoss:4.2281 | top1:83.3462 | AUROC:0.9745\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 10/10 | Loss:0.4042 | MainLoss:0.3985 | Alpha:0.0394 | SPLoss:0.1430 | CLSLoss:4.3553 | top1:81.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7175 | MainLoss:0.7175 | SPLoss:0.3364 | CLSLoss:4.4300 | top1:65.5341 | AUROC:0.7156\n",
      "Test | 20/10 | Loss:0.3784 | MainLoss:0.3784 | SPLoss:0.3364 | CLSLoss:4.4300 | top1:82.7692 | AUROC:0.9819\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 10/10 | Loss:0.4768 | MainLoss:0.4692 | Alpha:0.0404 | SPLoss:0.1882 | CLSLoss:4.3467 | top1:77.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7425 | MainLoss:0.7425 | SPLoss:0.2671 | CLSLoss:4.3743 | top1:65.0426 | AUROC:0.7202\n",
      "Test | 20/10 | Loss:0.2847 | MainLoss:0.2847 | SPLoss:0.2671 | CLSLoss:4.3743 | top1:87.5769 | AUROC:0.9828\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 10/10 | Loss:0.4584 | MainLoss:0.4512 | Alpha:0.0402 | SPLoss:0.1772 | CLSLoss:4.3334 | top1:78.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7493 | MainLoss:0.7493 | SPLoss:0.2980 | CLSLoss:4.3240 | top1:64.0465 | AUROC:0.7179\n",
      "Test | 20/10 | Loss:0.4689 | MainLoss:0.4689 | SPLoss:0.2980 | CLSLoss:4.3240 | top1:77.9487 | AUROC:0.9815\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 10/10 | Loss:0.4453 | MainLoss:0.4380 | Alpha:0.0398 | SPLoss:0.1843 | CLSLoss:4.3448 | top1:79.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.6629 | MainLoss:0.6629 | SPLoss:0.3803 | CLSLoss:4.2837 | top1:65.9502 | AUROC:0.7207\n",
      "Test | 20/10 | Loss:0.3732 | MainLoss:0.3732 | SPLoss:0.3803 | CLSLoss:4.2837 | top1:82.3205 | AUROC:0.9771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 10/10 | Loss:0.4106 | MainLoss:0.4039 | Alpha:0.0406 | SPLoss:0.1622 | CLSLoss:4.3751 | top1:81.4750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8215 | MainLoss:0.8215 | SPLoss:0.3786 | CLSLoss:4.3407 | top1:61.5170 | AUROC:0.7105\n",
      "Test | 20/10 | Loss:0.5588 | MainLoss:0.5588 | SPLoss:0.3786 | CLSLoss:4.3407 | top1:72.9744 | AUROC:0.9758\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 10/10 | Loss:0.4363 | MainLoss:0.4286 | Alpha:0.0383 | SPLoss:0.1997 | CLSLoss:4.3578 | top1:79.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7093 | MainLoss:0.7093 | SPLoss:0.3506 | CLSLoss:4.4247 | top1:66.9725 | AUROC:0.7319\n",
      "Test | 20/10 | Loss:0.3518 | MainLoss:0.3518 | SPLoss:0.3506 | CLSLoss:4.4247 | top1:84.5513 | AUROC:0.9769\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 10/10 | Loss:0.4509 | MainLoss:0.4400 | Alpha:0.0418 | SPLoss:0.2595 | CLSLoss:4.3739 | top1:79.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7238 | MainLoss:0.7238 | SPLoss:0.3879 | CLSLoss:4.3779 | top1:66.0813 | AUROC:0.7273\n",
      "Test | 20/10 | Loss:0.3004 | MainLoss:0.3004 | SPLoss:0.3879 | CLSLoss:4.3778 | top1:87.0385 | AUROC:0.9815\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 10/10 | Loss:0.3501 | MainLoss:0.3410 | Alpha:0.0419 | SPLoss:0.2180 | CLSLoss:4.4313 | top1:84.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7871 | MainLoss:0.7871 | SPLoss:0.3997 | CLSLoss:4.5201 | top1:66.6448 | AUROC:0.7350\n",
      "Test | 20/10 | Loss:0.4680 | MainLoss:0.4680 | SPLoss:0.3997 | CLSLoss:4.5201 | top1:81.0513 | AUROC:0.9779\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 10/10 | Loss:0.4355 | MainLoss:0.4217 | Alpha:0.0425 | SPLoss:0.3249 | CLSLoss:4.4129 | top1:79.9000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7397 | MainLoss:0.7397 | SPLoss:0.4165 | CLSLoss:4.4351 | top1:66.8054 | AUROC:0.7300\n",
      "Test | 20/10 | Loss:0.3869 | MainLoss:0.3869 | SPLoss:0.4165 | CLSLoss:4.4351 | top1:83.6410 | AUROC:0.9771\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 10/10 | Loss:0.3442 | MainLoss:0.3355 | Alpha:0.0427 | SPLoss:0.2049 | CLSLoss:4.5106 | top1:85.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8501 | MainLoss:0.8501 | SPLoss:0.4950 | CLSLoss:4.4262 | top1:62.8735 | AUROC:0.7222\n",
      "Test | 20/10 | Loss:0.5790 | MainLoss:0.5790 | SPLoss:0.4950 | CLSLoss:4.4262 | top1:74.4872 | AUROC:0.9675\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 10/10 | Loss:0.3516 | MainLoss:0.3429 | Alpha:0.0398 | SPLoss:0.2164 | CLSLoss:4.4753 | top1:84.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7472 | MainLoss:0.7472 | SPLoss:0.3879 | CLSLoss:4.5111 | top1:67.4476 | AUROC:0.7390\n",
      "Test | 20/10 | Loss:0.4141 | MainLoss:0.4141 | SPLoss:0.3879 | CLSLoss:4.5110 | top1:82.6410 | AUROC:0.9733\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 10/10 | Loss:0.2677 | MainLoss:0.2676 | Alpha:0.0433 | SPLoss:0.0025 | CLSLoss:4.5259 | top1:89.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7588 | MainLoss:0.7588 | SPLoss:0.0061 | CLSLoss:4.5384 | top1:67.6638 | AUROC:0.7418\n",
      "Test | 20/10 | Loss:0.4020 | MainLoss:0.4020 | SPLoss:0.0061 | CLSLoss:4.5384 | top1:83.6026 | AUROC:0.9728\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 10/10 | Loss:0.2379 | MainLoss:0.2378 | Alpha:0.0440 | SPLoss:0.0023 | CLSLoss:4.5522 | top1:90.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7776 | MainLoss:0.7776 | SPLoss:0.0055 | CLSLoss:4.5678 | top1:67.8014 | AUROC:0.7429\n",
      "Test | 20/10 | Loss:0.4061 | MainLoss:0.4061 | SPLoss:0.0055 | CLSLoss:4.5678 | top1:83.7436 | AUROC:0.9722\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 10/10 | Loss:0.2379 | MainLoss:0.2378 | Alpha:0.0441 | SPLoss:0.0023 | CLSLoss:4.5790 | top1:90.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.7954 | MainLoss:0.7954 | SPLoss:0.0055 | CLSLoss:4.5911 | top1:67.8932 | AUROC:0.7434\n",
      "Test | 20/10 | Loss:0.4185 | MainLoss:0.4185 | SPLoss:0.0055 | CLSLoss:4.5911 | top1:83.5000 | AUROC:0.9717\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 10/10 | Loss:0.2412 | MainLoss:0.2411 | Alpha:0.0440 | SPLoss:0.0031 | CLSLoss:4.5976 | top1:90.9250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8044 | MainLoss:0.8044 | SPLoss:0.0063 | CLSLoss:4.6097 | top1:68.2077 | AUROC:0.7458\n",
      "Test | 20/10 | Loss:0.4162 | MainLoss:0.4162 | SPLoss:0.0063 | CLSLoss:4.6097 | top1:84.0000 | AUROC:0.9712\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 10/10 | Loss:0.2539 | MainLoss:0.2537 | Alpha:0.0437 | SPLoss:0.0036 | CLSLoss:4.6138 | top1:89.6000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8088 | MainLoss:0.8088 | SPLoss:0.0075 | CLSLoss:4.6206 | top1:68.2143 | AUROC:0.7463\n",
      "Test | 20/10 | Loss:0.4332 | MainLoss:0.4332 | SPLoss:0.0075 | CLSLoss:4.6206 | top1:83.4103 | AUROC:0.9708\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 10/10 | Loss:0.2401 | MainLoss:0.2400 | Alpha:0.0440 | SPLoss:0.0031 | CLSLoss:4.6262 | top1:90.4500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8062 | MainLoss:0.8062 | SPLoss:0.0072 | CLSLoss:4.6318 | top1:68.4699 | AUROC:0.7467\n",
      "Test | 20/10 | Loss:0.4035 | MainLoss:0.4035 | SPLoss:0.0072 | CLSLoss:4.6318 | top1:84.3846 | AUROC:0.9706\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 10/10 | Loss:0.2423 | MainLoss:0.2421 | Alpha:0.0440 | SPLoss:0.0031 | CLSLoss:4.6375 | top1:90.3750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8103 | MainLoss:0.8103 | SPLoss:0.0066 | CLSLoss:4.6429 | top1:68.2929 | AUROC:0.7483\n",
      "Test | 20/10 | Loss:0.4211 | MainLoss:0.4211 | SPLoss:0.0066 | CLSLoss:4.6429 | top1:83.9487 | AUROC:0.9694\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 10/10 | Loss:0.2211 | MainLoss:0.2210 | Alpha:0.0445 | SPLoss:0.0023 | CLSLoss:4.6506 | top1:91.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8222 | MainLoss:0.8222 | SPLoss:0.0052 | CLSLoss:4.6591 | top1:68.3191 | AUROC:0.7489\n",
      "Test | 20/10 | Loss:0.4336 | MainLoss:0.4336 | SPLoss:0.0052 | CLSLoss:4.6591 | top1:83.6923 | AUROC:0.9689\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 10/10 | Loss:0.1897 | MainLoss:0.1896 | Alpha:0.0453 | SPLoss:0.0024 | CLSLoss:4.6693 | top1:92.4250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8390 | MainLoss:0.8390 | SPLoss:0.0061 | CLSLoss:4.6826 | top1:68.4535 | AUROC:0.7492\n",
      "Test | 20/10 | Loss:0.4153 | MainLoss:0.4153 | SPLoss:0.0061 | CLSLoss:4.6826 | top1:84.5385 | AUROC:0.9691\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 10/10 | Loss:0.2043 | MainLoss:0.2042 | Alpha:0.0449 | SPLoss:0.0027 | CLSLoss:4.6912 | top1:91.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8454 | MainLoss:0.8454 | SPLoss:0.0069 | CLSLoss:4.6954 | top1:68.4666 | AUROC:0.7493\n",
      "Test | 20/10 | Loss:0.4319 | MainLoss:0.4319 | SPLoss:0.0069 | CLSLoss:4.6954 | top1:84.0769 | AUROC:0.9695\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 10/10 | Loss:0.1915 | MainLoss:0.1914 | Alpha:0.0453 | SPLoss:0.0025 | CLSLoss:4.7048 | top1:92.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8526 | MainLoss:0.8526 | SPLoss:0.0058 | CLSLoss:4.7122 | top1:68.5944 | AUROC:0.7513\n",
      "Test | 20/10 | Loss:0.4419 | MainLoss:0.4419 | SPLoss:0.0058 | CLSLoss:4.7122 | top1:83.8846 | AUROC:0.9675\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 10/10 | Loss:0.2012 | MainLoss:0.2010 | Alpha:0.0450 | SPLoss:0.0039 | CLSLoss:4.7149 | top1:91.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8563 | MainLoss:0.8563 | SPLoss:0.0070 | CLSLoss:4.7259 | top1:68.5878 | AUROC:0.7514\n",
      "Test | 20/10 | Loss:0.4356 | MainLoss:0.4356 | SPLoss:0.0070 | CLSLoss:4.7259 | top1:84.1410 | AUROC:0.9678\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 10/10 | Loss:0.1743 | MainLoss:0.1742 | Alpha:0.0457 | SPLoss:0.0026 | CLSLoss:4.7347 | top1:93.4500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8731 | MainLoss:0.8731 | SPLoss:0.0062 | CLSLoss:4.7436 | top1:68.6566 | AUROC:0.7519\n",
      "Test | 20/10 | Loss:0.4504 | MainLoss:0.4504 | SPLoss:0.0062 | CLSLoss:4.7436 | top1:83.8718 | AUROC:0.9666\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 10/10 | Loss:0.1942 | MainLoss:0.1941 | Alpha:0.0452 | SPLoss:0.0032 | CLSLoss:4.7480 | top1:92.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8794 | MainLoss:0.8794 | SPLoss:0.0071 | CLSLoss:4.7542 | top1:68.7058 | AUROC:0.7536\n",
      "Test | 20/10 | Loss:0.4682 | MainLoss:0.4682 | SPLoss:0.0071 | CLSLoss:4.7542 | top1:83.6026 | AUROC:0.9669\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 10/10 | Loss:0.2063 | MainLoss:0.2061 | Alpha:0.0449 | SPLoss:0.0038 | CLSLoss:4.7562 | top1:91.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8800 | MainLoss:0.8800 | SPLoss:0.0080 | CLSLoss:4.7596 | top1:68.7942 | AUROC:0.7547\n",
      "Test | 20/10 | Loss:0.4808 | MainLoss:0.4808 | SPLoss:0.0080 | CLSLoss:4.7596 | top1:83.4231 | AUROC:0.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 10/10 | Loss:0.2099 | MainLoss:0.2097 | Alpha:0.0448 | SPLoss:0.0039 | CLSLoss:4.7600 | top1:92.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8731 | MainLoss:0.8731 | SPLoss:0.0093 | CLSLoss:4.7626 | top1:68.8008 | AUROC:0.7555\n",
      "Test | 20/10 | Loss:0.4424 | MainLoss:0.4424 | SPLoss:0.0093 | CLSLoss:4.7626 | top1:84.2308 | AUROC:0.9660\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 10/10 | Loss:0.1940 | MainLoss:0.1938 | Alpha:0.0451 | SPLoss:0.0037 | CLSLoss:4.7662 | top1:92.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8883 | MainLoss:0.8883 | SPLoss:0.0086 | CLSLoss:4.7704 | top1:68.8991 | AUROC:0.7564\n",
      "Test | 20/10 | Loss:0.4939 | MainLoss:0.4939 | SPLoss:0.0086 | CLSLoss:4.7704 | top1:82.9615 | AUROC:0.9654\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 10/10 | Loss:0.1840 | MainLoss:0.1838 | Alpha:0.0454 | SPLoss:0.0036 | CLSLoss:4.7723 | top1:92.9000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8837 | MainLoss:0.8837 | SPLoss:0.0068 | CLSLoss:4.7799 | top1:68.9056 | AUROC:0.7564\n",
      "Test | 20/10 | Loss:0.4710 | MainLoss:0.4710 | SPLoss:0.0068 | CLSLoss:4.7799 | top1:83.7821 | AUROC:0.9663\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 10/10 | Loss:0.1625 | MainLoss:0.1624 | Alpha:0.0460 | SPLoss:0.0022 | CLSLoss:4.7904 | top1:94.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9128 | MainLoss:0.9128 | SPLoss:0.0080 | CLSLoss:4.7942 | top1:68.7942 | AUROC:0.7557\n",
      "Test | 20/10 | Loss:0.5100 | MainLoss:0.5100 | SPLoss:0.0080 | CLSLoss:4.7942 | top1:82.9103 | AUROC:0.9668\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 10/10 | Loss:0.2002 | MainLoss:0.2001 | Alpha:0.0450 | SPLoss:0.0039 | CLSLoss:4.7938 | top1:91.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8878 | MainLoss:0.8878 | SPLoss:0.0090 | CLSLoss:4.7944 | top1:69.0400 | AUROC:0.7575\n",
      "Test | 20/10 | Loss:0.4695 | MainLoss:0.4695 | SPLoss:0.0090 | CLSLoss:4.7944 | top1:83.9231 | AUROC:0.9672\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 10/10 | Loss:0.2057 | MainLoss:0.2055 | Alpha:0.0449 | SPLoss:0.0042 | CLSLoss:4.7941 | top1:91.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8877 | MainLoss:0.8877 | SPLoss:0.0090 | CLSLoss:4.7951 | top1:69.2136 | AUROC:0.7581\n",
      "Test | 20/10 | Loss:0.4773 | MainLoss:0.4773 | SPLoss:0.0090 | CLSLoss:4.7951 | top1:83.4615 | AUROC:0.9653\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 10/10 | Loss:0.1886 | MainLoss:0.1884 | Alpha:0.0453 | SPLoss:0.0040 | CLSLoss:4.7957 | top1:92.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8881 | MainLoss:0.8881 | SPLoss:0.0084 | CLSLoss:4.8001 | top1:69.1153 | AUROC:0.7593\n",
      "Test | 20/10 | Loss:0.4960 | MainLoss:0.4960 | SPLoss:0.0084 | CLSLoss:4.8001 | top1:82.9615 | AUROC:0.9651\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 10/10 | Loss:0.1913 | MainLoss:0.1911 | Alpha:0.0452 | SPLoss:0.0041 | CLSLoss:4.8062 | top1:93.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8850 | MainLoss:0.8850 | SPLoss:0.0100 | CLSLoss:4.8054 | top1:69.2235 | AUROC:0.7595\n",
      "Test | 20/10 | Loss:0.4476 | MainLoss:0.4476 | SPLoss:0.0100 | CLSLoss:4.8054 | top1:84.2692 | AUROC:0.9648\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 10/10 | Loss:0.1520 | MainLoss:0.1519 | Alpha:0.0462 | SPLoss:0.0030 | CLSLoss:4.8111 | top1:94.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.8997 | MainLoss:0.8997 | SPLoss:0.0065 | CLSLoss:4.8208 | top1:69.1710 | AUROC:0.7591\n",
      "Test | 20/10 | Loss:0.4887 | MainLoss:0.4887 | SPLoss:0.0065 | CLSLoss:4.8208 | top1:83.4487 | AUROC:0.9647\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 10/10 | Loss:0.1624 | MainLoss:0.1622 | Alpha:0.0460 | SPLoss:0.0038 | CLSLoss:4.8244 | top1:94.1250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9097 | MainLoss:0.9097 | SPLoss:0.0071 | CLSLoss:4.8316 | top1:69.2104 | AUROC:0.7604\n",
      "Test | 20/10 | Loss:0.5091 | MainLoss:0.5091 | SPLoss:0.0071 | CLSLoss:4.8316 | top1:82.9359 | AUROC:0.9633\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 10/10 | Loss:0.1594 | MainLoss:0.1592 | Alpha:0.0461 | SPLoss:0.0033 | CLSLoss:4.8370 | top1:93.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9131 | MainLoss:0.9131 | SPLoss:0.0079 | CLSLoss:4.8422 | top1:69.3283 | AUROC:0.7614\n",
      "Test | 20/10 | Loss:0.5039 | MainLoss:0.5039 | SPLoss:0.0079 | CLSLoss:4.8422 | top1:82.9487 | AUROC:0.9619\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 10/10 | Loss:0.1410 | MainLoss:0.1409 | Alpha:0.0465 | SPLoss:0.0033 | CLSLoss:4.8477 | top1:94.8000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9265 | MainLoss:0.9265 | SPLoss:0.0068 | CLSLoss:4.8560 | top1:69.2628 | AUROC:0.7618\n",
      "Test | 20/10 | Loss:0.5089 | MainLoss:0.5089 | SPLoss:0.0068 | CLSLoss:4.8560 | top1:83.2949 | AUROC:0.9624\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 10/10 | Loss:0.1587 | MainLoss:0.1585 | Alpha:0.0461 | SPLoss:0.0040 | CLSLoss:4.8585 | top1:94.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9240 | MainLoss:0.9240 | SPLoss:0.0088 | CLSLoss:4.8609 | top1:69.4627 | AUROC:0.7625\n",
      "Test | 20/10 | Loss:0.4943 | MainLoss:0.4943 | SPLoss:0.0088 | CLSLoss:4.8609 | top1:83.7436 | AUROC:0.9612\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 10/10 | Loss:0.1922 | MainLoss:0.1920 | Alpha:0.0453 | SPLoss:0.0048 | CLSLoss:4.8606 | top1:92.7250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9133 | MainLoss:0.9133 | SPLoss:0.0103 | CLSLoss:4.8588 | top1:69.3611 | AUROC:0.7623\n",
      "Test | 20/10 | Loss:0.4939 | MainLoss:0.4939 | SPLoss:0.0103 | CLSLoss:4.8588 | top1:83.6538 | AUROC:0.9627\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 10/10 | Loss:0.1793 | MainLoss:0.1792 | Alpha:0.0456 | SPLoss:0.0041 | CLSLoss:4.8606 | top1:92.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9190 | MainLoss:0.9190 | SPLoss:0.0096 | CLSLoss:4.8626 | top1:69.2235 | AUROC:0.7620\n",
      "Test | 20/10 | Loss:0.5271 | MainLoss:0.5271 | SPLoss:0.0096 | CLSLoss:4.8626 | top1:82.7821 | AUROC:0.9618\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 10/10 | Loss:0.1371 | MainLoss:0.1370 | Alpha:0.0466 | SPLoss:0.0032 | CLSLoss:4.8705 | top1:94.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9333 | MainLoss:0.9333 | SPLoss:0.0075 | CLSLoss:4.8778 | top1:69.2923 | AUROC:0.7628\n",
      "Test | 20/10 | Loss:0.5288 | MainLoss:0.5288 | SPLoss:0.0075 | CLSLoss:4.8778 | top1:82.7564 | AUROC:0.9617\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 10/10 | Loss:0.1359 | MainLoss:0.1357 | Alpha:0.0466 | SPLoss:0.0034 | CLSLoss:4.8831 | top1:95.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9392 | MainLoss:0.9392 | SPLoss:0.0067 | CLSLoss:4.8904 | top1:69.3185 | AUROC:0.7629\n",
      "Test | 20/10 | Loss:0.5203 | MainLoss:0.5203 | SPLoss:0.0067 | CLSLoss:4.8904 | top1:83.1538 | AUROC:0.9608\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 10/10 | Loss:0.1616 | MainLoss:0.1614 | Alpha:0.0460 | SPLoss:0.0041 | CLSLoss:4.8928 | top1:93.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9312 | MainLoss:0.9312 | SPLoss:0.0102 | CLSLoss:4.8914 | top1:69.4528 | AUROC:0.7636\n",
      "Test | 20/10 | Loss:0.4854 | MainLoss:0.4854 | SPLoss:0.0102 | CLSLoss:4.8913 | top1:83.9615 | AUROC:0.9601\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 10/10 | Loss:0.1368 | MainLoss:0.1366 | Alpha:0.0466 | SPLoss:0.0036 | CLSLoss:4.8960 | top1:95.1250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9323 | MainLoss:0.9323 | SPLoss:0.0077 | CLSLoss:4.9003 | top1:69.5380 | AUROC:0.7652\n",
      "Test | 20/10 | Loss:0.4698 | MainLoss:0.4698 | SPLoss:0.0077 | CLSLoss:4.9003 | top1:84.2051 | AUROC:0.9600\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 10/10 | Loss:0.1816 | MainLoss:0.1814 | Alpha:0.0456 | SPLoss:0.0048 | CLSLoss:4.9010 | top1:92.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9226 | MainLoss:0.9226 | SPLoss:0.0106 | CLSLoss:4.8975 | top1:69.5708 | AUROC:0.7654\n",
      "Test | 20/10 | Loss:0.4617 | MainLoss:0.4617 | SPLoss:0.0106 | CLSLoss:4.8975 | top1:84.4103 | AUROC:0.9589\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 10/10 | Loss:0.1782 | MainLoss:0.1779 | Alpha:0.0456 | SPLoss:0.0048 | CLSLoss:4.8959 | top1:93.2500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9216 | MainLoss:0.9216 | SPLoss:0.0095 | CLSLoss:4.8981 | top1:69.5380 | AUROC:0.7650\n",
      "Test | 20/10 | Loss:0.4743 | MainLoss:0.4743 | SPLoss:0.0095 | CLSLoss:4.8981 | top1:84.0128 | AUROC:0.9600\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 10/10 | Loss:0.1444 | MainLoss:0.1443 | Alpha:0.0464 | SPLoss:0.0036 | CLSLoss:4.9030 | top1:94.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9357 | MainLoss:0.9357 | SPLoss:0.0089 | CLSLoss:4.9061 | top1:69.5806 | AUROC:0.7638\n",
      "Test | 20/10 | Loss:0.5117 | MainLoss:0.5117 | SPLoss:0.0089 | CLSLoss:4.9061 | top1:83.1795 | AUROC:0.9612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n",
      "Train | 10/10 | Loss:0.1266 | MainLoss:0.1264 | Alpha:0.0469 | SPLoss:0.0040 | CLSLoss:4.9113 | top1:95.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9480 | MainLoss:0.9480 | SPLoss:0.0077 | CLSLoss:4.9191 | top1:69.5708 | AUROC:0.7659\n",
      "Test | 20/10 | Loss:0.5055 | MainLoss:0.5055 | SPLoss:0.0077 | CLSLoss:4.9191 | top1:83.4744 | AUROC:0.9603\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 10/10 | Loss:0.1365 | MainLoss:0.1363 | Alpha:0.0467 | SPLoss:0.0037 | CLSLoss:4.9226 | top1:94.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9528 | MainLoss:0.9528 | SPLoss:0.0078 | CLSLoss:4.9269 | top1:69.5118 | AUROC:0.7654\n",
      "Test | 20/10 | Loss:0.5141 | MainLoss:0.5141 | SPLoss:0.0078 | CLSLoss:4.9269 | top1:83.5385 | AUROC:0.9607\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 10/10 | Loss:0.1833 | MainLoss:0.1830 | Alpha:0.0455 | SPLoss:0.0061 | CLSLoss:4.9213 | top1:93.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9335 | MainLoss:0.9335 | SPLoss:0.0122 | CLSLoss:4.9191 | top1:69.6494 | AUROC:0.7664\n",
      "Test | 20/10 | Loss:0.4899 | MainLoss:0.4899 | SPLoss:0.0122 | CLSLoss:4.9191 | top1:83.9231 | AUROC:0.9586\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 10/10 | Loss:0.1386 | MainLoss:0.1384 | Alpha:0.0466 | SPLoss:0.0041 | CLSLoss:4.9205 | top1:95.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9389 | MainLoss:0.9389 | SPLoss:0.0082 | CLSLoss:4.9262 | top1:69.7805 | AUROC:0.7670\n",
      "Test | 20/10 | Loss:0.5332 | MainLoss:0.5332 | SPLoss:0.0082 | CLSLoss:4.9262 | top1:82.8974 | AUROC:0.9585\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n",
      "Train | 10/10 | Loss:0.1697 | MainLoss:0.1695 | Alpha:0.0458 | SPLoss:0.0050 | CLSLoss:4.9256 | top1:93.7500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9307 | MainLoss:0.9307 | SPLoss:0.0104 | CLSLoss:4.9225 | top1:69.9050 | AUROC:0.7676\n",
      "Test | 20/10 | Loss:0.5131 | MainLoss:0.5131 | SPLoss:0.0104 | CLSLoss:4.9225 | top1:83.2821 | AUROC:0.9577\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 10/10 | Loss:0.1214 | MainLoss:0.1213 | Alpha:0.0470 | SPLoss:0.0029 | CLSLoss:4.9300 | top1:95.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9378 | MainLoss:0.9378 | SPLoss:0.0069 | CLSLoss:4.9354 | top1:69.9640 | AUROC:0.7687\n",
      "Test | 20/10 | Loss:0.5242 | MainLoss:0.5242 | SPLoss:0.0069 | CLSLoss:4.9354 | top1:83.0897 | AUROC:0.9567\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 10/10 | Loss:0.1600 | MainLoss:0.1598 | Alpha:0.0460 | SPLoss:0.0051 | CLSLoss:4.9352 | top1:94.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9131 | MainLoss:0.9131 | SPLoss:0.0109 | CLSLoss:4.9331 | top1:69.9935 | AUROC:0.7688\n",
      "Test | 20/10 | Loss:0.4931 | MainLoss:0.4931 | SPLoss:0.0109 | CLSLoss:4.9331 | top1:83.5513 | AUROC:0.9565\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.039308\n",
      "Train | 10/10 | Loss:0.1372 | MainLoss:0.1370 | Alpha:0.0466 | SPLoss:0.0042 | CLSLoss:4.9364 | top1:94.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9311 | MainLoss:0.9311 | SPLoss:0.0083 | CLSLoss:4.9430 | top1:69.9574 | AUROC:0.7692\n",
      "Test | 20/10 | Loss:0.5072 | MainLoss:0.5072 | SPLoss:0.0083 | CLSLoss:4.9430 | top1:83.3077 | AUROC:0.9566\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.039291\n",
      "Train | 10/10 | Loss:0.1352 | MainLoss:0.1350 | Alpha:0.0467 | SPLoss:0.0039 | CLSLoss:4.9477 | top1:95.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9421 | MainLoss:0.9421 | SPLoss:0.0092 | CLSLoss:4.9505 | top1:69.9443 | AUROC:0.7693\n",
      "Test | 20/10 | Loss:0.4913 | MainLoss:0.4913 | SPLoss:0.0092 | CLSLoss:4.9505 | top1:83.8462 | AUROC:0.9583\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.039274\n",
      "Train | 10/10 | Loss:0.1211 | MainLoss:0.1210 | Alpha:0.0470 | SPLoss:0.0032 | CLSLoss:4.9566 | top1:95.4750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9521 | MainLoss:0.9521 | SPLoss:0.0071 | CLSLoss:4.9606 | top1:70.1638 | AUROC:0.7704\n",
      "Test | 20/10 | Loss:0.5055 | MainLoss:0.5055 | SPLoss:0.0071 | CLSLoss:4.9606 | top1:83.6923 | AUROC:0.9570\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.039258\n",
      "Train | 10/10 | Loss:0.1231 | MainLoss:0.1229 | Alpha:0.0469 | SPLoss:0.0046 | CLSLoss:4.9620 | top1:95.6250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9617 | MainLoss:0.9617 | SPLoss:0.0082 | CLSLoss:4.9690 | top1:70.0033 | AUROC:0.7697\n",
      "Test | 20/10 | Loss:0.5198 | MainLoss:0.5198 | SPLoss:0.0082 | CLSLoss:4.9690 | top1:83.3077 | AUROC:0.9574\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.039241\n",
      "Train | 10/10 | Loss:0.1109 | MainLoss:0.1107 | Alpha:0.0473 | SPLoss:0.0037 | CLSLoss:4.9725 | top1:95.7500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9678 | MainLoss:0.9678 | SPLoss:0.0074 | CLSLoss:4.9799 | top1:70.0852 | AUROC:0.7702\n",
      "Test | 20/10 | Loss:0.5310 | MainLoss:0.5310 | SPLoss:0.0074 | CLSLoss:4.9799 | top1:83.4359 | AUROC:0.9577\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.039223\n",
      "Train | 10/10 | Loss:0.1620 | MainLoss:0.1617 | Alpha:0.0460 | SPLoss:0.0064 | CLSLoss:4.9767 | top1:93.6000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9548 | MainLoss:0.9548 | SPLoss:0.0116 | CLSLoss:4.9773 | top1:70.1343 | AUROC:0.7710\n",
      "Test | 20/10 | Loss:0.5386 | MainLoss:0.5386 | SPLoss:0.0116 | CLSLoss:4.9773 | top1:82.9231 | AUROC:0.9561\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.039206\n",
      "Train | 10/10 | Loss:0.1105 | MainLoss:0.1103 | Alpha:0.0473 | SPLoss:0.0028 | CLSLoss:4.9835 | top1:96.0000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9689 | MainLoss:0.9689 | SPLoss:0.0075 | CLSLoss:4.9872 | top1:70.0524 | AUROC:0.7702\n",
      "Test | 20/10 | Loss:0.5524 | MainLoss:0.5524 | SPLoss:0.0075 | CLSLoss:4.9872 | top1:82.7821 | AUROC:0.9568\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.039188\n",
      "Train | 10/10 | Loss:0.1086 | MainLoss:0.1084 | Alpha:0.0474 | SPLoss:0.0045 | CLSLoss:4.9911 | top1:96.0250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9864 | MainLoss:0.9864 | SPLoss:0.0079 | CLSLoss:4.9986 | top1:70.1180 | AUROC:0.7699\n",
      "Test | 20/10 | Loss:0.5400 | MainLoss:0.5400 | SPLoss:0.0079 | CLSLoss:4.9986 | top1:83.3590 | AUROC:0.9576\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.039170\n",
      "Train | 10/10 | Loss:0.1481 | MainLoss:0.1478 | Alpha:0.0464 | SPLoss:0.0056 | CLSLoss:4.9968 | top1:94.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9783 | MainLoss:0.9783 | SPLoss:0.0115 | CLSLoss:4.9961 | top1:70.0524 | AUROC:0.7703\n",
      "Test | 20/10 | Loss:0.5612 | MainLoss:0.5612 | SPLoss:0.0115 | CLSLoss:4.9961 | top1:82.8333 | AUROC:0.9559\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.039152\n",
      "Train | 10/10 | Loss:0.0975 | MainLoss:0.0974 | Alpha:0.0476 | SPLoss:0.0029 | CLSLoss:5.0019 | top1:96.5000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9905 | MainLoss:0.9905 | SPLoss:0.0065 | CLSLoss:5.0089 | top1:70.1081 | AUROC:0.7700\n",
      "Test | 20/10 | Loss:0.5497 | MainLoss:0.5497 | SPLoss:0.0065 | CLSLoss:5.0089 | top1:83.1795 | AUROC:0.9556\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.039134\n",
      "Train | 10/10 | Loss:0.1270 | MainLoss:0.1268 | Alpha:0.0469 | SPLoss:0.0050 | CLSLoss:5.0098 | top1:95.2750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9932 | MainLoss:0.9932 | SPLoss:0.0102 | CLSLoss:5.0122 | top1:70.0524 | AUROC:0.7700\n",
      "Test | 20/10 | Loss:0.5483 | MainLoss:0.5483 | SPLoss:0.0102 | CLSLoss:5.0122 | top1:83.1923 | AUROC:0.9562\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.039116\n",
      "Train | 10/10 | Loss:0.1213 | MainLoss:0.1211 | Alpha:0.0470 | SPLoss:0.0047 | CLSLoss:5.0132 | top1:95.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9862 | MainLoss:0.9862 | SPLoss:0.0100 | CLSLoss:5.0164 | top1:70.0229 | AUROC:0.7710\n",
      "Test | 20/10 | Loss:0.5166 | MainLoss:0.5166 | SPLoss:0.0100 | CLSLoss:5.0164 | top1:83.8077 | AUROC:0.9557\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.039097\n",
      "Train | 10/10 | Loss:0.1124 | MainLoss:0.1122 | Alpha:0.0472 | SPLoss:0.0042 | CLSLoss:5.0182 | top1:95.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9944 | MainLoss:0.9944 | SPLoss:0.0091 | CLSLoss:5.0222 | top1:70.0721 | AUROC:0.7719\n",
      "Test | 20/10 | Loss:0.5646 | MainLoss:0.5646 | SPLoss:0.0091 | CLSLoss:5.0222 | top1:82.7436 | AUROC:0.9534\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.039079\n",
      "Train | 10/10 | Loss:0.1422 | MainLoss:0.1420 | Alpha:0.0466 | SPLoss:0.0051 | CLSLoss:5.0237 | top1:94.1250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9845 | MainLoss:0.9845 | SPLoss:0.0128 | CLSLoss:5.0201 | top1:70.1900 | AUROC:0.7721\n",
      "Test | 20/10 | Loss:0.5510 | MainLoss:0.5510 | SPLoss:0.0128 | CLSLoss:5.0201 | top1:83.1538 | AUROC:0.9554\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.039060\n",
      "Train | 10/10 | Loss:0.1321 | MainLoss:0.1319 | Alpha:0.0467 | SPLoss:0.0044 | CLSLoss:5.0218 | top1:94.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9751 | MainLoss:0.9751 | SPLoss:0.0103 | CLSLoss:5.0214 | top1:70.2064 | AUROC:0.7710\n",
      "Test | 20/10 | Loss:0.5487 | MainLoss:0.5487 | SPLoss:0.0103 | CLSLoss:5.0214 | top1:83.2821 | AUROC:0.9570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [111 | 1000] LR: 0.039040\n",
      "Train | 10/10 | Loss:0.1353 | MainLoss:0.1350 | Alpha:0.0467 | SPLoss:0.0061 | CLSLoss:5.0178 | top1:95.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9772 | MainLoss:0.9772 | SPLoss:0.0109 | CLSLoss:5.0194 | top1:70.2425 | AUROC:0.7731\n",
      "Test | 20/10 | Loss:0.5702 | MainLoss:0.5702 | SPLoss:0.0109 | CLSLoss:5.0194 | top1:82.4744 | AUROC:0.9553\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.039021\n",
      "Train | 10/10 | Loss:0.0885 | MainLoss:0.0884 | Alpha:0.0478 | SPLoss:0.0036 | CLSLoss:5.0252 | top1:97.0250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9933 | MainLoss:0.9933 | SPLoss:0.0064 | CLSLoss:5.0343 | top1:70.2228 | AUROC:0.7720\n",
      "Test | 20/10 | Loss:0.5521 | MainLoss:0.5521 | SPLoss:0.0064 | CLSLoss:5.0343 | top1:83.3205 | AUROC:0.9560\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.039002\n",
      "Train | 10/10 | Loss:0.1269 | MainLoss:0.1267 | Alpha:0.0470 | SPLoss:0.0050 | CLSLoss:5.0334 | top1:95.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9856 | MainLoss:0.9856 | SPLoss:0.0106 | CLSLoss:5.0342 | top1:70.3539 | AUROC:0.7724\n",
      "Test | 20/10 | Loss:0.5531 | MainLoss:0.5531 | SPLoss:0.0106 | CLSLoss:5.0342 | top1:83.2949 | AUROC:0.9550\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.038982\n",
      "Train | 10/10 | Loss:0.1031 | MainLoss:0.1029 | Alpha:0.0475 | SPLoss:0.0038 | CLSLoss:5.0378 | top1:96.5500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9921 | MainLoss:0.9921 | SPLoss:0.0082 | CLSLoss:5.0408 | top1:70.3735 | AUROC:0.7731\n",
      "Test | 20/10 | Loss:0.5409 | MainLoss:0.5409 | SPLoss:0.0082 | CLSLoss:5.0408 | top1:83.5641 | AUROC:0.9542\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.038962\n",
      "Train | 10/10 | Loss:0.1470 | MainLoss:0.1467 | Alpha:0.0463 | SPLoss:0.0055 | CLSLoss:5.0389 | top1:94.2750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9765 | MainLoss:0.9765 | SPLoss:0.0132 | CLSLoss:5.0347 | top1:70.4423 | AUROC:0.7743\n",
      "Test | 20/10 | Loss:0.5292 | MainLoss:0.5292 | SPLoss:0.0132 | CLSLoss:5.0347 | top1:83.8205 | AUROC:0.9539\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.038942\n",
      "Train | 10/10 | Loss:0.1256 | MainLoss:0.1254 | Alpha:0.0468 | SPLoss:0.0053 | CLSLoss:5.0356 | top1:95.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9829 | MainLoss:0.9829 | SPLoss:0.0099 | CLSLoss:5.0370 | top1:70.2228 | AUROC:0.7734\n",
      "Test | 20/10 | Loss:0.5522 | MainLoss:0.5522 | SPLoss:0.0099 | CLSLoss:5.0370 | top1:83.4615 | AUROC:0.9550\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.038922\n",
      "Train | 10/10 | Loss:0.1275 | MainLoss:0.1271 | Alpha:0.0469 | SPLoss:0.0073 | CLSLoss:5.0321 | top1:95.4250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9708 | MainLoss:0.9708 | SPLoss:0.0105 | CLSLoss:5.0378 | top1:70.3604 | AUROC:0.7754\n",
      "Test | 20/10 | Loss:0.5563 | MainLoss:0.5563 | SPLoss:0.0105 | CLSLoss:5.0378 | top1:83.0000 | AUROC:0.9526\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.038901\n",
      "Train | 10/10 | Loss:0.1466 | MainLoss:0.1463 | Alpha:0.0464 | SPLoss:0.0068 | CLSLoss:5.0315 | top1:94.5000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9682 | MainLoss:0.9682 | SPLoss:0.0122 | CLSLoss:5.0342 | top1:70.4260 | AUROC:0.7748\n",
      "Test | 20/10 | Loss:0.5548 | MainLoss:0.5548 | SPLoss:0.0122 | CLSLoss:5.0342 | top1:82.8590 | AUROC:0.9539\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.038881\n",
      "Train | 10/10 | Loss:0.1036 | MainLoss:0.1034 | Alpha:0.0474 | SPLoss:0.0042 | CLSLoss:5.0373 | top1:96.5250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9719 | MainLoss:0.9719 | SPLoss:0.0083 | CLSLoss:5.0442 | top1:70.6684 | AUROC:0.7762\n",
      "Test | 20/10 | Loss:0.5478 | MainLoss:0.5478 | SPLoss:0.0083 | CLSLoss:5.0442 | top1:83.2692 | AUROC:0.9545\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.038860\n",
      "Train | 10/10 | Loss:0.1049 | MainLoss:0.1047 | Alpha:0.0474 | SPLoss:0.0030 | CLSLoss:5.0502 | top1:96.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9727 | MainLoss:0.9727 | SPLoss:0.0089 | CLSLoss:5.0498 | top1:70.7110 | AUROC:0.7764\n",
      "Test | 20/10 | Loss:0.5526 | MainLoss:0.5526 | SPLoss:0.0089 | CLSLoss:5.0498 | top1:83.2564 | AUROC:0.9551\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.038839\n",
      "Train | 10/10 | Loss:0.1199 | MainLoss:0.1197 | Alpha:0.0471 | SPLoss:0.0037 | CLSLoss:5.0527 | top1:95.7250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9837 | MainLoss:0.9837 | SPLoss:0.0101 | CLSLoss:5.0516 | top1:70.7176 | AUROC:0.7772\n",
      "Test | 20/10 | Loss:0.5642 | MainLoss:0.5642 | SPLoss:0.0101 | CLSLoss:5.0516 | top1:83.0385 | AUROC:0.9541\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.038818\n",
      "Train | 10/10 | Loss:0.1084 | MainLoss:0.1082 | Alpha:0.0474 | SPLoss:0.0045 | CLSLoss:5.0515 | top1:96.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9873 | MainLoss:0.9873 | SPLoss:0.0086 | CLSLoss:5.0561 | top1:70.5374 | AUROC:0.7765\n",
      "Test | 20/10 | Loss:0.5914 | MainLoss:0.5914 | SPLoss:0.0086 | CLSLoss:5.0561 | top1:82.5256 | AUROC:0.9540\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.038796\n",
      "Train | 10/10 | Loss:0.0903 | MainLoss:0.0902 | Alpha:0.0478 | SPLoss:0.0034 | CLSLoss:5.0598 | top1:96.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9976 | MainLoss:0.9976 | SPLoss:0.0071 | CLSLoss:5.0642 | top1:70.7110 | AUROC:0.7768\n",
      "Test | 20/10 | Loss:0.5997 | MainLoss:0.5997 | SPLoss:0.0071 | CLSLoss:5.0642 | top1:82.2436 | AUROC:0.9545\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.038775\n",
      "Train | 10/10 | Loss:0.1134 | MainLoss:0.1132 | Alpha:0.0472 | SPLoss:0.0037 | CLSLoss:5.0673 | top1:95.4500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9853 | MainLoss:0.9853 | SPLoss:0.0099 | CLSLoss:5.0660 | top1:70.6914 | AUROC:0.7765\n",
      "Test | 20/10 | Loss:0.5961 | MainLoss:0.5961 | SPLoss:0.0099 | CLSLoss:5.0660 | top1:82.0641 | AUROC:0.9529\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.038753\n",
      "Train | 10/10 | Loss:0.1212 | MainLoss:0.1211 | Alpha:0.0470 | SPLoss:0.0041 | CLSLoss:5.0692 | top1:95.4500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9845 | MainLoss:0.9845 | SPLoss:0.0106 | CLSLoss:5.0654 | top1:70.6717 | AUROC:0.7769\n",
      "Test | 20/10 | Loss:0.5824 | MainLoss:0.5824 | SPLoss:0.0106 | CLSLoss:5.0654 | top1:82.2821 | AUROC:0.9530\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.038731\n",
      "Train | 10/10 | Loss:0.0904 | MainLoss:0.0902 | Alpha:0.0478 | SPLoss:0.0044 | CLSLoss:5.0665 | top1:96.7250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0020 | MainLoss:1.0020 | SPLoss:0.0088 | CLSLoss:5.0714 | top1:70.5341 | AUROC:0.7778\n",
      "Test | 20/10 | Loss:0.6260 | MainLoss:0.6260 | SPLoss:0.0088 | CLSLoss:5.0714 | top1:81.5897 | AUROC:0.9520\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.038709\n",
      "Train | 10/10 | Loss:0.1201 | MainLoss:0.1199 | Alpha:0.0471 | SPLoss:0.0058 | CLSLoss:5.0706 | top1:95.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:0.9861 | MainLoss:0.9861 | SPLoss:0.0113 | CLSLoss:5.0710 | top1:70.7077 | AUROC:0.7781\n",
      "Test | 20/10 | Loss:0.5502 | MainLoss:0.5502 | SPLoss:0.0113 | CLSLoss:5.0710 | top1:83.1667 | AUROC:0.9513\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.038687\n",
      "Train | 10/10 | Loss:0.0874 | MainLoss:0.0873 | Alpha:0.0478 | SPLoss:0.0032 | CLSLoss:5.0758 | top1:96.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0017 | MainLoss:1.0017 | SPLoss:0.0082 | CLSLoss:5.0791 | top1:70.8814 | AUROC:0.7800\n",
      "Test | 20/10 | Loss:0.5797 | MainLoss:0.5797 | SPLoss:0.0082 | CLSLoss:5.0791 | top1:82.6026 | AUROC:0.9497\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.038664\n",
      "Train | 10/10 | Loss:0.0800 | MainLoss:0.0798 | Alpha:0.0479 | SPLoss:0.0026 | CLSLoss:5.0862 | top1:96.9250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0065 | MainLoss:1.0065 | SPLoss:0.0083 | CLSLoss:5.0889 | top1:70.9142 | AUROC:0.7792\n",
      "Test | 20/10 | Loss:0.5413 | MainLoss:0.5413 | SPLoss:0.0083 | CLSLoss:5.0889 | top1:83.6410 | AUROC:0.9508\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.038641\n",
      "Train | 10/10 | Loss:0.0925 | MainLoss:0.0923 | Alpha:0.0477 | SPLoss:0.0045 | CLSLoss:5.0878 | top1:97.0500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0144 | MainLoss:1.0144 | SPLoss:0.0076 | CLSLoss:5.0927 | top1:70.8585 | AUROC:0.7785\n",
      "Test | 20/10 | Loss:0.5773 | MainLoss:0.5773 | SPLoss:0.0076 | CLSLoss:5.0927 | top1:82.7821 | AUROC:0.9491\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.038619\n",
      "Train | 10/10 | Loss:0.0784 | MainLoss:0.0782 | Alpha:0.0481 | SPLoss:0.0040 | CLSLoss:5.0973 | top1:97.2750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0258 | MainLoss:1.0258 | SPLoss:0.0093 | CLSLoss:5.1011 | top1:70.9568 | AUROC:0.7797\n",
      "Test | 20/10 | Loss:0.5669 | MainLoss:0.5669 | SPLoss:0.0093 | CLSLoss:5.1011 | top1:83.4487 | AUROC:0.9509\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.038596\n",
      "Train | 10/10 | Loss:0.1012 | MainLoss:0.1009 | Alpha:0.0476 | SPLoss:0.0048 | CLSLoss:5.1027 | top1:96.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0245 | MainLoss:1.0245 | SPLoss:0.0104 | CLSLoss:5.1040 | top1:71.0780 | AUROC:0.7803\n",
      "Test | 20/10 | Loss:0.5845 | MainLoss:0.5845 | SPLoss:0.0104 | CLSLoss:5.1040 | top1:82.9487 | AUROC:0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [133 | 1000] LR: 0.038572\n",
      "Train | 10/10 | Loss:0.0970 | MainLoss:0.0968 | Alpha:0.0476 | SPLoss:0.0043 | CLSLoss:5.1066 | top1:96.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0217 | MainLoss:1.0217 | SPLoss:0.0092 | CLSLoss:5.1073 | top1:70.9961 | AUROC:0.7804\n",
      "Test | 20/10 | Loss:0.5909 | MainLoss:0.5909 | SPLoss:0.0092 | CLSLoss:5.1074 | top1:82.8846 | AUROC:0.9512\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.038549\n",
      "Train | 10/10 | Loss:0.0920 | MainLoss:0.0918 | Alpha:0.0477 | SPLoss:0.0044 | CLSLoss:5.1078 | top1:96.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0274 | MainLoss:1.0274 | SPLoss:0.0089 | CLSLoss:5.1097 | top1:70.8716 | AUROC:0.7803\n",
      "Test | 20/10 | Loss:0.5968 | MainLoss:0.5968 | SPLoss:0.0089 | CLSLoss:5.1097 | top1:82.7692 | AUROC:0.9497\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.038525\n",
      "Train | 10/10 | Loss:0.0694 | MainLoss:0.0692 | Alpha:0.0482 | SPLoss:0.0038 | CLSLoss:5.1126 | top1:97.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0369 | MainLoss:1.0369 | SPLoss:0.0064 | CLSLoss:5.1195 | top1:70.8716 | AUROC:0.7793\n",
      "Test | 20/10 | Loss:0.5842 | MainLoss:0.5842 | SPLoss:0.0064 | CLSLoss:5.1195 | top1:82.8846 | AUROC:0.9495\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.038502\n",
      "Train | 10/10 | Loss:0.0857 | MainLoss:0.0855 | Alpha:0.0479 | SPLoss:0.0043 | CLSLoss:5.1209 | top1:96.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0451 | MainLoss:1.0451 | SPLoss:0.0096 | CLSLoss:5.1239 | top1:70.4063 | AUROC:0.7776\n",
      "Test | 20/10 | Loss:0.5220 | MainLoss:0.5220 | SPLoss:0.0096 | CLSLoss:5.1239 | top1:84.2179 | AUROC:0.9512\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.038478\n",
      "Train | 10/10 | Loss:0.1206 | MainLoss:0.1203 | Alpha:0.0469 | SPLoss:0.0057 | CLSLoss:5.1236 | top1:96.0000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0213 | MainLoss:1.0213 | SPLoss:0.0129 | CLSLoss:5.1157 | top1:70.6094 | AUROC:0.7788\n",
      "Test | 20/10 | Loss:0.5266 | MainLoss:0.5266 | SPLoss:0.0129 | CLSLoss:5.1157 | top1:83.8462 | AUROC:0.9506\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.038453\n",
      "Train | 10/10 | Loss:0.1066 | MainLoss:0.1064 | Alpha:0.0471 | SPLoss:0.0049 | CLSLoss:5.1181 | top1:96.3500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0231 | MainLoss:1.0231 | SPLoss:0.0114 | CLSLoss:5.1161 | top1:70.5472 | AUROC:0.7781\n",
      "Test | 20/10 | Loss:0.5663 | MainLoss:0.5663 | SPLoss:0.0114 | CLSLoss:5.1161 | top1:83.3718 | AUROC:0.9521\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.038429\n",
      "Train | 10/10 | Loss:0.0806 | MainLoss:0.0804 | Alpha:0.0480 | SPLoss:0.0039 | CLSLoss:5.1181 | top1:97.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0297 | MainLoss:1.0297 | SPLoss:0.0086 | CLSLoss:5.1217 | top1:70.5799 | AUROC:0.7790\n",
      "Test | 20/10 | Loss:0.5507 | MainLoss:0.5507 | SPLoss:0.0086 | CLSLoss:5.1217 | top1:83.8077 | AUROC:0.9523\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.038405\n",
      "Train | 10/10 | Loss:0.0828 | MainLoss:0.0826 | Alpha:0.0480 | SPLoss:0.0037 | CLSLoss:5.1253 | top1:97.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0440 | MainLoss:1.0440 | SPLoss:0.0083 | CLSLoss:5.1282 | top1:70.6357 | AUROC:0.7780\n",
      "Test | 20/10 | Loss:0.5860 | MainLoss:0.5860 | SPLoss:0.0083 | CLSLoss:5.1282 | top1:82.9615 | AUROC:0.9496\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.038380\n",
      "Train | 10/10 | Loss:0.0815 | MainLoss:0.0813 | Alpha:0.0480 | SPLoss:0.0037 | CLSLoss:5.1310 | top1:97.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0497 | MainLoss:1.0497 | SPLoss:0.0086 | CLSLoss:5.1331 | top1:70.6094 | AUROC:0.7783\n",
      "Test | 20/10 | Loss:0.6020 | MainLoss:0.6020 | SPLoss:0.0086 | CLSLoss:5.1331 | top1:82.5385 | AUROC:0.9502\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.038355\n",
      "Train | 10/10 | Loss:0.0879 | MainLoss:0.0877 | Alpha:0.0478 | SPLoss:0.0059 | CLSLoss:5.1341 | top1:96.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0327 | MainLoss:1.0327 | SPLoss:0.0110 | CLSLoss:5.1362 | top1:70.8912 | AUROC:0.7803\n",
      "Test | 20/10 | Loss:0.5593 | MainLoss:0.5593 | SPLoss:0.0110 | CLSLoss:5.1362 | top1:83.5641 | AUROC:0.9513\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.038330\n",
      "Train | 10/10 | Loss:0.0876 | MainLoss:0.0874 | Alpha:0.0479 | SPLoss:0.0046 | CLSLoss:5.1372 | top1:96.9500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0366 | MainLoss:1.0366 | SPLoss:0.0099 | CLSLoss:5.1385 | top1:70.7438 | AUROC:0.7792\n",
      "Test | 20/10 | Loss:0.5482 | MainLoss:0.5482 | SPLoss:0.0099 | CLSLoss:5.1385 | top1:83.8205 | AUROC:0.9524\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.038305\n",
      "Train | 10/10 | Loss:0.0805 | MainLoss:0.0803 | Alpha:0.0480 | SPLoss:0.0031 | CLSLoss:5.1415 | top1:97.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0347 | MainLoss:1.0347 | SPLoss:0.0087 | CLSLoss:5.1405 | top1:70.7045 | AUROC:0.7788\n",
      "Test | 20/10 | Loss:0.5676 | MainLoss:0.5676 | SPLoss:0.0087 | CLSLoss:5.1405 | top1:83.3333 | AUROC:0.9500\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.038279\n",
      "Train | 10/10 | Loss:0.0552 | MainLoss:0.0551 | Alpha:0.0487 | SPLoss:0.0032 | CLSLoss:5.1443 | top1:98.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0536 | MainLoss:1.0536 | SPLoss:0.0057 | CLSLoss:5.1515 | top1:70.8617 | AUROC:0.7791\n",
      "Test | 20/10 | Loss:0.5949 | MainLoss:0.5949 | SPLoss:0.0057 | CLSLoss:5.1515 | top1:82.9231 | AUROC:0.9498\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.038254\n",
      "Train | 10/10 | Loss:0.0775 | MainLoss:0.0773 | Alpha:0.0481 | SPLoss:0.0037 | CLSLoss:5.1541 | top1:97.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0562 | MainLoss:1.0562 | SPLoss:0.0097 | CLSLoss:5.1537 | top1:70.8322 | AUROC:0.7796\n",
      "Test | 20/10 | Loss:0.6007 | MainLoss:0.6007 | SPLoss:0.0097 | CLSLoss:5.1537 | top1:82.9487 | AUROC:0.9487\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.038228\n",
      "Train | 10/10 | Loss:0.0693 | MainLoss:0.0691 | Alpha:0.0483 | SPLoss:0.0041 | CLSLoss:5.1558 | top1:97.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0570 | MainLoss:1.0570 | SPLoss:0.0081 | CLSLoss:5.1600 | top1:70.8519 | AUROC:0.7801\n",
      "Test | 20/10 | Loss:0.5708 | MainLoss:0.5708 | SPLoss:0.0081 | CLSLoss:5.1600 | top1:83.5769 | AUROC:0.9487\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.038202\n",
      "Train | 10/10 | Loss:0.0770 | MainLoss:0.0769 | Alpha:0.0481 | SPLoss:0.0029 | CLSLoss:5.1643 | top1:97.4500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0774 | MainLoss:1.0774 | SPLoss:0.0093 | CLSLoss:5.1632 | top1:70.6455 | AUROC:0.7784\n",
      "Test | 20/10 | Loss:0.6286 | MainLoss:0.6286 | SPLoss:0.0093 | CLSLoss:5.1632 | top1:82.3590 | AUROC:0.9498\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.038176\n",
      "Train | 10/10 | Loss:0.0731 | MainLoss:0.0729 | Alpha:0.0482 | SPLoss:0.0041 | CLSLoss:5.1651 | top1:97.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0764 | MainLoss:1.0764 | SPLoss:0.0096 | CLSLoss:5.1661 | top1:70.7241 | AUROC:0.7786\n",
      "Test | 20/10 | Loss:0.6252 | MainLoss:0.6252 | SPLoss:0.0096 | CLSLoss:5.1661 | top1:82.4615 | AUROC:0.9471\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.038150\n",
      "Train | 10/10 | Loss:0.0686 | MainLoss:0.0684 | Alpha:0.0483 | SPLoss:0.0046 | CLSLoss:5.1673 | top1:97.5250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0852 | MainLoss:1.0852 | SPLoss:0.0083 | CLSLoss:5.1708 | top1:70.6029 | AUROC:0.7783\n",
      "Test | 20/10 | Loss:0.6401 | MainLoss:0.6401 | SPLoss:0.0083 | CLSLoss:5.1708 | top1:82.3333 | AUROC:0.9487\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.038123\n",
      "Train | 10/10 | Loss:0.0772 | MainLoss:0.0770 | Alpha:0.0481 | SPLoss:0.0045 | CLSLoss:5.1716 | top1:97.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0812 | MainLoss:1.0812 | SPLoss:0.0088 | CLSLoss:5.1736 | top1:70.5439 | AUROC:0.7781\n",
      "Test | 20/10 | Loss:0.6059 | MainLoss:0.6059 | SPLoss:0.0088 | CLSLoss:5.1736 | top1:82.9872 | AUROC:0.9493\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.038097\n",
      "Train | 10/10 | Loss:0.0797 | MainLoss:0.0795 | Alpha:0.0480 | SPLoss:0.0046 | CLSLoss:5.1746 | top1:97.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0879 | MainLoss:1.0879 | SPLoss:0.0098 | CLSLoss:5.1762 | top1:70.8257 | AUROC:0.7806\n",
      "Test | 20/10 | Loss:0.6241 | MainLoss:0.6241 | SPLoss:0.0098 | CLSLoss:5.1762 | top1:82.7179 | AUROC:0.9475\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.038070\n",
      "Train | 10/10 | Loss:0.0762 | MainLoss:0.0760 | Alpha:0.0482 | SPLoss:0.0050 | CLSLoss:5.1776 | top1:97.3750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0754 | MainLoss:1.0754 | SPLoss:0.0098 | CLSLoss:5.1793 | top1:71.0747 | AUROC:0.7816\n",
      "Test | 20/10 | Loss:0.6036 | MainLoss:0.6036 | SPLoss:0.0098 | CLSLoss:5.1793 | top1:82.8718 | AUROC:0.9463\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.038043\n",
      "Train | 10/10 | Loss:0.0855 | MainLoss:0.0852 | Alpha:0.0479 | SPLoss:0.0043 | CLSLoss:5.1803 | top1:96.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.1115 | MainLoss:1.1115 | SPLoss:0.0113 | CLSLoss:5.1793 | top1:70.4227 | AUROC:0.7808\n",
      "Test | 20/10 | Loss:0.6767 | MainLoss:0.6767 | SPLoss:0.0113 | CLSLoss:5.1793 | top1:81.0641 | AUROC:0.9459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [155 | 1000] LR: 0.038015\n",
      "Train | 10/10 | Loss:0.1080 | MainLoss:0.1076 | Alpha:0.0471 | SPLoss:0.0082 | CLSLoss:5.1722 | top1:96.4250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0612 | MainLoss:1.0612 | SPLoss:0.0129 | CLSLoss:5.1731 | top1:71.0387 | AUROC:0.7822\n",
      "Test | 20/10 | Loss:0.6084 | MainLoss:0.6084 | SPLoss:0.0129 | CLSLoss:5.1731 | top1:82.6026 | AUROC:0.9456\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.037988\n",
      "Train | 10/10 | Loss:0.0879 | MainLoss:0.0877 | Alpha:0.0478 | SPLoss:0.0051 | CLSLoss:5.1736 | top1:96.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0533 | MainLoss:1.0533 | SPLoss:0.0109 | CLSLoss:5.1744 | top1:71.1042 | AUROC:0.7827\n",
      "Test | 20/10 | Loss:0.6132 | MainLoss:0.6132 | SPLoss:0.0109 | CLSLoss:5.1744 | top1:82.6923 | AUROC:0.9469\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.037961\n",
      "Train | 10/10 | Loss:0.0807 | MainLoss:0.0806 | Alpha:0.0480 | SPLoss:0.0040 | CLSLoss:5.1758 | top1:97.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0657 | MainLoss:1.0657 | SPLoss:0.0106 | CLSLoss:5.1751 | top1:71.2254 | AUROC:0.7831\n",
      "Test | 20/10 | Loss:0.6124 | MainLoss:0.6124 | SPLoss:0.0106 | CLSLoss:5.1751 | top1:82.5897 | AUROC:0.9431\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.037933\n",
      "Train | 10/10 | Loss:0.0848 | MainLoss:0.0845 | Alpha:0.0477 | SPLoss:0.0055 | CLSLoss:5.1744 | top1:97.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0695 | MainLoss:1.0695 | SPLoss:0.0113 | CLSLoss:5.1733 | top1:71.0157 | AUROC:0.7831\n",
      "Test | 20/10 | Loss:0.6712 | MainLoss:0.6712 | SPLoss:0.0113 | CLSLoss:5.1733 | top1:81.4872 | AUROC:0.9463\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.037905\n",
      "Train | 10/10 | Loss:0.0806 | MainLoss:0.0803 | Alpha:0.0480 | SPLoss:0.0061 | CLSLoss:5.1720 | top1:97.1250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0511 | MainLoss:1.0511 | SPLoss:0.0097 | CLSLoss:5.1756 | top1:71.2058 | AUROC:0.7845\n",
      "Test | 20/10 | Loss:0.6350 | MainLoss:0.6350 | SPLoss:0.0097 | CLSLoss:5.1756 | top1:82.2308 | AUROC:0.9453\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.037877\n",
      "Train | 10/10 | Loss:0.0676 | MainLoss:0.0675 | Alpha:0.0483 | SPLoss:0.0037 | CLSLoss:5.1793 | top1:97.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0572 | MainLoss:1.0572 | SPLoss:0.0091 | CLSLoss:5.1814 | top1:71.0878 | AUROC:0.7825\n",
      "Test | 20/10 | Loss:0.6113 | MainLoss:0.6113 | SPLoss:0.0091 | CLSLoss:5.1814 | top1:82.6410 | AUROC:0.9438\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.037849\n",
      "Train | 10/10 | Loss:0.1002 | MainLoss:0.0999 | Alpha:0.0475 | SPLoss:0.0062 | CLSLoss:5.1814 | top1:96.5250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0520 | MainLoss:1.0520 | SPLoss:0.0142 | CLSLoss:5.1778 | top1:70.9043 | AUROC:0.7806\n",
      "Test | 20/10 | Loss:0.6175 | MainLoss:0.6175 | SPLoss:0.0142 | CLSLoss:5.1778 | top1:82.3462 | AUROC:0.9464\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.037820\n",
      "Train | 10/10 | Loss:0.0843 | MainLoss:0.0841 | Alpha:0.0479 | SPLoss:0.0056 | CLSLoss:5.1775 | top1:97.2750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0425 | MainLoss:1.0425 | SPLoss:0.0121 | CLSLoss:5.1769 | top1:71.2713 | AUROC:0.7840\n",
      "Test | 20/10 | Loss:0.6359 | MainLoss:0.6359 | SPLoss:0.0121 | CLSLoss:5.1769 | top1:82.0128 | AUROC:0.9431\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.037792\n",
      "Train | 10/10 | Loss:0.0414 | MainLoss:0.0413 | Alpha:0.0490 | SPLoss:0.0024 | CLSLoss:5.1816 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0569 | MainLoss:1.0569 | SPLoss:0.0044 | CLSLoss:5.1881 | top1:71.3401 | AUROC:0.7846\n",
      "Test | 20/10 | Loss:0.6426 | MainLoss:0.6426 | SPLoss:0.0044 | CLSLoss:5.1881 | top1:81.9744 | AUROC:0.9436\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.037763\n",
      "Train | 10/10 | Loss:0.0777 | MainLoss:0.0775 | Alpha:0.0481 | SPLoss:0.0044 | CLSLoss:5.1880 | top1:97.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0643 | MainLoss:1.0643 | SPLoss:0.0098 | CLSLoss:5.1890 | top1:71.3237 | AUROC:0.7841\n",
      "Test | 20/10 | Loss:0.6166 | MainLoss:0.6166 | SPLoss:0.0098 | CLSLoss:5.1890 | top1:82.6538 | AUROC:0.9432\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.037734\n",
      "Train | 10/10 | Loss:0.0427 | MainLoss:0.0426 | Alpha:0.0488 | SPLoss:0.0033 | CLSLoss:5.1934 | top1:98.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0871 | MainLoss:1.0871 | SPLoss:0.0063 | CLSLoss:5.1992 | top1:71.2353 | AUROC:0.7848\n",
      "Test | 20/10 | Loss:0.6865 | MainLoss:0.6865 | SPLoss:0.0063 | CLSLoss:5.1992 | top1:81.5513 | AUROC:0.9428\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.037705\n",
      "Train | 10/10 | Loss:0.0798 | MainLoss:0.0796 | Alpha:0.0480 | SPLoss:0.0042 | CLSLoss:5.2005 | top1:97.2500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0770 | MainLoss:1.0770 | SPLoss:0.0109 | CLSLoss:5.1987 | top1:71.2549 | AUROC:0.7850\n",
      "Test | 20/10 | Loss:0.6598 | MainLoss:0.6598 | SPLoss:0.0109 | CLSLoss:5.1987 | top1:81.7564 | AUROC:0.9418\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.037675\n",
      "Train | 10/10 | Loss:0.0789 | MainLoss:0.0787 | Alpha:0.0480 | SPLoss:0.0041 | CLSLoss:5.1998 | top1:97.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0492 | MainLoss:1.0492 | SPLoss:0.0110 | CLSLoss:5.1979 | top1:71.2385 | AUROC:0.7847\n",
      "Test | 20/10 | Loss:0.6496 | MainLoss:0.6496 | SPLoss:0.0110 | CLSLoss:5.1979 | top1:81.3590 | AUROC:0.9403\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.037646\n",
      "Train | 10/10 | Loss:0.0876 | MainLoss:0.0874 | Alpha:0.0478 | SPLoss:0.0051 | CLSLoss:5.1973 | top1:96.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0387 | MainLoss:1.0387 | SPLoss:0.0125 | CLSLoss:5.1958 | top1:71.5662 | AUROC:0.7856\n",
      "Test | 20/10 | Loss:0.6476 | MainLoss:0.6476 | SPLoss:0.0125 | CLSLoss:5.1958 | top1:81.7821 | AUROC:0.9422\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.037616\n",
      "Train | 10/10 | Loss:0.0612 | MainLoss:0.0611 | Alpha:0.0485 | SPLoss:0.0025 | CLSLoss:5.2000 | top1:97.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0568 | MainLoss:1.0568 | SPLoss:0.0076 | CLSLoss:5.2023 | top1:71.4908 | AUROC:0.7849\n",
      "Test | 20/10 | Loss:0.6160 | MainLoss:0.6160 | SPLoss:0.0076 | CLSLoss:5.2023 | top1:82.6154 | AUROC:0.9417\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.037586\n",
      "Train | 10/10 | Loss:0.0567 | MainLoss:0.0565 | Alpha:0.0486 | SPLoss:0.0037 | CLSLoss:5.2034 | top1:98.0500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0695 | MainLoss:1.0695 | SPLoss:0.0065 | CLSLoss:5.2081 | top1:71.2942 | AUROC:0.7844\n",
      "Test | 20/10 | Loss:0.6432 | MainLoss:0.6432 | SPLoss:0.0065 | CLSLoss:5.2081 | top1:82.0769 | AUROC:0.9450\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.037556\n",
      "Train | 10/10 | Loss:0.0727 | MainLoss:0.0725 | Alpha:0.0483 | SPLoss:0.0043 | CLSLoss:5.2094 | top1:97.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0746 | MainLoss:1.0746 | SPLoss:0.0103 | CLSLoss:5.2090 | top1:71.2516 | AUROC:0.7840\n",
      "Test | 20/10 | Loss:0.5909 | MainLoss:0.5909 | SPLoss:0.0103 | CLSLoss:5.2090 | top1:83.2051 | AUROC:0.9440\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.037526\n",
      "Train | 10/10 | Loss:0.0618 | MainLoss:0.0616 | Alpha:0.0484 | SPLoss:0.0031 | CLSLoss:5.2107 | top1:97.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0737 | MainLoss:1.0737 | SPLoss:0.0085 | CLSLoss:5.2123 | top1:71.3827 | AUROC:0.7845\n",
      "Test | 20/10 | Loss:0.6107 | MainLoss:0.6107 | SPLoss:0.0085 | CLSLoss:5.2123 | top1:82.8077 | AUROC:0.9443\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.037496\n",
      "Train | 10/10 | Loss:0.0661 | MainLoss:0.0659 | Alpha:0.0484 | SPLoss:0.0042 | CLSLoss:5.2142 | top1:97.6250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0817 | MainLoss:1.0817 | SPLoss:0.0094 | CLSLoss:5.2149 | top1:71.3532 | AUROC:0.7838\n",
      "Test | 20/10 | Loss:0.6470 | MainLoss:0.6470 | SPLoss:0.0094 | CLSLoss:5.2149 | top1:82.1538 | AUROC:0.9452\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.037465\n",
      "Train | 10/10 | Loss:0.0891 | MainLoss:0.0889 | Alpha:0.0478 | SPLoss:0.0047 | CLSLoss:5.2156 | top1:97.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.1256 | MainLoss:1.1256 | SPLoss:0.0132 | CLSLoss:5.2101 | top1:70.8126 | AUROC:0.7841\n",
      "Test | 20/10 | Loss:0.7172 | MainLoss:0.7172 | SPLoss:0.0132 | CLSLoss:5.2101 | top1:80.8333 | AUROC:0.9447\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.037435\n",
      "Train | 10/10 | Loss:0.0851 | MainLoss:0.0848 | Alpha:0.0476 | SPLoss:0.0066 | CLSLoss:5.2089 | top1:96.7250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0400 | MainLoss:1.0400 | SPLoss:0.0124 | CLSLoss:5.2089 | top1:71.6252 | AUROC:0.7869\n",
      "Test | 20/10 | Loss:0.6500 | MainLoss:0.6500 | SPLoss:0.0124 | CLSLoss:5.2089 | top1:81.8333 | AUROC:0.9422\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.037404\n",
      "Train | 10/10 | Loss:0.0794 | MainLoss:0.0792 | Alpha:0.0481 | SPLoss:0.0043 | CLSLoss:5.2095 | top1:97.0250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0469 | MainLoss:1.0469 | SPLoss:0.0098 | CLSLoss:5.2090 | top1:71.4974 | AUROC:0.7860\n",
      "Test | 20/10 | Loss:0.6686 | MainLoss:0.6686 | SPLoss:0.0098 | CLSLoss:5.2090 | top1:81.6795 | AUROC:0.9435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [177 | 1000] LR: 0.037373\n",
      "Train | 10/10 | Loss:0.0685 | MainLoss:0.0682 | Alpha:0.0483 | SPLoss:0.0045 | CLSLoss:5.2109 | top1:97.3750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0568 | MainLoss:1.0568 | SPLoss:0.0093 | CLSLoss:5.2137 | top1:71.6055 | AUROC:0.7861\n",
      "Test | 20/10 | Loss:0.6360 | MainLoss:0.6360 | SPLoss:0.0093 | CLSLoss:5.2137 | top1:82.4744 | AUROC:0.9445\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.037341\n",
      "Train | 10/10 | Loss:0.0877 | MainLoss:0.0874 | Alpha:0.0479 | SPLoss:0.0064 | CLSLoss:5.2102 | top1:96.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0495 | MainLoss:1.0495 | SPLoss:0.0104 | CLSLoss:5.2119 | top1:71.4777 | AUROC:0.7856\n",
      "Test | 20/10 | Loss:0.6202 | MainLoss:0.6202 | SPLoss:0.0104 | CLSLoss:5.2119 | top1:82.3462 | AUROC:0.9438\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.037310\n",
      "Train | 10/10 | Loss:0.0466 | MainLoss:0.0465 | Alpha:0.0488 | SPLoss:0.0023 | CLSLoss:5.2165 | top1:98.4750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0644 | MainLoss:1.0644 | SPLoss:0.0058 | CLSLoss:5.2211 | top1:71.3532 | AUROC:0.7867\n",
      "Test | 20/10 | Loss:0.6478 | MainLoss:0.6478 | SPLoss:0.0058 | CLSLoss:5.2211 | top1:81.8846 | AUROC:0.9430\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.037278\n",
      "Train | 10/10 | Loss:0.0571 | MainLoss:0.0570 | Alpha:0.0486 | SPLoss:0.0031 | CLSLoss:5.2233 | top1:98.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0792 | MainLoss:1.0792 | SPLoss:0.0065 | CLSLoss:5.2251 | top1:71.3106 | AUROC:0.7868\n",
      "Test | 20/10 | Loss:0.6544 | MainLoss:0.6544 | SPLoss:0.0065 | CLSLoss:5.2251 | top1:81.9359 | AUROC:0.9424\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.037247\n",
      "Train | 10/10 | Loss:0.0717 | MainLoss:0.0714 | Alpha:0.0482 | SPLoss:0.0062 | CLSLoss:5.2239 | top1:97.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0719 | MainLoss:1.0719 | SPLoss:0.0107 | CLSLoss:5.2260 | top1:71.6022 | AUROC:0.7863\n",
      "Test | 20/10 | Loss:0.6191 | MainLoss:0.6191 | SPLoss:0.0107 | CLSLoss:5.2260 | top1:82.7692 | AUROC:0.9433\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.037215\n",
      "Train | 10/10 | Loss:0.0747 | MainLoss:0.0745 | Alpha:0.0482 | SPLoss:0.0050 | CLSLoss:5.2260 | top1:97.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0644 | MainLoss:1.0644 | SPLoss:0.0105 | CLSLoss:5.2256 | top1:71.6678 | AUROC:0.7878\n",
      "Test | 20/10 | Loss:0.6113 | MainLoss:0.6113 | SPLoss:0.0105 | CLSLoss:5.2256 | top1:82.8205 | AUROC:0.9435\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.037183\n",
      "Train | 10/10 | Loss:0.0732 | MainLoss:0.0729 | Alpha:0.0482 | SPLoss:0.0052 | CLSLoss:5.2256 | top1:97.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0501 | MainLoss:1.0501 | SPLoss:0.0110 | CLSLoss:5.2267 | top1:71.7038 | AUROC:0.7880\n",
      "Test | 20/10 | Loss:0.6171 | MainLoss:0.6171 | SPLoss:0.0110 | CLSLoss:5.2267 | top1:82.4487 | AUROC:0.9416\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.037151\n",
      "Train | 10/10 | Loss:0.0813 | MainLoss:0.0810 | Alpha:0.0480 | SPLoss:0.0046 | CLSLoss:5.2262 | top1:97.2500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0416 | MainLoss:1.0416 | SPLoss:0.0105 | CLSLoss:5.2251 | top1:71.6153 | AUROC:0.7890\n",
      "Test | 20/10 | Loss:0.6094 | MainLoss:0.6094 | SPLoss:0.0105 | CLSLoss:5.2251 | top1:82.5385 | AUROC:0.9406\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.037118\n",
      "Train | 10/10 | Loss:0.0513 | MainLoss:0.0512 | Alpha:0.0487 | SPLoss:0.0027 | CLSLoss:5.2276 | top1:98.4250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0557 | MainLoss:1.0557 | SPLoss:0.0066 | CLSLoss:5.2288 | top1:71.6514 | AUROC:0.7883\n",
      "Test | 20/10 | Loss:0.6444 | MainLoss:0.6444 | SPLoss:0.0066 | CLSLoss:5.2288 | top1:81.8077 | AUROC:0.9396\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.037086\n",
      "Train | 10/10 | Loss:0.0814 | MainLoss:0.0811 | Alpha:0.0481 | SPLoss:0.0058 | CLSLoss:5.2274 | top1:96.8000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0483 | MainLoss:1.0483 | SPLoss:0.0117 | CLSLoss:5.2271 | top1:71.6841 | AUROC:0.7887\n",
      "Test | 20/10 | Loss:0.6176 | MainLoss:0.6176 | SPLoss:0.0117 | CLSLoss:5.2271 | top1:82.5385 | AUROC:0.9400\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.037053\n",
      "Train | 10/10 | Loss:0.0729 | MainLoss:0.0726 | Alpha:0.0482 | SPLoss:0.0048 | CLSLoss:5.2268 | top1:97.6000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0413 | MainLoss:1.0413 | SPLoss:0.0090 | CLSLoss:5.2271 | top1:71.6809 | AUROC:0.7886\n",
      "Test | 20/10 | Loss:0.6125 | MainLoss:0.6125 | SPLoss:0.0090 | CLSLoss:5.2271 | top1:82.7179 | AUROC:0.9398\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.037020\n",
      "Train | 10/10 | Loss:0.0739 | MainLoss:0.0736 | Alpha:0.0482 | SPLoss:0.0053 | CLSLoss:5.2267 | top1:97.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0465 | MainLoss:1.0465 | SPLoss:0.0097 | CLSLoss:5.2285 | top1:71.8709 | AUROC:0.7894\n",
      "Test | 20/10 | Loss:0.6219 | MainLoss:0.6219 | SPLoss:0.0097 | CLSLoss:5.2285 | top1:82.5385 | AUROC:0.9408\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.036987\n",
      "Train | 10/10 | Loss:0.0703 | MainLoss:0.0701 | Alpha:0.0483 | SPLoss:0.0036 | CLSLoss:5.2300 | top1:97.3750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0681 | MainLoss:1.0681 | SPLoss:0.0102 | CLSLoss:5.2280 | top1:71.4744 | AUROC:0.7894\n",
      "Test | 20/10 | Loss:0.6719 | MainLoss:0.6719 | SPLoss:0.0102 | CLSLoss:5.2280 | top1:81.3077 | AUROC:0.9411\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.036954\n",
      "Train | 10/10 | Loss:0.0779 | MainLoss:0.0777 | Alpha:0.0480 | SPLoss:0.0041 | CLSLoss:5.2291 | top1:97.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0536 | MainLoss:1.0536 | SPLoss:0.0107 | CLSLoss:5.2273 | top1:71.6153 | AUROC:0.7886\n",
      "Test | 20/10 | Loss:0.6427 | MainLoss:0.6427 | SPLoss:0.0107 | CLSLoss:5.2273 | top1:81.9487 | AUROC:0.9427\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.036920\n",
      "Train | 10/10 | Loss:0.0523 | MainLoss:0.0521 | Alpha:0.0487 | SPLoss:0.0042 | CLSLoss:5.2303 | top1:98.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0669 | MainLoss:1.0669 | SPLoss:0.0071 | CLSLoss:5.2351 | top1:71.6383 | AUROC:0.7895\n",
      "Test | 20/10 | Loss:0.6570 | MainLoss:0.6570 | SPLoss:0.0071 | CLSLoss:5.2351 | top1:81.7692 | AUROC:0.9413\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.036887\n",
      "Train | 10/10 | Loss:0.0528 | MainLoss:0.0527 | Alpha:0.0487 | SPLoss:0.0030 | CLSLoss:5.2372 | top1:98.3500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0594 | MainLoss:1.0594 | SPLoss:0.0073 | CLSLoss:5.2389 | top1:71.8250 | AUROC:0.7899\n",
      "Test | 20/10 | Loss:0.5956 | MainLoss:0.5956 | SPLoss:0.0073 | CLSLoss:5.2389 | top1:83.0385 | AUROC:0.9431\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.036853\n",
      "Train | 10/10 | Loss:0.0975 | MainLoss:0.0972 | Alpha:0.0475 | SPLoss:0.0062 | CLSLoss:5.2382 | top1:96.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0546 | MainLoss:1.0546 | SPLoss:0.0146 | CLSLoss:5.2349 | top1:71.5433 | AUROC:0.7895\n",
      "Test | 20/10 | Loss:0.6552 | MainLoss:0.6552 | SPLoss:0.0146 | CLSLoss:5.2350 | top1:81.4615 | AUROC:0.9414\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.036819\n",
      "Train | 10/10 | Loss:0.1011 | MainLoss:0.1008 | Alpha:0.0474 | SPLoss:0.0064 | CLSLoss:5.2319 | top1:96.5500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0399 | MainLoss:1.0399 | SPLoss:0.0132 | CLSLoss:5.2294 | top1:71.5826 | AUROC:0.7894\n",
      "Test | 20/10 | Loss:0.6574 | MainLoss:0.6574 | SPLoss:0.0132 | CLSLoss:5.2294 | top1:81.3718 | AUROC:0.9392\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.036785\n",
      "Train | 10/10 | Loss:0.0440 | MainLoss:0.0439 | Alpha:0.0489 | SPLoss:0.0021 | CLSLoss:5.2337 | top1:98.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0491 | MainLoss:1.0491 | SPLoss:0.0061 | CLSLoss:5.2365 | top1:71.6907 | AUROC:0.7896\n",
      "Test | 20/10 | Loss:0.5988 | MainLoss:0.5988 | SPLoss:0.0061 | CLSLoss:5.2365 | top1:82.6282 | AUROC:0.9405\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.036751\n",
      "Train | 10/10 | Loss:0.0398 | MainLoss:0.0397 | Alpha:0.0489 | SPLoss:0.0018 | CLSLoss:5.2407 | top1:98.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0633 | MainLoss:1.0633 | SPLoss:0.0056 | CLSLoss:5.2439 | top1:71.5662 | AUROC:0.7881\n",
      "Test | 20/10 | Loss:0.6288 | MainLoss:0.6288 | SPLoss:0.0056 | CLSLoss:5.2439 | top1:82.1154 | AUROC:0.9417\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.036716\n",
      "Train | 10/10 | Loss:0.0520 | MainLoss:0.0519 | Alpha:0.0487 | SPLoss:0.0030 | CLSLoss:5.2469 | top1:98.3000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0767 | MainLoss:1.0767 | SPLoss:0.0074 | CLSLoss:5.2487 | top1:71.6350 | AUROC:0.7883\n",
      "Test | 20/10 | Loss:0.5922 | MainLoss:0.5922 | SPLoss:0.0074 | CLSLoss:5.2487 | top1:83.1410 | AUROC:0.9443\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.036682\n",
      "Train | 10/10 | Loss:0.0651 | MainLoss:0.0649 | Alpha:0.0483 | SPLoss:0.0051 | CLSLoss:5.2483 | top1:97.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0621 | MainLoss:1.0621 | SPLoss:0.0093 | CLSLoss:5.2505 | top1:71.7038 | AUROC:0.7892\n",
      "Test | 20/10 | Loss:0.6461 | MainLoss:0.6461 | SPLoss:0.0093 | CLSLoss:5.2505 | top1:81.7692 | AUROC:0.9405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [199 | 1000] LR: 0.036647\n",
      "Train | 10/10 | Loss:0.0885 | MainLoss:0.0883 | Alpha:0.0478 | SPLoss:0.0048 | CLSLoss:5.2498 | top1:97.1000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0519 | MainLoss:1.0519 | SPLoss:0.0131 | CLSLoss:5.2447 | top1:71.5498 | AUROC:0.7879\n",
      "Test | 20/10 | Loss:0.6483 | MainLoss:0.6483 | SPLoss:0.0131 | CLSLoss:5.2447 | top1:81.8462 | AUROC:0.9418\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.036612\n",
      "Train | 10/10 | Loss:0.0492 | MainLoss:0.0491 | Alpha:0.0487 | SPLoss:0.0021 | CLSLoss:5.2485 | top1:98.4750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0639 | MainLoss:1.0639 | SPLoss:0.0069 | CLSLoss:5.2501 | top1:71.7333 | AUROC:0.7895\n",
      "Test | 20/10 | Loss:0.6275 | MainLoss:0.6275 | SPLoss:0.0069 | CLSLoss:5.2501 | top1:82.3846 | AUROC:0.9400\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.036577\n",
      "Train | 10/10 | Loss:0.0718 | MainLoss:0.0715 | Alpha:0.0482 | SPLoss:0.0053 | CLSLoss:5.2480 | top1:97.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0530 | MainLoss:1.0530 | SPLoss:0.0110 | CLSLoss:5.2466 | top1:71.7759 | AUROC:0.7909\n",
      "Test | 20/10 | Loss:0.6315 | MainLoss:0.6315 | SPLoss:0.0110 | CLSLoss:5.2466 | top1:82.0000 | AUROC:0.9386\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.036542\n",
      "Train | 10/10 | Loss:0.0676 | MainLoss:0.0674 | Alpha:0.0484 | SPLoss:0.0051 | CLSLoss:5.2459 | top1:97.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0555 | MainLoss:1.0555 | SPLoss:0.0096 | CLSLoss:5.2469 | top1:71.6317 | AUROC:0.7891\n",
      "Test | 20/10 | Loss:0.5941 | MainLoss:0.5941 | SPLoss:0.0096 | CLSLoss:5.2469 | top1:83.1795 | AUROC:0.9418\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.036506\n",
      "Train | 10/10 | Loss:0.0432 | MainLoss:0.0431 | Alpha:0.0489 | SPLoss:0.0028 | CLSLoss:5.2497 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0877 | MainLoss:1.0877 | SPLoss:0.0068 | CLSLoss:5.2526 | top1:71.3794 | AUROC:0.7882\n",
      "Test | 20/10 | Loss:0.6693 | MainLoss:0.6693 | SPLoss:0.0068 | CLSLoss:5.2526 | top1:81.8077 | AUROC:0.9409\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.036471\n",
      "Train | 10/10 | Loss:0.0529 | MainLoss:0.0528 | Alpha:0.0486 | SPLoss:0.0029 | CLSLoss:5.2551 | top1:98.3500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0947 | MainLoss:1.0947 | SPLoss:0.0077 | CLSLoss:5.2555 | top1:71.5138 | AUROC:0.7876\n",
      "Test | 20/10 | Loss:0.6704 | MainLoss:0.6704 | SPLoss:0.0077 | CLSLoss:5.2555 | top1:81.6667 | AUROC:0.9405\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.036435\n",
      "Train | 10/10 | Loss:0.0611 | MainLoss:0.0608 | Alpha:0.0485 | SPLoss:0.0060 | CLSLoss:5.2543 | top1:97.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0714 | MainLoss:1.0714 | SPLoss:0.0097 | CLSLoss:5.2556 | top1:71.7136 | AUROC:0.7894\n",
      "Test | 20/10 | Loss:0.6390 | MainLoss:0.6390 | SPLoss:0.0097 | CLSLoss:5.2556 | top1:82.2051 | AUROC:0.9402\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.036399\n",
      "Train | 10/10 | Loss:0.0544 | MainLoss:0.0542 | Alpha:0.0487 | SPLoss:0.0032 | CLSLoss:5.2575 | top1:98.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0836 | MainLoss:1.0836 | SPLoss:0.0073 | CLSLoss:5.2599 | top1:71.7136 | AUROC:0.7887\n",
      "Test | 20/10 | Loss:0.6329 | MainLoss:0.6329 | SPLoss:0.0073 | CLSLoss:5.2599 | top1:82.4872 | AUROC:0.9404\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.036363\n",
      "Train | 10/10 | Loss:0.0848 | MainLoss:0.0845 | Alpha:0.0479 | SPLoss:0.0059 | CLSLoss:5.2577 | top1:96.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0711 | MainLoss:1.0711 | SPLoss:0.0132 | CLSLoss:5.2536 | top1:71.5891 | AUROC:0.7896\n",
      "Test | 20/10 | Loss:0.5722 | MainLoss:0.5722 | SPLoss:0.0132 | CLSLoss:5.2536 | top1:83.4872 | AUROC:0.9397\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.036327\n",
      "Train | 10/10 | Loss:0.0661 | MainLoss:0.0659 | Alpha:0.0483 | SPLoss:0.0055 | CLSLoss:5.2525 | top1:97.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0695 | MainLoss:1.0695 | SPLoss:0.0096 | CLSLoss:5.2535 | top1:71.8480 | AUROC:0.7895\n",
      "Test | 20/10 | Loss:0.6084 | MainLoss:0.6084 | SPLoss:0.0096 | CLSLoss:5.2535 | top1:82.7692 | AUROC:0.9407\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.036290\n",
      "Train | 10/10 | Loss:0.0674 | MainLoss:0.0672 | Alpha:0.0484 | SPLoss:0.0059 | CLSLoss:5.2523 | top1:97.5000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0688 | MainLoss:1.0688 | SPLoss:0.0101 | CLSLoss:5.2518 | top1:71.7235 | AUROC:0.7896\n",
      "Test | 20/10 | Loss:0.6691 | MainLoss:0.6691 | SPLoss:0.0101 | CLSLoss:5.2518 | top1:81.4615 | AUROC:0.9403\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.036254\n",
      "Train | 10/10 | Loss:0.0760 | MainLoss:0.0757 | Alpha:0.0481 | SPLoss:0.0055 | CLSLoss:5.2500 | top1:97.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0644 | MainLoss:1.0644 | SPLoss:0.0106 | CLSLoss:5.2497 | top1:71.7726 | AUROC:0.7905\n",
      "Test | 20/10 | Loss:0.6621 | MainLoss:0.6621 | SPLoss:0.0106 | CLSLoss:5.2497 | top1:81.6795 | AUROC:0.9425\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.036217\n",
      "Train | 10/10 | Loss:0.0673 | MainLoss:0.0671 | Alpha:0.0483 | SPLoss:0.0025 | CLSLoss:5.2522 | top1:97.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0648 | MainLoss:1.0648 | SPLoss:0.0099 | CLSLoss:5.2500 | top1:71.7431 | AUROC:0.7910\n",
      "Test | 20/10 | Loss:0.5832 | MainLoss:0.5832 | SPLoss:0.0099 | CLSLoss:5.2500 | top1:83.2949 | AUROC:0.9405\n",
      "\n",
      "Epoch: [212 | 1000] LR: 0.036180\n",
      "Train | 10/10 | Loss:0.0549 | MainLoss:0.0547 | Alpha:0.0485 | SPLoss:0.0046 | CLSLoss:5.2503 | top1:97.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0586 | MainLoss:1.0586 | SPLoss:0.0081 | CLSLoss:5.2526 | top1:71.7955 | AUROC:0.7914\n",
      "Test | 20/10 | Loss:0.6510 | MainLoss:0.6510 | SPLoss:0.0081 | CLSLoss:5.2526 | top1:82.2436 | AUROC:0.9393\n",
      "\n",
      "Epoch: [213 | 1000] LR: 0.036143\n",
      "Train | 10/10 | Loss:0.0807 | MainLoss:0.0804 | Alpha:0.0480 | SPLoss:0.0057 | CLSLoss:5.2516 | top1:97.0000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0598 | MainLoss:1.0598 | SPLoss:0.0125 | CLSLoss:5.2497 | top1:71.9168 | AUROC:0.7922\n",
      "Test | 20/10 | Loss:0.6508 | MainLoss:0.6508 | SPLoss:0.0125 | CLSLoss:5.2497 | top1:81.7436 | AUROC:0.9399\n",
      "\n",
      "Epoch: [214 | 1000] LR: 0.036106\n",
      "Train | 10/10 | Loss:0.0318 | MainLoss:0.0317 | Alpha:0.0492 | SPLoss:0.0016 | CLSLoss:5.2540 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0686 | MainLoss:1.0686 | SPLoss:0.0050 | CLSLoss:5.2572 | top1:72.0577 | AUROC:0.7935\n",
      "Test | 20/10 | Loss:0.6189 | MainLoss:0.6189 | SPLoss:0.0050 | CLSLoss:5.2572 | top1:82.8333 | AUROC:0.9407\n",
      "\n",
      "Epoch: [215 | 1000] LR: 0.036069\n",
      "Train | 10/10 | Loss:0.0691 | MainLoss:0.0688 | Alpha:0.0484 | SPLoss:0.0062 | CLSLoss:5.2554 | top1:97.3750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0576 | MainLoss:1.0576 | SPLoss:0.0108 | CLSLoss:5.2565 | top1:72.0052 | AUROC:0.7934\n",
      "Test | 20/10 | Loss:0.6371 | MainLoss:0.6371 | SPLoss:0.0108 | CLSLoss:5.2565 | top1:82.2692 | AUROC:0.9416\n",
      "\n",
      "Epoch: [216 | 1000] LR: 0.036031\n",
      "Train | 10/10 | Loss:0.0576 | MainLoss:0.0573 | Alpha:0.0486 | SPLoss:0.0049 | CLSLoss:5.2566 | top1:97.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0603 | MainLoss:1.0603 | SPLoss:0.0085 | CLSLoss:5.2593 | top1:72.0085 | AUROC:0.7929\n",
      "Test | 20/10 | Loss:0.6542 | MainLoss:0.6542 | SPLoss:0.0085 | CLSLoss:5.2593 | top1:81.8974 | AUROC:0.9405\n",
      "\n",
      "Epoch: [217 | 1000] LR: 0.035994\n",
      "Train | 10/10 | Loss:0.0874 | MainLoss:0.0871 | Alpha:0.0478 | SPLoss:0.0065 | CLSLoss:5.2555 | top1:96.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0471 | MainLoss:1.0471 | SPLoss:0.0137 | CLSLoss:5.2533 | top1:71.9594 | AUROC:0.7928\n",
      "Test | 20/10 | Loss:0.6352 | MainLoss:0.6352 | SPLoss:0.0137 | CLSLoss:5.2533 | top1:82.1667 | AUROC:0.9390\n",
      "\n",
      "Epoch: [218 | 1000] LR: 0.035956\n",
      "Train | 10/10 | Loss:0.0593 | MainLoss:0.0591 | Alpha:0.0486 | SPLoss:0.0041 | CLSLoss:5.2537 | top1:98.0000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0396 | MainLoss:1.0396 | SPLoss:0.0082 | CLSLoss:5.2553 | top1:72.2051 | AUROC:0.7943\n",
      "Test | 20/10 | Loss:0.6011 | MainLoss:0.6011 | SPLoss:0.0082 | CLSLoss:5.2553 | top1:82.6538 | AUROC:0.9379\n",
      "\n",
      "Epoch: [219 | 1000] LR: 0.035918\n",
      "Train | 10/10 | Loss:0.0520 | MainLoss:0.0518 | Alpha:0.0487 | SPLoss:0.0042 | CLSLoss:5.2573 | top1:98.1500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0524 | MainLoss:1.0524 | SPLoss:0.0087 | CLSLoss:5.2594 | top1:72.0020 | AUROC:0.7926\n",
      "Test | 20/10 | Loss:0.6482 | MainLoss:0.6482 | SPLoss:0.0087 | CLSLoss:5.2594 | top1:81.7949 | AUROC:0.9389\n",
      "\n",
      "Epoch: [220 | 1000] LR: 0.035880\n",
      "Train | 10/10 | Loss:0.0675 | MainLoss:0.0673 | Alpha:0.0483 | SPLoss:0.0044 | CLSLoss:5.2586 | top1:97.5250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0437 | MainLoss:1.0437 | SPLoss:0.0100 | CLSLoss:5.2576 | top1:72.0380 | AUROC:0.7926\n",
      "Test | 20/10 | Loss:0.6630 | MainLoss:0.6630 | SPLoss:0.0100 | CLSLoss:5.2576 | top1:81.3974 | AUROC:0.9406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [221 | 1000] LR: 0.035842\n",
      "Train | 10/10 | Loss:0.0583 | MainLoss:0.0582 | Alpha:0.0485 | SPLoss:0.0027 | CLSLoss:5.2600 | top1:97.9750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0652 | MainLoss:1.0652 | SPLoss:0.0094 | CLSLoss:5.2584 | top1:71.9070 | AUROC:0.7913\n",
      "Test | 20/10 | Loss:0.6240 | MainLoss:0.6240 | SPLoss:0.0094 | CLSLoss:5.2584 | top1:82.3077 | AUROC:0.9380\n",
      "\n",
      "Epoch: [222 | 1000] LR: 0.035803\n",
      "Train | 10/10 | Loss:0.0567 | MainLoss:0.0565 | Alpha:0.0484 | SPLoss:0.0047 | CLSLoss:5.2589 | top1:98.0750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0579 | MainLoss:1.0579 | SPLoss:0.0092 | CLSLoss:5.2602 | top1:71.9430 | AUROC:0.7928\n",
      "Test | 20/10 | Loss:0.6560 | MainLoss:0.6560 | SPLoss:0.0092 | CLSLoss:5.2602 | top1:81.7179 | AUROC:0.9376\n",
      "\n",
      "Epoch: [223 | 1000] LR: 0.035765\n",
      "Train | 10/10 | Loss:0.0374 | MainLoss:0.0372 | Alpha:0.0491 | SPLoss:0.0024 | CLSLoss:5.2632 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0572 | MainLoss:1.0572 | SPLoss:0.0049 | CLSLoss:5.2661 | top1:72.0642 | AUROC:0.7934\n",
      "Test | 20/10 | Loss:0.6470 | MainLoss:0.6470 | SPLoss:0.0049 | CLSLoss:5.2661 | top1:81.9615 | AUROC:0.9380\n",
      "\n",
      "Epoch: [224 | 1000] LR: 0.035726\n",
      "Train | 10/10 | Loss:0.0330 | MainLoss:0.0329 | Alpha:0.0492 | SPLoss:0.0024 | CLSLoss:5.2696 | top1:98.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.1142 | MainLoss:1.1142 | SPLoss:0.0055 | CLSLoss:5.2743 | top1:71.6514 | AUROC:0.7921\n",
      "Test | 20/10 | Loss:0.7001 | MainLoss:0.7001 | SPLoss:0.0055 | CLSLoss:5.2743 | top1:81.3462 | AUROC:0.9391\n",
      "\n",
      "Epoch: [225 | 1000] LR: 0.035687\n",
      "Train | 10/10 | Loss:0.0640 | MainLoss:0.0638 | Alpha:0.0483 | SPLoss:0.0050 | CLSLoss:5.2734 | top1:97.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0773 | MainLoss:1.0773 | SPLoss:0.0107 | CLSLoss:5.2723 | top1:71.7693 | AUROC:0.7926\n",
      "Test | 20/10 | Loss:0.6348 | MainLoss:0.6348 | SPLoss:0.0107 | CLSLoss:5.2723 | top1:82.4615 | AUROC:0.9420\n",
      "\n",
      "Epoch: [226 | 1000] LR: 0.035648\n",
      "Train | 10/10 | Loss:0.0519 | MainLoss:0.0517 | Alpha:0.0488 | SPLoss:0.0029 | CLSLoss:5.2737 | top1:98.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0795 | MainLoss:1.0795 | SPLoss:0.0073 | CLSLoss:5.2731 | top1:71.9266 | AUROC:0.7922\n",
      "Test | 20/10 | Loss:0.6286 | MainLoss:0.6286 | SPLoss:0.0073 | CLSLoss:5.2731 | top1:82.5000 | AUROC:0.9416\n",
      "\n",
      "Epoch: [227 | 1000] LR: 0.035609\n",
      "Train | 10/10 | Loss:0.0667 | MainLoss:0.0665 | Alpha:0.0484 | SPLoss:0.0053 | CLSLoss:5.2708 | top1:97.8000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0843 | MainLoss:1.0843 | SPLoss:0.0100 | CLSLoss:5.2711 | top1:71.7890 | AUROC:0.7926\n",
      "Test | 20/10 | Loss:0.6711 | MainLoss:0.6711 | SPLoss:0.0100 | CLSLoss:5.2711 | top1:81.6923 | AUROC:0.9397\n",
      "\n",
      "Epoch: [228 | 1000] LR: 0.035569\n",
      "Train | 10/10 | Loss:0.0612 | MainLoss:0.0610 | Alpha:0.0484 | SPLoss:0.0030 | CLSLoss:5.2725 | top1:97.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0569 | MainLoss:1.0569 | SPLoss:0.0100 | CLSLoss:5.2710 | top1:72.0577 | AUROC:0.7936\n",
      "Test | 20/10 | Loss:0.6282 | MainLoss:0.6282 | SPLoss:0.0100 | CLSLoss:5.2710 | top1:82.2949 | AUROC:0.9404\n",
      "\n",
      "Epoch: [229 | 1000] LR: 0.035530\n",
      "Train | 10/10 | Loss:0.0340 | MainLoss:0.0339 | Alpha:0.0491 | SPLoss:0.0021 | CLSLoss:5.2740 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0729 | MainLoss:1.0729 | SPLoss:0.0054 | CLSLoss:5.2768 | top1:72.0577 | AUROC:0.7941\n",
      "Test | 20/10 | Loss:0.6392 | MainLoss:0.6392 | SPLoss:0.0054 | CLSLoss:5.2768 | top1:82.3077 | AUROC:0.9415\n",
      "\n",
      "Epoch: [230 | 1000] LR: 0.035490\n",
      "Train | 10/10 | Loss:0.0718 | MainLoss:0.0717 | Alpha:0.0483 | SPLoss:0.0035 | CLSLoss:5.2771 | top1:97.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0602 | MainLoss:1.0602 | SPLoss:0.0106 | CLSLoss:5.2722 | top1:72.1265 | AUROC:0.7937\n",
      "Test | 20/10 | Loss:0.6294 | MainLoss:0.6294 | SPLoss:0.0106 | CLSLoss:5.2722 | top1:82.3974 | AUROC:0.9404\n",
      "\n",
      "Epoch: [231 | 1000] LR: 0.035450\n",
      "Train | 10/10 | Loss:0.0550 | MainLoss:0.0547 | Alpha:0.0487 | SPLoss:0.0047 | CLSLoss:5.2712 | top1:98.1750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0620 | MainLoss:1.0620 | SPLoss:0.0076 | CLSLoss:5.2736 | top1:72.0380 | AUROC:0.7947\n",
      "Test | 20/10 | Loss:0.6673 | MainLoss:0.6673 | SPLoss:0.0076 | CLSLoss:5.2736 | top1:81.4615 | AUROC:0.9372\n",
      "\n",
      "Epoch: [232 | 1000] LR: 0.035410\n",
      "Train | 10/10 | Loss:0.1141 | MainLoss:0.1138 | Alpha:0.0472 | SPLoss:0.0069 | CLSLoss:5.2704 | top1:95.6500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0558 | MainLoss:1.0558 | SPLoss:0.0177 | CLSLoss:5.2617 | top1:71.8185 | AUROC:0.7922\n",
      "Test | 20/10 | Loss:0.6083 | MainLoss:0.6083 | SPLoss:0.0177 | CLSLoss:5.2617 | top1:82.6667 | AUROC:0.9404\n",
      "\n",
      "Epoch: [233 | 1000] LR: 0.035370\n",
      "Train | 10/10 | Loss:0.0662 | MainLoss:0.0660 | Alpha:0.0485 | SPLoss:0.0044 | CLSLoss:5.2601 | top1:97.6250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0391 | MainLoss:1.0391 | SPLoss:0.0097 | CLSLoss:5.2595 | top1:71.9332 | AUROC:0.7923\n",
      "Test | 20/10 | Loss:0.6596 | MainLoss:0.6596 | SPLoss:0.0097 | CLSLoss:5.2595 | top1:81.5769 | AUROC:0.9389\n",
      "\n",
      "Epoch: [234 | 1000] LR: 0.035330\n",
      "Train | 10/10 | Loss:0.0338 | MainLoss:0.0337 | Alpha:0.0491 | SPLoss:0.0017 | CLSLoss:5.2632 | top1:98.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0457 | MainLoss:1.0457 | SPLoss:0.0049 | CLSLoss:5.2668 | top1:72.1330 | AUROC:0.7942\n",
      "Test | 20/10 | Loss:0.6331 | MainLoss:0.6331 | SPLoss:0.0049 | CLSLoss:5.2668 | top1:82.1795 | AUROC:0.9401\n",
      "\n",
      "Epoch: [235 | 1000] LR: 0.035289\n",
      "Train | 10/10 | Loss:0.0552 | MainLoss:0.0550 | Alpha:0.0487 | SPLoss:0.0029 | CLSLoss:5.2677 | top1:98.1250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0447 | MainLoss:1.0447 | SPLoss:0.0074 | CLSLoss:5.2678 | top1:72.2477 | AUROC:0.7949\n",
      "Test | 20/10 | Loss:0.6770 | MainLoss:0.6770 | SPLoss:0.0074 | CLSLoss:5.2678 | top1:81.1795 | AUROC:0.9386\n",
      "\n",
      "Epoch: [236 | 1000] LR: 0.035249\n",
      "Train | 10/10 | Loss:0.0429 | MainLoss:0.0428 | Alpha:0.0489 | SPLoss:0.0031 | CLSLoss:5.2698 | top1:98.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0424 | MainLoss:1.0424 | SPLoss:0.0065 | CLSLoss:5.2726 | top1:72.1461 | AUROC:0.7948\n",
      "Test | 20/10 | Loss:0.6177 | MainLoss:0.6177 | SPLoss:0.0065 | CLSLoss:5.2726 | top1:82.7436 | AUROC:0.9403\n",
      "\n",
      "Epoch: [237 | 1000] LR: 0.035208\n",
      "Train | 10/10 | Loss:0.0395 | MainLoss:0.0394 | Alpha:0.0490 | SPLoss:0.0021 | CLSLoss:5.2751 | top1:98.8750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0493 | MainLoss:1.0493 | SPLoss:0.0052 | CLSLoss:5.2772 | top1:72.3886 | AUROC:0.7953\n",
      "Test | 20/10 | Loss:0.6395 | MainLoss:0.6395 | SPLoss:0.0052 | CLSLoss:5.2772 | top1:82.1154 | AUROC:0.9381\n",
      "\n",
      "Epoch: [238 | 1000] LR: 0.035167\n",
      "Train | 10/10 | Loss:0.0264 | MainLoss:0.0263 | Alpha:0.0493 | SPLoss:0.0013 | CLSLoss:5.2807 | top1:99.2500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0583 | MainLoss:1.0583 | SPLoss:0.0037 | CLSLoss:5.2839 | top1:72.1658 | AUROC:0.7943\n",
      "Test | 20/10 | Loss:0.6271 | MainLoss:0.6271 | SPLoss:0.0037 | CLSLoss:5.2839 | top1:82.3974 | AUROC:0.9374\n",
      "\n",
      "Epoch: [239 | 1000] LR: 0.035126\n",
      "Train | 10/10 | Loss:0.0454 | MainLoss:0.0452 | Alpha:0.0489 | SPLoss:0.0035 | CLSLoss:5.2842 | top1:98.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0688 | MainLoss:1.0688 | SPLoss:0.0066 | CLSLoss:5.2861 | top1:72.1560 | AUROC:0.7943\n",
      "Test | 20/10 | Loss:0.6402 | MainLoss:0.6402 | SPLoss:0.0066 | CLSLoss:5.2861 | top1:82.3333 | AUROC:0.9377\n",
      "\n",
      "Epoch: [240 | 1000] LR: 0.035085\n",
      "Train | 10/10 | Loss:0.0749 | MainLoss:0.0747 | Alpha:0.0482 | SPLoss:0.0043 | CLSLoss:5.2859 | top1:97.2250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0767 | MainLoss:1.0767 | SPLoss:0.0118 | CLSLoss:5.2823 | top1:72.0085 | AUROC:0.7942\n",
      "Test | 20/10 | Loss:0.6540 | MainLoss:0.6540 | SPLoss:0.0118 | CLSLoss:5.2823 | top1:81.9487 | AUROC:0.9381\n",
      "\n",
      "Epoch: [241 | 1000] LR: 0.035044\n",
      "Train | 10/10 | Loss:0.0243 | MainLoss:0.0243 | Alpha:0.0494 | SPLoss:0.0016 | CLSLoss:5.2858 | top1:99.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0848 | MainLoss:1.0848 | SPLoss:0.0034 | CLSLoss:5.2894 | top1:72.2838 | AUROC:0.7950\n",
      "Test | 20/10 | Loss:0.6362 | MainLoss:0.6362 | SPLoss:0.0034 | CLSLoss:5.2894 | top1:82.3974 | AUROC:0.9367\n",
      "\n",
      "Epoch: [242 | 1000] LR: 0.035002\n",
      "Train | 10/10 | Loss:0.0585 | MainLoss:0.0583 | Alpha:0.0486 | SPLoss:0.0036 | CLSLoss:5.2898 | top1:97.8500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.1119 | MainLoss:1.1119 | SPLoss:0.0095 | CLSLoss:5.2885 | top1:71.8119 | AUROC:0.7935\n",
      "Test | 20/10 | Loss:0.6908 | MainLoss:0.6908 | SPLoss:0.0095 | CLSLoss:5.2885 | top1:81.4744 | AUROC:0.9371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [243 | 1000] LR: 0.034961\n",
      "Train | 10/10 | Loss:0.0579 | MainLoss:0.0577 | Alpha:0.0486 | SPLoss:0.0039 | CLSLoss:5.2883 | top1:98.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0474 | MainLoss:1.0474 | SPLoss:0.0100 | CLSLoss:5.2867 | top1:72.3067 | AUROC:0.7960\n",
      "Test | 20/10 | Loss:0.6118 | MainLoss:0.6118 | SPLoss:0.0100 | CLSLoss:5.2867 | top1:82.7564 | AUROC:0.9382\n",
      "\n",
      "Epoch: [244 | 1000] LR: 0.034919\n",
      "Train | 10/10 | Loss:0.0392 | MainLoss:0.0390 | Alpha:0.0490 | SPLoss:0.0036 | CLSLoss:5.2878 | top1:98.5750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0717 | MainLoss:1.0717 | SPLoss:0.0054 | CLSLoss:5.2914 | top1:72.2903 | AUROC:0.7950\n",
      "Test | 20/10 | Loss:0.6308 | MainLoss:0.6308 | SPLoss:0.0054 | CLSLoss:5.2914 | top1:82.7821 | AUROC:0.9392\n",
      "\n",
      "Epoch: [245 | 1000] LR: 0.034877\n",
      "Train | 10/10 | Loss:0.0636 | MainLoss:0.0635 | Alpha:0.0485 | SPLoss:0.0027 | CLSLoss:5.2926 | top1:97.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0598 | MainLoss:1.0598 | SPLoss:0.0098 | CLSLoss:5.2879 | top1:72.3886 | AUROC:0.7955\n",
      "Test | 20/10 | Loss:0.6551 | MainLoss:0.6551 | SPLoss:0.0098 | CLSLoss:5.2879 | top1:82.1795 | AUROC:0.9377\n",
      "\n",
      "Epoch: [246 | 1000] LR: 0.034835\n",
      "Train | 10/10 | Loss:0.0483 | MainLoss:0.0482 | Alpha:0.0488 | SPLoss:0.0031 | CLSLoss:5.2896 | top1:98.3250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0658 | MainLoss:1.0658 | SPLoss:0.0075 | CLSLoss:5.2907 | top1:72.2772 | AUROC:0.7944\n",
      "Test | 20/10 | Loss:0.6189 | MainLoss:0.6189 | SPLoss:0.0075 | CLSLoss:5.2907 | top1:82.9487 | AUROC:0.9404\n",
      "\n",
      "Epoch: [247 | 1000] LR: 0.034793\n",
      "Train | 10/10 | Loss:0.0443 | MainLoss:0.0441 | Alpha:0.0490 | SPLoss:0.0038 | CLSLoss:5.2912 | top1:98.4000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0671 | MainLoss:1.0671 | SPLoss:0.0068 | CLSLoss:5.2934 | top1:72.5262 | AUROC:0.7960\n",
      "Test | 20/10 | Loss:0.6408 | MainLoss:0.6408 | SPLoss:0.0068 | CLSLoss:5.2934 | top1:82.1667 | AUROC:0.9392\n",
      "\n",
      "Epoch: [248 | 1000] LR: 0.034750\n",
      "Train | 10/10 | Loss:0.0674 | MainLoss:0.0671 | Alpha:0.0484 | SPLoss:0.0049 | CLSLoss:5.2924 | top1:97.7000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0698 | MainLoss:1.0698 | SPLoss:0.0100 | CLSLoss:5.2912 | top1:72.3296 | AUROC:0.7964\n",
      "Test | 20/10 | Loss:0.6377 | MainLoss:0.6377 | SPLoss:0.0100 | CLSLoss:5.2912 | top1:82.4615 | AUROC:0.9395\n",
      "\n",
      "Epoch: [249 | 1000] LR: 0.034708\n",
      "Train | 10/10 | Loss:0.0474 | MainLoss:0.0473 | Alpha:0.0489 | SPLoss:0.0026 | CLSLoss:5.2920 | top1:98.5500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0543 | MainLoss:1.0543 | SPLoss:0.0072 | CLSLoss:5.2911 | top1:72.5590 | AUROC:0.7974\n",
      "Test | 20/10 | Loss:0.6146 | MainLoss:0.6146 | SPLoss:0.0072 | CLSLoss:5.2911 | top1:82.8077 | AUROC:0.9394\n",
      "\n",
      "Epoch: [250 | 1000] LR: 0.034665\n",
      "Train | 10/10 | Loss:0.0581 | MainLoss:0.0580 | Alpha:0.0486 | SPLoss:0.0032 | CLSLoss:5.2927 | top1:97.7750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0616 | MainLoss:1.0616 | SPLoss:0.0081 | CLSLoss:5.2915 | top1:72.3034 | AUROC:0.7963\n",
      "Test | 20/10 | Loss:0.5914 | MainLoss:0.5914 | SPLoss:0.0081 | CLSLoss:5.2915 | top1:83.3205 | AUROC:0.9416\n",
      "\n",
      "Epoch: [251 | 1000] LR: 0.034622\n",
      "Train | 10/10 | Loss:0.0819 | MainLoss:0.0817 | Alpha:0.0480 | SPLoss:0.0048 | CLSLoss:5.2895 | top1:97.2000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0478 | MainLoss:1.0478 | SPLoss:0.0110 | CLSLoss:5.2838 | top1:72.2608 | AUROC:0.7958\n",
      "Test | 20/10 | Loss:0.5938 | MainLoss:0.5938 | SPLoss:0.0110 | CLSLoss:5.2838 | top1:83.0513 | AUROC:0.9419\n",
      "\n",
      "Epoch: [252 | 1000] LR: 0.003458\n",
      "Train | 10/10 | Loss:0.0450 | MainLoss:0.0450 | Alpha:0.0489 | SPLoss:0.0000 | CLSLoss:5.2840 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0454 | MainLoss:1.0454 | SPLoss:0.0001 | CLSLoss:5.2842 | top1:72.4279 | AUROC:0.7961\n",
      "Test | 20/10 | Loss:0.6076 | MainLoss:0.6076 | SPLoss:0.0001 | CLSLoss:5.2842 | top1:82.7179 | AUROC:0.9418\n",
      "\n",
      "Epoch: [253 | 1000] LR: 0.003454\n",
      "Train | 10/10 | Loss:0.0348 | MainLoss:0.0348 | Alpha:0.0491 | SPLoss:0.0000 | CLSLoss:5.2844 | top1:98.8250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0458 | MainLoss:1.0458 | SPLoss:0.0001 | CLSLoss:5.2846 | top1:72.4181 | AUROC:0.7960\n",
      "Test | 20/10 | Loss:0.6111 | MainLoss:0.6111 | SPLoss:0.0001 | CLSLoss:5.2846 | top1:82.5769 | AUROC:0.9415\n",
      "\n",
      "Epoch: [254 | 1000] LR: 0.003449\n",
      "Train | 10/10 | Loss:0.0382 | MainLoss:0.0382 | Alpha:0.0490 | SPLoss:0.0000 | CLSLoss:5.2849 | top1:98.6750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0451 | MainLoss:1.0451 | SPLoss:0.0001 | CLSLoss:5.2850 | top1:72.4345 | AUROC:0.7959\n",
      "Test | 20/10 | Loss:0.6193 | MainLoss:0.6193 | SPLoss:0.0001 | CLSLoss:5.2850 | top1:82.4231 | AUROC:0.9417\n",
      "\n",
      "Epoch: [255 | 1000] LR: 0.003445\n",
      "Train | 10/10 | Loss:0.0442 | MainLoss:0.0442 | Alpha:0.0489 | SPLoss:0.0000 | CLSLoss:5.2852 | top1:98.4750 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0452 | MainLoss:1.0452 | SPLoss:0.0001 | CLSLoss:5.2853 | top1:72.4148 | AUROC:0.7962\n",
      "Test | 20/10 | Loss:0.6246 | MainLoss:0.6246 | SPLoss:0.0001 | CLSLoss:5.2853 | top1:82.3462 | AUROC:0.9413\n",
      "\n",
      "Epoch: [256 | 1000] LR: 0.003441\n",
      "Train | 10/10 | Loss:0.0532 | MainLoss:0.0532 | Alpha:0.0487 | SPLoss:0.0000 | CLSLoss:5.2853 | top1:98.1250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0446 | MainLoss:1.0446 | SPLoss:0.0001 | CLSLoss:5.2854 | top1:72.4083 | AUROC:0.7965\n",
      "Test | 20/10 | Loss:0.6283 | MainLoss:0.6283 | SPLoss:0.0001 | CLSLoss:5.2854 | top1:82.2949 | AUROC:0.9407\n",
      "\n",
      "Epoch: [257 | 1000] LR: 0.003436\n",
      "Train | 10/10 | Loss:0.0655 | MainLoss:0.0655 | Alpha:0.0484 | SPLoss:0.0000 | CLSLoss:5.2855 | top1:97.5500 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0442 | MainLoss:1.0442 | SPLoss:0.0001 | CLSLoss:5.2852 | top1:72.3788 | AUROC:0.7961\n",
      "Test | 20/10 | Loss:0.6264 | MainLoss:0.6264 | SPLoss:0.0001 | CLSLoss:5.2852 | top1:82.3333 | AUROC:0.9411\n",
      "\n",
      "Epoch: [258 | 1000] LR: 0.003432\n",
      "Train | 10/10 | Loss:0.0397 | MainLoss:0.0397 | Alpha:0.0490 | SPLoss:0.0000 | CLSLoss:5.2853 | top1:98.6250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0443 | MainLoss:1.0443 | SPLoss:0.0001 | CLSLoss:5.2857 | top1:72.4312 | AUROC:0.7964\n",
      "Test | 20/10 | Loss:0.6330 | MainLoss:0.6330 | SPLoss:0.0001 | CLSLoss:5.2857 | top1:82.1538 | AUROC:0.9406\n",
      "\n",
      "Epoch: [259 | 1000] LR: 0.003427\n",
      "Train | 10/10 | Loss:0.0468 | MainLoss:0.0468 | Alpha:0.0488 | SPLoss:0.0001 | CLSLoss:5.2857 | top1:98.5250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0449 | MainLoss:1.0449 | SPLoss:0.0001 | CLSLoss:5.2860 | top1:72.3657 | AUROC:0.7963\n",
      "Test | 20/10 | Loss:0.6189 | MainLoss:0.6189 | SPLoss:0.0001 | CLSLoss:5.2860 | top1:82.3718 | AUROC:0.9406\n",
      "\n",
      "Epoch: [260 | 1000] LR: 0.003423\n",
      "Train | 10/10 | Loss:0.0403 | MainLoss:0.0403 | Alpha:0.0490 | SPLoss:0.0000 | CLSLoss:5.2862 | top1:98.4250 | AUROC:0.0000\n",
      "Test | 77/10 | Loss:1.0437 | MainLoss:1.0437 | SPLoss:0.0001 | CLSLoss:5.2863 | top1:72.4771 | AUROC:0.7964\n",
      "Test | 20/10 | Loss:0.6337 | MainLoss:0.6337 | SPLoss:0.0001 | CLSLoss:5.2863 | top1:82.1667 | AUROC:0.9404\n",
      "\n",
      "Epoch: [261 | 1000] LR: 0.003419\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
