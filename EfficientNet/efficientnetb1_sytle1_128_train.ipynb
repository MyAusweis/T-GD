{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b1' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 190\n",
    "test_batch = 190\n",
    "lr = 0.1\n",
    "schedule = [20, 75, 125, 175]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b1_2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 4\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        16, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1920, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1920, 1920, kernel_size=(3, 3), stride=(1, 1), groups=1920, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1920, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1920, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        80, 1920, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 6.52M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=(3,64,64), device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-5, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 300] LR: 0.100000\n",
      "1/363 Data:1.199 | Batch:3.562 | Total:0:00:03 | ETA:0:21:30 | Loss:0.7181949615478516 | top1:47.894737243652344\n",
      "11/363 Data:0.000 | Batch:0.323 | Total:0:00:06 | ETA:0:03:48 | Loss:5.107670469717546 | top1:50.19138717651367\n",
      "21/363 Data:0.000 | Batch:0.323 | Total:0:00:09 | ETA:0:01:49 | Loss:3.4429485968181064 | top1:50.20050048828125\n",
      "31/363 Data:0.001 | Batch:0.326 | Total:0:00:13 | ETA:0:01:47 | Loss:2.6384854201347596 | top1:50.135826110839844\n",
      "41/363 Data:0.000 | Batch:0.316 | Total:0:00:16 | ETA:0:01:45 | Loss:2.178675684987045 | top1:50.1668815612793\n",
      "51/363 Data:0.000 | Batch:0.325 | Total:0:00:19 | ETA:0:01:40 | Loss:1.893785727959053 | top1:50.19607925415039\n",
      "61/363 Data:0.000 | Batch:0.324 | Total:0:00:22 | ETA:0:01:37 | Loss:1.6994556784629822 | top1:50.120792388916016\n",
      "71/363 Data:0.000 | Batch:0.315 | Total:0:00:26 | ETA:0:01:35 | Loss:1.5593682611492319 | top1:50.35581970214844\n",
      "81/363 Data:0.000 | Batch:0.319 | Total:0:00:29 | ETA:0:01:31 | Loss:1.4551852139425867 | top1:50.25990676879883\n",
      "91/363 Data:0.000 | Batch:0.317 | Total:0:00:32 | ETA:0:01:28 | Loss:1.3737408492591354 | top1:50.231346130371094\n",
      "101/363 Data:0.000 | Batch:0.327 | Total:0:00:35 | ETA:0:01:24 | Loss:1.3078757766449804 | top1:50.270973205566406\n",
      "111/363 Data:0.000 | Batch:0.313 | Total:0:00:38 | ETA:0:01:22 | Loss:1.2549588583611153 | top1:50.19914627075195\n",
      "121/363 Data:0.000 | Batch:0.320 | Total:0:00:42 | ETA:0:01:18 | Loss:1.2098615696607542 | top1:50.04349899291992\n",
      "131/363 Data:0.000 | Batch:0.315 | Total:0:00:45 | ETA:0:01:14 | Loss:1.1713264570891402 | top1:50.11651611328125\n",
      "141/363 Data:0.000 | Batch:0.324 | Total:0:00:48 | ETA:0:01:10 | Loss:1.1382407956089533 | top1:50.164241790771484\n",
      "151/363 Data:0.000 | Batch:0.320 | Total:0:00:51 | ETA:0:01:09 | Loss:1.1103372889638736 | top1:50.1568489074707\n",
      "161/363 Data:0.000 | Batch:0.318 | Total:0:00:54 | ETA:0:01:06 | Loss:1.0852904375295462 | top1:50.107879638671875\n",
      "171/363 Data:0.000 | Batch:0.325 | Total:0:00:58 | ETA:0:01:02 | Loss:1.0631156989008363 | top1:50.08926010131836\n",
      "181/363 Data:0.000 | Batch:0.316 | Total:0:01:01 | ETA:0:00:59 | Loss:1.0436384193805042 | top1:50.084327697753906\n",
      "191/363 Data:0.000 | Batch:0.323 | Total:0:01:04 | ETA:0:00:56 | Loss:1.026044546309566 | top1:49.98897933959961\n",
      "201/363 Data:0.000 | Batch:0.328 | Total:0:01:07 | ETA:0:00:53 | Loss:1.0102436500995313 | top1:50.03404235839844\n",
      "211/363 Data:0.000 | Batch:0.360 | Total:0:01:11 | ETA:0:00:53 | Loss:0.995851200621275 | top1:50.01247024536133\n",
      "221/363 Data:0.000 | Batch:0.362 | Total:0:01:14 | ETA:0:00:51 | Loss:0.982558982944057 | top1:50.01667022705078\n",
      "231/363 Data:0.000 | Batch:0.334 | Total:0:01:18 | ETA:0:00:48 | Loss:0.9706226564072943 | top1:50.01139450073242\n",
      "241/363 Data:0.000 | Batch:0.314 | Total:0:01:21 | ETA:0:00:40 | Loss:0.9594634521551647 | top1:50.03712463378906\n",
      "251/363 Data:0.000 | Batch:0.316 | Total:0:01:24 | ETA:0:00:36 | Loss:0.9496204777067876 | top1:50.0\n",
      "261/363 Data:0.000 | Batch:0.315 | Total:0:01:27 | ETA:0:00:33 | Loss:0.9403174581198857 | top1:50.00403594970703\n",
      "271/363 Data:0.000 | Batch:0.317 | Total:0:01:31 | ETA:0:00:30 | Loss:0.9315831357702558 | top1:50.060203552246094\n",
      "281/363 Data:0.000 | Batch:0.326 | Total:0:01:34 | ETA:0:00:27 | Loss:0.9232719279693115 | top1:50.155460357666016\n",
      "291/363 Data:0.000 | Batch:0.326 | Total:0:01:37 | ETA:0:00:24 | Loss:0.915676220995454 | top1:50.144691467285156\n",
      "301/363 Data:0.000 | Batch:0.325 | Total:0:01:40 | ETA:0:00:21 | Loss:0.908909525586125 | top1:50.111907958984375\n",
      "311/363 Data:0.000 | Batch:0.322 | Total:0:01:44 | ETA:0:00:17 | Loss:0.9023100986741364 | top1:50.07784652709961\n",
      "321/363 Data:0.000 | Batch:0.340 | Total:0:01:47 | ETA:0:00:14 | Loss:0.8960969622632796 | top1:50.0573844909668\n",
      "331/363 Data:0.000 | Batch:0.316 | Total:0:01:50 | ETA:0:00:11 | Loss:0.8903708837903879 | top1:50.03498458862305\n",
      "341/363 Data:0.000 | Batch:0.325 | Total:0:01:53 | ETA:0:00:08 | Loss:0.8848844899460018 | top1:50.02314758300781\n",
      "351/363 Data:0.000 | Batch:0.326 | Total:0:01:57 | ETA:0:00:04 | Loss:0.8796964092472 | top1:50.04348373413086\n",
      "361/363 Data:0.000 | Batch:0.327 | Total:0:02:00 | ETA:0:00:01 | Loss:0.8747166659363089 | top1:50.07289505004883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 Data:0.000 | Batch:0.391 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6954568112507845 | top1:50.0\n",
      "\n",
      "Epoch: [2 | 300] LR: 0.170000\n",
      "1/363 Data:1.104 | Batch:1.484 | Total:0:00:01 | ETA:0:08:58 | Loss:0.7008553147315979 | top1:50.52631759643555\n",
      "11/363 Data:0.001 | Batch:0.320 | Total:0:00:05 | ETA:0:02:46 | Loss:0.7246841138059442 | top1:49.330142974853516\n",
      "21/363 Data:0.000 | Batch:0.322 | Total:0:00:08 | ETA:0:01:58 | Loss:0.7133762240409851 | top1:49.77443313598633\n",
      "31/363 Data:0.000 | Batch:0.322 | Total:0:00:11 | ETA:0:01:47 | Loss:0.7131567924253402 | top1:50.27164840698242\n",
      "41/363 Data:0.000 | Batch:0.322 | Total:0:00:14 | ETA:0:01:44 | Loss:0.7109273017906561 | top1:50.397945404052734\n",
      "51/363 Data:0.000 | Batch:0.327 | Total:0:00:17 | ETA:0:01:41 | Loss:0.7107974618088966 | top1:50.08256149291992\n",
      "61/363 Data:0.000 | Batch:0.315 | Total:0:00:21 | ETA:0:01:38 | Loss:0.709744499355066 | top1:50.19844436645508\n",
      "71/363 Data:0.000 | Batch:0.313 | Total:0:00:24 | ETA:0:01:34 | Loss:0.7087685902353743 | top1:50.19273376464844\n",
      "81/363 Data:0.000 | Batch:0.317 | Total:0:00:27 | ETA:0:01:31 | Loss:0.70928841534956 | top1:49.98050308227539\n",
      "91/363 Data:0.000 | Batch:0.314 | Total:0:00:30 | ETA:0:01:27 | Loss:0.7092125907049074 | top1:50.17929458618164\n",
      "101/363 Data:0.000 | Batch:0.384 | Total:0:00:33 | ETA:0:01:26 | Loss:0.709033712892249 | top1:50.26055145263672\n",
      "111/363 Data:0.000 | Batch:0.322 | Total:0:00:37 | ETA:0:01:25 | Loss:0.7086983135154655 | top1:50.29872131347656\n",
      "121/363 Data:0.000 | Batch:0.314 | Total:0:00:40 | ETA:0:01:18 | Loss:0.708844309503382 | top1:50.2479362487793\n",
      "131/363 Data:0.000 | Batch:0.322 | Total:0:00:43 | ETA:0:01:15 | Loss:0.7087780287247578 | top1:50.20490264892578\n",
      "141/363 Data:0.000 | Batch:0.318 | Total:0:00:46 | ETA:0:01:12 | Loss:0.7087315528105337 | top1:50.11198425292969\n",
      "151/363 Data:0.000 | Batch:0.314 | Total:0:00:50 | ETA:0:01:09 | Loss:0.7083374036858413 | top1:50.04182815551758\n",
      "161/363 Data:0.000 | Batch:0.325 | Total:0:00:53 | ETA:0:01:05 | Loss:0.7078776807518479 | top1:50.029422760009766\n",
      "171/363 Data:0.000 | Batch:0.322 | Total:0:00:56 | ETA:0:01:02 | Loss:0.7078385750452677 | top1:49.889198303222656\n",
      "181/363 Data:0.000 | Batch:0.324 | Total:0:00:59 | ETA:0:00:59 | Loss:0.7073653435838815 | top1:49.92439651489258\n",
      "191/363 Data:0.000 | Batch:0.324 | Total:0:01:02 | ETA:0:00:56 | Loss:0.7073460558322088 | top1:49.82915496826172\n",
      "201/363 Data:0.000 | Batch:0.323 | Total:0:01:06 | ETA:0:00:52 | Loss:0.7070340271019817 | top1:49.934539794921875\n",
      "211/363 Data:0.000 | Batch:0.318 | Total:0:01:09 | ETA:0:00:49 | Loss:0.7067231824047757 | top1:50.00498962402344\n",
      "221/363 Data:0.000 | Batch:0.324 | Total:0:01:12 | ETA:0:00:46 | Loss:0.7065869909605829 | top1:50.011905670166016\n",
      "231/363 Data:0.000 | Batch:0.329 | Total:0:01:15 | ETA:0:00:43 | Loss:0.7065926970857562 | top1:50.034175872802734\n",
      "241/363 Data:0.000 | Batch:0.326 | Total:0:01:18 | ETA:0:00:40 | Loss:0.7061267974960359 | top1:50.09608840942383\n",
      "251/363 Data:0.000 | Batch:0.322 | Total:0:01:22 | ETA:0:00:37 | Loss:0.7061259005649156 | top1:50.09016799926758\n",
      "261/363 Data:0.002 | Batch:0.318 | Total:0:01:25 | ETA:0:00:34 | Loss:0.7058427374938439 | top1:50.08671188354492\n",
      "271/363 Data:0.001 | Batch:0.321 | Total:0:01:28 | ETA:0:00:30 | Loss:0.7058672172556943 | top1:50.08933639526367\n",
      "281/363 Data:0.001 | Batch:0.317 | Total:0:01:31 | ETA:0:00:27 | Loss:0.7057903061133687 | top1:50.084285736083984\n",
      "291/363 Data:0.000 | Batch:0.326 | Total:0:01:35 | ETA:0:00:24 | Loss:0.7056764483042189 | top1:50.07053756713867\n",
      "301/363 Data:0.000 | Batch:0.319 | Total:0:01:38 | ETA:0:00:21 | Loss:0.705547903463294 | top1:50.05070877075195\n",
      "311/363 Data:0.000 | Batch:0.315 | Total:0:01:41 | ETA:0:00:17 | Loss:0.7055440261432979 | top1:50.05415344238281\n",
      "321/363 Data:0.000 | Batch:0.325 | Total:0:01:44 | ETA:0:00:14 | Loss:0.7054093476396484 | top1:50.04098892211914\n",
      "331/363 Data:0.000 | Batch:0.334 | Total:0:01:48 | ETA:0:00:11 | Loss:0.7051903549638039 | top1:50.089046478271484\n",
      "341/363 Data:0.000 | Batch:0.314 | Total:0:01:51 | ETA:0:00:08 | Loss:0.705038746780664 | top1:50.10032272338867\n",
      "351/363 Data:0.000 | Batch:0.324 | Total:0:01:54 | ETA:0:00:04 | Loss:0.7049707846763806 | top1:50.05548095703125\n",
      "361/363 Data:0.000 | Batch:0.315 | Total:0:01:57 | ETA:0:00:01 | Loss:0.7048042116072699 | top1:50.05685806274414\n",
      "42/42 Data:0.000 | Batch:0.036 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6932833491991728 | top1:50.0\n",
      "\n",
      "Epoch: [3 | 300] LR: 0.240000\n",
      "1/363 Data:1.109 | Batch:1.463 | Total:0:00:01 | ETA:0:08:50 | Loss:0.6920202970504761 | top1:54.210533142089844\n",
      "11/363 Data:0.000 | Batch:0.325 | Total:0:00:04 | ETA:0:02:34 | Loss:0.7005275596271862 | top1:49.71291732788086\n",
      "21/363 Data:0.000 | Batch:0.324 | Total:0:00:07 | ETA:0:01:51 | Loss:0.7016263689313617 | top1:49.899749755859375\n",
      "31/363 Data:0.000 | Batch:0.313 | Total:0:00:11 | ETA:0:01:46 | Loss:0.7015181510679184 | top1:49.76231002807617\n",
      "41/363 Data:0.000 | Batch:0.313 | Total:0:00:14 | ETA:0:01:43 | Loss:0.701439165487522 | top1:49.17843246459961\n",
      "51/363 Data:0.000 | Batch:0.314 | Total:0:00:17 | ETA:0:01:39 | Loss:0.7009462515513102 | top1:49.08152770996094\n",
      "61/363 Data:0.000 | Batch:0.314 | Total:0:00:20 | ETA:0:01:36 | Loss:0.7009914605343928 | top1:49.344261169433594\n",
      "71/363 Data:0.000 | Batch:0.340 | Total:0:00:23 | ETA:0:01:36 | Loss:0.7012811168818407 | top1:49.503334045410156\n",
      "81/363 Data:0.001 | Batch:0.324 | Total:0:00:27 | ETA:0:01:33 | Loss:0.7010302852701258 | top1:49.532161712646484\n",
      "91/363 Data:0.000 | Batch:0.317 | Total:0:00:30 | ETA:0:01:28 | Loss:0.7009854113662636 | top1:49.78600311279297\n",
      "101/363 Data:0.000 | Batch:0.322 | Total:0:00:33 | ETA:0:01:24 | Loss:0.7016115831856681 | top1:49.682125091552734\n",
      "111/363 Data:0.000 | Batch:0.319 | Total:0:00:36 | ETA:0:01:22 | Loss:0.701243418831009 | top1:49.81507873535156\n",
      "121/363 Data:0.001 | Batch:0.322 | Total:0:00:40 | ETA:0:01:18 | Loss:0.7011508798796283 | top1:49.991302490234375\n",
      "131/363 Data:0.000 | Batch:0.321 | Total:0:00:43 | ETA:0:01:15 | Loss:0.7011448759159059 | top1:49.97991180419922\n",
      "141/363 Data:0.000 | Batch:0.314 | Total:0:00:46 | ETA:0:01:11 | Loss:0.7019513350851992 | top1:49.832027435302734\n",
      "151/363 Data:0.001 | Batch:0.316 | Total:0:00:49 | ETA:0:01:08 | Loss:0.7020491176093651 | top1:49.82920837402344\n",
      "161/363 Data:0.001 | Batch:0.324 | Total:0:00:52 | ETA:0:01:05 | Loss:0.7023100790029727 | top1:49.68617248535156\n",
      "171/363 Data:0.000 | Batch:0.322 | Total:0:00:55 | ETA:0:01:02 | Loss:0.7021844624078761 | top1:49.809173583984375\n",
      "181/363 Data:0.000 | Batch:0.323 | Total:0:00:59 | ETA:0:00:59 | Loss:0.7021324120832412 | top1:49.688865661621094\n",
      "191/363 Data:0.000 | Batch:0.322 | Total:0:01:02 | ETA:0:00:56 | Loss:0.7019389814731338 | top1:49.78782272338867\n",
      "201/363 Data:0.000 | Batch:0.324 | Total:0:01:05 | ETA:0:00:53 | Loss:0.7018144700064588 | top1:49.79575729370117\n",
      "211/363 Data:0.000 | Batch:0.322 | Total:0:01:08 | ETA:0:00:50 | Loss:0.7015264068169617 | top1:49.8677978515625\n",
      "221/363 Data:0.000 | Batch:0.323 | Total:0:01:12 | ETA:0:00:46 | Loss:0.7012552168574269 | top1:49.93331527709961\n",
      "231/363 Data:0.000 | Batch:0.324 | Total:0:01:15 | ETA:0:00:43 | Loss:0.7011666989429689 | top1:49.96354675292969\n",
      "241/363 Data:0.001 | Batch:0.315 | Total:0:01:18 | ETA:0:00:40 | Loss:0.7009817366778108 | top1:50.04367446899414\n",
      "251/363 Data:0.000 | Batch:0.324 | Total:0:01:21 | ETA:0:00:36 | Loss:0.7009795224999051 | top1:50.03355026245117\n",
      "261/363 Data:0.000 | Batch:0.322 | Total:0:01:25 | ETA:0:00:33 | Loss:0.7009514307610376 | top1:50.05847930908203\n",
      "271/363 Data:0.000 | Batch:0.322 | Total:0:01:28 | ETA:0:00:30 | Loss:0.7009138511115774 | top1:50.0407829284668\n",
      "281/363 Data:0.001 | Batch:0.321 | Total:0:01:31 | ETA:0:00:27 | Loss:0.7007319239959173 | top1:50.084285736083984\n",
      "291/363 Data:0.000 | Batch:0.314 | Total:0:01:34 | ETA:0:00:23 | Loss:0.7007048701502613 | top1:50.1193733215332\n",
      "301/363 Data:0.000 | Batch:0.314 | Total:0:01:37 | ETA:0:00:20 | Loss:0.7005397372467573 | top1:50.1731071472168\n",
      "311/363 Data:0.000 | Batch:0.315 | Total:0:01:41 | ETA:0:00:17 | Loss:0.7003882680675224 | top1:50.201385498046875\n",
      "321/363 Data:0.000 | Batch:0.318 | Total:0:01:44 | ETA:0:00:14 | Loss:0.7002422389954421 | top1:50.24266052246094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331/363 Data:0.000 | Batch:0.351 | Total:0:01:47 | ETA:0:00:12 | Loss:0.7002588539325219 | top1:50.17013931274414\n",
      "341/363 Data:0.000 | Batch:0.350 | Total:0:01:51 | ETA:0:00:08 | Loss:0.700230688468452 | top1:50.200645446777344\n",
      "351/363 Data:0.000 | Batch:0.315 | Total:0:01:54 | ETA:0:00:05 | Loss:0.7001496021903818 | top1:50.190433502197266\n",
      "361/363 Data:0.000 | Batch:0.322 | Total:0:01:57 | ETA:0:00:01 | Loss:0.7000208416143613 | top1:50.2041130065918\n",
      "42/42 Data:0.000 | Batch:0.035 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6936797223029992 | top1:50.0\n",
      "\n",
      "Epoch: [4 | 300] LR: 0.310000\n",
      "1/363 Data:1.082 | Batch:1.430 | Total:0:00:01 | ETA:0:08:38 | Loss:0.699485719203949 | top1:47.36842346191406\n",
      "11/363 Data:0.000 | Batch:0.326 | Total:0:00:04 | ETA:0:02:32 | Loss:0.6982612393119119 | top1:50.14353942871094\n",
      "21/363 Data:0.001 | Batch:0.469 | Total:0:00:08 | ETA:0:02:02 | Loss:0.6975717516172499 | top1:50.85212707519531\n",
      "31/363 Data:0.000 | Batch:0.323 | Total:0:00:11 | ETA:0:02:01 | Loss:0.6979936938132009 | top1:50.47538375854492\n",
      "41/363 Data:0.000 | Batch:0.315 | Total:0:00:15 | ETA:0:01:47 | Loss:0.6972749669377397 | top1:50.57766342163086\n",
      "51/363 Data:0.000 | Batch:0.324 | Total:0:00:18 | ETA:0:01:41 | Loss:0.6975081679867763 | top1:50.99071502685547\n",
      "61/363 Data:0.000 | Batch:0.324 | Total:0:00:21 | ETA:0:01:41 | Loss:0.6975640019432443 | top1:51.09577178955078\n",
      "71/363 Data:0.000 | Batch:0.332 | Total:0:00:25 | ETA:0:01:39 | Loss:0.6970955628744313 | top1:51.237953186035156\n",
      "81/363 Data:0.000 | Batch:0.326 | Total:0:00:28 | ETA:0:01:35 | Loss:0.697056105107437 | top1:51.01364517211914\n",
      "91/363 Data:0.000 | Batch:0.314 | Total:0:00:31 | ETA:0:01:28 | Loss:0.6969032837794378 | top1:51.05263137817383\n",
      "101/363 Data:0.000 | Batch:0.314 | Total:0:00:34 | ETA:0:01:26 | Loss:0.6970845077297475 | top1:51.02657699584961\n",
      "111/363 Data:0.000 | Batch:0.314 | Total:0:00:38 | ETA:0:01:23 | Loss:0.6973092362687394 | top1:50.91038513183594\n",
      "121/363 Data:0.000 | Batch:0.437 | Total:0:00:41 | ETA:0:01:17 | Loss:0.6972185905314674 | top1:50.800350189208984\n",
      "131/363 Data:0.000 | Batch:0.323 | Total:0:00:44 | ETA:0:01:15 | Loss:0.6973070088233656 | top1:50.76737976074219\n",
      "141/363 Data:0.000 | Batch:0.316 | Total:0:00:48 | ETA:0:01:15 | Loss:0.6974000512285435 | top1:50.6457633972168\n",
      "151/363 Data:0.000 | Batch:0.318 | Total:0:00:51 | ETA:0:01:11 | Loss:0.6973077437735551 | top1:50.634368896484375\n",
      "161/363 Data:0.000 | Batch:0.315 | Total:0:00:54 | ETA:0:01:09 | Loss:0.6972692408917113 | top1:50.65707778930664\n",
      "171/363 Data:0.000 | Batch:0.325 | Total:0:00:58 | ETA:0:01:05 | Loss:0.6971589483712849 | top1:50.65250778198242\n",
      "181/363 Data:0.000 | Batch:0.325 | Total:0:01:01 | ETA:0:01:01 | Loss:0.6972144776286341 | top1:50.581565856933594\n",
      "191/363 Data:0.000 | Batch:0.443 | Total:0:01:04 | ETA:0:00:56 | Loss:0.6972047546147052 | top1:50.523563385009766\n",
      "201/363 Data:0.000 | Batch:0.315 | Total:0:01:07 | ETA:0:00:52 | Loss:0.6971579976935884 | top1:50.47394561767578\n",
      "211/363 Data:0.001 | Batch:0.315 | Total:0:01:11 | ETA:0:00:50 | Loss:0.6973060561582376 | top1:50.40159606933594\n",
      "221/363 Data:0.000 | Batch:0.324 | Total:0:01:14 | ETA:0:00:46 | Loss:0.6971083404791302 | top1:50.519168853759766\n",
      "231/363 Data:0.000 | Batch:0.315 | Total:0:01:17 | ETA:0:00:43 | Loss:0.6970880939846947 | top1:50.48758316040039\n",
      "241/363 Data:0.000 | Batch:0.315 | Total:0:01:21 | ETA:0:00:41 | Loss:0.6970709193791591 | top1:50.50447463989258\n",
      "251/363 Data:0.000 | Batch:0.326 | Total:0:01:24 | ETA:0:00:38 | Loss:0.6970446610830695 | top1:50.48228073120117\n",
      "261/363 Data:0.001 | Batch:0.324 | Total:0:01:27 | ETA:0:00:35 | Loss:0.6969409157946649 | top1:50.536399841308594\n",
      "271/363 Data:0.000 | Batch:0.325 | Total:0:01:31 | ETA:0:00:31 | Loss:0.6969239267475931 | top1:50.4719352722168\n",
      "281/363 Data:0.001 | Batch:0.588 | Total:0:01:36 | ETA:0:00:41 | Loss:0.6968976484074711 | top1:50.413936614990234\n",
      "291/363 Data:0.001 | Batch:0.589 | Total:0:01:42 | ETA:0:00:44 | Loss:0.6969401486141166 | top1:50.37800598144531\n",
      "301/363 Data:0.000 | Batch:0.615 | Total:0:01:48 | ETA:0:00:38 | Loss:0.6969255605013267 | top1:50.35845184326172\n",
      "311/363 Data:0.001 | Batch:0.617 | Total:0:01:54 | ETA:0:00:32 | Loss:0.6969514592667485 | top1:50.343544006347656\n",
      "321/363 Data:0.001 | Batch:0.578 | Total:0:02:00 | ETA:0:00:26 | Loss:0.696887411989527 | top1:50.36399459838867\n",
      "331/363 Data:0.000 | Batch:0.623 | Total:0:02:06 | ETA:0:00:20 | Loss:0.6968407953253686 | top1:50.32914733886719\n",
      "341/363 Data:0.002 | Batch:0.620 | Total:0:02:10 | ETA:0:00:10 | Loss:0.6968957666427859 | top1:50.23151397705078\n",
      "351/363 Data:0.001 | Batch:0.610 | Total:0:02:16 | ETA:0:00:08 | Loss:0.6970502688334539 | top1:50.19643020629883\n",
      "361/363 Data:0.000 | Batch:0.325 | Total:0:02:20 | ETA:0:00:01 | Loss:0.6970327296745744 | top1:50.15745544433594\n",
      "42/42 Data:0.000 | Batch:0.042 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6936741816691864 | top1:50.0\n",
      "\n",
      "Epoch: [5 | 300] LR: 0.380000\n",
      "1/363 Data:1.115 | Batch:1.444 | Total:0:00:01 | ETA:0:08:43 | Loss:0.6939852237701416 | top1:53.157901763916016\n",
      "11/363 Data:0.000 | Batch:0.322 | Total:0:00:04 | ETA:0:02:39 | Loss:0.6977422562512484 | top1:50.33492660522461\n",
      "21/363 Data:0.001 | Batch:0.362 | Total:0:00:08 | ETA:0:01:53 | Loss:0.6975149398758298 | top1:49.77443313598633\n",
      "31/363 Data:0.000 | Batch:0.339 | Total:0:00:11 | ETA:0:01:59 | Loss:0.6967745442544261 | top1:49.219017028808594\n",
      "41/363 Data:0.000 | Batch:0.357 | Total:0:00:15 | ETA:0:01:55 | Loss:0.6961283785540883 | top1:49.66624069213867\n",
      "51/363 Data:0.000 | Batch:0.323 | Total:0:00:18 | ETA:0:01:46 | Loss:0.6965306050637189 | top1:49.680084228515625\n",
      "61/363 Data:0.000 | Batch:0.370 | Total:0:00:22 | ETA:0:01:55 | Loss:0.6962482684948406 | top1:49.965484619140625\n",
      "71/363 Data:0.000 | Batch:0.322 | Total:0:00:25 | ETA:0:01:37 | Loss:0.6963382548009845 | top1:49.79985046386719\n",
      "81/363 Data:0.000 | Batch:0.332 | Total:0:00:28 | ETA:0:01:31 | Loss:0.6962752459961691 | top1:49.76607894897461\n",
      "91/363 Data:0.000 | Batch:0.317 | Total:0:00:32 | ETA:0:01:29 | Loss:0.6961889961263635 | top1:49.791786193847656\n",
      "101/363 Data:0.000 | Batch:0.318 | Total:0:00:35 | ETA:0:01:24 | Loss:0.6962903602288502 | top1:49.89577865600586\n",
      "111/363 Data:0.000 | Batch:0.315 | Total:0:00:38 | ETA:0:01:21 | Loss:0.6961191054937002 | top1:49.962066650390625\n",
      "121/363 Data:0.000 | Batch:0.324 | Total:0:00:41 | ETA:0:01:18 | Loss:0.6961506841596493 | top1:49.99565124511719\n",
      "131/363 Data:0.001 | Batch:0.324 | Total:0:00:45 | ETA:0:01:18 | Loss:0.6961211671356027 | top1:50.02812576293945\n",
      "141/363 Data:0.001 | Batch:0.315 | Total:0:00:48 | ETA:0:01:12 | Loss:0.696194866447584 | top1:50.06718826293945\n",
      "151/363 Data:0.001 | Batch:0.320 | Total:0:00:51 | ETA:0:01:09 | Loss:0.6962081797865053 | top1:50.08713912963867\n",
      "161/363 Data:0.000 | Batch:0.324 | Total:0:00:54 | ETA:0:01:06 | Loss:0.6960654691879794 | top1:50.22883605957031\n",
      "171/363 Data:0.001 | Batch:0.368 | Total:0:00:58 | ETA:0:01:08 | Loss:0.6959525259614688 | top1:50.22468566894531\n",
      "181/363 Data:0.000 | Batch:0.355 | Total:0:01:01 | ETA:0:01:06 | Loss:0.6960409296810298 | top1:50.16865539550781\n",
      "191/363 Data:0.000 | Batch:0.326 | Total:0:01:05 | ETA:0:00:59 | Loss:0.6960179799514291 | top1:50.23422622680664\n",
      "201/363 Data:0.003 | Batch:0.428 | Total:0:01:09 | ETA:0:00:58 | Loss:0.6959172982481582 | top1:50.28803253173828\n",
      "211/363 Data:0.000 | Batch:0.323 | Total:0:01:12 | ETA:0:00:55 | Loss:0.6959508038810079 | top1:50.276878356933594\n",
      "221/363 Data:0.000 | Batch:0.317 | Total:0:01:15 | ETA:0:00:48 | Loss:0.6959851235825552 | top1:50.23100662231445\n",
      "231/363 Data:0.000 | Batch:0.319 | Total:0:01:19 | ETA:0:00:45 | Loss:0.695922903168253 | top1:50.28936004638672\n",
      "241/363 Data:0.000 | Batch:0.315 | Total:0:01:22 | ETA:0:00:42 | Loss:0.6960436333264552 | top1:50.32539749145508\n",
      "251/363 Data:0.001 | Batch:0.414 | Total:0:01:25 | ETA:0:00:38 | Loss:0.6959855755961749 | top1:50.30614471435547\n",
      "261/363 Data:0.001 | Batch:0.315 | Total:0:01:29 | ETA:0:00:34 | Loss:0.6958808627165141 | top1:50.381126403808594\n",
      "271/363 Data:0.000 | Batch:0.315 | Total:0:01:32 | ETA:0:00:31 | Loss:0.6959014451371788 | top1:50.291316986083984\n",
      "281/363 Data:0.000 | Batch:0.315 | Total:0:01:35 | ETA:0:00:27 | Loss:0.6958016649259792 | top1:50.333396911621094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291/363 Data:0.000 | Batch:0.314 | Total:0:01:39 | ETA:0:00:24 | Loss:0.6957915052515534 | top1:50.32917404174805\n",
      "301/363 Data:0.000 | Batch:0.315 | Total:0:01:42 | ETA:0:00:21 | Loss:0.6957692046498143 | top1:50.346214294433594\n",
      "311/363 Data:0.000 | Batch:0.323 | Total:0:01:45 | ETA:0:00:18 | Loss:0.6957360162995636 | top1:50.30461883544922\n",
      "321/363 Data:0.000 | Batch:0.345 | Total:0:01:49 | ETA:0:00:14 | Loss:0.6956656549207146 | top1:50.34103775024414\n",
      "331/363 Data:0.001 | Batch:0.336 | Total:0:01:52 | ETA:0:00:11 | Loss:0.6957335801643187 | top1:50.286216735839844\n",
      "341/363 Data:0.000 | Batch:0.314 | Total:0:01:55 | ETA:0:00:08 | Loss:0.6957700591632697 | top1:50.27318572998047\n",
      "351/363 Data:0.001 | Batch:0.374 | Total:0:01:59 | ETA:0:00:05 | Loss:0.6957709052284219 | top1:50.301395416259766\n",
      "361/363 Data:0.000 | Batch:0.317 | Total:0:02:02 | ETA:0:00:01 | Loss:0.6957540976043554 | top1:50.33095169067383\n",
      "42/42 Data:0.000 | Batch:0.035 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6949891787308913 | top1:50.0\n",
      "\n",
      "Epoch: [6 | 300] LR: 0.450000\n",
      "1/363 Data:1.137 | Batch:1.524 | Total:0:00:01 | ETA:0:09:12 | Loss:0.6967126727104187 | top1:46.842105865478516\n",
      "11/363 Data:0.001 | Batch:0.610 | Total:0:00:07 | ETA:0:03:56 | Loss:0.6960889534516768 | top1:50.33492660522461\n",
      "21/363 Data:0.000 | Batch:0.621 | Total:0:00:13 | ETA:0:03:31 | Loss:0.6961992269470578 | top1:50.20050048828125\n",
      "31/363 Data:0.000 | Batch:0.629 | Total:0:00:19 | ETA:0:03:20 | Loss:0.6963345177711979 | top1:49.89813232421875\n",
      "41/363 Data:0.001 | Batch:0.619 | Total:0:00:25 | ETA:0:03:14 | Loss:0.6959434994837133 | top1:50.05134963989258\n",
      "51/363 Data:0.001 | Batch:0.598 | Total:0:00:31 | ETA:0:03:09 | Loss:0.6960876864545485 | top1:50.0\n",
      "61/363 Data:0.000 | Batch:0.332 | Total:0:00:36 | ETA:0:02:47 | Loss:0.695750289275998 | top1:49.98274230957031\n",
      "71/363 Data:0.000 | Batch:0.323 | Total:0:00:39 | ETA:0:01:35 | Loss:0.6956002452004124 | top1:50.1111946105957\n",
      "81/363 Data:0.000 | Batch:0.326 | Total:0:00:42 | ETA:0:01:31 | Loss:0.6954995570359407 | top1:50.305389404296875\n",
      "91/363 Data:0.000 | Batch:0.322 | Total:0:00:46 | ETA:0:01:29 | Loss:0.6955750060605479 | top1:50.27183532714844\n",
      "101/363 Data:0.000 | Batch:0.324 | Total:0:00:49 | ETA:0:01:25 | Loss:0.6954333646462696 | top1:50.34914016723633\n",
      "111/363 Data:0.000 | Batch:0.322 | Total:0:00:52 | ETA:0:01:22 | Loss:0.6956949679701178 | top1:50.341392517089844\n",
      "121/363 Data:0.000 | Batch:0.314 | Total:0:00:55 | ETA:0:01:17 | Loss:0.6956770434852474 | top1:50.36537551879883\n",
      "131/363 Data:0.000 | Batch:0.313 | Total:0:00:58 | ETA:0:01:13 | Loss:0.6956237777498843 | top1:50.34954071044922\n",
      "141/363 Data:0.000 | Batch:0.314 | Total:0:01:02 | ETA:0:01:10 | Loss:0.6957063607290281 | top1:50.283687591552734\n",
      "151/363 Data:0.000 | Batch:0.314 | Total:0:01:05 | ETA:0:01:07 | Loss:0.6957117121740682 | top1:50.30324172973633\n",
      "161/363 Data:0.000 | Batch:0.315 | Total:0:01:08 | ETA:0:01:04 | Loss:0.6958613195774718 | top1:50.30729293823242\n",
      "171/363 Data:0.000 | Batch:0.313 | Total:0:01:11 | ETA:0:01:01 | Loss:0.6957443269372684 | top1:50.26777648925781\n",
      "181/363 Data:0.000 | Batch:0.326 | Total:0:01:14 | ETA:0:00:58 | Loss:0.6957198621818373 | top1:50.20064163208008\n",
      "191/363 Data:0.000 | Batch:0.318 | Total:0:01:17 | ETA:0:00:55 | Loss:0.6957561111575021 | top1:50.20942687988281\n",
      "201/363 Data:0.000 | Batch:0.324 | Total:0:01:21 | ETA:0:00:52 | Loss:0.6957337921531639 | top1:50.240901947021484\n",
      "211/363 Data:0.000 | Batch:0.322 | Total:0:01:24 | ETA:0:00:49 | Loss:0.6956454179298256 | top1:50.246944427490234\n",
      "221/363 Data:0.000 | Batch:0.318 | Total:0:01:27 | ETA:0:00:46 | Loss:0.6957897220262036 | top1:50.23815155029297\n",
      "231/363 Data:0.000 | Batch:0.322 | Total:0:01:30 | ETA:0:00:43 | Loss:0.6957693954050799 | top1:50.23468017578125\n",
      "241/363 Data:0.000 | Batch:0.322 | Total:0:01:33 | ETA:0:00:40 | Loss:0.6957364643757769 | top1:50.28608703613281\n",
      "251/363 Data:0.000 | Batch:0.318 | Total:0:01:37 | ETA:0:00:36 | Loss:0.6956943207053075 | top1:50.27888488769531\n",
      "261/363 Data:0.000 | Batch:0.323 | Total:0:01:40 | ETA:0:00:33 | Loss:0.695683954319278 | top1:50.19963836669922\n",
      "271/363 Data:0.000 | Batch:0.321 | Total:0:01:43 | ETA:0:00:30 | Loss:0.6957096582849087 | top1:50.15342712402344\n",
      "281/363 Data:0.000 | Batch:0.327 | Total:0:01:46 | ETA:0:00:27 | Loss:0.6955617273829585 | top1:50.2828254699707\n",
      "291/363 Data:0.000 | Batch:0.322 | Total:0:01:49 | ETA:0:00:23 | Loss:0.6955662787575083 | top1:50.245975494384766\n",
      "301/363 Data:0.000 | Batch:0.323 | Total:0:01:53 | ETA:0:00:20 | Loss:0.6955103319744731 | top1:50.312992095947266\n",
      "311/363 Data:0.000 | Batch:0.322 | Total:0:01:56 | ETA:0:00:17 | Loss:0.6955191682772621 | top1:50.31815719604492\n",
      "321/363 Data:0.000 | Batch:0.322 | Total:0:01:59 | ETA:0:00:14 | Loss:0.6955397091921988 | top1:50.288570404052734\n",
      "331/363 Data:0.000 | Batch:0.325 | Total:0:02:02 | ETA:0:00:11 | Loss:0.6955288853890226 | top1:50.276676177978516\n",
      "341/363 Data:0.001 | Batch:0.323 | Total:0:02:05 | ETA:0:00:08 | Loss:0.695519383002586 | top1:50.282447814941406\n",
      "351/363 Data:0.000 | Batch:0.323 | Total:0:02:09 | ETA:0:00:04 | Loss:0.6955350712153986 | top1:50.25790786743164\n",
      "361/363 Data:0.000 | Batch:0.322 | Total:0:02:12 | ETA:0:00:01 | Loss:0.695597436124268 | top1:50.249305725097656\n",
      "42/42 Data:0.000 | Batch:0.034 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6951030550858913 | top1:50.0\n",
      "\n",
      "Epoch: [7 | 300] LR: 0.520000\n",
      "1/363 Data:1.185 | Batch:1.515 | Total:0:00:01 | ETA:0:09:09 | Loss:0.6905212998390198 | top1:53.157901763916016\n",
      "11/363 Data:0.000 | Batch:0.314 | Total:0:00:04 | ETA:0:02:37 | Loss:0.6936981027776544 | top1:51.818180084228516\n",
      "21/363 Data:0.000 | Batch:0.323 | Total:0:00:07 | ETA:0:01:51 | Loss:0.6937459253129505 | top1:51.10275650024414\n",
      "31/363 Data:0.000 | Batch:0.322 | Total:0:00:11 | ETA:0:01:48 | Loss:0.6948538268766096 | top1:50.32258224487305\n",
      "41/363 Data:0.000 | Batch:0.322 | Total:0:00:14 | ETA:0:01:45 | Loss:0.6945709339002284 | top1:50.44929504394531\n",
      "51/363 Data:0.000 | Batch:0.324 | Total:0:00:17 | ETA:0:01:41 | Loss:0.6950345904219384 | top1:50.495357513427734\n",
      "61/363 Data:0.000 | Batch:0.325 | Total:0:00:20 | ETA:0:01:38 | Loss:0.6951283050365136 | top1:50.379634857177734\n",
      "71/363 Data:0.000 | Batch:0.326 | Total:0:00:24 | ETA:0:01:36 | Loss:0.6952258709450843 | top1:50.37805938720703\n",
      "81/363 Data:0.000 | Batch:0.323 | Total:0:00:27 | ETA:0:01:31 | Loss:0.6950972014003329 | top1:50.389862060546875\n",
      "91/363 Data:0.000 | Batch:0.323 | Total:0:00:30 | ETA:0:01:29 | Loss:0.6949731982671298 | top1:50.41642379760742\n",
      "101/363 Data:0.000 | Batch:0.324 | Total:0:00:33 | ETA:0:01:25 | Loss:0.6949153034993918 | top1:50.55758285522461\n",
      "111/363 Data:0.000 | Batch:0.324 | Total:0:00:37 | ETA:0:01:23 | Loss:0.6950770364151345 | top1:50.393550872802734\n",
      "121/363 Data:0.000 | Batch:0.322 | Total:0:00:40 | ETA:0:01:19 | Loss:0.6951177327100896 | top1:50.35232925415039\n",
      "131/363 Data:0.000 | Batch:0.314 | Total:0:00:43 | ETA:0:01:15 | Loss:0.6950918718148734 | top1:50.43792724609375\n",
      "141/363 Data:0.000 | Batch:0.322 | Total:0:00:46 | ETA:0:01:12 | Loss:0.694985679700865 | top1:50.488990783691406\n",
      "151/363 Data:0.000 | Batch:0.324 | Total:0:00:50 | ETA:0:01:09 | Loss:0.6949891373021713 | top1:50.55768585205078\n",
      "161/363 Data:0.000 | Batch:0.322 | Total:0:00:53 | ETA:0:01:06 | Loss:0.6950700397817244 | top1:50.53612518310547\n",
      "171/363 Data:0.000 | Batch:0.324 | Total:0:00:56 | ETA:0:01:02 | Loss:0.6950797323595014 | top1:50.60326385498047\n",
      "181/363 Data:0.000 | Batch:0.321 | Total:0:00:59 | ETA:0:00:59 | Loss:0.6952585663584715 | top1:50.622276306152344\n",
      "191/363 Data:0.000 | Batch:0.315 | Total:0:01:02 | ETA:0:00:56 | Loss:0.6952736583679758 | top1:50.636539459228516\n",
      "201/363 Data:0.000 | Batch:0.322 | Total:0:01:06 | ETA:0:00:52 | Loss:0.6951459930903876 | top1:50.67033386230469\n",
      "211/363 Data:0.000 | Batch:0.323 | Total:0:01:09 | ETA:0:00:50 | Loss:0.6951230702242015 | top1:50.661014556884766\n",
      "221/363 Data:0.000 | Batch:0.322 | Total:0:01:12 | ETA:0:00:46 | Loss:0.6950581847812256 | top1:50.7382698059082\n",
      "231/363 Data:0.000 | Batch:0.323 | Total:0:01:15 | ETA:0:00:43 | Loss:0.6950999792520102 | top1:50.70175552368164\n",
      "241/363 Data:0.000 | Batch:0.325 | Total:0:01:19 | ETA:0:00:40 | Loss:0.695130461973768 | top1:50.64861297607422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/363 Data:0.000 | Batch:0.323 | Total:0:01:22 | ETA:0:00:37 | Loss:0.6951448941135786 | top1:50.61228942871094\n",
      "261/363 Data:0.000 | Batch:0.323 | Total:0:01:25 | ETA:0:00:33 | Loss:0.6951685123059942 | top1:50.641258239746094\n",
      "271/363 Data:0.000 | Batch:0.317 | Total:0:01:28 | ETA:0:00:30 | Loss:0.6952348424059879 | top1:50.57875061035156\n",
      "281/363 Data:0.000 | Batch:0.323 | Total:0:01:31 | ETA:0:00:27 | Loss:0.695175001629731 | top1:50.65742492675781\n",
      "291/363 Data:0.000 | Batch:0.322 | Total:0:01:35 | ETA:0:00:24 | Loss:0.6951316513146731 | top1:50.65473175048828\n",
      "301/363 Data:0.000 | Batch:0.322 | Total:0:01:38 | ETA:0:00:21 | Loss:0.6951636042309758 | top1:50.62773132324219\n",
      "311/363 Data:0.000 | Batch:0.314 | Total:0:01:41 | ETA:0:00:17 | Loss:0.6951389446902505 | top1:50.629547119140625\n",
      "321/363 Data:0.000 | Batch:0.314 | Total:0:01:44 | ETA:0:00:14 | Loss:0.6951227418358824 | top1:50.61485290527344\n",
      "331/363 Data:0.000 | Batch:0.314 | Total:0:01:47 | ETA:0:00:11 | Loss:0.6951657549852331 | top1:50.62967300415039\n",
      "341/363 Data:0.000 | Batch:0.314 | Total:0:01:51 | ETA:0:00:07 | Loss:0.6951876861259035 | top1:50.606571197509766\n",
      "351/363 Data:0.000 | Batch:0.314 | Total:0:01:54 | ETA:0:00:04 | Loss:0.6951946178053179 | top1:50.59379196166992\n",
      "361/363 Data:0.000 | Batch:0.313 | Total:0:01:57 | ETA:0:00:01 | Loss:0.6951596045097816 | top1:50.63566207885742\n",
      "42/42 Data:0.000 | Batch:0.034 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6932711119835193 | top1:50.0\n",
      "\n",
      "Epoch: [8 | 300] LR: 0.590000\n",
      "1/363 Data:1.160 | Batch:1.489 | Total:0:00:01 | ETA:0:09:00 | Loss:0.6919334530830383 | top1:52.6315803527832\n",
      "11/363 Data:0.000 | Batch:0.424 | Total:0:00:04 | ETA:0:02:40 | Loss:0.6950747966766357 | top1:49.71291732788086\n",
      "21/363 Data:0.000 | Batch:0.394 | Total:0:00:09 | ETA:0:02:29 | Loss:0.6946912407875061 | top1:50.701751708984375\n",
      "31/363 Data:0.000 | Batch:0.316 | Total:0:00:12 | ETA:0:01:51 | Loss:0.6954667818161749 | top1:50.98472213745117\n",
      "41/363 Data:0.001 | Batch:0.634 | Total:0:00:16 | ETA:0:02:07 | Loss:0.6947997503164338 | top1:51.61745834350586\n",
      "51/363 Data:0.001 | Batch:0.368 | Total:0:00:20 | ETA:0:01:53 | Loss:0.6949671986056309 | top1:51.310630798339844\n",
      "61/363 Data:0.000 | Batch:0.317 | Total:0:00:23 | ETA:0:02:00 | Loss:0.6950086120699273 | top1:51.25970458984375\n",
      "71/363 Data:0.001 | Batch:0.325 | Total:0:00:27 | ETA:0:01:45 | Loss:0.6950902846497549 | top1:50.90437316894531\n",
      "81/363 Data:0.000 | Batch:0.325 | Total:0:00:30 | ETA:0:01:40 | Loss:0.6946502695848913 | top1:51.23456573486328\n",
      "91/363 Data:0.001 | Batch:0.334 | Total:0:00:34 | ETA:0:01:37 | Loss:0.6949275391442435 | top1:51.058414459228516\n",
      "101/363 Data:0.000 | Batch:0.315 | Total:0:00:37 | ETA:0:01:34 | Loss:0.6950216151700161 | top1:50.90672302246094\n",
      "111/363 Data:0.000 | Batch:0.325 | Total:0:00:41 | ETA:0:01:30 | Loss:0.6950317261455295 | top1:50.881935119628906\n",
      "121/363 Data:0.000 | Batch:0.431 | Total:0:00:45 | ETA:0:01:37 | Loss:0.6948587145687135 | top1:50.856895446777344\n",
      "131/363 Data:0.000 | Batch:0.327 | Total:0:00:49 | ETA:0:01:24 | Loss:0.6949310061585812 | top1:50.91603088378906\n",
      "141/363 Data:0.000 | Batch:0.316 | Total:0:00:52 | ETA:0:01:20 | Loss:0.6949312407919701 | top1:50.99664306640625\n",
      "151/363 Data:0.000 | Batch:0.323 | Total:0:00:56 | ETA:0:01:20 | Loss:0.6951004077267173 | top1:50.833045959472656\n",
      "161/363 Data:0.000 | Batch:0.324 | Total:0:00:59 | ETA:0:01:13 | Loss:0.6951942136569053 | top1:50.8336067199707\n",
      "171/363 Data:0.000 | Batch:0.315 | Total:0:01:03 | ETA:0:01:09 | Loss:0.6951906213983458 | top1:50.78178024291992\n",
      "181/363 Data:0.001 | Batch:0.326 | Total:0:01:06 | ETA:0:01:05 | Loss:0.6952966965364488 | top1:50.7647590637207\n",
      "191/363 Data:0.000 | Batch:0.475 | Total:0:01:10 | ETA:0:01:05 | Loss:0.695299755216269 | top1:50.829429626464844\n",
      "201/363 Data:0.001 | Batch:0.603 | Total:0:01:14 | ETA:0:01:08 | Loss:0.6953258241587017 | top1:50.73055648803711\n",
      "211/363 Data:0.001 | Batch:0.587 | Total:0:01:20 | ETA:0:01:32 | Loss:0.6953110322003117 | top1:50.65851974487305\n",
      "221/363 Data:0.001 | Batch:0.630 | Total:0:01:26 | ETA:0:01:26 | Loss:0.695393148860241 | top1:50.559654235839844\n",
      "231/363 Data:0.001 | Batch:0.590 | Total:0:01:32 | ETA:0:01:19 | Loss:0.695435149587078 | top1:50.462520599365234\n",
      "241/363 Data:0.001 | Batch:0.566 | Total:0:01:38 | ETA:0:01:14 | Loss:0.6953667478937331 | top1:50.532867431640625\n",
      "251/363 Data:0.001 | Batch:0.601 | Total:0:01:44 | ETA:0:01:08 | Loss:0.6953256633652158 | top1:50.5074462890625\n",
      "261/363 Data:0.000 | Batch:0.324 | Total:0:01:50 | ETA:0:00:56 | Loss:0.695285372578778 | top1:50.55656433105469\n",
      "271/363 Data:0.000 | Batch:0.634 | Total:0:01:55 | ETA:0:00:46 | Loss:0.6953248401409585 | top1:50.559329986572266\n",
      "281/363 Data:0.000 | Batch:0.322 | Total:0:02:00 | ETA:0:00:50 | Loss:0.6953184818882111 | top1:50.46076202392578\n",
      "291/363 Data:0.001 | Batch:0.319 | Total:0:02:03 | ETA:0:00:26 | Loss:0.695286390298011 | top1:50.49557113647461\n",
      "301/363 Data:0.000 | Batch:0.317 | Total:0:02:07 | ETA:0:00:23 | Loss:0.6952551415196289 | top1:50.49833679199219\n",
      "311/363 Data:0.000 | Batch:0.315 | Total:0:02:11 | ETA:0:00:19 | Loss:0.6952309970687056 | top1:50.436622619628906\n",
      "321/363 Data:0.000 | Batch:0.315 | Total:0:02:14 | ETA:0:00:16 | Loss:0.6952974556762481 | top1:50.383670806884766\n",
      "331/363 Data:0.000 | Batch:0.456 | Total:0:02:18 | ETA:0:00:12 | Loss:0.6952696849572334 | top1:50.383209228515625\n",
      "341/363 Data:0.001 | Batch:0.397 | Total:0:02:22 | ETA:0:00:09 | Loss:0.6952597570559147 | top1:50.407466888427734\n",
      "351/363 Data:0.000 | Batch:0.322 | Total:0:02:25 | ETA:0:00:05 | Loss:0.6952244459394037 | top1:50.40485763549805\n",
      "361/363 Data:0.000 | Batch:0.314 | Total:0:02:29 | ETA:0:00:01 | Loss:0.6952724580619474 | top1:50.41551208496094\n",
      "42/42 Data:0.000 | Batch:0.036 | Total:0:00:08 | ETA:0:00:00 | Loss:0.6954142060799476 | top1:50.0\n",
      "\n",
      "Epoch: [9 | 300] LR: 0.660000\n",
      "1/363 Data:1.169 | Batch:1.524 | Total:0:00:01 | ETA:0:09:12 | Loss:0.6911472678184509 | top1:53.157901763916016\n",
      "11/363 Data:0.000 | Batch:0.399 | Total:0:00:05 | ETA:0:02:48 | Loss:0.6944803487170826 | top1:50.52631378173828\n",
      "21/363 Data:0.000 | Batch:0.324 | Total:0:00:09 | ETA:0:02:15 | Loss:0.6952597498893738 | top1:50.92731857299805\n",
      "31/363 Data:0.000 | Batch:0.314 | Total:0:00:12 | ETA:0:02:12 | Loss:0.6957237797398721 | top1:50.0\n",
      "41/363 Data:0.000 | Batch:0.314 | Total:0:00:16 | ETA:0:01:55 | Loss:0.6952578832463521 | top1:50.14120864868164\n",
      "51/363 Data:0.000 | Batch:0.503 | Total:0:00:19 | ETA:0:01:59 | Loss:0.6949774693040287 | top1:50.16511917114258\n",
      "61/363 Data:0.000 | Batch:0.474 | Total:0:00:23 | ETA:0:01:48 | Loss:0.6947796510868385 | top1:50.181190490722656\n",
      "71/363 Data:0.000 | Batch:0.325 | Total:0:00:27 | ETA:0:01:57 | Loss:0.6949342430477411 | top1:50.467010498046875\n",
      "81/363 Data:0.001 | Batch:0.325 | Total:0:00:30 | ETA:0:01:42 | Loss:0.6947998389785672 | top1:50.67576217651367\n",
      "91/363 Data:0.000 | Batch:0.315 | Total:0:00:34 | ETA:0:01:38 | Loss:0.694727184353294 | top1:50.780799865722656\n",
      "101/363 Data:0.000 | Batch:0.496 | Total:0:00:38 | ETA:0:01:38 | Loss:0.694722037504215 | top1:50.708702087402344\n",
      "111/363 Data:0.000 | Batch:0.324 | Total:0:00:42 | ETA:0:01:41 | Loss:0.6947281919084154 | top1:50.715980529785156\n",
      "121/363 Data:0.000 | Batch:0.328 | Total:0:00:45 | ETA:0:01:28 | Loss:0.695154988076076 | top1:50.31753158569336\n",
      "131/363 Data:0.000 | Batch:0.325 | Total:0:00:49 | ETA:0:01:24 | Loss:0.6951869020935233 | top1:50.325435638427734\n",
      "141/363 Data:0.002 | Batch:0.414 | Total:0:00:53 | ETA:0:01:28 | Loss:0.6956052429287146 | top1:50.3284797668457\n",
      "151/363 Data:0.000 | Batch:0.316 | Total:0:00:56 | ETA:0:01:24 | Loss:0.6957427910621593 | top1:50.338096618652344\n",
      "161/363 Data:0.000 | Batch:0.315 | Total:0:01:00 | ETA:0:01:13 | Loss:0.6953922713765447 | top1:50.53612518310547\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
