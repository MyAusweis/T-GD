{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import resnext50_32x4d\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/star/128/32x4d/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'resnext32x4d' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 400\n",
    "test_batch = 400\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/star/128/32x4d/to_style2/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = 'fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/star/128/32x4d/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "student_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 22.98M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in student_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += 0.1*sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + (sp_alpha*loss_sp) + (sp_alpha*loss_cls)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 10/10 | Loss:1.0619 | MainLoss:1.0348 | Alpha:0.0051 | SPLoss:0.2674 | CLSLoss:5.0755 | top1:51.8056 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.7193 | MainLoss:0.7193 | SPLoss:0.4010 | CLSLoss:4.9145 | top1:53.8846 | AUROC:0.5549\n",
      "Test | 77/10 | Loss:0.2307 | MainLoss:0.2307 | SPLoss:0.4010 | CLSLoss:4.9145 | top1:91.8545 | AUROC:0.9986\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 10/10 | Loss:0.8574 | MainLoss:0.7030 | Alpha:0.0327 | SPLoss:0.0096 | CLSLoss:4.7061 | top1:53.7778 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6893 | MainLoss:0.6893 | SPLoss:0.0226 | CLSLoss:4.4618 | top1:54.8846 | AUROC:0.5702\n",
      "Test | 77/10 | Loss:0.2825 | MainLoss:0.2825 | SPLoss:0.0226 | CLSLoss:4.4618 | top1:90.7831 | AUROC:0.9982\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 10/10 | Loss:0.8234 | MainLoss:0.6806 | Alpha:0.0336 | SPLoss:0.0038 | CLSLoss:4.2502 | top1:56.6667 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6827 | MainLoss:0.6827 | SPLoss:0.0087 | CLSLoss:3.9967 | top1:55.9103 | AUROC:0.5836\n",
      "Test | 77/10 | Loss:0.2977 | MainLoss:0.2977 | SPLoss:0.0087 | CLSLoss:3.9967 | top1:90.2654 | AUROC:0.9980\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 10/10 | Loss:0.8041 | MainLoss:0.6767 | Alpha:0.0337 | SPLoss:0.0042 | CLSLoss:3.7770 | top1:57.0556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6776 | MainLoss:0.6776 | SPLoss:0.0103 | CLSLoss:3.5152 | top1:57.0513 | AUROC:0.5994\n",
      "Test | 77/10 | Loss:0.3021 | MainLoss:0.3021 | SPLoss:0.0103 | CLSLoss:3.5152 | top1:90.2785 | AUROC:0.9979\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 10/10 | Loss:0.7830 | MainLoss:0.6714 | Alpha:0.0338 | SPLoss:0.0055 | CLSLoss:3.2962 | top1:58.6389 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6725 | MainLoss:0.6725 | SPLoss:0.0137 | CLSLoss:3.0386 | top1:57.9103 | AUROC:0.6145\n",
      "Test | 77/10 | Loss:0.2980 | MainLoss:0.2980 | SPLoss:0.0137 | CLSLoss:3.0386 | top1:91.1861 | AUROC:0.9977\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 10/10 | Loss:0.7611 | MainLoss:0.6649 | Alpha:0.0339 | SPLoss:0.0074 | CLSLoss:2.8284 | top1:59.4167 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6668 | MainLoss:0.6668 | SPLoss:0.0193 | CLSLoss:2.5824 | top1:58.5641 | AUROC:0.6315\n",
      "Test | 77/10 | Loss:0.2861 | MainLoss:0.2861 | SPLoss:0.0193 | CLSLoss:2.5824 | top1:92.2543 | AUROC:0.9975\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 10/10 | Loss:0.7346 | MainLoss:0.6525 | Alpha:0.0342 | SPLoss:0.0125 | CLSLoss:2.3908 | top1:61.9444 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6639 | MainLoss:0.6639 | SPLoss:0.0277 | CLSLoss:2.1617 | top1:58.9487 | AUROC:0.6494\n",
      "Test | 77/10 | Loss:0.2600 | MainLoss:0.2600 | SPLoss:0.0277 | CLSLoss:2.1617 | top1:94.7706 | AUROC:0.9978\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 10/10 | Loss:0.7089 | MainLoss:0.6399 | Alpha:0.0343 | SPLoss:0.0166 | CLSLoss:1.9916 | top1:63.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6470 | MainLoss:0.6470 | SPLoss:0.0436 | CLSLoss:1.8053 | top1:62.3462 | AUROC:0.6695\n",
      "Test | 77/10 | Loss:0.2761 | MainLoss:0.2761 | SPLoss:0.0436 | CLSLoss:1.8053 | top1:90.5439 | AUROC:0.9967\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 10/10 | Loss:0.6795 | MainLoss:0.6207 | Alpha:0.0349 | SPLoss:0.0248 | CLSLoss:1.6600 | top1:66.6667 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6362 | MainLoss:0.6362 | SPLoss:0.0621 | CLSLoss:1.4896 | top1:63.6154 | AUROC:0.6882\n",
      "Test | 77/10 | Loss:0.2530 | MainLoss:0.2530 | SPLoss:0.0621 | CLSLoss:1.4896 | top1:91.9954 | AUROC:0.9973\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 10/10 | Loss:0.6514 | MainLoss:0.6015 | Alpha:0.0354 | SPLoss:0.0382 | CLSLoss:1.3721 | top1:67.8889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6271 | MainLoss:0.6271 | SPLoss:0.0901 | CLSLoss:1.2315 | top1:64.8333 | AUROC:0.7093\n",
      "Test | 77/10 | Loss:0.2730 | MainLoss:0.2730 | SPLoss:0.0901 | CLSLoss:1.2315 | top1:89.2267 | AUROC:0.9960\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.6291 | MainLoss:0.5869 | Alpha:0.0359 | SPLoss:0.0526 | CLSLoss:1.1226 | top1:69.2778 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.7210 | MainLoss:0.7210 | SPLoss:0.1404 | CLSLoss:0.9902 | top1:56.6923 | AUROC:0.7150\n",
      "Test | 77/10 | Loss:0.1854 | MainLoss:0.1854 | SPLoss:0.1404 | CLSLoss:0.9902 | top1:97.4869 | AUROC:0.9971\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.6033 | MainLoss:0.5703 | Alpha:0.0341 | SPLoss:0.0633 | CLSLoss:0.9024 | top1:69.6111 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5988 | MainLoss:0.5988 | SPLoss:0.1326 | CLSLoss:0.8687 | top1:67.6154 | AUROC:0.7467\n",
      "Test | 77/10 | Loss:0.2589 | MainLoss:0.2589 | SPLoss:0.1326 | CLSLoss:0.8687 | top1:89.9607 | AUROC:0.9948\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 10/10 | Loss:0.6105 | MainLoss:0.5801 | Alpha:0.0371 | SPLoss:0.0776 | CLSLoss:0.7406 | top1:70.8611 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6135 | MainLoss:0.6135 | SPLoss:0.1133 | CLSLoss:0.7081 | top1:66.7821 | AUROC:0.7523\n",
      "Test | 77/10 | Loss:0.2043 | MainLoss:0.2043 | SPLoss:0.1133 | CLSLoss:0.7081 | top1:95.5406 | AUROC:0.9954\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 10/10 | Loss:0.5689 | MainLoss:0.5423 | Alpha:0.0372 | SPLoss:0.0697 | CLSLoss:0.6467 | top1:73.0278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5748 | MainLoss:0.5748 | SPLoss:0.1221 | CLSLoss:0.6825 | top1:70.0897 | AUROC:0.7697\n",
      "Test | 77/10 | Loss:0.2321 | MainLoss:0.2321 | SPLoss:0.1221 | CLSLoss:0.6825 | top1:91.7104 | AUROC:0.9938\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 10/10 | Loss:0.6486 | MainLoss:0.6206 | Alpha:0.0383 | SPLoss:0.1859 | CLSLoss:0.5449 | top1:69.8611 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6030 | MainLoss:0.6030 | SPLoss:0.2893 | CLSLoss:0.3869 | top1:69.9744 | AUROC:0.7643\n",
      "Test | 77/10 | Loss:0.3179 | MainLoss:0.3179 | SPLoss:0.2893 | CLSLoss:0.3869 | top1:92.2379 | AUROC:0.9960\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 10/10 | Loss:0.5404 | MainLoss:0.5197 | Alpha:0.0365 | SPLoss:0.0652 | CLSLoss:0.5017 | top1:77.3889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6827 | MainLoss:0.6827 | SPLoss:0.1912 | CLSLoss:0.5899 | top1:63.5128 | AUROC:0.7833\n",
      "Test | 77/10 | Loss:0.3531 | MainLoss:0.3531 | SPLoss:0.1912 | CLSLoss:0.5899 | top1:83.4043 | AUROC:0.9899\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 10/10 | Loss:0.4997 | MainLoss:0.4744 | Alpha:0.0363 | SPLoss:0.1008 | CLSLoss:0.5957 | top1:78.0000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6710 | MainLoss:0.6710 | SPLoss:0.2409 | CLSLoss:0.6618 | top1:64.4359 | AUROC:0.7940\n",
      "Test | 77/10 | Loss:0.1606 | MainLoss:0.1606 | SPLoss:0.2409 | CLSLoss:0.6618 | top1:96.2877 | AUROC:0.9944\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 10/10 | Loss:0.6449 | MainLoss:0.6197 | Alpha:0.0372 | SPLoss:0.2543 | CLSLoss:0.4242 | top1:72.3333 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5481 | MainLoss:0.5481 | SPLoss:0.2242 | CLSLoss:0.4744 | top1:72.5000 | AUROC:0.8002\n",
      "Test | 77/10 | Loss:0.2535 | MainLoss:0.2535 | SPLoss:0.2242 | CLSLoss:0.4744 | top1:92.6212 | AUROC:0.9930\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 10/10 | Loss:0.4188 | MainLoss:0.3913 | Alpha:0.0393 | SPLoss:0.0931 | CLSLoss:0.6064 | top1:84.0833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5501 | MainLoss:0.5501 | SPLoss:0.2504 | CLSLoss:0.7143 | top1:72.9103 | AUROC:0.8108\n",
      "Test | 77/10 | Loss:0.1919 | MainLoss:0.1919 | SPLoss:0.2504 | CLSLoss:0.7144 | top1:93.0701 | AUROC:0.9910\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 10/10 | Loss:0.6411 | MainLoss:0.6071 | Alpha:0.0413 | SPLoss:0.3515 | CLSLoss:0.4696 | top1:72.7222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5404 | MainLoss:0.5404 | SPLoss:0.3900 | CLSLoss:0.4108 | top1:73.2949 | AUROC:0.8132\n",
      "Test | 77/10 | Loss:0.2759 | MainLoss:0.2759 | SPLoss:0.3900 | CLSLoss:0.4108 | top1:92.3296 | AUROC:0.9913\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 10/10 | Loss:0.4686 | MainLoss:0.4415 | Alpha:0.0395 | SPLoss:0.1131 | CLSLoss:0.5716 | top1:81.3889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.6732 | MainLoss:0.6732 | SPLoss:0.3147 | CLSLoss:0.4780 | top1:62.5385 | AUROC:0.8112\n",
      "Test | 77/10 | Loss:0.2218 | MainLoss:0.2218 | SPLoss:0.3147 | CLSLoss:0.4780 | top1:95.3277 | AUROC:0.9916\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 10/10 | Loss:0.3756 | MainLoss:0.3491 | Alpha:0.0374 | SPLoss:0.1229 | CLSLoss:0.5835 | top1:85.8889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5164 | MainLoss:0.5164 | SPLoss:0.2897 | CLSLoss:0.7348 | top1:75.5256 | AUROC:0.8350\n",
      "Test | 77/10 | Loss:0.2031 | MainLoss:0.2031 | SPLoss:0.2897 | CLSLoss:0.7348 | top1:91.9004 | AUROC:0.9879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 10/10 | Loss:0.4827 | MainLoss:0.4427 | Alpha:0.0438 | SPLoss:0.3050 | CLSLoss:0.6079 | top1:82.0833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5046 | MainLoss:0.5046 | SPLoss:0.4156 | CLSLoss:0.5761 | top1:75.2436 | AUROC:0.8324\n",
      "Test | 77/10 | Loss:0.2306 | MainLoss:0.2306 | SPLoss:0.4156 | CLSLoss:0.5761 | top1:91.3368 | AUROC:0.9852\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 10/10 | Loss:0.4171 | MainLoss:0.3846 | Alpha:0.0423 | SPLoss:0.1680 | CLSLoss:0.6003 | top1:83.4722 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4931 | MainLoss:0.4931 | SPLoss:0.3196 | CLSLoss:0.5650 | top1:76.1667 | AUROC:0.8423\n",
      "Test | 77/10 | Loss:0.2308 | MainLoss:0.2308 | SPLoss:0.3196 | CLSLoss:0.5650 | top1:91.4679 | AUROC:0.9861\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 10/10 | Loss:0.2656 | MainLoss:0.2302 | Alpha:0.0433 | SPLoss:0.1338 | CLSLoss:0.6849 | top1:92.1389 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5997 | MainLoss:0.5997 | SPLoss:0.3436 | CLSLoss:0.7637 | top1:75.0513 | AUROC:0.8503\n",
      "Test | 77/10 | Loss:0.1698 | MainLoss:0.1698 | SPLoss:0.3436 | CLSLoss:0.7637 | top1:93.3191 | AUROC:0.9831\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 10/10 | Loss:0.5745 | MainLoss:0.5259 | Alpha:0.0443 | SPLoss:0.6064 | CLSLoss:0.4922 | top1:80.8333 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4926 | MainLoss:0.4926 | SPLoss:0.6127 | CLSLoss:0.5185 | top1:75.6923 | AUROC:0.8475\n",
      "Test | 77/10 | Loss:0.2487 | MainLoss:0.2487 | SPLoss:0.6127 | CLSLoss:0.5185 | top1:90.4260 | AUROC:0.9851\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 10/10 | Loss:0.2774 | MainLoss:0.2434 | Alpha:0.0432 | SPLoss:0.1403 | CLSLoss:0.6469 | top1:91.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5081 | MainLoss:0.5081 | SPLoss:0.3183 | CLSLoss:0.7228 | top1:76.8333 | AUROC:0.8590\n",
      "Test | 77/10 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:0.3183 | CLSLoss:0.7228 | top1:90.6455 | AUROC:0.9843\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 10/10 | Loss:0.3980 | MainLoss:0.3559 | Alpha:0.0455 | SPLoss:0.2812 | CLSLoss:0.6453 | top1:84.5278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4728 | MainLoss:0.4728 | SPLoss:0.5475 | CLSLoss:0.4941 | top1:77.2949 | AUROC:0.8552\n",
      "Test | 77/10 | Loss:0.2298 | MainLoss:0.2298 | SPLoss:0.5475 | CLSLoss:0.4941 | top1:91.8283 | AUROC:0.9803\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 10/10 | Loss:0.1998 | MainLoss:0.1653 | Alpha:0.0445 | SPLoss:0.1312 | CLSLoss:0.6442 | top1:95.1667 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5161 | MainLoss:0.5161 | SPLoss:0.3301 | CLSLoss:0.7137 | top1:77.8333 | AUROC:0.8636\n",
      "Test | 77/10 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:0.3301 | CLSLoss:0.7137 | top1:90.9109 | AUROC:0.9775\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 10/10 | Loss:0.3675 | MainLoss:0.3203 | Alpha:0.0466 | SPLoss:0.3831 | CLSLoss:0.6296 | top1:86.3333 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4702 | MainLoss:0.4702 | SPLoss:0.7738 | CLSLoss:0.4595 | top1:77.6282 | AUROC:0.8579\n",
      "Test | 77/10 | Loss:0.2264 | MainLoss:0.2264 | SPLoss:0.7738 | CLSLoss:0.4595 | top1:91.6809 | AUROC:0.9804\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 10/10 | Loss:0.2019 | MainLoss:0.1690 | Alpha:0.0445 | SPLoss:0.1559 | CLSLoss:0.5830 | top1:94.6389 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5382 | MainLoss:0.5382 | SPLoss:0.3402 | CLSLoss:0.6797 | top1:77.9103 | AUROC:0.8696\n",
      "Test | 77/10 | Loss:0.2385 | MainLoss:0.2385 | SPLoss:0.3402 | CLSLoss:0.6797 | top1:90.3440 | AUROC:0.9771\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 10/10 | Loss:0.1474 | MainLoss:0.1077 | Alpha:0.0477 | SPLoss:0.1592 | CLSLoss:0.6749 | top1:95.9722 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5547 | MainLoss:0.5547 | SPLoss:0.3574 | CLSLoss:0.6471 | top1:77.7564 | AUROC:0.8691\n",
      "Test | 77/10 | Loss:0.2405 | MainLoss:0.2405 | SPLoss:0.3574 | CLSLoss:0.6471 | top1:90.4128 | AUROC:0.9767\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 10/10 | Loss:0.1489 | MainLoss:0.1083 | Alpha:0.0479 | SPLoss:0.2119 | CLSLoss:0.6332 | top1:96.0000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.8228 | MainLoss:0.8228 | SPLoss:0.4701 | CLSLoss:0.6037 | top1:71.3205 | AUROC:0.8697\n",
      "Test | 77/10 | Loss:0.4072 | MainLoss:0.4072 | SPLoss:0.4701 | CLSLoss:0.6037 | top1:84.5446 | AUROC:0.9696\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 10/10 | Loss:0.2862 | MainLoss:0.2417 | Alpha:0.0445 | SPLoss:0.4863 | CLSLoss:0.5115 | top1:90.0278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5113 | MainLoss:0.5113 | SPLoss:0.6274 | CLSLoss:0.5393 | top1:79.1410 | AUROC:0.8748\n",
      "Test | 77/10 | Loss:0.2245 | MainLoss:0.2245 | SPLoss:0.6274 | CLSLoss:0.5393 | top1:90.7962 | AUROC:0.9728\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 10/10 | Loss:0.3869 | MainLoss:0.3404 | Alpha:0.0474 | SPLoss:0.5007 | CLSLoss:0.4778 | top1:86.1944 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5190 | MainLoss:0.5190 | SPLoss:1.4757 | CLSLoss:0.2367 | top1:75.5513 | AUROC:0.8396\n",
      "Test | 77/10 | Loss:0.3300 | MainLoss:0.3300 | SPLoss:1.4757 | CLSLoss:0.2367 | top1:89.8788 | AUROC:0.9687\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 10/10 | Loss:0.2129 | MainLoss:0.1847 | Alpha:0.0414 | SPLoss:0.2141 | CLSLoss:0.4672 | top1:95.0556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5094 | MainLoss:0.5094 | SPLoss:0.5002 | CLSLoss:0.6046 | top1:78.6282 | AUROC:0.8761\n",
      "Test | 77/10 | Loss:0.2521 | MainLoss:0.2521 | SPLoss:0.5002 | CLSLoss:0.6046 | top1:89.6265 | AUROC:0.9747\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 10/10 | Loss:0.1057 | MainLoss:0.0699 | Alpha:0.0480 | SPLoss:0.1331 | CLSLoss:0.6125 | top1:97.8056 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5338 | MainLoss:0.5338 | SPLoss:0.2733 | CLSLoss:0.5954 | top1:80.3077 | AUROC:0.8808\n",
      "Test | 77/10 | Loss:0.2382 | MainLoss:0.2382 | SPLoss:0.2733 | CLSLoss:0.5954 | top1:90.8421 | AUROC:0.9725\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 10/10 | Loss:0.0914 | MainLoss:0.0586 | Alpha:0.0486 | SPLoss:0.1152 | CLSLoss:0.5610 | top1:98.2778 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5734 | MainLoss:0.5734 | SPLoss:0.3001 | CLSLoss:0.5239 | top1:79.0641 | AUROC:0.8791\n",
      "Test | 77/10 | Loss:0.2753 | MainLoss:0.2753 | SPLoss:0.3001 | CLSLoss:0.5239 | top1:89.5773 | AUROC:0.9712\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 10/10 | Loss:0.1017 | MainLoss:0.0672 | Alpha:0.0487 | SPLoss:0.2163 | CLSLoss:0.4916 | top1:97.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5775 | MainLoss:0.5775 | SPLoss:0.4024 | CLSLoss:0.4750 | top1:79.6667 | AUROC:0.8785\n",
      "Test | 77/10 | Loss:0.2350 | MainLoss:0.2350 | SPLoss:0.4024 | CLSLoss:0.4750 | top1:91.1435 | AUROC:0.9714\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 10/10 | Loss:0.0934 | MainLoss:0.0627 | Alpha:0.0485 | SPLoss:0.1837 | CLSLoss:0.4491 | top1:97.9445 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5824 | MainLoss:0.5824 | SPLoss:0.3610 | CLSLoss:0.4239 | top1:79.8462 | AUROC:0.8838\n",
      "Test | 77/10 | Loss:0.2206 | MainLoss:0.2206 | SPLoss:0.3610 | CLSLoss:0.4239 | top1:91.6153 | AUROC:0.9729\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 10/10 | Loss:0.1123 | MainLoss:0.0797 | Alpha:0.0484 | SPLoss:0.2618 | CLSLoss:0.4100 | top1:97.2222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5644 | MainLoss:0.5644 | SPLoss:0.4141 | CLSLoss:0.4118 | top1:80.0769 | AUROC:0.8856\n",
      "Test | 77/10 | Loss:0.2857 | MainLoss:0.2857 | SPLoss:0.4141 | CLSLoss:0.4118 | top1:89.6658 | AUROC:0.9687\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 10/10 | Loss:0.2385 | MainLoss:0.2062 | Alpha:0.0481 | SPLoss:0.2977 | CLSLoss:0.3735 | top1:92.0278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:1.3352 | MainLoss:1.3352 | SPLoss:1.7945 | CLSLoss:0.2105 | top1:50.6538 | AUROC:0.8074\n",
      "Test | 77/10 | Loss:1.0430 | MainLoss:1.0430 | SPLoss:1.7945 | CLSLoss:0.2105 | top1:52.4312 | AUROC:0.8630\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 10/10 | Loss:0.4382 | MainLoss:0.4162 | Alpha:0.0249 | SPLoss:0.6295 | CLSLoss:0.2499 | top1:81.2222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5911 | MainLoss:0.5911 | SPLoss:0.9164 | CLSLoss:0.5010 | top1:77.8333 | AUROC:0.8747\n",
      "Test | 77/10 | Loss:0.2572 | MainLoss:0.2572 | SPLoss:0.9164 | CLSLoss:0.5010 | top1:89.6822 | AUROC:0.9624\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 10/10 | Loss:0.1969 | MainLoss:0.1567 | Alpha:0.0465 | SPLoss:0.3636 | CLSLoss:0.5011 | top1:94.1111 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5017 | MainLoss:0.5017 | SPLoss:0.5694 | CLSLoss:0.5319 | top1:80.0641 | AUROC:0.8872\n",
      "Test | 77/10 | Loss:0.2709 | MainLoss:0.2709 | SPLoss:0.5694 | CLSLoss:0.5319 | top1:89.1809 | AUROC:0.9671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 10/10 | Loss:0.1192 | MainLoss:0.0884 | Alpha:0.0483 | SPLoss:0.1192 | CLSLoss:0.5200 | top1:96.8889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.7391 | MainLoss:0.7391 | SPLoss:0.5737 | CLSLoss:0.4290 | top1:74.0513 | AUROC:0.8758\n",
      "Test | 77/10 | Loss:0.2829 | MainLoss:0.2829 | SPLoss:0.5737 | CLSLoss:0.4290 | top1:88.9351 | AUROC:0.9601\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 10/10 | Loss:0.1284 | MainLoss:0.0966 | Alpha:0.0438 | SPLoss:0.2775 | CLSLoss:0.4491 | top1:96.3889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4840 | MainLoss:0.4840 | SPLoss:0.4207 | CLSLoss:0.4693 | top1:81.6538 | AUROC:0.8961\n",
      "Test | 77/10 | Loss:0.2630 | MainLoss:0.2630 | SPLoss:0.4207 | CLSLoss:0.4693 | top1:89.8460 | AUROC:0.9642\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 10/10 | Loss:0.0660 | MainLoss:0.0389 | Alpha:0.0490 | SPLoss:0.1008 | CLSLoss:0.4516 | top1:98.8333 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5035 | MainLoss:0.5035 | SPLoss:0.2356 | CLSLoss:0.4165 | top1:81.5000 | AUROC:0.8966\n",
      "Test | 77/10 | Loss:0.2744 | MainLoss:0.2744 | SPLoss:0.2356 | CLSLoss:0.4165 | top1:89.6756 | AUROC:0.9633\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 10/10 | Loss:0.0739 | MainLoss:0.0471 | Alpha:0.0489 | SPLoss:0.1573 | CLSLoss:0.3895 | top1:98.5833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5223 | MainLoss:0.5223 | SPLoss:0.2964 | CLSLoss:0.3782 | top1:81.6795 | AUROC:0.8988\n",
      "Test | 77/10 | Loss:0.3035 | MainLoss:0.3035 | SPLoss:0.2964 | CLSLoss:0.3782 | top1:89.0367 | AUROC:0.9613\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 10/10 | Loss:0.0449 | MainLoss:0.0241 | Alpha:0.0494 | SPLoss:0.0606 | CLSLoss:0.3615 | top1:99.2778 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5240 | MainLoss:0.5240 | SPLoss:0.2047 | CLSLoss:0.3308 | top1:81.9359 | AUROC:0.8971\n",
      "Test | 77/10 | Loss:0.2845 | MainLoss:0.2845 | SPLoss:0.2047 | CLSLoss:0.3308 | top1:89.5577 | AUROC:0.9607\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 10/10 | Loss:0.0481 | MainLoss:0.0272 | Alpha:0.0493 | SPLoss:0.1072 | CLSLoss:0.3151 | top1:99.2222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5363 | MainLoss:0.5363 | SPLoss:0.1935 | CLSLoss:0.2998 | top1:81.8333 | AUROC:0.9001\n",
      "Test | 77/10 | Loss:0.2715 | MainLoss:0.2715 | SPLoss:0.1935 | CLSLoss:0.2998 | top1:90.3703 | AUROC:0.9644\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 10/10 | Loss:0.1216 | MainLoss:0.0944 | Alpha:0.0490 | SPLoss:0.2796 | CLSLoss:0.2752 | top1:96.5833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5374 | MainLoss:0.5374 | SPLoss:0.7337 | CLSLoss:0.2872 | top1:80.0897 | AUROC:0.8922\n",
      "Test | 77/10 | Loss:0.2697 | MainLoss:0.2697 | SPLoss:0.7337 | CLSLoss:0.2872 | top1:89.4299 | AUROC:0.9589\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 10/10 | Loss:0.0668 | MainLoss:0.0527 | Alpha:0.0483 | SPLoss:0.0023 | CLSLoss:0.2909 | top1:98.3056 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4731 | MainLoss:0.4731 | SPLoss:0.0077 | CLSLoss:0.2930 | top1:81.5000 | AUROC:0.8983\n",
      "Test | 77/10 | Loss:0.2675 | MainLoss:0.2675 | SPLoss:0.0077 | CLSLoss:0.2930 | top1:89.4430 | AUROC:0.9611\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 10/10 | Loss:0.0497 | MainLoss:0.0352 | Alpha:0.0491 | SPLoss:0.0011 | CLSLoss:0.2946 | top1:99.0556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4818 | MainLoss:0.4818 | SPLoss:0.0027 | CLSLoss:0.2971 | top1:81.5385 | AUROC:0.8990\n",
      "Test | 77/10 | Loss:0.2767 | MainLoss:0.2767 | SPLoss:0.0027 | CLSLoss:0.2971 | top1:89.3054 | AUROC:0.9611\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 10/10 | Loss:0.0401 | MainLoss:0.0254 | Alpha:0.0493 | SPLoss:0.0008 | CLSLoss:0.2986 | top1:99.3056 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4949 | MainLoss:0.4949 | SPLoss:0.0020 | CLSLoss:0.3004 | top1:81.2949 | AUROC:0.8993\n",
      "Test | 77/10 | Loss:0.2824 | MainLoss:0.2824 | SPLoss:0.0020 | CLSLoss:0.3004 | top1:89.2890 | AUROC:0.9616\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 10/10 | Loss:0.0658 | MainLoss:0.0512 | Alpha:0.0487 | SPLoss:0.0020 | CLSLoss:0.2980 | top1:98.4445 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.4972 | MainLoss:0.4972 | SPLoss:0.0041 | CLSLoss:0.2968 | top1:81.2564 | AUROC:0.8999\n",
      "Test | 77/10 | Loss:0.2848 | MainLoss:0.2848 | SPLoss:0.0041 | CLSLoss:0.2968 | top1:89.2300 | AUROC:0.9624\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 10/10 | Loss:0.0351 | MainLoss:0.0203 | Alpha:0.0495 | SPLoss:0.0006 | CLSLoss:0.2977 | top1:99.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5046 | MainLoss:0.5046 | SPLoss:0.0015 | CLSLoss:0.2985 | top1:81.2564 | AUROC:0.9007\n",
      "Test | 77/10 | Loss:0.2859 | MainLoss:0.2859 | SPLoss:0.0015 | CLSLoss:0.2985 | top1:89.3119 | AUROC:0.9624\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 10/10 | Loss:0.0463 | MainLoss:0.0316 | Alpha:0.0492 | SPLoss:0.0015 | CLSLoss:0.2971 | top1:99.0556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5029 | MainLoss:0.5029 | SPLoss:0.0025 | CLSLoss:0.2974 | top1:81.3974 | AUROC:0.9012\n",
      "Test | 77/10 | Loss:0.2822 | MainLoss:0.2822 | SPLoss:0.0025 | CLSLoss:0.2974 | top1:89.5052 | AUROC:0.9621\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 10/10 | Loss:0.0563 | MainLoss:0.0417 | Alpha:0.0489 | SPLoss:0.0020 | CLSLoss:0.2958 | top1:98.6111 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5079 | MainLoss:0.5079 | SPLoss:0.0039 | CLSLoss:0.2940 | top1:81.4615 | AUROC:0.9023\n",
      "Test | 77/10 | Loss:0.2905 | MainLoss:0.2905 | SPLoss:0.0039 | CLSLoss:0.2940 | top1:89.2628 | AUROC:0.9619\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 10/10 | Loss:0.0486 | MainLoss:0.0342 | Alpha:0.0491 | SPLoss:0.0012 | CLSLoss:0.2927 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5063 | MainLoss:0.5063 | SPLoss:0.0028 | CLSLoss:0.2910 | top1:81.6795 | AUROC:0.9025\n",
      "Test | 77/10 | Loss:0.2885 | MainLoss:0.2885 | SPLoss:0.0028 | CLSLoss:0.2910 | top1:89.3414 | AUROC:0.9622\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 10/10 | Loss:0.0382 | MainLoss:0.0238 | Alpha:0.0494 | SPLoss:0.0007 | CLSLoss:0.2905 | top1:99.3889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5105 | MainLoss:0.5105 | SPLoss:0.0022 | CLSLoss:0.2895 | top1:81.6410 | AUROC:0.9024\n",
      "Test | 77/10 | Loss:0.2888 | MainLoss:0.2888 | SPLoss:0.0022 | CLSLoss:0.2895 | top1:89.3676 | AUROC:0.9622\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 10/10 | Loss:0.0339 | MainLoss:0.0196 | Alpha:0.0495 | SPLoss:0.0006 | CLSLoss:0.2887 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5131 | MainLoss:0.5131 | SPLoss:0.0013 | CLSLoss:0.2872 | top1:81.5769 | AUROC:0.9022\n",
      "Test | 77/10 | Loss:0.2899 | MainLoss:0.2899 | SPLoss:0.0013 | CLSLoss:0.2872 | top1:89.3250 | AUROC:0.9619\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 10/10 | Loss:0.0515 | MainLoss:0.0374 | Alpha:0.0491 | SPLoss:0.0013 | CLSLoss:0.2853 | top1:98.7222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5074 | MainLoss:0.5074 | SPLoss:0.0042 | CLSLoss:0.2827 | top1:81.9744 | AUROC:0.9027\n",
      "Test | 77/10 | Loss:0.2900 | MainLoss:0.2900 | SPLoss:0.0042 | CLSLoss:0.2827 | top1:89.3676 | AUROC:0.9613\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 10/10 | Loss:0.0414 | MainLoss:0.0275 | Alpha:0.0493 | SPLoss:0.0016 | CLSLoss:0.2807 | top1:99.1667 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5095 | MainLoss:0.5095 | SPLoss:0.0027 | CLSLoss:0.2793 | top1:81.9231 | AUROC:0.9031\n",
      "Test | 77/10 | Loss:0.2883 | MainLoss:0.2883 | SPLoss:0.0027 | CLSLoss:0.2793 | top1:89.4692 | AUROC:0.9616\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 10/10 | Loss:0.0378 | MainLoss:0.0240 | Alpha:0.0494 | SPLoss:0.0008 | CLSLoss:0.2779 | top1:99.3333 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5129 | MainLoss:0.5129 | SPLoss:0.0020 | CLSLoss:0.2756 | top1:81.8974 | AUROC:0.9038\n",
      "Test | 77/10 | Loss:0.2922 | MainLoss:0.2922 | SPLoss:0.0020 | CLSLoss:0.2756 | top1:89.3611 | AUROC:0.9614\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 10/10 | Loss:0.0284 | MainLoss:0.0147 | Alpha:0.0496 | SPLoss:0.0006 | CLSLoss:0.2744 | top1:99.6945 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5167 | MainLoss:0.5167 | SPLoss:0.0010 | CLSLoss:0.2736 | top1:81.8718 | AUROC:0.9033\n",
      "Test | 77/10 | Loss:0.2929 | MainLoss:0.2929 | SPLoss:0.0010 | CLSLoss:0.2736 | top1:89.4102 | AUROC:0.9616\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 10/10 | Loss:0.0432 | MainLoss:0.0297 | Alpha:0.0493 | SPLoss:0.0012 | CLSLoss:0.2725 | top1:99.0278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5212 | MainLoss:0.5212 | SPLoss:0.0028 | CLSLoss:0.2699 | top1:81.8846 | AUROC:0.9033\n",
      "Test | 77/10 | Loss:0.2967 | MainLoss:0.2967 | SPLoss:0.0028 | CLSLoss:0.2699 | top1:89.3676 | AUROC:0.9609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 10/10 | Loss:0.0419 | MainLoss:0.0286 | Alpha:0.0493 | SPLoss:0.0015 | CLSLoss:0.2689 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5288 | MainLoss:0.5288 | SPLoss:0.0028 | CLSLoss:0.2680 | top1:81.8846 | AUROC:0.9032\n",
      "Test | 77/10 | Loss:0.3012 | MainLoss:0.3012 | SPLoss:0.0028 | CLSLoss:0.2680 | top1:89.3021 | AUROC:0.9609\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 10/10 | Loss:0.0381 | MainLoss:0.0249 | Alpha:0.0494 | SPLoss:0.0008 | CLSLoss:0.2671 | top1:99.3611 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5181 | MainLoss:0.5181 | SPLoss:0.0025 | CLSLoss:0.2652 | top1:82.2179 | AUROC:0.9036\n",
      "Test | 77/10 | Loss:0.2914 | MainLoss:0.2914 | SPLoss:0.0025 | CLSLoss:0.2652 | top1:89.5446 | AUROC:0.9610\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 10/10 | Loss:0.0322 | MainLoss:0.0191 | Alpha:0.0495 | SPLoss:0.0005 | CLSLoss:0.2642 | top1:99.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5251 | MainLoss:0.5251 | SPLoss:0.0018 | CLSLoss:0.2622 | top1:82.1410 | AUROC:0.9035\n",
      "Test | 77/10 | Loss:0.2963 | MainLoss:0.2963 | SPLoss:0.0018 | CLSLoss:0.2622 | top1:89.3840 | AUROC:0.9607\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 10/10 | Loss:0.0344 | MainLoss:0.0215 | Alpha:0.0495 | SPLoss:0.0008 | CLSLoss:0.2604 | top1:99.4445 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5232 | MainLoss:0.5232 | SPLoss:0.0019 | CLSLoss:0.2581 | top1:82.2692 | AUROC:0.9036\n",
      "Test | 77/10 | Loss:0.2943 | MainLoss:0.2943 | SPLoss:0.0019 | CLSLoss:0.2581 | top1:89.4790 | AUROC:0.9607\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 10/10 | Loss:0.0469 | MainLoss:0.0342 | Alpha:0.0492 | SPLoss:0.0022 | CLSLoss:0.2552 | top1:98.9445 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5162 | MainLoss:0.5162 | SPLoss:0.0038 | CLSLoss:0.2531 | top1:82.1795 | AUROC:0.9034\n",
      "Test | 77/10 | Loss:0.2896 | MainLoss:0.2896 | SPLoss:0.0038 | CLSLoss:0.2531 | top1:89.5708 | AUROC:0.9609\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 10/10 | Loss:0.0342 | MainLoss:0.0217 | Alpha:0.0494 | SPLoss:0.0010 | CLSLoss:0.2518 | top1:99.2778 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5209 | MainLoss:0.5209 | SPLoss:0.0022 | CLSLoss:0.2509 | top1:82.3846 | AUROC:0.9039\n",
      "Test | 77/10 | Loss:0.2947 | MainLoss:0.2947 | SPLoss:0.0022 | CLSLoss:0.2509 | top1:89.4430 | AUROC:0.9609\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 10/10 | Loss:0.0444 | MainLoss:0.0321 | Alpha:0.0492 | SPLoss:0.0011 | CLSLoss:0.2496 | top1:99.0278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5219 | MainLoss:0.5219 | SPLoss:0.0036 | CLSLoss:0.2461 | top1:82.0513 | AUROC:0.9041\n",
      "Test | 77/10 | Loss:0.2962 | MainLoss:0.2962 | SPLoss:0.0036 | CLSLoss:0.2461 | top1:89.3906 | AUROC:0.9605\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 10/10 | Loss:0.0389 | MainLoss:0.0267 | Alpha:0.0493 | SPLoss:0.0019 | CLSLoss:0.2451 | top1:99.2500 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5232 | MainLoss:0.5232 | SPLoss:0.0030 | CLSLoss:0.2444 | top1:82.1795 | AUROC:0.9048\n",
      "Test | 77/10 | Loss:0.2967 | MainLoss:0.2967 | SPLoss:0.0030 | CLSLoss:0.2444 | top1:89.4070 | AUROC:0.9609\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 10/10 | Loss:0.0451 | MainLoss:0.0331 | Alpha:0.0492 | SPLoss:0.0015 | CLSLoss:0.2431 | top1:98.8889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5141 | MainLoss:0.5141 | SPLoss:0.0033 | CLSLoss:0.2402 | top1:82.2051 | AUROC:0.9052\n",
      "Test | 77/10 | Loss:0.2925 | MainLoss:0.2925 | SPLoss:0.0033 | CLSLoss:0.2402 | top1:89.5052 | AUROC:0.9608\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 10/10 | Loss:0.0393 | MainLoss:0.0274 | Alpha:0.0493 | SPLoss:0.0014 | CLSLoss:0.2383 | top1:99.0833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5185 | MainLoss:0.5185 | SPLoss:0.0028 | CLSLoss:0.2370 | top1:82.1410 | AUROC:0.9046\n",
      "Test | 77/10 | Loss:0.2958 | MainLoss:0.2958 | SPLoss:0.0028 | CLSLoss:0.2370 | top1:89.4201 | AUROC:0.9608\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 10/10 | Loss:0.0259 | MainLoss:0.0141 | Alpha:0.0496 | SPLoss:0.0007 | CLSLoss:0.2367 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5145 | MainLoss:0.5145 | SPLoss:0.0015 | CLSLoss:0.2363 | top1:82.3333 | AUROC:0.9054\n",
      "Test | 77/10 | Loss:0.2930 | MainLoss:0.2930 | SPLoss:0.0015 | CLSLoss:0.2363 | top1:89.5315 | AUROC:0.9609\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 10/10 | Loss:0.0341 | MainLoss:0.0225 | Alpha:0.0494 | SPLoss:0.0011 | CLSLoss:0.2349 | top1:99.2778 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5199 | MainLoss:0.5199 | SPLoss:0.0025 | CLSLoss:0.2341 | top1:82.4359 | AUROC:0.9050\n",
      "Test | 77/10 | Loss:0.2963 | MainLoss:0.2963 | SPLoss:0.0025 | CLSLoss:0.2341 | top1:89.4692 | AUROC:0.9606\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 10/10 | Loss:0.0443 | MainLoss:0.0327 | Alpha:0.0492 | SPLoss:0.0021 | CLSLoss:0.2327 | top1:98.7222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5196 | MainLoss:0.5196 | SPLoss:0.0044 | CLSLoss:0.2311 | top1:82.4103 | AUROC:0.9067\n",
      "Test | 77/10 | Loss:0.3012 | MainLoss:0.3012 | SPLoss:0.0044 | CLSLoss:0.2311 | top1:89.3218 | AUROC:0.9603\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 10/10 | Loss:0.0331 | MainLoss:0.0217 | Alpha:0.0495 | SPLoss:0.0009 | CLSLoss:0.2300 | top1:99.4445 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5161 | MainLoss:0.5161 | SPLoss:0.0022 | CLSLoss:0.2288 | top1:82.3590 | AUROC:0.9072\n",
      "Test | 77/10 | Loss:0.3009 | MainLoss:0.3009 | SPLoss:0.0022 | CLSLoss:0.2288 | top1:89.3087 | AUROC:0.9598\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 10/10 | Loss:0.0390 | MainLoss:0.0277 | Alpha:0.0493 | SPLoss:0.0016 | CLSLoss:0.2277 | top1:99.1667 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5143 | MainLoss:0.5143 | SPLoss:0.0032 | CLSLoss:0.2266 | top1:82.6026 | AUROC:0.9062\n",
      "Test | 77/10 | Loss:0.3001 | MainLoss:0.3001 | SPLoss:0.0032 | CLSLoss:0.2266 | top1:89.3283 | AUROC:0.9591\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 10/10 | Loss:0.0270 | MainLoss:0.0157 | Alpha:0.0496 | SPLoss:0.0010 | CLSLoss:0.2254 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5182 | MainLoss:0.5182 | SPLoss:0.0016 | CLSLoss:0.2251 | top1:82.6282 | AUROC:0.9060\n",
      "Test | 77/10 | Loss:0.3016 | MainLoss:0.3016 | SPLoss:0.0016 | CLSLoss:0.2251 | top1:89.3087 | AUROC:0.9593\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 10/10 | Loss:0.0250 | MainLoss:0.0138 | Alpha:0.0497 | SPLoss:0.0009 | CLSLoss:0.2244 | top1:99.6945 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5210 | MainLoss:0.5210 | SPLoss:0.0013 | CLSLoss:0.2239 | top1:82.5256 | AUROC:0.9064\n",
      "Test | 77/10 | Loss:0.3004 | MainLoss:0.3004 | SPLoss:0.0013 | CLSLoss:0.2239 | top1:89.4004 | AUROC:0.9594\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 10/10 | Loss:0.0325 | MainLoss:0.0214 | Alpha:0.0495 | SPLoss:0.0008 | CLSLoss:0.2230 | top1:99.4722 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5174 | MainLoss:0.5174 | SPLoss:0.0024 | CLSLoss:0.2214 | top1:82.5513 | AUROC:0.9060\n",
      "Test | 77/10 | Loss:0.2975 | MainLoss:0.2975 | SPLoss:0.0024 | CLSLoss:0.2214 | top1:89.4856 | AUROC:0.9594\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 10/10 | Loss:0.0290 | MainLoss:0.0180 | Alpha:0.0496 | SPLoss:0.0007 | CLSLoss:0.2206 | top1:99.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5221 | MainLoss:0.5221 | SPLoss:0.0021 | CLSLoss:0.2188 | top1:82.3718 | AUROC:0.9062\n",
      "Test | 77/10 | Loss:0.3008 | MainLoss:0.3008 | SPLoss:0.0021 | CLSLoss:0.2188 | top1:89.4725 | AUROC:0.9595\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 10/10 | Loss:0.0367 | MainLoss:0.0259 | Alpha:0.0494 | SPLoss:0.0024 | CLSLoss:0.2164 | top1:99.1389 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5295 | MainLoss:0.5295 | SPLoss:0.0034 | CLSLoss:0.2155 | top1:82.2436 | AUROC:0.9062\n",
      "Test | 77/10 | Loss:0.3075 | MainLoss:0.3075 | SPLoss:0.0034 | CLSLoss:0.2155 | top1:89.2431 | AUROC:0.9597\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 10/10 | Loss:0.0407 | MainLoss:0.0300 | Alpha:0.0493 | SPLoss:0.0016 | CLSLoss:0.2146 | top1:98.8889 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5319 | MainLoss:0.5319 | SPLoss:0.0040 | CLSLoss:0.2139 | top1:82.2051 | AUROC:0.9063\n",
      "Test | 77/10 | Loss:0.3086 | MainLoss:0.3086 | SPLoss:0.0040 | CLSLoss:0.2139 | top1:89.2661 | AUROC:0.9597\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 10/10 | Loss:0.0270 | MainLoss:0.0164 | Alpha:0.0496 | SPLoss:0.0013 | CLSLoss:0.2130 | top1:99.5000 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5343 | MainLoss:0.5343 | SPLoss:0.0024 | CLSLoss:0.2122 | top1:82.1667 | AUROC:0.9063\n",
      "Test | 77/10 | Loss:0.3106 | MainLoss:0.3106 | SPLoss:0.0024 | CLSLoss:0.2122 | top1:89.2431 | AUROC:0.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n",
      "Train | 10/10 | Loss:0.0261 | MainLoss:0.0155 | Alpha:0.0496 | SPLoss:0.0013 | CLSLoss:0.2113 | top1:99.4722 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5290 | MainLoss:0.5290 | SPLoss:0.0022 | CLSLoss:0.2106 | top1:82.3077 | AUROC:0.9070\n",
      "Test | 77/10 | Loss:0.3087 | MainLoss:0.3087 | SPLoss:0.0022 | CLSLoss:0.2106 | top1:89.2988 | AUROC:0.9593\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 10/10 | Loss:0.0237 | MainLoss:0.0133 | Alpha:0.0497 | SPLoss:0.0003 | CLSLoss:0.2100 | top1:99.5833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5251 | MainLoss:0.5251 | SPLoss:0.0014 | CLSLoss:0.2092 | top1:82.5256 | AUROC:0.9062\n",
      "Test | 77/10 | Loss:0.3014 | MainLoss:0.3014 | SPLoss:0.0014 | CLSLoss:0.2092 | top1:89.5642 | AUROC:0.9598\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 10/10 | Loss:0.0303 | MainLoss:0.0199 | Alpha:0.0495 | SPLoss:0.0015 | CLSLoss:0.2082 | top1:99.3333 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5347 | MainLoss:0.5347 | SPLoss:0.0024 | CLSLoss:0.2073 | top1:82.2949 | AUROC:0.9061\n",
      "Test | 77/10 | Loss:0.3107 | MainLoss:0.3107 | SPLoss:0.0024 | CLSLoss:0.2073 | top1:89.3152 | AUROC:0.9590\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 10/10 | Loss:0.0268 | MainLoss:0.0165 | Alpha:0.0496 | SPLoss:0.0009 | CLSLoss:0.2067 | top1:99.5278 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5344 | MainLoss:0.5344 | SPLoss:0.0024 | CLSLoss:0.2054 | top1:82.3590 | AUROC:0.9075\n",
      "Test | 77/10 | Loss:0.3132 | MainLoss:0.3132 | SPLoss:0.0024 | CLSLoss:0.2054 | top1:89.2693 | AUROC:0.9587\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n",
      "Train | 10/10 | Loss:0.0282 | MainLoss:0.0180 | Alpha:0.0496 | SPLoss:0.0015 | CLSLoss:0.2043 | top1:99.4167 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5359 | MainLoss:0.5359 | SPLoss:0.0030 | CLSLoss:0.2030 | top1:82.2564 | AUROC:0.9076\n",
      "Test | 77/10 | Loss:0.3167 | MainLoss:0.3167 | SPLoss:0.0030 | CLSLoss:0.2030 | top1:89.1448 | AUROC:0.9584\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 10/10 | Loss:0.0247 | MainLoss:0.0147 | Alpha:0.0496 | SPLoss:0.0009 | CLSLoss:0.2018 | top1:99.6389 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5392 | MainLoss:0.5392 | SPLoss:0.0018 | CLSLoss:0.2006 | top1:82.2308 | AUROC:0.9077\n",
      "Test | 77/10 | Loss:0.3189 | MainLoss:0.3189 | SPLoss:0.0018 | CLSLoss:0.2006 | top1:89.0826 | AUROC:0.9580\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 10/10 | Loss:0.0212 | MainLoss:0.0113 | Alpha:0.0497 | SPLoss:0.0010 | CLSLoss:0.1996 | top1:99.6945 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5333 | MainLoss:0.5333 | SPLoss:0.0015 | CLSLoss:0.1991 | top1:82.3974 | AUROC:0.9078\n",
      "Test | 77/10 | Loss:0.3117 | MainLoss:0.3117 | SPLoss:0.0015 | CLSLoss:0.1991 | top1:89.3054 | AUROC:0.9583\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.039308\n",
      "Train | 10/10 | Loss:0.0205 | MainLoss:0.0107 | Alpha:0.0497 | SPLoss:0.0002 | CLSLoss:0.1986 | top1:99.7222 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5384 | MainLoss:0.5384 | SPLoss:0.0012 | CLSLoss:0.1976 | top1:82.4872 | AUROC:0.9075\n",
      "Test | 77/10 | Loss:0.3147 | MainLoss:0.3147 | SPLoss:0.0012 | CLSLoss:0.1976 | top1:89.2300 | AUROC:0.9578\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.039291\n",
      "Train | 10/10 | Loss:0.0269 | MainLoss:0.0172 | Alpha:0.0496 | SPLoss:0.0005 | CLSLoss:0.1967 | top1:99.5833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5368 | MainLoss:0.5368 | SPLoss:0.0020 | CLSLoss:0.1945 | top1:82.4487 | AUROC:0.9071\n",
      "Test | 77/10 | Loss:0.3144 | MainLoss:0.3144 | SPLoss:0.0020 | CLSLoss:0.1945 | top1:89.2235 | AUROC:0.9577\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.039274\n",
      "Train | 10/10 | Loss:0.0304 | MainLoss:0.0208 | Alpha:0.0495 | SPLoss:0.0017 | CLSLoss:0.1924 | top1:99.3056 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5290 | MainLoss:0.5290 | SPLoss:0.0035 | CLSLoss:0.1915 | top1:82.7949 | AUROC:0.9068\n",
      "Test | 77/10 | Loss:0.3052 | MainLoss:0.3052 | SPLoss:0.0035 | CLSLoss:0.1915 | top1:89.4299 | AUROC:0.9585\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.039258\n",
      "Train | 10/10 | Loss:0.0344 | MainLoss:0.0250 | Alpha:0.0494 | SPLoss:0.0010 | CLSLoss:0.1900 | top1:99.0833 | AUROC:0.0000\n",
      "Test | 20/10 | Loss:0.5225 | MainLoss:0.5225 | SPLoss:0.0033 | CLSLoss:0.1877 | top1:82.6667 | AUROC:0.9085\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, sou