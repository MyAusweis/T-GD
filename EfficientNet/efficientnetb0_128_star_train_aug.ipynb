{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = './log/star/128/b0/aug/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 400\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.04\n",
    "schedule = [75, 175, 250]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/star/128/b0/aug' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.2\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.2\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.', 'Train AUROC.', 'Valid AUROC.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        arc.update(auroc, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [159 | 400] LR: 0.022469\n",
      "1/1373 | Loss:0.0743 | top1:98.5000 | AUROC:0.9965\n",
      "101/1373 | Loss:0.1065 | top1:95.5495 | AUROC:0.9955\n",
      "201/1373 | Loss:0.1037 | top1:95.7587 | AUROC:0.9954\n",
      "301/1373 | Loss:0.0994 | top1:95.9634 | AUROC:0.9955\n",
      "401/1373 | Loss:0.1021 | top1:95.8554 | AUROC:0.9955\n",
      "501/1373 | Loss:0.1041 | top1:95.7605 | AUROC:0.9953\n",
      "601/1373 | Loss:0.1031 | top1:95.8278 | AUROC:0.9952\n",
      "701/1373 | Loss:0.1038 | top1:95.8031 | AUROC:0.9952\n",
      "801/1373 | Loss:0.1033 | top1:95.8302 | AUROC:0.9951\n",
      "901/1373 | Loss:0.1030 | top1:95.8430 | AUROC:0.9952\n",
      "1001/1373 | Loss:0.1032 | top1:95.8367 | AUROC:0.9951\n",
      "1101/1373 | Loss:0.1037 | top1:95.8088 | AUROC:0.9951\n",
      "1201/1373 | Loss:0.1044 | top1:95.7914 | AUROC:0.9951\n",
      "1301/1373 | Loss:0.1037 | top1:95.8305 | AUROC:0.9951\n",
      "1373/1373 | Loss:0.1042 | top1:95.8185 | AUROC:0.9951\n",
      "153/153 | Loss:0.0684 | top1:97.3984 | AUROC:0.9985\n",
      "\n",
      "Epoch: [160 | 400] LR: 0.006800\n",
      "1/1373 | Loss:0.0959 | top1:95.5000 | AUROC:0.9993\n",
      "101/1373 | Loss:0.0859 | top1:96.4802 | AUROC:0.9960\n",
      "201/1373 | Loss:0.0808 | top1:96.7811 | AUROC:0.9964\n",
      "301/1373 | Loss:0.0792 | top1:96.8671 | AUROC:0.9964\n",
      "401/1373 | Loss:0.0780 | top1:96.9551 | AUROC:0.9965\n",
      "501/1373 | Loss:0.0770 | top1:96.9950 | AUROC:0.9966\n",
      "601/1373 | Loss:0.0773 | top1:96.9576 | AUROC:0.9967\n",
      "701/1373 | Loss:0.0765 | top1:96.9850 | AUROC:0.9967\n",
      "801/1373 | Loss:0.0765 | top1:96.9744 | AUROC:0.9967\n",
      "901/1373 | Loss:0.0761 | top1:96.9867 | AUROC:0.9968\n",
      "1001/1373 | Loss:0.0759 | top1:96.9960 | AUROC:0.9967\n",
      "1101/1373 | Loss:0.0759 | top1:96.9905 | AUROC:0.9968\n",
      "1201/1373 | Loss:0.0756 | top1:96.9979 | AUROC:0.9968\n",
      "1301/1373 | Loss:0.0757 | top1:96.9946 | AUROC:0.9968\n",
      "1373/1373 | Loss:0.0758 | top1:96.9934 | AUROC:0.9968\n",
      "153/153 | Loss:0.0309 | top1:98.8598 | AUROC:0.9994\n",
      "\n",
      "Epoch: [161 | 400] LR: 0.009600\n",
      "1/1373 | Loss:0.0646 | top1:97.5000 | AUROC:0.9979\n",
      "101/1373 | Loss:0.0774 | top1:96.9901 | AUROC:0.9967\n",
      "201/1373 | Loss:0.0748 | top1:97.0547 | AUROC:0.9970\n",
      "301/1373 | Loss:0.0747 | top1:97.0615 | AUROC:0.9969\n",
      "401/1373 | Loss:0.0753 | top1:97.0224 | AUROC:0.9969\n",
      "501/1373 | Loss:0.0748 | top1:97.0220 | AUROC:0.9969\n",
      "601/1373 | Loss:0.0744 | top1:97.0283 | AUROC:0.9969\n",
      "701/1373 | Loss:0.0745 | top1:97.0285 | AUROC:0.9969\n",
      "801/1373 | Loss:0.0744 | top1:97.0300 | AUROC:0.9970\n",
      "901/1373 | Loss:0.0746 | top1:97.0327 | AUROC:0.9970\n",
      "1001/1373 | Loss:0.0748 | top1:97.0210 | AUROC:0.9970\n",
      "1101/1373 | Loss:0.0748 | top1:97.0191 | AUROC:0.9970\n",
      "1201/1373 | Loss:0.0748 | top1:97.0229 | AUROC:0.9970\n",
      "1301/1373 | Loss:0.0751 | top1:97.0050 | AUROC:0.9970\n",
      "1373/1373 | Loss:0.0751 | top1:97.0062 | AUROC:0.9970\n",
      "153/153 | Loss:0.0299 | top1:98.8893 | AUROC:0.9994\n",
      "\n",
      "Epoch: [162 | 400] LR: 0.012400\n",
      "1/1373 | Loss:0.0523 | top1:97.5000 | AUROC:0.9986\n",
      "101/1373 | Loss:0.0737 | top1:97.0396 | AUROC:0.9972\n",
      "201/1373 | Loss:0.0767 | top1:96.9627 | AUROC:0.9970\n",
      "301/1373 | Loss:0.0782 | top1:96.9120 | AUROC:0.9969\n",
      "401/1373 | Loss:0.0793 | top1:96.8766 | AUROC:0.9968\n",
      "501/1373 | Loss:0.0794 | top1:96.8852 | AUROC:0.9968\n",
      "601/1373 | Loss:0.0788 | top1:96.9185 | AUROC:0.9967\n",
      "701/1373 | Loss:0.0782 | top1:96.9565 | AUROC:0.9968\n",
      "801/1373 | Loss:0.0791 | top1:96.9126 | AUROC:0.9967\n",
      "901/1373 | Loss:0.0792 | top1:96.8946 | AUROC:0.9967\n",
      "1001/1373 | Loss:0.0793 | top1:96.8916 | AUROC:0.9967\n",
      "1101/1373 | Loss:0.0799 | top1:96.8692 | AUROC:0.9967\n",
      "1201/1373 | Loss:0.0796 | top1:96.8747 | AUROC:0.9967\n",
      "1301/1373 | Loss:0.0797 | top1:96.8663 | AUROC:0.9967\n",
      "1373/1373 | Loss:0.0795 | top1:96.8732 | AUROC:0.9967\n",
      "153/153 | Loss:0.0387 | top1:98.5518 | AUROC:0.9992\n",
      "\n",
      "Epoch: [163 | 400] LR: 0.015200\n",
      "1/1373 | Loss:0.1338 | top1:95.0000 | AUROC:0.9913\n",
      "101/1373 | Loss:0.0813 | top1:96.7673 | AUROC:0.9964\n",
      "201/1373 | Loss:0.0817 | top1:96.7736 | AUROC:0.9964\n",
      "301/1373 | Loss:0.0788 | top1:96.8920 | AUROC:0.9967\n",
      "401/1373 | Loss:0.0824 | top1:96.7357 | AUROC:0.9966\n",
      "501/1373 | Loss:0.0824 | top1:96.7345 | AUROC:0.9965\n",
      "601/1373 | Loss:0.0832 | top1:96.7213 | AUROC:0.9964\n",
      "701/1373 | Loss:0.0831 | top1:96.7411 | AUROC:0.9964\n",
      "801/1373 | Loss:0.0835 | top1:96.7310 | AUROC:0.9964\n",
      "901/1373 | Loss:0.0842 | top1:96.6909 | AUROC:0.9964\n",
      "1001/1373 | Loss:0.0849 | top1:96.6538 | AUROC:0.9964\n",
      "1101/1373 | Loss:0.0851 | top1:96.6412 | AUROC:0.9964\n",
      "1201/1373 | Loss:0.0852 | top1:96.6374 | AUROC:0.9963\n",
      "1301/1373 | Loss:0.0851 | top1:96.6311 | AUROC:0.9963\n",
      "1373/1373 | Loss:0.0851 | top1:96.6268 | AUROC:0.9963\n",
      "153/153 | Loss:0.0366 | top1:98.7156 | AUROC:0.9991\n",
      "\n",
      "Epoch: [164 | 400] LR: 0.018000\n",
      "1/1373 | Loss:0.0754 | top1:97.0000 | AUROC:0.9968\n",
      "101/1373 | Loss:0.1001 | top1:95.9802 | AUROC:0.9958\n",
      "201/1373 | Loss:0.0908 | top1:96.3532 | AUROC:0.9961\n",
      "301/1373 | Loss:0.0895 | top1:96.4036 | AUROC:0.9961\n",
      "401/1373 | Loss:0.0932 | top1:96.2319 | AUROC:0.9960\n",
      "501/1373 | Loss:0.0931 | top1:96.2385 | AUROC:0.9960\n",
      "601/1373 | Loss:0.0930 | top1:96.2820 | AUROC:0.9960\n",
      "701/1373 | Loss:0.0926 | top1:96.3131 | AUROC:0.9960\n",
      "801/1373 | Loss:0.0925 | top1:96.3240 | AUROC:0.9959\n",
      "901/1373 | Loss:0.0931 | top1:96.3196 | AUROC:0.9959\n",
      "1001/1373 | Loss:0.0933 | top1:96.3002 | AUROC:0.9959\n",
      "1101/1373 | Loss:0.0938 | top1:96.2838 | AUROC:0.9959\n",
      "1201/1373 | Loss:0.0935 | top1:96.3031 | AUROC:0.9959\n",
      "1301/1373 | Loss:0.0934 | top1:96.3052 | AUROC:0.9959\n",
      "1373/1373 | Loss:0.0936 | top1:96.3014 | AUROC:0.9958\n",
      "153/153 | Loss:0.0376 | top1:98.6304 | AUROC:0.9990\n",
      "\n",
      "Epoch: [165 | 400] LR: 0.020800\n",
      "1/1373 | Loss:0.0512 | top1:97.5000 | AUROC:0.9985\n",
      "101/1373 | Loss:0.0929 | top1:96.2970 | AUROC:0.9951\n",
      "201/1373 | Loss:0.0924 | top1:96.3134 | AUROC:0.9954\n",
      "301/1373 | Loss:0.0954 | top1:96.1645 | AUROC:0.9953\n",
      "401/1373 | Loss:0.0948 | top1:96.2070 | AUROC:0.9955\n",
      "501/1373 | Loss:0.0963 | top1:96.1347 | AUROC:0.9954\n",
      "601/1373 | Loss:0.0966 | top1:96.1198 | AUROC:0.9955\n",
      "701/1373 | Loss:0.0968 | top1:96.1006 | AUROC:0.9955\n",
      "801/1373 | Loss:0.0961 | top1:96.1236 | AUROC:0.9955\n",
      "901/1373 | Loss:0.0964 | top1:96.1204 | AUROC:0.9955\n",
      "1001/1373 | Loss:0.0964 | top1:96.1164 | AUROC:0.9955\n",
      "1101/1373 | Loss:0.0963 | top1:96.1181 | AUROC:0.9956\n",
      "1201/1373 | Loss:0.0964 | top1:96.1220 | AUROC:0.9956\n",
      "1301/1373 | Loss:0.0962 | top1:96.1303 | AUROC:0.9956\n",
      "1373/1373 | Loss:0.0957 | top1:96.1443 | AUROC:0.9956\n",
      "153/153 | Loss:0.0425 | top1:98.3486 | AUROC:0.9989\n",
      "\n",
      "Epoch: [166 | 400] LR: 0.023600\n",
      "1/1373 | Loss:0.1102 | top1:94.5000 | AUROC:0.9944\n",
      "101/1373 | Loss:0.1026 | top1:96.0050 | AUROC:0.9950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1594583eaac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: [%d | %d] LR: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-061456a0ae46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, use_cuda)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# compute gradient and do SGD step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc, train_auroc, test_auroc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
