{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b1' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 756\n",
    "test_batch = 500\n",
    "lr = 0.04\n",
    "schedule = [75, 150, 225]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/64/b1' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 4\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (64, 64)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 6.52M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=(3,64,64), device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 300] LR: 0.040000\n",
      "1/170 Data:1.570 | Batch:5.456 | Total:0:00:05 | ETA:0:15:23 | Loss:0.7146677374839783 | top1:48.94179916381836\n",
      "11/170 Data:0.001 | Batch:0.505 | Total:0:00:10 | ETA:0:02:42 | Loss:1.4604379317977212 | top1:49.49494934082031\n",
      "21/170 Data:0.002 | Batch:0.517 | Total:0:00:15 | ETA:0:01:17 | Loss:1.3155791220210848 | top1:50.35273742675781\n",
      "31/170 Data:0.001 | Batch:0.510 | Total:0:00:20 | ETA:0:01:12 | Loss:1.1421904198585018 | top1:50.166412353515625\n",
      "41/170 Data:0.001 | Batch:0.506 | Total:0:00:26 | ETA:0:01:07 | Loss:1.053890917359329 | top1:49.90966796875\n",
      "51/170 Data:0.001 | Batch:0.509 | Total:0:00:31 | ETA:0:01:01 | Loss:0.9852287488825181 | top1:49.95850372314453\n",
      "61/170 Data:0.001 | Batch:0.512 | Total:0:00:36 | ETA:0:00:56 | Loss:0.9385558767396895 | top1:49.91326141357422\n",
      "71/170 Data:0.001 | Batch:0.524 | Total:0:00:41 | ETA:0:00:53 | Loss:0.9050314837778118 | top1:49.919891357421875\n",
      "81/170 Data:0.001 | Batch:0.526 | Total:0:00:46 | ETA:0:00:47 | Loss:0.8799595921127884 | top1:49.87752151489258\n",
      "91/170 Data:0.001 | Batch:0.517 | Total:0:00:52 | ETA:0:00:42 | Loss:0.8597677258344797 | top1:49.91423797607422\n",
      "101/170 Data:0.001 | Batch:0.517 | Total:0:00:57 | ETA:0:00:37 | Loss:0.8433982057146506 | top1:49.950233459472656\n",
      "111/170 Data:0.001 | Batch:0.511 | Total:0:01:02 | ETA:0:00:32 | Loss:0.8303749620377481 | top1:49.94518280029297\n",
      "121/170 Data:0.001 | Batch:0.522 | Total:0:01:07 | ETA:0:00:26 | Loss:0.8194227583152204 | top1:49.89177322387695\n",
      "131/170 Data:0.001 | Batch:0.511 | Total:0:01:13 | ETA:0:00:21 | Loss:0.8104529535497418 | top1:49.88993835449219\n",
      "141/170 Data:0.001 | Batch:0.522 | Total:0:01:18 | ETA:0:00:16 | Loss:0.8024089649213967 | top1:49.98030090332031\n",
      "151/170 Data:0.001 | Batch:0.521 | Total:0:01:23 | ETA:0:00:10 | Loss:0.7954499429424867 | top1:50.04905700683594\n",
      "161/170 Data:0.001 | Batch:0.530 | Total:0:01:28 | ETA:0:00:05 | Loss:0.7895748045133508 | top1:50.051761627197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 Data:0.000 | Batch:0.343 | Total:0:00:12 | ETA:0:00:00 | Loss:0.6988236948708507 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [2 | 300] LR: 0.068000\n",
      "1/170 Data:1.980 | Batch:2.543 | Total:0:00:02 | ETA:0:07:10 | Loss:0.6981512904167175 | top1:50.925926208496094\n",
      "11/170 Data:0.001 | Batch:0.534 | Total:0:00:07 | ETA:0:01:57 | Loss:1.1397061944007874 | top1:49.4348258972168\n",
      "21/170 Data:0.001 | Batch:0.523 | Total:0:00:13 | ETA:0:01:21 | Loss:1.055970311164856 | top1:49.88032531738281\n",
      "31/170 Data:0.001 | Batch:0.512 | Total:0:00:18 | ETA:0:01:14 | Loss:1.011167566622457 | top1:49.735450744628906\n",
      "41/170 Data:0.001 | Batch:0.534 | Total:0:00:23 | ETA:0:01:08 | Loss:0.9467596920525155 | top1:49.87095260620117\n",
      "51/170 Data:0.001 | Batch:0.526 | Total:0:00:29 | ETA:0:01:05 | Loss:0.8988683504216811 | top1:49.815853118896484\n",
      "61/170 Data:0.001 | Batch:0.517 | Total:0:00:34 | ETA:0:01:00 | Loss:0.8696197144320754 | top1:49.770145416259766\n",
      "71/170 Data:0.001 | Batch:0.524 | Total:0:00:39 | ETA:0:00:51 | Loss:0.8457907010132159 | top1:49.82674026489258\n",
      "81/170 Data:0.003 | Batch:0.523 | Total:0:00:44 | ETA:0:00:47 | Loss:0.8286492390397154 | top1:49.83832931518555\n",
      "91/170 Data:0.001 | Batch:0.516 | Total:0:00:50 | ETA:0:00:42 | Loss:0.8160304165148473 | top1:49.87935256958008\n",
      "101/170 Data:0.001 | Batch:0.508 | Total:0:00:55 | ETA:0:00:36 | Loss:0.8044355948372642 | top1:49.96333312988281\n",
      "111/170 Data:0.001 | Batch:0.516 | Total:0:01:00 | ETA:0:00:31 | Loss:0.7956246948456979 | top1:49.951141357421875\n",
      "121/170 Data:0.001 | Batch:0.508 | Total:0:01:05 | ETA:0:00:26 | Loss:0.7875716326650509 | top1:49.96501541137695\n",
      "131/170 Data:0.001 | Batch:0.508 | Total:0:01:10 | ETA:0:00:20 | Loss:0.7805507542522809 | top1:50.00605773925781\n",
      "141/170 Data:0.001 | Batch:0.520 | Total:0:01:16 | ETA:0:00:16 | Loss:0.7747981560991165 | top1:50.058162689208984\n",
      "151/170 Data:0.001 | Batch:0.534 | Total:0:01:21 | ETA:0:00:10 | Loss:0.7701030128839 | top1:50.05168533325195\n",
      "161/170 Data:0.001 | Batch:0.518 | Total:0:01:26 | ETA:0:00:05 | Loss:0.7654896641369933 | top1:50.03697204589844\n",
      "65/65 Data:0.000 | Batch:0.046 | Total:0:00:15 | ETA:0:00:00 | Loss:0.7085633068069862 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [3 | 300] LR: 0.096000\n",
      "1/170 Data:1.656 | Batch:2.215 | Total:0:00:02 | ETA:0:06:15 | Loss:0.712694525718689 | top1:48.67724609375\n",
      "11/170 Data:0.001 | Batch:0.510 | Total:0:00:07 | ETA:0:01:53 | Loss:0.9544897512956099 | top1:49.074073791503906\n",
      "21/170 Data:0.011 | Batch:0.543 | Total:0:00:12 | ETA:0:01:18 | Loss:0.8474914601870945 | top1:49.79214096069336\n",
      "31/170 Data:0.001 | Batch:0.520 | Total:0:00:18 | ETA:0:01:14 | Loss:0.8090474297923427 | top1:49.79092025756836\n",
      "41/170 Data:0.001 | Batch:0.507 | Total:0:00:23 | ETA:0:01:08 | Loss:0.7812338340573195 | top1:49.98064422607422\n",
      "51/170 Data:0.001 | Batch:0.540 | Total:0:00:28 | ETA:0:01:02 | Loss:0.7669765446700302 | top1:49.800289154052734\n",
      "61/170 Data:0.001 | Batch:0.514 | Total:0:00:33 | ETA:0:00:58 | Loss:0.7552470525757211 | top1:49.94361877441406\n",
      "71/170 Data:0.001 | Batch:0.529 | Total:0:00:38 | ETA:0:00:52 | Loss:0.7495581835088595 | top1:49.795066833496094\n",
      "81/170 Data:0.009 | Batch:0.518 | Total:0:00:44 | ETA:0:00:49 | Loss:0.7428767401495098 | top1:49.87588882446289\n",
      "91/170 Data:0.001 | Batch:0.530 | Total:0:00:49 | ETA:0:00:42 | Loss:0.7381111632336627 | top1:49.93022918701172\n",
      "101/170 Data:0.010 | Batch:0.547 | Total:0:00:55 | ETA:0:00:38 | Loss:0.733961301274819 | top1:50.020957946777344\n",
      "111/170 Data:0.001 | Batch:0.526 | Total:0:01:00 | ETA:0:00:31 | Loss:0.7310181990400091 | top1:50.01311111450195\n",
      "121/170 Data:0.002 | Batch:0.511 | Total:0:01:05 | ETA:0:00:27 | Loss:0.7281218900168238 | top1:50.048099517822266\n",
      "131/170 Data:0.001 | Batch:0.510 | Total:0:01:10 | ETA:0:00:21 | Loss:0.7261123684526399 | top1:50.05149841308594\n",
      "141/170 Data:0.001 | Batch:0.509 | Total:0:01:16 | ETA:0:00:16 | Loss:0.7246325612068176 | top1:50.07035827636719\n",
      "151/170 Data:0.002 | Batch:0.538 | Total:0:01:21 | ETA:0:00:11 | Loss:0.7229572929293904 | top1:50.07884216308594\n",
      "161/170 Data:0.001 | Batch:0.525 | Total:0:01:26 | ETA:0:00:05 | Loss:0.7216157835462819 | top1:50.11009216308594\n",
      "65/65 Data:0.000 | Batch:0.049 | Total:0:00:13 | ETA:0:00:00 | Loss:0.6931747428353331 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [4 | 300] LR: 0.124000\n",
      "1/170 Data:1.161 | Batch:1.673 | Total:0:00:01 | ETA:0:04:43 | Loss:0.6946496963500977 | top1:50.52909851074219\n",
      "11/170 Data:0.001 | Batch:0.517 | Total:0:00:06 | ETA:0:01:43 | Loss:0.7381540916182778 | top1:50.50505065917969\n",
      "21/170 Data:0.005 | Batch:0.515 | Total:0:00:12 | ETA:0:01:18 | Loss:0.7173620121819633 | top1:50.81254959106445\n",
      "31/170 Data:0.001 | Batch:0.610 | Total:0:00:17 | ETA:0:01:19 | Loss:0.7103298498738196 | top1:50.96432876586914\n",
      "41/170 Data:0.001 | Batch:0.521 | Total:0:00:23 | ETA:0:01:09 | Loss:0.7119200738464914 | top1:50.54523468017578\n",
      "51/170 Data:0.001 | Batch:0.520 | Total:0:00:28 | ETA:0:01:02 | Loss:0.7131035912270639 | top1:50.264549255371094\n",
      "61/170 Data:0.001 | Batch:0.544 | Total:0:00:33 | ETA:0:00:57 | Loss:0.7101056282637549 | top1:50.3296012878418\n",
      "71/170 Data:0.001 | Batch:0.511 | Total:0:00:38 | ETA:0:00:53 | Loss:0.7079620638363798 | top1:50.348388671875\n",
      "81/170 Data:0.001 | Batch:0.532 | Total:0:00:43 | ETA:0:00:47 | Loss:0.7076516210297008 | top1:50.31680679321289\n",
      "91/170 Data:0.001 | Batch:0.520 | Total:0:00:49 | ETA:0:00:42 | Loss:0.7066849496338393 | top1:50.302345275878906\n",
      "101/170 Data:0.001 | Batch:0.512 | Total:0:00:54 | ETA:0:00:37 | Loss:0.7062854743239904 | top1:50.20037841796875\n",
      "111/170 Data:0.001 | Batch:0.557 | Total:0:00:59 | ETA:0:00:32 | Loss:0.7053433807046564 | top1:50.25621032714844\n",
      "121/170 Data:0.001 | Batch:0.509 | Total:0:01:05 | ETA:0:00:27 | Loss:0.7044856410381223 | top1:50.29515838623047\n",
      "131/170 Data:0.001 | Batch:0.530 | Total:0:01:10 | ETA:0:00:22 | Loss:0.7038234526874455 | top1:50.291812896728516\n",
      "141/170 Data:0.001 | Batch:0.512 | Total:0:01:16 | ETA:0:00:16 | Loss:0.7031812938392585 | top1:50.33490753173828\n",
      "151/170 Data:0.001 | Batch:0.523 | Total:0:01:21 | ETA:0:00:11 | Loss:0.7027606518063324 | top1:50.49668884277344\n",
      "161/170 Data:0.001 | Batch:0.512 | Total:0:01:26 | ETA:0:00:05 | Loss:0.7026842258731771 | top1:50.555389404296875\n",
      "65/65 Data:0.000 | Batch:0.079 | Total:0:00:11 | ETA:0:00:00 | Loss:0.6736322552244239 | top1:63.429908752441406\n",
      "\n",
      "Epoch: [5 | 300] LR: 0.152000\n",
      "1/170 Data:1.806 | Batch:2.381 | Total:0:00:02 | ETA:0:06:43 | Loss:0.6837247014045715 | top1:57.67195510864258\n",
      "11/170 Data:0.001 | Batch:0.522 | Total:0:00:07 | ETA:0:01:54 | Loss:0.7286650104956194 | top1:51.92400360107422\n",
      "21/170 Data:0.008 | Batch:0.569 | Total:0:00:12 | ETA:0:01:19 | Loss:0.7191666137604487 | top1:50.76845932006836\n",
      "31/170 Data:0.001 | Batch:0.516 | Total:0:00:18 | ETA:0:01:13 | Loss:0.7132720678083359 | top1:50.11094284057617\n",
      "41/170 Data:0.001 | Batch:0.511 | Total:0:00:23 | ETA:0:01:08 | Loss:0.7095911779054781 | top1:50.00645446777344\n",
      "51/170 Data:0.001 | Batch:0.530 | Total:0:00:28 | ETA:0:01:03 | Loss:0.7072433373507332 | top1:50.10633850097656\n",
      "61/170 Data:0.001 | Batch:0.519 | Total:0:00:33 | ETA:0:00:57 | Loss:0.7054106905812123 | top1:50.21250534057617\n",
      "71/170 Data:0.001 | Batch:0.514 | Total:0:00:39 | ETA:0:00:53 | Loss:0.704150334210463 | top1:50.20866012573242\n",
      "81/170 Data:0.001 | Batch:0.568 | Total:0:00:44 | ETA:0:00:48 | Loss:0.7029906202245642 | top1:50.280879974365234\n",
      "91/170 Data:0.001 | Batch:0.557 | Total:0:00:49 | ETA:0:00:43 | Loss:0.7019663872299614 | top1:50.32705307006836\n",
      "101/170 Data:0.001 | Batch:0.574 | Total:0:00:55 | ETA:0:00:37 | Loss:0.7011139032864334 | top1:50.5788688659668\n",
      "111/170 Data:0.009 | Batch:0.519 | Total:0:01:00 | ETA:0:00:32 | Loss:0.7019285108592059 | top1:50.539825439453125\n",
      "121/170 Data:0.001 | Batch:0.569 | Total:0:01:05 | ETA:0:00:26 | Loss:0.7012136435705768 | top1:50.71384811401367\n",
      "131/170 Data:0.001 | Batch:0.515 | Total:0:01:10 | ETA:0:00:21 | Loss:0.700548775778472 | top1:50.97035598754883\n",
      "141/170 Data:0.001 | Batch:0.516 | Total:0:01:16 | ETA:0:00:16 | Loss:0.7021078336323406 | top1:50.95688247680664\n",
      "151/170 Data:0.001 | Batch:0.545 | Total:0:01:21 | ETA:0:00:10 | Loss:0.7026562931521839 | top1:50.84971618652344\n",
      "161/170 Data:0.001 | Batch:0.510 | Total:0:01:26 | ETA:0:00:05 | Loss:0.7023499852381878 | top1:50.826515197753906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 Data:0.000 | Batch:0.070 | Total:0:00:11 | ETA:0:00:00 | Loss:0.6939394388614786 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [6 | 300] LR: 0.180000\n",
      "1/170 Data:2.112 | Batch:2.694 | Total:0:00:02 | ETA:0:07:36 | Loss:0.6946014761924744 | top1:49.338623046875\n",
      "11/170 Data:0.021 | Batch:0.786 | Total:0:00:10 | ETA:0:02:34 | Loss:0.6963306231932207 | top1:49.98797607421875\n",
      "21/170 Data:0.001 | Batch:0.516 | Total:0:00:18 | ETA:0:02:03 | Loss:0.6956309335572379 | top1:50.176368713378906\n",
      "31/170 Data:0.001 | Batch:0.816 | Total:0:00:26 | ETA:0:01:44 | Loss:0.6951173620839273 | top1:50.17494583129883\n",
      "41/170 Data:0.001 | Batch:0.886 | Total:0:00:33 | ETA:0:01:39 | Loss:0.6950761879362711 | top1:50.23551559448242\n",
      "51/170 Data:0.002 | Batch:0.759 | Total:0:00:41 | ETA:0:01:28 | Loss:0.6950677750157375 | top1:50.176368713378906\n",
      "61/170 Data:0.001 | Batch:0.723 | Total:0:00:49 | ETA:0:01:30 | Loss:0.6954300892157633 | top1:50.02602005004883\n",
      "71/170 Data:0.006 | Batch:0.556 | Total:0:00:57 | ETA:0:01:22 | Loss:0.6956036292331319 | top1:49.990684509277344\n",
      "81/170 Data:0.001 | Batch:0.809 | Total:0:01:05 | ETA:0:01:11 | Loss:0.6957813574944014 | top1:49.9183464050293\n",
      "91/170 Data:0.002 | Batch:0.895 | Total:0:01:13 | ETA:0:01:05 | Loss:0.6958669076909075 | top1:49.88807678222656\n",
      "101/170 Data:0.002 | Batch:0.734 | Total:0:01:22 | ETA:0:00:59 | Loss:0.6960373497245336 | top1:49.834983825683594\n",
      "111/170 Data:0.001 | Batch:0.510 | Total:0:01:29 | ETA:0:00:43 | Loss:0.6961133716342686 | top1:49.78669357299805\n",
      "121/170 Data:0.001 | Batch:0.521 | Total:0:01:34 | ETA:0:00:26 | Loss:0.6959335927135688 | top1:49.869911193847656\n",
      "131/170 Data:0.001 | Batch:0.519 | Total:0:01:39 | ETA:0:00:21 | Loss:0.6958276724997368 | top1:49.98485565185547\n",
      "141/170 Data:0.004 | Batch:0.531 | Total:0:01:45 | ETA:0:00:16 | Loss:0.6960535201620548 | top1:49.939022064208984\n",
      "151/170 Data:0.001 | Batch:0.554 | Total:0:01:50 | ETA:0:00:11 | Loss:0.6958307106763322 | top1:50.05431365966797\n",
      "161/170 Data:0.001 | Batch:0.540 | Total:0:01:55 | ETA:0:00:05 | Loss:0.6957838283562512 | top1:50.13227462768555\n",
      "65/65 Data:0.000 | Batch:0.079 | Total:0:00:12 | ETA:0:00:00 | Loss:0.81234987595371 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [7 | 300] LR: 0.208000\n",
      "1/170 Data:1.661 | Batch:2.197 | Total:0:00:02 | ETA:0:06:12 | Loss:0.8010894060134888 | top1:51.719573974609375\n",
      "11/170 Data:0.011 | Batch:0.548 | Total:0:00:07 | ETA:0:01:52 | Loss:0.7246580069715326 | top1:50.38479995727539\n",
      "21/170 Data:0.001 | Batch:0.528 | Total:0:00:12 | ETA:0:01:20 | Loss:0.7223177694139027 | top1:50.60469055175781\n",
      "31/170 Data:0.009 | Batch:0.520 | Total:0:00:18 | ETA:0:01:13 | Loss:0.714142216790107 | top1:50.187747955322266\n",
      "41/170 Data:0.001 | Batch:0.516 | Total:0:00:23 | ETA:0:01:07 | Loss:0.7091697498065669 | top1:50.354888916015625\n",
      "51/170 Data:0.001 | Batch:0.524 | Total:0:00:28 | ETA:0:01:02 | Loss:0.7061989669706307 | top1:50.44091796875\n",
      "61/170 Data:0.001 | Batch:0.527 | Total:0:00:33 | ETA:0:00:57 | Loss:0.7048906449411736 | top1:50.29924392700195\n",
      "71/170 Data:0.001 | Batch:0.514 | Total:0:00:38 | ETA:0:00:52 | Loss:0.7033898175602228 | top1:50.355838775634766\n",
      "81/170 Data:0.001 | Batch:0.511 | Total:0:00:44 | ETA:0:00:47 | Loss:0.7023214646327642 | top1:50.41315460205078\n",
      "91/170 Data:0.010 | Batch:0.535 | Total:0:00:49 | ETA:0:00:41 | Loss:0.7016365678755792 | top1:50.255828857421875\n",
      "101/170 Data:0.001 | Batch:0.529 | Total:0:00:54 | ETA:0:00:37 | Loss:0.7008830480056234 | top1:50.182044982910156\n",
      "111/170 Data:0.012 | Batch:0.535 | Total:0:00:59 | ETA:0:00:31 | Loss:0.7002333853695843 | top1:50.23476028442383\n",
      "121/170 Data:0.001 | Batch:0.515 | Total:0:01:04 | ETA:0:00:26 | Loss:0.6996827770855801 | top1:50.23065948486328\n",
      "131/170 Data:0.001 | Batch:0.520 | Total:0:01:10 | ETA:0:00:21 | Loss:0.6991534692640523 | top1:50.339271545410156\n",
      "141/170 Data:0.001 | Batch:0.510 | Total:0:01:15 | ETA:0:00:16 | Loss:0.6987027851402337 | top1:50.39963912963867\n",
      "151/170 Data:0.001 | Batch:0.516 | Total:0:01:20 | ETA:0:00:10 | Loss:0.6985299788563457 | top1:50.51245880126953\n",
      "161/170 Data:0.008 | Batch:0.517 | Total:0:01:26 | ETA:0:00:05 | Loss:0.6984728900542171 | top1:50.65726852416992\n",
      "65/65 Data:0.000 | Batch:0.068 | Total:0:00:13 | ETA:0:00:00 | Loss:0.8600844425204387 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [8 | 300] LR: 0.236000\n",
      "1/170 Data:1.762 | Batch:2.322 | Total:0:00:02 | ETA:0:06:33 | Loss:0.8820810317993164 | top1:49.73544692993164\n",
      "11/170 Data:0.009 | Batch:0.529 | Total:0:00:07 | ETA:0:01:51 | Loss:0.724151535467668 | top1:48.18422317504883\n",
      "21/170 Data:0.001 | Batch:0.512 | Total:0:00:12 | ETA:0:01:20 | Loss:0.7235819356782096 | top1:48.85361862182617\n",
      "31/170 Data:0.001 | Batch:0.515 | Total:0:00:18 | ETA:0:01:14 | Loss:0.7147433469372411 | top1:49.03140640258789\n",
      "41/170 Data:0.001 | Batch:0.512 | Total:0:00:23 | ETA:0:01:09 | Loss:0.7099253913251365 | top1:49.30958938598633\n",
      "51/170 Data:0.001 | Batch:0.519 | Total:0:00:28 | ETA:0:01:04 | Loss:0.706700389291726 | top1:49.533145904541016\n",
      "61/170 Data:0.010 | Batch:0.576 | Total:0:00:34 | ETA:0:00:58 | Loss:0.7046675984976721 | top1:49.74845886230469\n",
      "71/170 Data:0.001 | Batch:0.525 | Total:0:00:39 | ETA:0:00:53 | Loss:0.70301173606389 | top1:49.95528793334961\n",
      "81/170 Data:0.001 | Batch:0.540 | Total:0:00:44 | ETA:0:00:47 | Loss:0.7018963782875626 | top1:50.11920928955078\n",
      "91/170 Data:0.001 | Batch:0.526 | Total:0:00:49 | ETA:0:00:42 | Loss:0.7009421154692933 | top1:50.3779296875\n",
      "101/170 Data:0.001 | Batch:0.514 | Total:0:00:55 | ETA:0:00:37 | Loss:0.7003577888602077 | top1:50.749122619628906\n",
      "111/170 Data:0.001 | Batch:0.526 | Total:0:01:00 | ETA:0:00:32 | Loss:0.7040960520237416 | top1:51.3882942199707\n",
      "121/170 Data:0.001 | Batch:0.515 | Total:0:01:05 | ETA:0:00:26 | Loss:0.736564934746293 | top1:51.31837844848633\n",
      "131/170 Data:0.001 | Batch:0.525 | Total:0:01:10 | ETA:0:00:21 | Loss:0.7411417110275677 | top1:51.19047546386719\n",
      "141/170 Data:0.001 | Batch:0.535 | Total:0:01:16 | ETA:0:00:16 | Loss:0.737859326896938 | top1:51.1257438659668\n",
      "151/170 Data:0.001 | Batch:0.513 | Total:0:01:21 | ETA:0:00:10 | Loss:0.7350140016599996 | top1:51.05732727050781\n",
      "161/170 Data:0.001 | Batch:0.519 | Total:0:01:26 | ETA:0:00:05 | Loss:0.7325647006864133 | top1:51.04341125488281\n",
      "65/65 Data:0.000 | Batch:0.050 | Total:0:00:10 | ETA:0:00:00 | Loss:0.692551942814919 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [9 | 300] LR: 0.264000\n",
      "1/170 Data:1.865 | Batch:2.419 | Total:0:00:02 | ETA:0:06:49 | Loss:0.6915267109870911 | top1:51.85184860229492\n",
      "11/170 Data:0.001 | Batch:0.527 | Total:0:00:07 | ETA:0:01:53 | Loss:0.6953743425282565 | top1:50.06012725830078\n",
      "21/170 Data:0.001 | Batch:0.514 | Total:0:00:12 | ETA:0:01:20 | Loss:0.6949068137577602 | top1:50.031497955322266\n",
      "31/170 Data:0.005 | Batch:0.590 | Total:0:00:18 | ETA:0:01:13 | Loss:0.694250108734254 | top1:50.285884857177734\n",
      "41/170 Data:0.001 | Batch:0.511 | Total:0:00:23 | ETA:0:01:08 | Loss:0.6933414005651707 | top1:50.9162483215332\n",
      "51/170 Data:0.009 | Batch:0.546 | Total:0:00:28 | ETA:0:01:02 | Loss:0.6936259117780947 | top1:51.4498405456543\n",
      "61/170 Data:0.001 | Batch:0.514 | Total:0:00:34 | ETA:0:01:01 | Loss:0.6925301551818848 | top1:52.027496337890625\n",
      "71/170 Data:0.001 | Batch:0.556 | Total:0:00:39 | ETA:0:00:53 | Loss:0.6912667020945482 | top1:52.82062911987305\n",
      "81/170 Data:0.001 | Batch:0.515 | Total:0:00:44 | ETA:0:00:48 | Loss:0.6942384655092969 | top1:52.380950927734375\n",
      "91/170 Data:0.001 | Batch:0.517 | Total:0:00:49 | ETA:0:00:41 | Loss:0.6941993629539406 | top1:52.06843566894531\n",
      "101/170 Data:0.001 | Batch:0.524 | Total:0:00:55 | ETA:0:00:36 | Loss:0.6942531023875321 | top1:51.88197708129883\n",
      "111/170 Data:0.001 | Batch:0.516 | Total:0:01:00 | ETA:0:00:31 | Loss:0.6942778718364131 | top1:51.645694732666016\n",
      "121/170 Data:0.001 | Batch:0.519 | Total:0:01:05 | ETA:0:00:26 | Loss:0.6941267355414461 | top1:51.63212203979492\n",
      "131/170 Data:0.001 | Batch:0.533 | Total:0:01:10 | ETA:0:00:21 | Loss:0.6938743978056289 | top1:51.75996780395508\n",
      "141/170 Data:0.001 | Batch:0.527 | Total:0:01:16 | ETA:0:00:16 | Loss:0.6937451286518828 | top1:51.82652282714844\n",
      "151/170 Data:0.001 | Batch:0.508 | Total:0:01:21 | ETA:0:00:10 | Loss:0.6935093754174694 | top1:51.78089904785156\n",
      "161/170 Data:0.001 | Batch:0.525 | Total:0:01:26 | ETA:0:00:05 | Loss:0.6931752273755044 | top1:51.940582275390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 Data:0.000 | Batch:0.062 | Total:0:00:12 | ETA:0:00:00 | Loss:0.6666027292655636 | top1:55.90031433105469\n",
      "\n",
      "Epoch: [10 | 300] LR: 0.292000\n",
      "1/170 Data:1.452 | Batch:2.020 | Total:0:00:02 | ETA:0:05:42 | Loss:0.6838404536247253 | top1:53.96825408935547\n",
      "11/170 Data:0.001 | Batch:0.519 | Total:0:00:07 | ETA:0:01:48 | Loss:0.6777945377609946 | top1:57.455509185791016\n",
      "21/170 Data:0.002 | Batch:0.532 | Total:0:00:12 | ETA:0:01:18 | Loss:0.6706057957240513 | top1:58.711265563964844\n",
      "31/170 Data:0.001 | Batch:0.510 | Total:0:00:17 | ETA:0:01:14 | Loss:0.670417224207232 | top1:59.66461944580078\n",
      "41/170 Data:0.001 | Batch:0.561 | Total:0:00:23 | ETA:0:01:08 | Loss:0.6784354462856199 | top1:57.155765533447266\n",
      "51/170 Data:0.001 | Batch:0.509 | Total:0:00:28 | ETA:0:01:04 | Loss:0.6815501790420682 | top1:55.84604263305664\n",
      "61/170 Data:0.001 | Batch:0.514 | Total:0:00:33 | ETA:0:00:58 | Loss:0.6844917633494393 | top1:54.82478713989258\n",
      "71/170 Data:0.001 | Batch:0.520 | Total:0:00:38 | ETA:0:00:52 | Loss:0.6858606800227098 | top1:54.21044921875\n",
      "81/170 Data:0.001 | Batch:0.530 | Total:0:00:44 | ETA:0:00:48 | Loss:0.6868812758245586 | top1:53.76249313354492\n",
      "91/170 Data:0.001 | Batch:0.544 | Total:0:00:49 | ETA:0:00:43 | Loss:0.6875817828126006 | top1:53.463863372802734\n",
      "101/170 Data:0.001 | Batch:0.603 | Total:0:00:54 | ETA:0:00:37 | Loss:0.6881385888203536 | top1:53.131385803222656\n",
      "111/170 Data:0.001 | Batch:0.529 | Total:0:01:00 | ETA:0:00:32 | Loss:0.6885612462017987 | top1:52.98274612426758\n",
      "121/170 Data:0.001 | Batch:0.510 | Total:0:01:05 | ETA:0:00:26 | Loss:0.6888281520733164 | top1:52.870697021484375\n",
      "131/170 Data:0.001 | Batch:0.515 | Total:0:01:10 | ETA:0:00:21 | Loss:0.6889625250838185 | top1:52.83937072753906\n",
      "141/170 Data:0.001 | Batch:0.516 | Total:0:01:15 | ETA:0:00:15 | Loss:0.6890885580516031 | top1:52.83687973022461\n",
      "151/170 Data:0.001 | Batch:0.509 | Total:0:01:21 | ETA:0:00:10 | Loss:0.6887046915805892 | top1:53.024810791015625\n",
      "161/170 Data:0.001 | Batch:0.515 | Total:0:01:26 | ETA:0:00:05 | Loss:0.690298550987836 | top1:52.875545501708984\n",
      "65/65 Data:0.000 | Batch:0.061 | Total:0:00:12 | ETA:0:00:00 | Loss:0.686628536643269 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [11 | 300] LR: 0.320000\n",
      "1/170 Data:1.443 | Batch:2.015 | Total:0:00:02 | ETA:0:05:41 | Loss:0.6922810077667236 | top1:48.80952072143555\n",
      "11/170 Data:0.001 | Batch:0.511 | Total:0:00:07 | ETA:0:01:48 | Loss:0.682824736291712 | top1:61.02693557739258\n",
      "21/170 Data:0.001 | Batch:0.536 | Total:0:00:12 | ETA:0:01:19 | Loss:0.7000656837508792 | top1:58.74905776977539\n",
      "31/170 Data:0.001 | Batch:0.546 | Total:0:00:18 | ETA:0:01:20 | Loss:0.6984218224402396 | top1:56.0291862487793\n",
      "41/170 Data:0.001 | Batch:0.538 | Total:0:00:23 | ETA:0:01:14 | Loss:0.6965363825239786 | top1:55.674930572509766\n",
      "51/170 Data:0.001 | Batch:0.531 | Total:0:00:29 | ETA:0:01:04 | Loss:0.6895578328300925 | top1:57.35034942626953\n",
      "61/170 Data:0.011 | Batch:0.537 | Total:0:00:34 | ETA:0:00:59 | Loss:0.7054305565161784 | top1:55.930694580078125\n",
      "71/170 Data:0.001 | Batch:0.510 | Total:0:00:39 | ETA:0:00:53 | Loss:0.7158416341727888 | top1:55.00968933105469\n",
      "81/170 Data:0.001 | Batch:0.525 | Total:0:00:45 | ETA:0:00:48 | Loss:0.7125776881053124 | top1:54.85662078857422\n",
      "91/170 Data:0.001 | Batch:0.519 | Total:0:00:50 | ETA:0:00:42 | Loss:0.7083311113682422 | top1:55.222686767578125\n",
      "101/170 Data:0.001 | Batch:0.544 | Total:0:00:55 | ETA:0:00:37 | Loss:0.7004889916665483 | top1:56.24443435668945\n",
      "111/170 Data:0.001 | Batch:0.546 | Total:0:01:01 | ETA:0:00:31 | Loss:0.6922889754578874 | top1:57.16907501220703\n",
      "121/170 Data:0.001 | Batch:0.511 | Total:0:01:06 | ETA:0:00:27 | Loss:0.684574741469927 | top1:58.04582595825195\n",
      "131/170 Data:0.001 | Batch:0.521 | Total:0:01:11 | ETA:0:00:21 | Loss:0.6762907454985698 | top1:58.90080261230469\n",
      "141/170 Data:0.001 | Batch:0.508 | Total:0:01:16 | ETA:0:00:15 | Loss:0.6691098686651136 | top1:59.701114654541016\n",
      "151/170 Data:0.001 | Batch:0.527 | Total:0:01:21 | ETA:0:00:10 | Loss:0.663500058729917 | top1:60.38053512573242\n",
      "161/170 Data:0.001 | Batch:0.512 | Total:0:01:27 | ETA:0:00:05 | Loss:0.6571116180893797 | top1:61.037166595458984\n",
      "65/65 Data:0.000 | Batch:0.063 | Total:0:00:13 | ETA:0:00:00 | Loss:0.4324659409924088 | top1:83.07476806640625\n",
      "\n",
      "Epoch: [12 | 300] LR: 0.320000\n",
      "1/170 Data:1.696 | Batch:2.295 | Total:0:00:02 | ETA:0:06:28 | Loss:0.5501129031181335 | top1:71.16402435302734\n",
      "11/170 Data:0.001 | Batch:0.513 | Total:0:00:07 | ETA:0:01:51 | Loss:0.5433666380968961 | top1:72.33045196533203\n",
      "21/170 Data:0.001 | Batch:0.509 | Total:0:00:12 | ETA:0:01:19 | Loss:0.5459742404165722 | top1:72.13404083251953\n",
      "31/170 Data:0.001 | Batch:0.514 | Total:0:00:17 | ETA:0:01:12 | Loss:0.5441654324531555 | top1:72.2478256225586\n",
      "41/170 Data:0.001 | Batch:0.514 | Total:0:00:23 | ETA:0:01:07 | Loss:0.5431447596084781 | top1:72.37385559082031\n",
      "51/170 Data:0.001 | Batch:0.582 | Total:0:00:28 | ETA:0:01:05 | Loss:0.5417031014666838 | top1:72.52568054199219\n",
      "61/170 Data:0.001 | Batch:0.535 | Total:0:00:33 | ETA:0:00:59 | Loss:0.5410382908875825 | top1:72.60820007324219\n",
      "71/170 Data:0.001 | Batch:0.516 | Total:0:00:39 | ETA:0:00:52 | Loss:0.5395661160139971 | top1:72.68052673339844\n",
      "81/170 Data:0.001 | Batch:0.512 | Total:0:00:44 | ETA:0:00:48 | Loss:0.5386377392727651 | top1:72.77255249023438\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
