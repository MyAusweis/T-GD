{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 128\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 110\n",
    "test_batch = 110\n",
    "lr = 0.04\n",
    "schedule = [75,150,225]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b1/siamese' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'\n",
    "\n",
    "# iterative training\n",
    "feedback = 0\n",
    "# iter_time = [1000, 2000, 2500]\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# constrastive\n",
    "thresh = 0.5\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "#         img0 = img0.convert(\"L\")\n",
    "#         img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_ = SiameseNetworkDataset(datasets.ImageFolder(train_dir), transform=train_aug, should_invert=False)\n",
    "train_loader = DataLoader(train_, shuffle=True, num_workers=num_workers, batch_size=train_batch)\n",
    "val_ = SiameseNetworkDataset(datasets.ImageFolder(val_dir), transform=val_aug, should_invert=False)\n",
    "val_loader = DataLoader(val_, shuffle=True, num_workers=num_workers, batch_size=test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.0, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    pt = torch.load(pretrained)['state_dict']\n",
    "    model.load_state_dict(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.17M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=1.0).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=50, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs0, inputs1, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs0.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            target_index = targets[targets==0]\n",
    "            target_index = target_index.long().cuda()\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs0.size(), lam)\n",
    "            inputs0[target_index, :, bbx1:bbx2, bby1:bby2] = inputs1[target_index, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "\n",
    "        \n",
    "        outputs0 = model(inputs0)\n",
    "        outputs1 = model(inputs1)\n",
    "        \n",
    "        loss = criterion(outputs0, outputs1, targets)\n",
    "            \n",
    "        # compute output\n",
    "        outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        pred = outputs.data\n",
    "        pred[pred < thresh] = 0.\n",
    "        pred[pred >= thresh] = 1.\n",
    "        prec1 = [accuracy_score(targets.data.cpu().numpy(), pred.cpu().numpy())]\n",
    "        \n",
    "        losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "        top1.update(prec1[0], inputs0.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "#         if batch_idx % 10 == 0:\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs0, inputs1, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs0 = model(inputs0)\n",
    "            outputs1 = model(inputs1)\n",
    "            loss = criterion(outputs0, outputs1, targets)\n",
    "            outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            pred = outputs.data\n",
    "            pred[pred < thresh] = 0.\n",
    "            pred[pred >= thresh] = 1.\n",
    "            prec1 = [accuracy_score(targets.data.cpu().numpy(), pred.cpu().numpy())]\n",
    "            losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "            top1.update(prec1[0], inputs0.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(val_loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,)\n",
    "            bar.next()\n",
    "        print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "             batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "        bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 300] LR: 0.040000\n",
      "1168/1168 Data:0.011 | Batch:0.574 | Total:0:12:03 | ETA:0:00:01 | Loss:0.27401352973196785 | top1:0.49848095349380694\n",
      "292/292 Data:0.027 | Batch:0.802 | Total:0:01:01 | ETA:0:00:00 | Loss:0.27290395770767395 | top1:0.4988785046728972\n",
      "\n",
      "Epoch: [2 | 300] LR: 0.042400\n",
      "1168/1168 Data:0.015 | Batch:0.597 | Total:0:13:49 | ETA:0:00:01 | Loss:0.2611437612041614 | top1:0.5002181194983252\n",
      "292/292 Data:0.046 | Batch:0.275 | Total:0:01:16 | ETA:0:00:00 | Loss:0.26491390395665837 | top1:0.5011214953271028\n",
      "\n",
      "Epoch: [3 | 300] LR: 0.044800\n",
      "1168/1168 Data:0.023 | Batch:0.658 | Total:0:14:55 | ETA:0:00:01 | Loss:0.25914395962552117 | top1:0.4993612214691906\n",
      "292/292 Data:0.018 | Batch:0.250 | Total:0:01:03 | ETA:0:00:00 | Loss:0.25919572001677066 | top1:0.5030218068535826\n",
      "\n",
      "Epoch: [4 | 300] LR: 0.047200\n",
      "1168/1168 Data:0.047 | Batch:0.688 | Total:0:13:16 | ETA:0:00:01 | Loss:0.257877073613823 | top1:0.4997351406091766\n",
      "292/292 Data:0.028 | Batch:0.189 | Total:0:01:01 | ETA:0:00:00 | Loss:0.2603433718376813 | top1:0.5014641744548287\n",
      "\n",
      "Epoch: [5 | 300] LR: 0.049600\n",
      "1168/1168 Data:0.008 | Batch:0.592 | Total:0:11:35 | ETA:0:00:01 | Loss:0.2577097272786097 | top1:0.5011996572407883\n",
      "292/292 Data:0.008 | Batch:0.142 | Total:0:00:51 | ETA:0:00:00 | Loss:0.25639538780736776 | top1:0.5119314641744548\n",
      "\n",
      "Epoch: [6 | 300] LR: 0.052000\n",
      "1168/1168 Data:0.008 | Batch:0.599 | Total:0:11:24 | ETA:0:00:01 | Loss:0.2568521601911682 | top1:0.5023759445353275\n",
      "292/292 Data:0.007 | Batch:0.147 | Total:0:00:52 | ETA:0:00:00 | Loss:0.25166992319521503 | top1:0.5361370716510904\n",
      "\n",
      "Epoch: [7 | 300] LR: 0.054400\n",
      "1168/1168 Data:0.009 | Batch:0.593 | Total:0:11:25 | ETA:0:00:01 | Loss:0.2552336712320748 | top1:0.5124250214224507\n",
      "292/292 Data:0.008 | Batch:0.150 | Total:0:00:51 | ETA:0:00:00 | Loss:0.23795724694973955 | top1:0.5928971962616822\n",
      "\n",
      "Epoch: [8 | 300] LR: 0.056800\n",
      "1168/1168 Data:0.010 | Batch:0.603 | Total:0:11:24 | ETA:0:00:01 | Loss:0.2452520350538571 | top1:0.5647425410921555\n",
      "292/292 Data:0.008 | Batch:0.152 | Total:0:00:52 | ETA:0:00:00 | Loss:0.15566850083639316 | top1:0.8306230529595016\n",
      "\n",
      "Epoch: [9 | 300] LR: 0.059200\n",
      "1168/1168 Data:0.008 | Batch:0.594 | Total:0:11:25 | ETA:0:00:01 | Loss:0.2258828740372245 | top1:0.6321570460387941\n",
      "292/292 Data:0.009 | Batch:0.120 | Total:0:00:50 | ETA:0:00:00 | Loss:0.08539537015107004 | top1:0.9432710280373832\n",
      "\n",
      "Epoch: [10 | 300] LR: 0.061600\n",
      "1168/1168 Data:0.010 | Batch:0.548 | Total:0:11:24 | ETA:0:00:01 | Loss:0.2154475934196969 | top1:0.6606528004985589\n",
      "292/292 Data:0.008 | Batch:0.149 | Total:0:00:48 | ETA:0:00:00 | Loss:0.11507216934288773 | top1:0.9513707165109034\n",
      "\n",
      "Epoch: [11 | 300] LR: 0.064000\n",
      "1168/1168 Data:0.009 | Batch:0.594 | Total:0:11:24 | ETA:0:00:01 | Loss:0.21083435215751842 | top1:0.6696502298044714\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07438614410217677 | top1:0.975981308411215\n",
      "\n",
      "Epoch: [12 | 300] LR: 0.066400\n",
      "1168/1168 Data:0.008 | Batch:0.592 | Total:0:11:25 | ETA:0:00:01 | Loss:0.20856881130170699 | top1:0.6769961829087793\n",
      "292/292 Data:0.008 | Batch:0.153 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08199218846973601 | top1:0.9822741433021807\n",
      "\n",
      "Epoch: [13 | 300] LR: 0.068800\n",
      "1168/1168 Data:0.009 | Batch:0.589 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20755879429026147 | top1:0.6767235335358729\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.06445149497776016 | top1:0.9802803738317757\n",
      "\n",
      "Epoch: [14 | 300] LR: 0.071200\n",
      "1168/1168 Data:0.008 | Batch:0.600 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20716136642879365 | top1:0.6793409675157747\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.08194218852159761 | top1:0.9866043613707165\n",
      "\n",
      "Epoch: [15 | 300] LR: 0.073600\n",
      "1168/1168 Data:0.009 | Batch:0.592 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20620048694765863 | top1:0.6805016748461479\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.08030470166691925 | top1:0.9854205607476636\n",
      "\n",
      "Epoch: [16 | 300] LR: 0.076000\n",
      "1168/1168 Data:0.008 | Batch:0.587 | Total:0:11:25 | ETA:0:00:01 | Loss:0.2048393786847336 | top1:0.6840616966580977\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.06694581799232328 | top1:0.9871339563862929\n",
      "\n",
      "Epoch: [17 | 300] LR: 0.078400\n",
      "1168/1168 Data:0.008 | Batch:0.589 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20502392638669426 | top1:0.6830879488977175\n",
      "292/292 Data:0.009 | Batch:0.136 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08163871557054304 | top1:0.9827102803738318\n",
      "\n",
      "Epoch: [18 | 300] LR: 0.080800\n",
      "1168/1168 Data:0.009 | Batch:0.605 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20392310910482334 | top1:0.6847939549739036\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.0749049990429221 | top1:0.983208722741433\n",
      "\n",
      "Epoch: [19 | 300] LR: 0.083200\n",
      "1168/1168 Data:0.009 | Batch:0.595 | Total:0:11:23 | ETA:0:00:01 | Loss:0.2053547000517269 | top1:0.6820518812806731\n",
      "292/292 Data:0.009 | Batch:0.150 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07506532542221829 | top1:0.9881931464174455\n",
      "\n",
      "Epoch: [20 | 300] LR: 0.085600\n",
      "1168/1168 Data:0.009 | Batch:0.592 | Total:0:11:21 | ETA:0:00:01 | Loss:0.2054833219974799 | top1:0.6802056555269923\n",
      "292/292 Data:0.007 | Batch:0.144 | Total:0:00:49 | ETA:0:00:00 | Loss:0.06582015206996711 | top1:0.9888785046728972\n",
      "\n",
      "Epoch: [21 | 300] LR: 0.088000\n",
      "1168/1168 Data:0.008 | Batch:0.544 | Total:0:11:23 | ETA:0:00:01 | Loss:0.20412131739204115 | top1:0.684513515618914\n",
      "292/292 Data:0.008 | Batch:0.138 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0680938686247091 | top1:0.9905607476635514\n",
      "\n",
      "Epoch: [22 | 300] LR: 0.090400\n",
      "1168/1168 Data:0.008 | Batch:0.592 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20375729693800543 | top1:0.6851133442393083\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07373490712004845 | top1:0.9886915887850467\n",
      "\n",
      "Epoch: [23 | 300] LR: 0.092800\n",
      "1168/1168 Data:0.007 | Batch:0.588 | Total:0:11:23 | ETA:0:00:01 | Loss:0.203979551038787 | top1:0.6850977642751421\n",
      "292/292 Data:0.007 | Batch:0.147 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07304887458496376 | top1:0.9896573208722741\n",
      "\n",
      "Epoch: [24 | 300] LR: 0.095200\n",
      "1168/1168 Data:0.008 | Batch:0.583 | Total:0:11:22 | ETA:0:00:01 | Loss:0.20320987041270314 | top1:0.6861182519280206\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:51 | ETA:0:00:00 | Loss:0.06766517631872049 | top1:0.9909968847352025\n",
      "\n",
      "Epoch: [25 | 300] LR: 0.097600\n",
      "1168/1168 Data:0.009 | Batch:0.593 | Total:0:11:23 | ETA:0:00:01 | Loss:0.20452220548956232 | top1:0.6839837968372673\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.09597021627277601 | top1:0.9902803738317757\n",
      "\n",
      "Epoch: [26 | 300] LR: 0.100000\n",
      "1168/1168 Data:0.008 | Batch:0.604 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20280875240060744 | top1:0.6858456025551142\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07596405556895465 | top1:0.988006230529595\n",
      "\n",
      "Epoch: [27 | 300] LR: 0.102400\n",
      "1168/1168 Data:0.009 | Batch:0.581 | Total:0:11:23 | ETA:0:00:01 | Loss:0.2034268700090758 | top1:0.6861649918205188\n",
      "292/292 Data:0.009 | Batch:0.134 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07915837620254432 | top1:0.9919003115264797\n",
      "\n",
      "Epoch: [28 | 300] LR: 0.104800\n",
      "1168/1168 Data:0.009 | Batch:0.588 | Total:0:11:23 | ETA:0:00:01 | Loss:0.2042377295663649 | top1:0.6831035288618836\n",
      "292/292 Data:0.008 | Batch:0.138 | Total:0:00:51 | ETA:0:00:00 | Loss:0.0683124729986317 | top1:0.9861682242990654\n",
      "\n",
      "Epoch: [29 | 300] LR: 0.107200\n",
      "1168/1168 Data:0.009 | Batch:0.610 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20210713494150886 | top1:0.6870608397600686\n",
      "292/292 Data:0.008 | Batch:0.152 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07717491066622957 | top1:0.9892211838006231\n",
      "\n",
      "Epoch: [30 | 300] LR: 0.109600\n",
      "1168/1168 Data:0.009 | Batch:0.588 | Total:0:11:23 | ETA:0:00:01 | Loss:0.20299759018135152 | top1:0.6856664329672042\n",
      "292/292 Data:0.007 | Batch:0.139 | Total:0:00:52 | ETA:0:00:00 | Loss:0.055350217911359675 | top1:0.9927102803738318\n",
      "\n",
      "Epoch: [31 | 300] LR: 0.112000\n",
      "1168/1168 Data:0.009 | Batch:0.587 | Total:0:11:23 | ETA:0:00:01 | Loss:0.2022547667340121 | top1:0.6868894601542417\n",
      "292/292 Data:0.007 | Batch:0.153 | Total:0:00:50 | ETA:0:00:00 | Loss:0.07563825144760333 | top1:0.9878816199376947\n",
      "\n",
      "Epoch: [32 | 300] LR: 0.114400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 Data:0.009 | Batch:0.541 | Total:0:11:25 | ETA:0:00:01 | Loss:0.20271335913773367 | top1:0.6873101191867259\n",
      "292/292 Data:0.009 | Batch:0.153 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0929545858196009 | top1:0.9891277258566978\n",
      "\n",
      "Epoch: [33 | 300] LR: 0.116800\n",
      "1168/1168 Data:0.009 | Batch:0.603 | Total:0:11:23 | ETA:0:00:01 | Loss:0.20192106747433172 | top1:0.6870997896704838\n",
      "292/292 Data:0.007 | Batch:0.148 | Total:0:00:51 | ETA:0:00:00 | Loss:0.0555863542638928 | top1:0.9901557632398754\n",
      "\n",
      "Epoch: [34 | 300] LR: 0.119200\n",
      "1168/1168 Data:0.009 | Batch:0.593 | Total:0:11:23 | ETA:0:00:01 | Loss:0.20162976402294563 | top1:0.6882371270546078\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08490532774745119 | top1:0.9872274143302181\n",
      "\n",
      "Epoch: [35 | 300] LR: 0.121600\n",
      "1168/1168 Data:0.008 | Batch:0.593 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20141425612602054 | top1:0.6894523642595622\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07748952263797926 | top1:0.9899376947040498\n",
      "\n",
      "Epoch: [36 | 300] LR: 0.124000\n",
      "1168/1168 Data:0.019 | Batch:0.585 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20160171560681367 | top1:0.6861260419101036\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.08339139027898185 | top1:0.9825545171339564\n",
      "\n",
      "Epoch: [37 | 300] LR: 0.126400\n",
      "1168/1168 Data:0.009 | Batch:0.603 | Total:0:11:23 | ETA:0:00:01 | Loss:0.2007404197559667 | top1:0.6893121445820675\n",
      "292/292 Data:0.008 | Batch:0.145 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08801511932292089 | top1:0.9866355140186915\n",
      "\n",
      "Epoch: [38 | 300] LR: 0.128800\n",
      "1168/1168 Data:0.009 | Batch:0.582 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20127114353521114 | top1:0.6875516086313002\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07172644356747282 | top1:0.9849532710280374\n",
      "\n",
      "Epoch: [39 | 300] LR: 0.131200\n",
      "1168/1168 Data:0.009 | Batch:0.597 | Total:0:11:24 | ETA:0:00:01 | Loss:0.2001831952894255 | top1:0.687902157825037\n",
      "292/292 Data:0.008 | Batch:0.153 | Total:0:00:52 | ETA:0:00:00 | Loss:0.06835442614680695 | top1:0.9918068535825545\n",
      "\n",
      "Epoch: [40 | 300] LR: 0.133600\n",
      "1168/1168 Data:0.009 | Batch:0.592 | Total:0:11:24 | ETA:0:00:01 | Loss:0.20108458035862536 | top1:0.6870842097063177\n",
      "292/292 Data:0.009 | Batch:0.153 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07794586830885611 | top1:0.982772585669782\n",
      "\n",
      "Epoch: [41 | 300] LR: 0.136000\n",
      "1168/1168 Data:0.008 | Batch:0.607 | Total:0:11:24 | ETA:0:00:01 | Loss:0.2006477089319458 | top1:0.6862974215159305\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07728139197575712 | top1:0.9758255451713396\n",
      "\n",
      "Epoch: [42 | 300] LR: 0.138400\n",
      "1168/1168 Data:0.008 | Batch:0.585 | Total:0:11:22 | ETA:0:00:01 | Loss:0.1993269054370756 | top1:0.687536028667134\n",
      "292/292 Data:0.008 | Batch:0.146 | Total:0:00:50 | ETA:0:00:00 | Loss:0.07902555151891857 | top1:0.9857943925233645\n",
      "\n",
      "Epoch: [43 | 300] LR: 0.140800\n",
      "1168/1168 Data:0.007 | Batch:0.542 | Total:0:11:24 | ETA:0:00:01 | Loss:0.19919188930984225 | top1:0.6882604970008569\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0854244777857329 | top1:0.9757009345794393\n",
      "\n",
      "Epoch: [44 | 300] LR: 0.143200\n",
      "1168/1168 Data:0.008 | Batch:0.592 | Total:0:11:23 | ETA:0:00:01 | Loss:0.19865231678183506 | top1:0.6897328036145517\n",
      "292/292 Data:0.007 | Batch:0.152 | Total:0:00:52 | ETA:0:00:00 | Loss:0.08317946359236664 | top1:0.972398753894081\n",
      "\n",
      "Epoch: [45 | 300] LR: 0.145600\n",
      "1168/1168 Data:0.008 | Batch:0.574 | Total:0:11:19 | ETA:0:00:01 | Loss:0.19888352824258518 | top1:0.6870374698138194\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07297040184269256 | top1:0.9767289719626169\n",
      "\n",
      "Epoch: [46 | 300] LR: 0.148000\n",
      "1168/1168 Data:0.008 | Batch:0.597 | Total:0:11:21 | ETA:0:00:01 | Loss:0.19836035324859128 | top1:0.6875983485237984\n",
      "292/292 Data:0.007 | Batch:0.152 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08091362215624977 | top1:0.9806853582554517\n",
      "\n",
      "Epoch: [47 | 300] LR: 0.150400\n",
      "1168/1168 Data:0.008 | Batch:0.581 | Total:0:11:21 | ETA:0:00:01 | Loss:0.1964227805407311 | top1:0.6931759756952559\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:50 | ETA:0:00:00 | Loss:0.06760568460826748 | top1:0.9828971962616823\n",
      "\n",
      "Epoch: [48 | 300] LR: 0.152800\n",
      "1168/1168 Data:0.008 | Batch:0.572 | Total:0:11:14 | ETA:0:00:01 | Loss:0.19560294711344653 | top1:0.693004596089429\n",
      "292/292 Data:0.008 | Batch:0.146 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08006444262845494 | top1:0.9803426791277259\n",
      "\n",
      "Epoch: [49 | 300] LR: 0.155200\n",
      "1168/1168 Data:0.008 | Batch:0.593 | Total:0:11:19 | ETA:0:00:01 | Loss:0.19603118898016628 | top1:0.69111163044325\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07177134611849845 | top1:0.9546105919003115\n",
      "\n",
      "Epoch: [50 | 300] LR: 0.157600\n",
      "1168/1168 Data:0.008 | Batch:0.584 | Total:0:11:20 | ETA:0:00:01 | Loss:0.1945077832325361 | top1:0.6951702111085144\n",
      "292/292 Data:0.007 | Batch:0.138 | Total:0:00:51 | ETA:0:00:00 | Loss:0.09537513072129536 | top1:0.9452959501557633\n",
      "\n",
      "Epoch: [51 | 300] LR: 0.160000\n",
      "1168/1168 Data:0.009 | Batch:0.586 | Total:0:11:18 | ETA:0:00:01 | Loss:0.194879773560506 | top1:0.6932538755160863\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08800118809734178 | top1:0.9613395638629284\n",
      "\n",
      "Epoch: [52 | 300] LR: 0.160000\n",
      "1168/1168 Data:0.008 | Batch:0.572 | Total:0:11:19 | ETA:0:00:01 | Loss:0.19373761924444965 | top1:0.6942899431331307\n",
      "292/292 Data:0.009 | Batch:0.147 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08624595566261037 | top1:0.9193146417445482\n",
      "\n",
      "Epoch: [53 | 300] LR: 0.159996\n",
      "1168/1168 Data:0.009 | Batch:0.578 | Total:0:11:17 | ETA:0:00:01 | Loss:0.19233083192517642 | top1:0.6970475967905274\n",
      "292/292 Data:0.008 | Batch:0.140 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08641222121188202 | top1:0.9371651090342679\n",
      "\n",
      "Epoch: [54 | 300] LR: 0.159982\n",
      "1168/1168 Data:0.010 | Batch:0.575 | Total:0:11:17 | ETA:0:00:01 | Loss:0.19132342114853063 | top1:0.6999065202150035\n",
      "292/292 Data:0.009 | Batch:0.142 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07176389927827866 | top1:0.9472585669781931\n",
      "\n",
      "Epoch: [55 | 300] LR: 0.159961\n",
      "1168/1168 Data:0.008 | Batch:0.595 | Total:0:11:16 | ETA:0:00:01 | Loss:0.19054840086307298 | top1:0.7018540157357638\n",
      "292/292 Data:0.007 | Batch:0.152 | Total:0:00:50 | ETA:0:00:00 | Loss:0.08206930387521459 | top1:0.940809968847352\n",
      "\n",
      "Epoch: [56 | 300] LR: 0.159930\n",
      "1168/1168 Data:0.008 | Batch:0.602 | Total:0:11:14 | ETA:0:00:01 | Loss:0.18968515525848925 | top1:0.7017839058970164\n",
      "292/292 Data:0.010 | Batch:0.154 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07231490134589397 | top1:0.947570093457944\n",
      "\n",
      "Epoch: [57 | 300] LR: 0.159890\n",
      "1168/1168 Data:0.010 | Batch:0.540 | Total:0:11:20 | ETA:0:00:01 | Loss:0.1880774919610812 | top1:0.7052114980135545\n",
      "292/292 Data:0.009 | Batch:0.142 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07470450272487703 | top1:0.942367601246106\n",
      "\n",
      "Epoch: [58 | 300] LR: 0.159842\n",
      "1168/1168 Data:0.009 | Batch:0.574 | Total:0:11:18 | ETA:0:00:01 | Loss:0.18642596532722366 | top1:0.7099555971021266\n",
      "292/292 Data:0.009 | Batch:0.129 | Total:0:00:51 | ETA:0:00:00 | Loss:0.05941904250335099 | top1:0.9601246105919004\n",
      "\n",
      "Epoch: [59 | 300] LR: 0.159785\n",
      "1168/1168 Data:0.009 | Batch:0.574 | Total:0:11:19 | ETA:0:00:01 | Loss:0.1854393884041622 | top1:0.7097296876217185\n",
      "292/292 Data:0.008 | Batch:0.147 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07343681702430932 | top1:0.9298130841121496\n",
      "\n",
      "Epoch: [60 | 300] LR: 0.159719\n",
      "1168/1168 Data:0.008 | Batch:0.595 | Total:0:11:19 | ETA:0:00:01 | Loss:0.18323082854810083 | top1:0.7144971566565397\n",
      "292/292 Data:0.010 | Batch:0.134 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07245676169212548 | top1:0.9260436137071651\n",
      "\n",
      "Epoch: [61 | 300] LR: 0.159645\n",
      "1168/1168 Data:0.019 | Batch:0.579 | Total:0:11:18 | ETA:0:00:01 | Loss:0.18152928918012717 | top1:0.7176365194360053\n",
      "292/292 Data:0.009 | Batch:0.141 | Total:0:00:51 | ETA:0:00:00 | Loss:0.04832065999275799 | top1:0.9644548286604362\n",
      "\n",
      "Epoch: [62 | 300] LR: 0.159562\n",
      "1168/1168 Data:0.010 | Batch:0.592 | Total:0:11:18 | ETA:0:00:01 | Loss:0.17943944733497313 | top1:0.7221702890083352\n",
      "292/292 Data:0.008 | Batch:0.150 | Total:0:00:51 | ETA:0:00:00 | Loss:0.0813249740552308 | top1:0.8992834890965732\n",
      "\n",
      "Epoch: [63 | 300] LR: 0.159470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 Data:0.008 | Batch:0.596 | Total:0:11:18 | ETA:0:00:01 | Loss:0.17715319041114846 | top1:0.7263379294227623\n",
      "292/292 Data:0.009 | Batch:0.146 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07605526787446482 | top1:0.9076323987538941\n",
      "\n",
      "Epoch: [64 | 300] LR: 0.159369\n",
      "1168/1168 Data:0.009 | Batch:0.573 | Total:0:11:17 | ETA:0:00:01 | Loss:0.17619062042711325 | top1:0.7288696735997507\n",
      "292/292 Data:0.009 | Batch:0.156 | Total:0:00:51 | ETA:0:00:00 | Loss:0.06303253152010226 | top1:0.9350778816199377\n",
      "\n",
      "Epoch: [65 | 300] LR: 0.159260\n",
      "1168/1168 Data:0.009 | Batch:0.575 | Total:0:11:18 | ETA:0:00:01 | Loss:0.17431357956228852 | top1:0.7304821998909402\n",
      "292/292 Data:0.008 | Batch:0.153 | Total:0:00:51 | ETA:0:00:00 | Loss:0.06576938046449998 | top1:0.9383489096573209\n",
      "\n",
      "Epoch: [66 | 300] LR: 0.159142\n",
      "1168/1168 Data:0.009 | Batch:0.599 | Total:0:11:19 | ETA:0:00:01 | Loss:0.17056537926477966 | top1:0.7398145984264236\n",
      "292/292 Data:0.009 | Batch:0.142 | Total:0:00:52 | ETA:0:00:00 | Loss:0.056219737130765605 | top1:0.9499376947040499\n",
      "\n",
      "Epoch: [67 | 300] LR: 0.159015\n",
      "1168/1168 Data:0.015 | Batch:0.596 | Total:0:11:25 | ETA:0:00:01 | Loss:0.16915477517331473 | top1:0.7412323751655371\n",
      "292/292 Data:0.007 | Batch:0.147 | Total:0:00:51 | ETA:0:00:00 | Loss:0.059851590704138036 | top1:0.9451713395638629\n",
      "\n",
      "Epoch: [68 | 300] LR: 0.158880\n",
      "1168/1168 Data:0.008 | Batch:0.596 | Total:0:11:26 | ETA:0:00:01 | Loss:0.16780639597515623 | top1:0.7443639479629197\n",
      "292/292 Data:0.009 | Batch:0.153 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08224592062287799 | top1:0.9061059190031152\n",
      "\n",
      "Epoch: [69 | 300] LR: 0.158736\n",
      "1168/1168 Data:0.009 | Batch:0.603 | Total:0:11:24 | ETA:0:00:01 | Loss:0.16662227025298657 | top1:0.7470125418711537\n",
      "292/292 Data:0.007 | Batch:0.117 | Total:0:00:47 | ETA:0:00:00 | Loss:0.05910475653279979 | top1:0.9351713395638629\n",
      "\n",
      "Epoch: [70 | 300] LR: 0.158583\n",
      "1168/1168 Data:0.009 | Batch:0.545 | Total:0:11:25 | ETA:0:00:01 | Loss:0.1631400538185466 | top1:0.7530731479317597\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:50 | ETA:0:00:00 | Loss:0.07633614789381206 | top1:0.906822429906542\n",
      "\n",
      "Epoch: [71 | 300] LR: 0.158422\n",
      "1168/1168 Data:0.007 | Batch:0.598 | Total:0:11:26 | ETA:0:00:01 | Loss:0.16094076625792103 | top1:0.7573810080236816\n",
      "292/292 Data:0.008 | Batch:0.151 | Total:0:00:51 | ETA:0:00:00 | Loss:0.05299456180581051 | top1:0.9406853582554517\n",
      "\n",
      "Epoch: [72 | 300] LR: 0.158252\n",
      "1168/1168 Data:0.007 | Batch:0.580 | Total:0:11:25 | ETA:0:00:01 | Loss:0.15873508759947444 | top1:0.7607930201760535\n",
      "292/292 Data:0.009 | Batch:0.153 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07215030940824023 | top1:0.9233333333333333\n",
      "\n",
      "Epoch: [73 | 300] LR: 0.158073\n",
      "1168/1168 Data:0.009 | Batch:0.604 | Total:0:11:25 | ETA:0:00:01 | Loss:0.15682120030010063 | top1:0.7648126509309029\n",
      "292/292 Data:0.009 | Batch:0.154 | Total:0:00:51 | ETA:0:00:00 | Loss:0.06661721227568722 | top1:0.9161682242990654\n",
      "\n",
      "Epoch: [74 | 300] LR: 0.157886\n",
      "1168/1168 Data:0.008 | Batch:0.581 | Total:0:11:25 | ETA:0:00:01 | Loss:0.1545855013818034 | top1:0.7689257614707486\n",
      "292/292 Data:0.016 | Batch:0.150 | Total:0:00:52 | ETA:0:00:00 | Loss:0.05389111251033541 | top1:0.9450155763239876\n",
      "\n",
      "Epoch: [75 | 300] LR: 0.157691\n",
      "1168/1168 Data:0.009 | Batch:0.587 | Total:0:11:26 | ETA:0:00:01 | Loss:0.15382460679064067 | top1:0.7704292280127756\n",
      "292/292 Data:0.009 | Batch:0.137 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07333252660094577 | top1:0.9128971962616822\n",
      "\n",
      "Epoch: [76 | 300] LR: 0.157487\n",
      "1168/1168 Data:0.009 | Batch:0.596 | Total:0:11:25 | ETA:0:00:01 | Loss:0.1513955165546678 | top1:0.7749629975851056\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08540880959074816 | top1:0.8896261682242991\n",
      "\n",
      "Epoch: [77 | 300] LR: 0.015727\n",
      "1168/1168 Data:0.009 | Batch:0.587 | Total:0:11:26 | ETA:0:00:01 | Loss:0.14543276601299426 | top1:0.7853314637376334\n",
      "292/292 Data:0.007 | Batch:0.149 | Total:0:00:52 | ETA:0:00:00 | Loss:0.0626702428838917 | top1:0.9202180685358256\n",
      "\n",
      "Epoch: [78 | 300] LR: 0.015705\n",
      "1168/1168 Data:0.009 | Batch:0.599 | Total:0:11:25 | ETA:0:00:01 | Loss:0.14257631052505695 | top1:0.7892731946716522\n",
      "292/292 Data:0.009 | Batch:0.139 | Total:0:00:52 | ETA:0:00:00 | Loss:0.05994323728043249 | top1:0.9252959501557633\n",
      "\n",
      "Epoch: [79 | 300] LR: 0.015682\n",
      "1168/1168 Data:0.008 | Batch:0.587 | Total:0:11:25 | ETA:0:00:01 | Loss:0.141592433106317 | top1:0.7931370257848407\n",
      "292/292 Data:0.008 | Batch:0.152 | Total:0:00:50 | ETA:0:00:00 | Loss:0.06071270141936258 | top1:0.9248909657320872\n",
      "\n",
      "Epoch: [80 | 300] LR: 0.015659\n",
      "1168/1168 Data:0.008 | Batch:0.540 | Total:0:11:25 | ETA:0:00:01 | Loss:0.13988636854809375 | top1:0.7951468411622653\n",
      "292/292 Data:0.008 | Batch:0.152 | Total:0:00:47 | ETA:0:00:00 | Loss:0.06718243429918898 | top1:0.9130529595015576\n",
      "\n",
      "Epoch: [81 | 300] LR: 0.015634\n",
      "1168/1168 Data:0.008 | Batch:0.589 | Total:0:11:25 | ETA:0:00:01 | Loss:0.13943341742138016 | top1:0.7955597102126665\n",
      "292/292 Data:0.008 | Batch:0.154 | Total:0:00:52 | ETA:0:00:00 | Loss:0.0572212576802321 | top1:0.9287538940809968\n",
      "\n",
      "Epoch: [82 | 300] LR: 0.015608\n",
      "1168/1168 Data:0.008 | Batch:0.596 | Total:0:11:24 | ETA:0:00:01 | Loss:0.13827357268808432 | top1:0.7974215159305134\n",
      "292/292 Data:0.010 | Batch:0.139 | Total:0:00:51 | ETA:0:00:00 | Loss:0.060762875806250116 | top1:0.9237694704049845\n",
      "\n",
      "Epoch: [83 | 300] LR: 0.015582\n",
      "1168/1168 Data:0.008 | Batch:0.589 | Total:0:11:22 | ETA:0:00:01 | Loss:0.1376541886469086 | top1:0.7998052504479239\n",
      "292/292 Data:0.007 | Batch:0.147 | Total:0:00:52 | ETA:0:00:00 | Loss:0.06034566585171817 | top1:0.9248286604361371\n",
      "\n",
      "Epoch: [84 | 300] LR: 0.015555\n",
      "1168/1168 Data:0.009 | Batch:0.595 | Total:0:11:25 | ETA:0:00:01 | Loss:0.1378862948349223 | top1:0.7985354833683882\n",
      "292/292 Data:0.009 | Batch:0.139 | Total:0:00:52 | ETA:0:00:00 | Loss:0.061995690961214614 | top1:0.9210280373831776\n",
      "\n",
      "Epoch: [85 | 300] LR: 0.015527\n",
      "1168/1168 Data:0.009 | Batch:0.594 | Total:0:11:29 | ETA:0:00:01 | Loss:0.13685317256431245 | top1:0.8003427592116538\n",
      "292/292 Data:0.009 | Batch:0.145 | Total:0:00:52 | ETA:0:00:00 | Loss:0.06258612238102798 | top1:0.9198130841121496\n",
      "\n",
      "Epoch: [86 | 300] LR: 0.015498\n",
      "1168/1168 Data:0.008 | Batch:0.599 | Total:0:11:26 | ETA:0:00:01 | Loss:0.13532168264305275 | top1:0.8029524032094726\n",
      "292/292 Data:0.011 | Batch:0.158 | Total:0:00:52 | ETA:0:00:00 | Loss:0.06782361061959252 | top1:0.9117133956386293\n",
      "\n",
      "Epoch: [87 | 300] LR: 0.015469\n",
      "1168/1168 Data:0.015 | Batch:0.607 | Total:0:11:55 | ETA:0:00:01 | Loss:0.13453624205346176 | top1:0.8055308872789593\n",
      "292/292 Data:0.028 | Batch:0.217 | Total:0:00:56 | ETA:0:00:00 | Loss:0.061832876329388574 | top1:0.9195327102803739\n",
      "\n",
      "Epoch: [88 | 300] LR: 0.015438\n",
      "1168/1168 Data:0.016 | Batch:0.605 | Total:0:11:52 | ETA:0:00:01 | Loss:0.13417454993459232 | top1:0.8059749162576926\n",
      "292/292 Data:0.009 | Batch:0.144 | Total:0:00:56 | ETA:0:00:00 | Loss:0.06993443385590553 | top1:0.9079127725856698\n",
      "\n",
      "Epoch: [89 | 300] LR: 0.015407\n",
      "1168/1168 Data:0.010 | Batch:0.493 | Total:0:11:50 | ETA:0:00:01 | Loss:0.1328002085211753 | top1:0.809075329126743\n",
      "292/292 Data:0.013 | Batch:0.204 | Total:0:00:51 | ETA:0:00:00 | Loss:0.05647743629564377 | top1:0.9283177570093458\n",
      "\n",
      "Epoch: [90 | 300] LR: 0.015375\n",
      "1168/1168 Data:0.015 | Batch:0.652 | Total:0:12:15 | ETA:0:00:01 | Loss:0.13164814230255725 | top1:0.8100490768871231\n",
      "292/292 Data:0.014 | Batch:0.143 | Total:0:00:56 | ETA:0:00:00 | Loss:0.058849864872607675 | top1:0.9249844236760124\n",
      "\n",
      "Epoch: [91 | 300] LR: 0.015342\n",
      "1168/1168 Data:0.008 | Batch:0.591 | Total:0:11:50 | ETA:0:00:01 | Loss:0.13172343343292023 | top1:0.8103684661525279\n",
      "292/292 Data:0.015 | Batch:0.128 | Total:0:00:55 | ETA:0:00:00 | Loss:0.06545459526152904 | top1:0.913613707165109\n",
      "\n",
      "Epoch: [92 | 300] LR: 0.015308\n",
      "1168/1168 Data:0.014 | Batch:0.641 | Total:0:12:14 | ETA:0:00:01 | Loss:0.13021723383394387 | top1:0.8134922489678273\n",
      "292/292 Data:0.015 | Batch:0.164 | Total:0:00:55 | ETA:0:00:00 | Loss:0.07456653257860944 | top1:0.9005919003115265\n",
      "\n",
      "Epoch: [93 | 300] LR: 0.015274\n",
      "1168/1168 Data:0.007 | Batch:0.608 | Total:0:12:04 | ETA:0:00:01 | Loss:0.13017055741491335 | top1:0.8140141777673912\n",
      "292/292 Data:0.017 | Batch:0.268 | Total:0:00:59 | ETA:0:00:00 | Loss:0.0728433007531077 | top1:0.9050155763239875\n",
      "\n",
      "Epoch: [94 | 300] LR: 0.015239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 Data:0.013 | Batch:0.652 | Total:0:12:19 | ETA:0:00:01 | Loss:0.1292696843544642 | top1:0.8162888525356392\n",
      "292/292 Data:0.014 | Batch:0.146 | Total:0:00:55 | ETA:0:00:00 | Loss:0.06519606962359026 | top1:0.9158566978193147\n",
      "\n",
      "Epoch: [95 | 300] LR: 0.015203\n",
      "1168/1168 Data:0.014 | Batch:0.603 | Total:0:11:54 | ETA:0:00:01 | Loss:0.12876989210428288 | top1:0.8160084131806496\n",
      "292/292 Data:0.045 | Batch:0.324 | Total:0:00:55 | ETA:0:00:00 | Loss:0.060549149046596897 | top1:0.9195327102803739\n",
      "\n",
      "Epoch: [96 | 300] LR: 0.015166\n",
      "1168/1168 Data:0.014 | Batch:0.700 | Total:0:12:18 | ETA:0:00:01 | Loss:0.12891534049999234 | top1:0.8172937602243515\n",
      "292/292 Data:0.028 | Batch:0.200 | Total:0:00:56 | ETA:0:00:00 | Loss:0.06986832070480625 | top1:0.9079439252336449\n",
      "\n",
      "Epoch: [97 | 300] LR: 0.015128\n",
      "1168/1168 Data:0.010 | Batch:0.593 | Total:0:11:59 | ETA:0:00:01 | Loss:0.12726135533414748 | top1:0.8185635273038872\n",
      "292/292 Data:0.023 | Batch:0.207 | Total:0:00:58 | ETA:0:00:00 | Loss:0.06586375788109511 | top1:0.9127725856697819\n",
      "\n",
      "Epoch: [98 | 300] LR: 0.015090\n",
      "1168/1168 Data:0.015 | Batch:0.640 | Total:0:12:16 | ETA:0:00:01 | Loss:0.12675207377041928 | top1:0.8200903637921633\n",
      "292/292 Data:0.015 | Batch:0.153 | Total:0:00:56 | ETA:0:00:00 | Loss:0.059002102949769694 | top1:0.9221495327102803\n",
      "\n",
      "Epoch: [99 | 300] LR: 0.015050\n",
      "1168/1168 Data:0.011 | Batch:0.592 | Total:0:12:02 | ETA:0:00:01 | Loss:0.12466145970801971 | top1:0.8233543662849575\n",
      "292/292 Data:0.014 | Batch:0.128 | Total:0:00:56 | ETA:0:00:00 | Loss:0.06513118740066746 | top1:0.9134579439252336\n",
      "\n",
      "Epoch: [100 | 300] LR: 0.015010\n",
      "1168/1168 Data:0.014 | Batch:0.580 | Total:0:12:25 | ETA:0:00:01 | Loss:0.12474546324854338 | top1:0.8242112643140921\n",
      "292/292 Data:0.013 | Batch:0.199 | Total:0:00:56 | ETA:0:00:00 | Loss:0.06181616955132128 | top1:0.9171339563862928\n",
      "\n",
      "Epoch: [101 | 300] LR: 0.014970\n",
      "1168/1168 Data:0.012 | Batch:0.547 | Total:0:12:06 | ETA:0:00:01 | Loss:0.12465091346545207 | top1:0.8241489444574277\n",
      "292/292 Data:0.013 | Batch:0.160 | Total:0:00:51 | ETA:0:00:00 | Loss:0.08023919097939941 | top1:0.891619937694704\n",
      "\n",
      "Epoch: [102 | 300] LR: 0.014928\n",
      "1168/1168 Data:0.029 | Batch:0.734 | Total:0:12:13 | ETA:0:00:01 | Loss:0.12387330712305243 | top1:0.8258315805873646\n",
      "292/292 Data:0.013 | Batch:0.162 | Total:0:01:01 | ETA:0:00:00 | Loss:0.06293658600261325 | top1:0.9155451713395638\n",
      "\n",
      "Epoch: [103 | 300] LR: 0.014886\n",
      "1168/1168 Data:0.014 | Batch:0.580 | Total:0:12:23 | ETA:0:00:01 | Loss:0.1224080779315982 | top1:0.8279115058035367\n",
      "292/292 Data:0.014 | Batch:0.153 | Total:0:00:55 | ETA:0:00:00 | Loss:0.07988068361496814 | top1:0.8907476635514019\n",
      "\n",
      "Epoch: [104 | 300] LR: 0.014843\n",
      "1168/1168 Data:0.010 | Batch:0.290 | Total:0:09:28 | ETA:0:00:01 | Loss:0.1211273795139003 | top1:0.829672041754304\n",
      "292/292 Data:0.012 | Batch:0.103 | Total:0:00:32 | ETA:0:00:00 | Loss:0.065517196824014 | top1:0.91196261682243\n",
      "\n",
      "Epoch: [105 | 300] LR: 0.014799\n",
      "1168/1168 Data:0.010 | Batch:0.331 | Total:0:08:19 | ETA:0:00:01 | Loss:0.1202549868308793 | top1:0.8316506972033965\n",
      "292/292 Data:0.008 | Batch:0.077 | Total:0:00:34 | ETA:0:00:00 | Loss:0.06785406290417148 | top1:0.9107476635514019\n",
      "\n",
      "Epoch: [106 | 300] LR: 0.014755\n",
      "1168/1168 Data:0.009 | Batch:0.298 | Total:0:08:16 | ETA:0:00:01 | Loss:0.12046654194261441 | top1:0.8319233465763028\n",
      "292/292 Data:0.012 | Batch:0.104 | Total:0:00:32 | ETA:0:00:00 | Loss:0.05740501543628835 | top1:0.924423676012461\n",
      "\n",
      "Epoch: [107 | 300] LR: 0.014709\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):    \n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
