{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b1/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b1' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 4000\n",
    "start_epoch = 0\n",
    "train_batch = 100\n",
    "test_batch = 200\n",
    "lr = 0.01\n",
    "schedule = [500, 1000, 2000, 3000]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b1/to_star/l2sp' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 4\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'\n",
    "\n",
    "# iterative training\n",
    "feedback = 0\n",
    "iter_time = []\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(target_dir, '100_shot_style1')\n",
    "source_train_dir = os.path.join(target_dir, '100_shot_style1_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "source_train_loader = DataLoader(datasets.ImageFolder(source_train_dir, transform=val_aug),\n",
    "                                batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b1/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes)\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 6.52M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in student_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_bn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        16, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(24, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(24, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(10, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(10, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(10, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(80, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(480, 1920, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1920, 1920, kernel_size=(3, 3), stride=(1, 1), groups=1920, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(480, 1920, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1920, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        80, 1920, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(80, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): GroupNorm(320, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_conv_stem.weight\n",
      "_bn0.weight\n",
      "_bn0.bias\n",
      "_blocks.0._depthwise_conv.weight\n",
      "_blocks.0._bn1.weight\n",
      "_blocks.0._bn1.bias\n",
      "_blocks.0._se_reduce.weight\n",
      "_blocks.0._se_reduce.bias\n",
      "_blocks.0._se_expand.weight\n",
      "_blocks.0._se_expand.bias\n",
      "_blocks.0._project_conv.weight\n",
      "_blocks.0._bn2.weight\n",
      "_blocks.0._bn2.bias\n",
      "_blocks.1._depthwise_conv.weight\n",
      "_blocks.1._bn1.weight\n",
      "_blocks.1._bn1.bias\n",
      "_blocks.1._se_reduce.weight\n",
      "_blocks.1._se_reduce.bias\n",
      "_blocks.1._se_expand.weight\n",
      "_blocks.1._se_expand.bias\n",
      "_blocks.1._project_conv.weight\n",
      "_blocks.1._bn2.weight\n",
      "_blocks.1._bn2.bias\n",
      "_blocks.2._expand_conv.weight\n",
      "_blocks.2._bn0.weight\n",
      "_blocks.2._bn0.bias\n",
      "_blocks.2._depthwise_conv.weight\n",
      "_blocks.2._bn1.weight\n",
      "_blocks.2._bn1.bias\n",
      "_blocks.2._se_reduce.weight\n",
      "_blocks.2._se_reduce.bias\n",
      "_blocks.2._se_expand.weight\n",
      "_blocks.2._se_expand.bias\n",
      "_blocks.2._project_conv.weight\n",
      "_blocks.2._bn2.weight\n",
      "_blocks.2._bn2.bias\n",
      "_blocks.3._expand_conv.weight\n",
      "_blocks.3._bn0.weight\n",
      "_blocks.3._bn0.bias\n",
      "_blocks.3._depthwise_conv.weight\n",
      "_blocks.3._bn1.weight\n",
      "_blocks.3._bn1.bias\n",
      "_blocks.3._se_reduce.weight\n",
      "_blocks.3._se_reduce.bias\n",
      "_blocks.3._se_expand.weight\n",
      "_blocks.3._se_expand.bias\n",
      "_blocks.3._project_conv.weight\n",
      "_blocks.3._bn2.weight\n",
      "_blocks.3._bn2.bias\n",
      "_blocks.4._expand_conv.weight\n",
      "_blocks.4._bn0.weight\n",
      "_blocks.4._bn0.bias\n",
      "_blocks.4._depthwise_conv.weight\n",
      "_blocks.4._bn1.weight\n",
      "_blocks.4._bn1.bias\n",
      "_blocks.4._se_reduce.weight\n",
      "_blocks.4._se_reduce.bias\n",
      "_blocks.4._se_expand.weight\n",
      "_blocks.4._se_expand.bias\n",
      "_blocks.4._project_conv.weight\n",
      "_blocks.4._bn2.weight\n",
      "_blocks.4._bn2.bias\n",
      "_blocks.5._expand_conv.weight\n",
      "_blocks.5._bn0.weight\n",
      "_blocks.5._bn0.bias\n",
      "_blocks.5._depthwise_conv.weight\n",
      "_blocks.5._bn1.weight\n",
      "_blocks.5._bn1.bias\n",
      "_blocks.5._se_reduce.weight\n",
      "_blocks.5._se_reduce.bias\n",
      "_blocks.5._se_expand.weight\n",
      "_blocks.5._se_expand.bias\n",
      "_blocks.5._project_conv.weight\n",
      "_blocks.5._bn2.weight\n",
      "_blocks.5._bn2.bias\n",
      "_blocks.6._expand_conv.weight\n",
      "_blocks.6._bn0.weight\n",
      "_blocks.6._bn0.bias\n",
      "_blocks.6._depthwise_conv.weight\n",
      "_blocks.6._bn1.weight\n",
      "_blocks.6._bn1.bias\n",
      "_blocks.6._se_reduce.weight\n",
      "_blocks.6._se_reduce.bias\n",
      "_blocks.6._se_expand.weight\n",
      "_blocks.6._se_expand.bias\n",
      "_blocks.6._project_conv.weight\n",
      "_blocks.6._bn2.weight\n",
      "_blocks.6._bn2.bias\n",
      "_blocks.7._expand_conv.weight\n",
      "_blocks.7._bn0.weight\n",
      "_blocks.7._bn0.bias\n",
      "_blocks.7._depthwise_conv.weight\n",
      "_blocks.7._bn1.weight\n",
      "_blocks.7._bn1.bias\n",
      "_blocks.7._se_reduce.weight\n",
      "_blocks.7._se_reduce.bias\n",
      "_blocks.7._se_expand.weight\n",
      "_blocks.7._se_expand.bias\n",
      "_blocks.7._project_conv.weight\n",
      "_blocks.7._bn2.weight\n",
      "_blocks.7._bn2.bias\n",
      "_blocks.8._expand_conv.weight\n",
      "_blocks.8._bn0.weight\n",
      "_blocks.8._bn0.bias\n",
      "_blocks.8._depthwise_conv.weight\n",
      "_blocks.8._bn1.weight\n",
      "_blocks.8._bn1.bias\n",
      "_blocks.8._se_reduce.weight\n",
      "_blocks.8._se_reduce.bias\n",
      "_blocks.8._se_expand.weight\n",
      "_blocks.8._se_expand.bias\n",
      "_blocks.8._project_conv.weight\n",
      "_blocks.8._bn2.weight\n",
      "_blocks.8._bn2.bias\n",
      "_blocks.9._expand_conv.weight\n",
      "_blocks.9._bn0.weight\n",
      "_blocks.9._bn0.bias\n",
      "_blocks.9._depthwise_conv.weight\n",
      "_blocks.9._bn1.weight\n",
      "_blocks.9._bn1.bias\n",
      "_blocks.9._se_reduce.weight\n",
      "_blocks.9._se_reduce.bias\n",
      "_blocks.9._se_expand.weight\n",
      "_blocks.9._se_expand.bias\n",
      "_blocks.9._project_conv.weight\n",
      "_blocks.9._bn2.weight\n",
      "_blocks.9._bn2.bias\n",
      "_blocks.10._expand_conv.weight\n",
      "_blocks.10._bn0.weight\n",
      "_blocks.10._bn0.bias\n",
      "_blocks.10._depthwise_conv.weight\n",
      "_blocks.10._bn1.weight\n",
      "_blocks.10._bn1.bias\n",
      "_blocks.10._se_reduce.weight\n",
      "_blocks.10._se_reduce.bias\n",
      "_blocks.10._se_expand.weight\n",
      "_blocks.10._se_expand.bias\n",
      "_blocks.10._project_conv.weight\n",
      "_blocks.10._bn2.weight\n",
      "_blocks.10._bn2.bias\n",
      "_blocks.11._expand_conv.weight\n",
      "_blocks.11._bn0.weight\n",
      "_blocks.11._bn0.bias\n",
      "_blocks.11._depthwise_conv.weight\n",
      "_blocks.11._bn1.weight\n",
      "_blocks.11._bn1.bias\n",
      "_blocks.11._se_reduce.weight\n",
      "_blocks.11._se_reduce.bias\n",
      "_blocks.11._se_expand.weight\n",
      "_blocks.11._se_expand.bias\n",
      "_blocks.11._project_conv.weight\n",
      "_blocks.11._bn2.weight\n",
      "_blocks.11._bn2.bias\n",
      "_blocks.12._expand_conv.weight\n",
      "_blocks.12._bn0.weight\n",
      "_blocks.12._bn0.bias\n",
      "_blocks.12._depthwise_conv.weight\n",
      "_blocks.12._bn1.weight\n",
      "_blocks.12._bn1.bias\n",
      "_blocks.12._se_reduce.weight\n",
      "_blocks.12._se_reduce.bias\n",
      "_blocks.12._se_expand.weight\n",
      "_blocks.12._se_expand.bias\n",
      "_blocks.12._project_conv.weight\n",
      "_blocks.12._bn2.weight\n",
      "_blocks.12._bn2.bias\n",
      "_blocks.13._expand_conv.weight\n",
      "_blocks.13._bn0.weight\n",
      "_blocks.13._bn0.bias\n",
      "_blocks.13._depthwise_conv.weight\n",
      "_blocks.13._bn1.weight\n",
      "_blocks.13._bn1.bias\n",
      "_blocks.13._se_reduce.weight\n",
      "_blocks.13._se_reduce.bias\n",
      "_blocks.13._se_expand.weight\n",
      "_blocks.13._se_expand.bias\n",
      "_blocks.13._project_conv.weight\n",
      "_blocks.13._bn2.weight\n",
      "_blocks.13._bn2.bias\n",
      "_blocks.14._expand_conv.weight\n",
      "_blocks.14._bn0.weight\n",
      "_blocks.14._bn0.bias\n",
      "_blocks.14._depthwise_conv.weight\n",
      "_blocks.14._bn1.weight\n",
      "_blocks.14._bn1.bias\n",
      "_blocks.14._se_reduce.weight\n",
      "_blocks.14._se_reduce.bias\n",
      "_blocks.14._se_expand.weight\n",
      "_blocks.14._se_expand.bias\n",
      "_blocks.14._project_conv.weight\n",
      "_blocks.14._bn2.weight\n",
      "_blocks.14._bn2.bias\n",
      "_blocks.15._expand_conv.weight\n",
      "_blocks.15._bn0.weight\n",
      "_blocks.15._bn0.bias\n",
      "_blocks.15._depthwise_conv.weight\n",
      "_blocks.15._bn1.weight\n",
      "_blocks.15._bn1.bias\n",
      "_blocks.15._se_reduce.weight\n",
      "_blocks.15._se_reduce.bias\n",
      "_blocks.15._se_expand.weight\n",
      "_blocks.15._se_expand.bias\n",
      "_blocks.15._project_conv.weight\n",
      "_blocks.15._bn2.weight\n",
      "_blocks.15._bn2.bias\n",
      "_blocks.16._expand_conv.weight\n",
      "_blocks.16._bn0.weight\n",
      "_blocks.16._bn0.bias\n",
      "_blocks.16._depthwise_conv.weight\n",
      "_blocks.16._bn1.weight\n",
      "_blocks.16._bn1.bias\n",
      "_blocks.16._se_reduce.weight\n",
      "_blocks.16._se_reduce.bias\n",
      "_blocks.16._se_expand.weight\n",
      "_blocks.16._se_expand.bias\n",
      "_blocks.16._project_conv.weight\n",
      "_blocks.16._bn2.weight\n",
      "_blocks.16._bn2.bias\n",
      "_blocks.17._expand_conv.weight\n",
      "_blocks.17._bn0.weight\n",
      "_blocks.17._bn0.bias\n",
      "_blocks.17._depthwise_conv.weight\n",
      "_blocks.17._bn1.weight\n",
      "_blocks.17._bn1.bias\n",
      "_blocks.17._se_reduce.weight\n",
      "_blocks.17._se_reduce.bias\n",
      "_blocks.17._se_expand.weight\n",
      "_blocks.17._se_expand.bias\n",
      "_blocks.17._project_conv.weight\n",
      "_blocks.17._bn2.weight\n",
      "_blocks.17._bn2.bias\n",
      "_blocks.18._expand_conv.weight\n",
      "_blocks.18._bn0.weight\n",
      "_blocks.18._bn0.bias\n",
      "_blocks.18._depthwise_conv.weight\n",
      "_blocks.18._bn1.weight\n",
      "_blocks.18._bn1.bias\n",
      "_blocks.18._se_reduce.weight\n",
      "_blocks.18._se_reduce.bias\n",
      "_blocks.18._se_expand.weight\n",
      "_blocks.18._se_expand.bias\n",
      "_blocks.18._project_conv.weight\n",
      "_blocks.18._bn2.weight\n",
      "_blocks.18._bn2.bias\n",
      "_blocks.19._expand_conv.weight\n",
      "_blocks.19._bn0.weight\n",
      "_blocks.19._bn0.bias\n",
      "_blocks.19._depthwise_conv.weight\n",
      "_blocks.19._bn1.weight\n",
      "_blocks.19._bn1.bias\n",
      "_blocks.19._se_reduce.weight\n",
      "_blocks.19._se_reduce.bias\n",
      "_blocks.19._se_expand.weight\n",
      "_blocks.19._se_expand.bias\n",
      "_blocks.19._project_conv.weight\n",
      "_blocks.19._bn2.weight\n",
      "_blocks.19._bn2.bias\n",
      "_blocks.20._expand_conv.weight\n",
      "_blocks.20._bn0.weight\n",
      "_blocks.20._bn0.bias\n",
      "_blocks.20._depthwise_conv.weight\n",
      "_blocks.20._bn1.weight\n",
      "_blocks.20._bn1.bias\n",
      "_blocks.20._se_reduce.weight\n",
      "_blocks.20._se_reduce.bias\n",
      "_blocks.20._se_expand.weight\n",
      "_blocks.20._se_expand.bias\n",
      "_blocks.20._project_conv.weight\n",
      "_blocks.20._bn2.weight\n",
      "_blocks.20._bn2.bias\n",
      "_blocks.21._expand_conv.weight\n",
      "_blocks.21._bn0.weight\n",
      "_blocks.21._bn0.bias\n",
      "_blocks.21._depthwise_conv.weight\n",
      "_blocks.21._bn1.weight\n",
      "_blocks.21._bn1.bias\n",
      "_blocks.21._se_reduce.weight\n",
      "_blocks.21._se_reduce.bias\n",
      "_blocks.21._se_expand.weight\n",
      "_blocks.21._se_expand.bias\n",
      "_blocks.21._project_conv.weight\n",
      "_blocks.21._bn2.weight\n",
      "_blocks.21._bn2.bias\n",
      "_blocks.22._expand_conv.weight\n",
      "_blocks.22._bn0.weight\n",
      "_blocks.22._bn0.bias\n",
      "_blocks.22._depthwise_conv.weight\n",
      "_blocks.22._bn1.weight\n",
      "_blocks.22._bn1.bias\n",
      "_blocks.22._se_reduce.weight\n",
      "_blocks.22._se_reduce.bias\n",
      "_blocks.22._se_expand.weight\n",
      "_blocks.22._se_expand.bias\n",
      "_blocks.22._project_conv.weight\n",
      "_blocks.22._bn2.weight\n",
      "_blocks.22._bn2.bias\n",
      "_conv_head.weight\n",
      "_bn1.weight\n",
      "_bn1.bias\n"
     ]
    }
   ],
   "source": [
    "for i, param in teacher_model.named_parameters():\n",
    "    if not i.startswith('_fc'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9097e-04, 1.3666e-04, 1.6796e-04,  ..., 1.3282e-04, 7.9702e-02,\n",
       "        8.5992e-05], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model_weights['_bn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=50, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.', 'Source Loss', 'Source ACC.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, source_train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    student_model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "#     source_set = []\n",
    "#     for inputs, targets in source_train_loader:\n",
    "#         source_set.append((inputs, targets))\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#         inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "        \n",
    "        # cutmix\n",
    "#         source_inputs, source_targets = source_set[batch_idx]\n",
    "#         source_inputs, source_targets = source_inputs.cuda(), source_targets.cuda()\n",
    "\n",
    "#         prob_delta = cm_prob_init - cm_prob_low\n",
    "#         prob_step = epoch / (epochs+1) * prob_delta\n",
    "#         lam = cm_prob_init - prob_step\n",
    "\n",
    "#         rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "#         st = source_targets[rand_index]\n",
    "#         tt = targets[rand_index]\n",
    "#         rand_index = rand_index[st == tt]\n",
    "        \n",
    "#         bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "#         inputs[rand_index, :, bbx1:bbx2, bby1:bby2] = source_inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "#         lam = 1 - ((bbx2 - bbx1)*(bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "            \n",
    "            \n",
    "        # compute output\n",
    "        loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "#         if batch_idx % 10 == 0:\n",
    "        print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "#         inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 4000] LR: 0.010000\n",
      "1/4 Data:2.200 | Batch:3.984 | Total:0:00:03 | ETA:0:00:12 | Loss:3.1258292198181152 | top1:68.0\n",
      "2/4 Data:0.001 | Batch:0.341 | Total:0:00:04 | ETA:0:00:05 | Loss:2.6153738498687744 | top1:73.0\n",
      "3/4 Data:0.012 | Batch:0.272 | Total:0:00:04 | ETA:0:00:02 | Loss:2.3423405090967813 | top1:73.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.287 | Total:0:00:04 | ETA:0:00:00 | Loss:2.2284773886203766 | top1:70.0\n",
      "153/153 Data:0.000 | Batch:0.443 | Total:0:00:21 | ETA:0:00:00 | Loss:1.373285769478834 | top1:50.409568786621094\n",
      "39/39 Data:0.000 | Batch:0.132 | Total:0:00:08 | ETA:0:00:00 | Loss:1.1798848402805817 | top1:65.03845977783203\n",
      "\n",
      "Epoch: [2 | 4000] LR: 0.010600\n",
      "1/4 Data:0.511 | Batch:0.849 | Total:0:00:00 | ETA:0:00:03 | Loss:1.2829450368881226 | top1:60.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:1.1835493445396423 | top1:65.0\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:1.1202774047851562 | top1:68.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:1.0610871016979218 | top1:70.25\n",
      "\n",
      "Epoch: [3 | 4000] LR: 0.011200\n",
      "1/4 Data:0.487 | Batch:0.777 | Total:0:00:00 | ETA:0:00:03 | Loss:0.946424126625061 | top1:66.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:0.942044734954834 | top1:69.0\n",
      "3/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:01 | Loss:0.9309960206349691 | top1:70.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:0.9290968179702759 | top1:72.0\n",
      "\n",
      "Epoch: [4 | 4000] LR: 0.011800\n",
      "1/4 Data:0.514 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:0.9242690801620483 | top1:74.0\n",
      "2/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:02 | Loss:0.9240847229957581 | top1:74.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:0.9302016894022623 | top1:72.0\n",
      "4/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:00 | Loss:0.9348676800727844 | top1:69.25\n",
      "\n",
      "Epoch: [5 | 4000] LR: 0.012400\n",
      "1/4 Data:0.499 | Batch:0.774 | Total:0:00:00 | ETA:0:00:03 | Loss:0.9461036920547485 | top1:61.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:0.9490434229373932 | top1:57.5\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:0.9529180526733398 | top1:58.333335876464844\n",
      "4/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:00 | Loss:0.957054927945137 | top1:55.75\n",
      "\n",
      "Epoch: [6 | 4000] LR: 0.013000\n",
      "1/4 Data:0.522 | Batch:0.800 | Total:0:00:00 | ETA:0:00:03 | Loss:0.9306929707527161 | top1:62.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:0.9344602823257446 | top1:67.0\n",
      "3/4 Data:0.007 | Batch:0.284 | Total:0:00:01 | ETA:0:00:01 | Loss:0.9266798893610636 | top1:70.33333587646484\n",
      "4/4 Data:0.010 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:0.9254281520843506 | top1:70.75\n",
      "\n",
      "Epoch: [7 | 4000] LR: 0.013600\n",
      "1/4 Data:0.521 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:0.9008257389068604 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8959656953811646 | top1:73.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:0.8869855403900146 | top1:73.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:0.8903092294931412 | top1:72.75\n",
      "\n",
      "Epoch: [8 | 4000] LR: 0.014200\n",
      "1/4 Data:0.484 | Batch:0.776 | Total:0:00:00 | ETA:0:00:03 | Loss:0.8376807570457458 | top1:74.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8332865536212921 | top1:75.5\n",
      "3/4 Data:0.009 | Batch:0.281 | Total:0:00:01 | ETA:0:00:01 | Loss:0.8211353222529093 | top1:76.0\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:0.8222907036542892 | top1:74.0\n",
      "\n",
      "Epoch: [9 | 4000] LR: 0.014800\n",
      "1/4 Data:0.505 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:0.7650171518325806 | top1:72.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:0.784731388092041 | top1:72.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:0.7915152907371521 | top1:71.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:0.7709695398807526 | top1:73.5\n",
      "\n",
      "Epoch: [10 | 4000] LR: 0.015400\n",
      "1/4 Data:0.511 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:0.7351049780845642 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:02 | Loss:0.7577874064445496 | top1:72.5\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:0.7410022219022115 | top1:74.0\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:0.7227199673652649 | top1:75.0\n",
      "\n",
      "Epoch: [11 | 4000] LR: 0.016000\n",
      "1/4 Data:0.518 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6661368608474731 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6822782158851624 | top1:77.0\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:0.676668643951416 | top1:77.0\n",
      "4/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6917869299650192 | top1:75.25\n",
      "\n",
      "Epoch: [12 | 4000] LR: 0.016600\n",
      "1/4 Data:0.529 | Batch:0.803 | Total:0:00:00 | ETA:0:00:03 | Loss:0.62556391954422 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6672857701778412 | top1:76.0\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6663105289141337 | top1:76.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:0.652150347828865 | top1:76.75\n",
      "\n",
      "Epoch: [13 | 4000] LR: 0.017200\n",
      "1/4 Data:0.499 | Batch:0.776 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6560819149017334 | top1:70.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6404418349266052 | top1:74.5\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6395483016967773 | top1:75.0\n",
      "4/4 Data:0.009 | Batch:0.281 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6220458447933197 | top1:75.75\n",
      "\n",
      "Epoch: [14 | 4000] LR: 0.017800\n",
      "1/4 Data:0.498 | Batch:0.778 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5635938048362732 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5708063542842865 | top1:77.5\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5909993847211202 | top1:77.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.257 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6067564636468887 | top1:76.5\n",
      "\n",
      "Epoch: [15 | 4000] LR: 0.018400\n",
      "1/4 Data:0.504 | Batch:0.784 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5925359129905701 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5855711996555328 | top1:78.5\n",
      "3/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6076765060424805 | top1:77.0\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5807500183582306 | top1:79.75\n",
      "\n",
      "Epoch: [16 | 4000] LR: 0.019000\n",
      "1/4 Data:0.498 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6232571005821228 | top1:76.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6005742847919464 | top1:77.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5792905688285828 | top1:80.0\n",
      "4/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5735228210687637 | top1:80.25\n",
      "\n",
      "Epoch: [17 | 4000] LR: 0.019600\n",
      "1/4 Data:0.537 | Batch:0.808 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5510175824165344 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:0.604562371969223 | top1:77.0\n",
      "3/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5799571474393209 | top1:78.0\n",
      "4/4 Data:0.009 | Batch:0.289 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5757296234369278 | top1:78.25\n",
      "\n",
      "Epoch: [18 | 4000] LR: 0.020200\n",
      "1/4 Data:0.538 | Batch:0.820 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5936464667320251 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:02 | Loss:0.564146488904953 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5461424787839254 | top1:83.0\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:0.548083633184433 | top1:82.5\n",
      "\n",
      "Epoch: [19 | 4000] LR: 0.020800\n",
      "1/4 Data:0.490 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5472390651702881 | top1:82.0\n",
      "2/4 Data:0.010 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5049454122781754 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5001300275325775 | top1:84.66667175292969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5182725414633751 | top1:83.75\n",
      "\n",
      "Epoch: [20 | 4000] LR: 0.021400\n",
      "1/4 Data:0.552 | Batch:0.826 | Total:0:00:00 | ETA:0:00:03 | Loss:0.49501368403434753 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:0.4956454336643219 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:01 | Loss:0.490856796503067 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:00 | Loss:0.4891681373119354 | top1:85.0\n",
      "\n",
      "Epoch: [21 | 4000] LR: 0.022000\n",
      "1/4 Data:0.535 | Batch:0.834 | Total:0:00:00 | ETA:0:00:03 | Loss:0.4559907019138336 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:0.44226670265197754 | top1:87.5\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.46217767397562665 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:0.48380914330482483 | top1:85.5\n",
      "\n",
      "Epoch: [22 | 4000] LR: 0.022600\n",
      "1/4 Data:0.512 | Batch:0.793 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5400198698043823 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5171906799077988 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5120462675889333 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5005876049399376 | top1:84.75\n",
      "\n",
      "Epoch: [23 | 4000] LR: 0.023200\n",
      "1/4 Data:0.498 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:0.57425856590271 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5296096950769424 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5345512926578522 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.250 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5562253370881081 | top1:82.25\n",
      "\n",
      "Epoch: [24 | 4000] LR: 0.023800\n",
      "1/4 Data:0.544 | Batch:0.825 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5426467061042786 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5365352928638458 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5565079053243002 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5530508011579514 | top1:84.5\n",
      "\n",
      "Epoch: [25 | 4000] LR: 0.024400\n",
      "1/4 Data:0.508 | Batch:0.792 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6106293201446533 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:0.564190149307251 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5706422527631124 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5674818605184555 | top1:85.25\n",
      "\n",
      "Epoch: [26 | 4000] LR: 0.025000\n",
      "1/4 Data:0.541 | Batch:0.809 | Total:0:00:00 | ETA:0:00:03 | Loss:0.657982349395752 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6174457967281342 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:0.628687858581543 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6165903210639954 | top1:85.0\n",
      "\n",
      "Epoch: [27 | 4000] LR: 0.025600\n",
      "1/4 Data:0.521 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6788512468338013 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6986947953701019 | top1:80.0\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6717101335525513 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6539570689201355 | top1:83.5\n",
      "\n",
      "Epoch: [28 | 4000] LR: 0.026200\n",
      "1/4 Data:0.567 | Batch:0.843 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6366139650344849 | top1:83.0\n",
      "2/4 Data:0.010 | Batch:0.292 | Total:0:00:01 | ETA:0:00:02 | Loss:0.656905323266983 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6221337914466858 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6002379208803177 | top1:87.5\n",
      "\n",
      "Epoch: [29 | 4000] LR: 0.026800\n",
      "1/4 Data:0.522 | Batch:0.837 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6701359152793884 | top1:83.0\n",
      "2/4 Data:0.008 | Batch:0.278 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6908515393733978 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.283 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6713400681813558 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6757249981164932 | top1:84.0\n",
      "\n",
      "Epoch: [30 | 4000] LR: 0.027400\n",
      "1/4 Data:0.502 | Batch:0.801 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6495063304901123 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6952527165412903 | top1:79.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6677160263061523 | top1:82.0\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6510394960641861 | top1:83.75\n",
      "\n",
      "Epoch: [31 | 4000] LR: 0.028000\n",
      "1/4 Data:0.511 | Batch:0.779 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6215043067932129 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:02 | Loss:0.649250864982605 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:01 | Loss:0.670921782652537 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6785080879926682 | top1:86.75\n",
      "\n",
      "Epoch: [32 | 4000] LR: 0.028600\n",
      "1/4 Data:0.509 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:0.7211705446243286 | top1:88.0\n",
      "2/4 Data:0.008 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:0.7721788883209229 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:0.7885111172993978 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:0.7884191572666168 | top1:86.0\n",
      "\n",
      "Epoch: [33 | 4000] LR: 0.029200\n",
      "1/4 Data:0.521 | Batch:0.807 | Total:0:00:00 | ETA:0:00:03 | Loss:0.8941096067428589 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8504149913787842 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:0.8669642806053162 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:0.8614068031311035 | top1:86.0\n",
      "\n",
      "Epoch: [34 | 4000] LR: 0.029800\n",
      "1/4 Data:0.536 | Batch:0.807 | Total:0:00:00 | ETA:0:00:03 | Loss:0.8007559776306152 | top1:90.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8624502718448639 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:0.8764100670814514 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:0.8753519356250763 | top1:86.5\n",
      "\n",
      "Epoch: [35 | 4000] LR: 0.030400\n",
      "1/4 Data:0.504 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:0.8314366340637207 | top1:90.0\n",
      "2/4 Data:0.009 | Batch:0.287 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8442674577236176 | top1:89.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:0.8938681483268738 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:0.895627960562706 | top1:85.0\n",
      "\n",
      "Epoch: [36 | 4000] LR: 0.031000\n",
      "1/4 Data:0.489 | Batch:0.778 | Total:0:00:00 | ETA:0:00:03 | Loss:0.9459834098815918 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8746790885925293 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.8500078916549683 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:0.8527786582708359 | top1:84.25\n",
      "\n",
      "Epoch: [37 | 4000] LR: 0.031600\n",
      "1/4 Data:0.541 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:0.7681078910827637 | top1:89.0\n",
      "2/4 Data:0.010 | Batch:0.262 | Total:0:00:01 | ETA:0:00:02 | Loss:0.8102073073387146 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:0.7835878928502401 | top1:87.0\n",
      "4/4 Data:0.009 | Batch:0.303 | Total:0:00:01 | ETA:0:00:00 | Loss:0.7707147896289825 | top1:87.5\n",
      "\n",
      "Epoch: [38 | 4000] LR: 0.032200\n",
      "1/4 Data:0.539 | Batch:0.838 | Total:0:00:00 | ETA:0:00:03 | Loss:0.70487380027771 | top1:91.0\n",
      "2/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:02 | Loss:0.7122021317481995 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.283 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6783689260482788 | top1:89.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6901136636734009 | top1:88.5\n",
      "\n",
      "Epoch: [39 | 4000] LR: 0.032800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 Data:0.537 | Batch:0.835 | Total:0:00:00 | ETA:0:00:03 | Loss:0.7011773586273193 | top1:87.0\n",
      "2/4 Data:0.006 | Batch:0.275 | Total:0:00:01 | ETA:0:00:02 | Loss:0.685376912355423 | top1:88.0\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:0.7008503874142965 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6860953271389008 | top1:86.25\n",
      "\n",
      "Epoch: [40 | 4000] LR: 0.033400\n",
      "1/4 Data:0.506 | Batch:0.792 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6221504211425781 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6098925173282623 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5919881264368693 | top1:90.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5786881446838379 | top1:90.5\n",
      "\n",
      "Epoch: [41 | 4000] LR: 0.034000\n",
      "1/4 Data:0.495 | Batch:0.779 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6002225279808044 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:0.620046854019165 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6294383605321249 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6228664219379425 | top1:86.0\n",
      "\n",
      "Epoch: [42 | 4000] LR: 0.034600\n",
      "1/4 Data:0.531 | Batch:0.801 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5562407374382019 | top1:90.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6224205493927002 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.257 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6558580199877421 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.299 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6330577731132507 | top1:83.75\n",
      "\n",
      "Epoch: [43 | 4000] LR: 0.035200\n",
      "1/4 Data:0.532 | Batch:0.803 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6149567365646362 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6310990452766418 | top1:81.0\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6320784489313761 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6198820918798447 | top1:83.0\n",
      "\n",
      "Epoch: [44 | 4000] LR: 0.035800\n",
      "1/4 Data:0.522 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6405078768730164 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6751767992973328 | top1:78.0\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:0.6772018074989319 | top1:77.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6494767665863037 | top1:79.5\n",
      "\n",
      "Epoch: [45 | 4000] LR: 0.036400\n",
      "1/4 Data:0.495 | Batch:0.773 | Total:0:00:00 | ETA:0:00:03 | Loss:0.6383288502693176 | top1:79.0\n",
      "2/4 Data:0.010 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:0.6338625848293304 | top1:81.0\n",
      "3/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:01 | Loss:0.616124153137207 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.290 | Total:0:00:01 | ETA:0:00:00 | Loss:0.6032171547412872 | top1:84.25\n",
      "\n",
      "Epoch: [46 | 4000] LR: 0.037000\n",
      "1/4 Data:0.509 | Batch:0.801 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5921972393989563 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:0.582131952047348 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5756977399190267 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5714935660362244 | top1:84.75\n",
      "\n",
      "Epoch: [47 | 4000] LR: 0.037600\n",
      "1/4 Data:0.500 | Batch:0.786 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5176211595535278 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5184897184371948 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5196892221768697 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5330849885940552 | top1:84.75\n",
      "\n",
      "Epoch: [48 | 4000] LR: 0.038200\n",
      "1/4 Data:0.513 | Batch:0.799 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5577449202537537 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:0.5230469703674316 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:0.5229667027791342 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:0.5230461806058884 | top1:87.0\n",
      "\n",
      "Epoch: [49 | 4000] LR: 0.038800\n",
      "1/4 Data:0.506 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:0.5740441083908081 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:0.9046210646629333 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:1.6089502573013306 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:2.5817563235759735 | top1:83.5\n",
      "\n",
      "Epoch: [50 | 4000] LR: 0.039400\n",
      "1/4 Data:0.491 | Batch:0.792 | Total:0:00:00 | ETA:0:00:03 | Loss:8.502007484436035 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:10.576651573181152 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:13.044204076131185 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:15.730672359466553 | top1:85.5\n",
      "\n",
      "Epoch: [51 | 4000] LR: 0.040000\n",
      "1/4 Data:0.520 | Batch:0.824 | Total:0:00:00 | ETA:0:00:03 | Loss:30.009498596191406 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:32.96082878112793 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:35.922105153401695 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:38.751487731933594 | top1:83.5\n",
      "153/153 Data:0.000 | Batch:0.091 | Total:0:00:20 | ETA:0:00:00 | Loss:52.51588843409387 | top1:53.361732482910156\n",
      "39/39 Data:0.000 | Batch:0.133 | Total:0:00:08 | ETA:0:00:00 | Loss:51.74307260757838 | top1:97.44871520996094\n",
      "\n",
      "Epoch: [52 | 4000] LR: 0.040000\n",
      "1/4 Data:0.511 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:52.05253219604492 | top1:72.0\n",
      "2/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:02 | Loss:54.018619537353516 | top1:80.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:55.838199615478516 | top1:83.0\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:57.41801071166992 | top1:84.25\n",
      "\n",
      "Epoch: [53 | 4000] LR: 0.040000\n",
      "1/4 Data:0.506 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:64.20697021484375 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:64.9244384765625 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:65.40732065836589 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:65.7087230682373 | top1:85.25\n",
      "\n",
      "Epoch: [54 | 4000] LR: 0.040000\n",
      "1/4 Data:0.505 | Batch:0.778 | Total:0:00:00 | ETA:0:00:03 | Loss:66.33698272705078 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:65.96075820922852 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:65.43355814615886 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:64.77857112884521 | top1:82.75\n",
      "\n",
      "Epoch: [55 | 4000] LR: 0.040000\n",
      "1/4 Data:0.527 | Batch:0.819 | Total:0:00:00 | ETA:0:00:03 | Loss:60.95689392089844 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:02 | Loss:59.83671760559082 | top1:87.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:58.701727549235024 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:57.509714126586914 | top1:88.0\n",
      "\n",
      "Epoch: [56 | 4000] LR: 0.040000\n",
      "1/4 Data:0.549 | Batch:0.819 | Total:0:00:00 | ETA:0:00:03 | Loss:51.321617126464844 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:50.02227783203125 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:48.734536488850914 | top1:83.0\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:47.45939636230469 | top1:83.25\n",
      "\n",
      "Epoch: [57 | 4000] LR: 0.040000\n",
      "1/4 Data:0.530 | Batch:0.823 | Total:0:00:00 | ETA:0:00:03 | Loss:41.26499557495117 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:40.152265548706055 | top1:89.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:39.09291966756185 | top1:88.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:38.07147789001465 | top1:85.75\n",
      "\n",
      "Epoch: [58 | 4000] LR: 0.040000\n",
      "1/4 Data:0.518 | Batch:0.803 | Total:0:00:00 | ETA:0:00:03 | Loss:32.928993225097656 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:02 | Loss:32.06612682342529 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:31.142632166544598 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:30.273503303527832 | top1:85.0\n",
      "\n",
      "Epoch: [59 | 4000] LR: 0.040000\n",
      "1/4 Data:0.514 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:26.433372497558594 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.257 | Total:0:00:01 | ETA:0:00:02 | Loss:26.169713973999023 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:01 | Loss:26.169313430786133 | top1:80.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.292 | Total:0:00:01 | ETA:0:00:00 | Loss:27.780853748321533 | top1:80.25\n",
      "\n",
      "Epoch: [60 | 4000] LR: 0.040000\n",
      "1/4 Data:0.493 | Batch:0.789 | Total:0:00:00 | ETA:0:00:03 | Loss:48.5048828125 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:59.68563461303711 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:72.17843882242839 | top1:87.0\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:85.43483543395996 | top1:86.75\n",
      "\n",
      "Epoch: [61 | 4000] LR: 0.040000\n",
      "1/4 Data:0.537 | Batch:0.842 | Total:0:00:00 | ETA:0:00:03 | Loss:153.49327087402344 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:02 | Loss:167.1468734741211 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.285 | Total:0:00:01 | ETA:0:00:01 | Loss:180.1727549235026 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:00 | Loss:192.39001083374023 | top1:84.0\n",
      "\n",
      "Epoch: [62 | 4000] LR: 0.039999\n",
      "1/4 Data:0.512 | Batch:0.813 | Total:0:00:00 | ETA:0:00:03 | Loss:248.96070861816406 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:257.35694122314453 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:264.80682881673175 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:00 | Loss:271.3310127258301 | top1:83.25\n",
      "\n",
      "Epoch: [63 | 4000] LR: 0.039999\n",
      "1/4 Data:0.486 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:299.2670593261719 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:302.07708740234375 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:304.0225423177083 | top1:83.0\n",
      "4/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:00 | Loss:305.1229934692383 | top1:84.75\n",
      "\n",
      "Epoch: [64 | 4000] LR: 0.039999\n",
      "1/4 Data:0.508 | Batch:0.784 | Total:0:00:00 | ETA:0:00:03 | Loss:306.7015075683594 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:304.7555847167969 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:302.23277791341144 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:299.1494827270508 | top1:82.75\n",
      "\n",
      "Epoch: [65 | 4000] LR: 0.039999\n",
      "1/4 Data:0.502 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:281.0938720703125 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:02 | Loss:276.18145751953125 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:270.9488220214844 | top1:87.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:265.46143341064453 | top1:87.25\n",
      "\n",
      "Epoch: [66 | 4000] LR: 0.039999\n",
      "1/4 Data:0.514 | Batch:0.793 | Total:0:00:00 | ETA:0:00:03 | Loss:236.92575073242188 | top1:91.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:230.8341827392578 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:01 | Loss:224.57921346028647 | top1:87.0\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:218.2982406616211 | top1:86.5\n",
      "\n",
      "Epoch: [67 | 4000] LR: 0.039999\n",
      "1/4 Data:0.496 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:186.92564392089844 | top1:88.0\n",
      "2/4 Data:0.002 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:180.77019500732422 | top1:86.5\n",
      "3/4 Data:0.006 | Batch:0.281 | Total:0:00:01 | ETA:0:00:01 | Loss:174.69563802083334 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.291 | Total:0:00:01 | ETA:0:00:00 | Loss:168.69927978515625 | top1:86.5\n",
      "\n",
      "Epoch: [68 | 4000] LR: 0.039998\n",
      "1/4 Data:0.530 | Batch:0.839 | Total:0:00:00 | ETA:0:00:03 | Loss:139.50343322753906 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:02 | Loss:134.0395050048828 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:128.72846221923828 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:123.63224411010742 | top1:87.5\n",
      "\n",
      "Epoch: [69 | 4000] LR: 0.039998\n",
      "1/4 Data:0.506 | Batch:0.819 | Total:0:00:00 | ETA:0:00:03 | Loss:104.88658142089844 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:108.1241340637207 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:113.66671244303386 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:00 | Loss:120.89364624023438 | top1:84.25\n",
      "\n",
      "Epoch: [70 | 4000] LR: 0.039998\n",
      "1/4 Data:0.507 | Batch:0.804 | Total:0:00:00 | ETA:0:00:03 | Loss:162.6287078857422 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:173.08499145507812 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:01 | Loss:183.46607971191406 | top1:87.0\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:193.5064353942871 | top1:86.25\n",
      "\n",
      "Epoch: [71 | 4000] LR: 0.039998\n",
      "1/4 Data:0.516 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:241.11512756347656 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:248.80374908447266 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:01 | Loss:255.6127471923828 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:00 | Loss:261.58987045288086 | top1:85.25\n",
      "\n",
      "Epoch: [72 | 4000] LR: 0.039998\n",
      "1/4 Data:0.498 | Batch:0.778 | Total:0:00:00 | ETA:0:00:03 | Loss:287.0245361328125 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:289.4308624267578 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:291.0262858072917 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:296.267333984375 | top1:87.75\n",
      "\n",
      "Epoch: [73 | 4000] LR: 0.039997\n",
      "1/4 Data:0.513 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:356.1636657714844 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.257 | Total:0:00:01 | ETA:0:00:02 | Loss:386.83177185058594 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:420.83953857421875 | top1:87.0\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:456.737060546875 | top1:84.75\n",
      "\n",
      "Epoch: [74 | 4000] LR: 0.039997\n",
      "1/4 Data:0.503 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:639.5000610351562 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:675.3065490722656 | top1:80.5\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:709.0802001953125 | top1:77.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:00 | Loss:740.3463897705078 | top1:79.5\n",
      "\n",
      "Epoch: [75 | 4000] LR: 0.039997\n",
      "1/4 Data:0.527 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:883.2179565429688 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:903.0182189941406 | top1:75.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:919.6281941731771 | top1:79.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:00 | Loss:933.126953125 | top1:81.5\n",
      "\n",
      "Epoch: [76 | 4000] LR: 0.039996\n",
      "1/4 Data:0.507 | Batch:0.784 | Total:0:00:00 | ETA:0:00:03 | Loss:985.4990234375 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:987.15234375 | top1:80.0\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:986.2591756184896 | top1:79.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:982.9602661132812 | top1:81.25\n",
      "\n",
      "Epoch: [77 | 4000] LR: 0.039996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 Data:0.504 | Batch:0.782 | Total:0:00:00 | ETA:0:00:03 | Loss:955.4873657226562 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:943.9866638183594 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:930.9735107421875 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:916.6408996582031 | top1:83.75\n",
      "\n",
      "Epoch: [78 | 4000] LR: 0.039996\n",
      "1/4 Data:0.517 | Batch:0.805 | Total:0:00:00 | ETA:0:00:03 | Loss:838.961181640625 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:820.5375061035156 | top1:87.5\n",
      "3/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:01 | Loss:801.5166625976562 | top1:88.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:782.0586853027344 | top1:87.0\n",
      "\n",
      "Epoch: [79 | 4000] LR: 0.039996\n",
      "1/4 Data:0.518 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:683.1930541992188 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:662.8709411621094 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:642.5758463541666 | top1:88.0\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:622.4666595458984 | top1:87.0\n",
      "\n",
      "Epoch: [80 | 4000] LR: 0.039995\n",
      "1/4 Data:0.492 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:523.0628662109375 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:504.1805114746094 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:485.6295166015625 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:467.60926818847656 | top1:82.25\n",
      "\n",
      "Epoch: [81 | 4000] LR: 0.039995\n",
      "1/4 Data:0.535 | Batch:0.817 | Total:0:00:00 | ETA:0:00:03 | Loss:379.85284423828125 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:363.9446563720703 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:348.60732014973956 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:333.8482971191406 | top1:84.75\n",
      "\n",
      "Epoch: [82 | 4000] LR: 0.039994\n",
      "1/4 Data:0.491 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:263.3431091308594 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:02 | Loss:251.55231475830078 | top1:81.0\n",
      "3/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:01 | Loss:240.91192626953125 | top1:80.0\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:231.26446533203125 | top1:81.0\n",
      "\n",
      "Epoch: [83 | 4000] LR: 0.039994\n",
      "1/4 Data:0.468 | Batch:0.778 | Total:0:00:00 | ETA:0:00:03 | Loss:187.45143127441406 | top1:84.0\n",
      "2/4 Data:0.001 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:180.9780044555664 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:01 | Loss:175.03490702311197 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:169.54304122924805 | top1:83.0\n",
      "\n",
      "Epoch: [84 | 4000] LR: 0.039994\n",
      "1/4 Data:0.505 | Batch:0.816 | Total:0:00:00 | ETA:0:00:03 | Loss:143.96803283691406 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:139.84451293945312 | top1:89.0\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:135.93195597330728 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:132.2458324432373 | top1:85.0\n",
      "\n",
      "Epoch: [85 | 4000] LR: 0.039993\n",
      "1/4 Data:0.512 | Batch:0.808 | Total:0:00:00 | ETA:0:00:03 | Loss:114.58063507080078 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:02 | Loss:111.38739395141602 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:108.3710428873698 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:105.45636749267578 | top1:83.25\n",
      "\n",
      "Epoch: [86 | 4000] LR: 0.039993\n",
      "1/4 Data:0.512 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:91.24028778076172 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:88.60387802124023 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:86.05319468180339 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:00 | Loss:83.58125495910645 | top1:83.5\n",
      "\n",
      "Epoch: [87 | 4000] LR: 0.039992\n",
      "1/4 Data:0.483 | Batch:0.765 | Total:0:00:00 | ETA:0:00:03 | Loss:71.38060760498047 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:02 | Loss:69.15340805053711 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.298 | Total:0:00:01 | ETA:0:00:01 | Loss:66.96822611490886 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.294 | Total:0:00:01 | ETA:0:00:00 | Loss:64.85634136199951 | top1:84.25\n",
      "\n",
      "Epoch: [88 | 4000] LR: 0.039992\n",
      "1/4 Data:0.498 | Batch:0.809 | Total:0:00:00 | ETA:0:00:03 | Loss:54.58976364135742 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:52.61429023742676 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:50.75852076212565 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:48.9714469909668 | top1:87.25\n",
      "\n",
      "Epoch: [89 | 4000] LR: 0.039992\n",
      "1/4 Data:0.546 | Batch:0.840 | Total:0:00:00 | ETA:0:00:03 | Loss:40.41941452026367 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:38.9433479309082 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:01 | Loss:37.52421315511068 | top1:81.0\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:36.16469764709473 | top1:83.5\n",
      "\n",
      "Epoch: [90 | 4000] LR: 0.039991\n",
      "1/4 Data:0.494 | Batch:0.805 | Total:0:00:00 | ETA:0:00:03 | Loss:29.934715270996094 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:28.98000431060791 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:28.093272527058918 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:27.24072027206421 | top1:82.75\n",
      "\n",
      "Epoch: [91 | 4000] LR: 0.039991\n",
      "1/4 Data:0.511 | Batch:0.792 | Total:0:00:00 | ETA:0:00:03 | Loss:23.37735939025879 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:02 | Loss:22.654688835144043 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:22.02372932434082 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:21.42247438430786 | top1:86.0\n",
      "\n",
      "Epoch: [92 | 4000] LR: 0.039990\n",
      "1/4 Data:0.512 | Batch:0.789 | Total:0:00:00 | ETA:0:00:03 | Loss:18.678619384765625 | top1:85.0\n",
      "2/4 Data:0.010 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:18.20703125 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:17.759296417236328 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:00 | Loss:17.33918571472168 | top1:83.0\n",
      "\n",
      "Epoch: [93 | 4000] LR: 0.039990\n",
      "1/4 Data:0.519 | Batch:0.807 | Total:0:00:00 | ETA:0:00:03 | Loss:15.277018547058105 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:14.87306547164917 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:01 | Loss:14.482983271280924 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:14.125107526779175 | top1:83.75\n",
      "\n",
      "Epoch: [94 | 4000] LR: 0.039989\n",
      "1/4 Data:0.519 | Batch:0.828 | Total:0:00:00 | ETA:0:00:03 | Loss:12.27739429473877 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:11.937106609344482 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:11.617986679077148 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:11.286286115646362 | top1:86.5\n",
      "\n",
      "Epoch: [95 | 4000] LR: 0.039989\n",
      "1/4 Data:0.542 | Batch:0.815 | Total:0:00:00 | ETA:0:00:03 | Loss:9.715614318847656 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:9.461548328399658 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:9.203139940897623 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:8.948303461074829 | top1:85.0\n",
      "\n",
      "Epoch: [96 | 4000] LR: 0.039988\n",
      "1/4 Data:0.529 | Batch:0.819 | Total:0:00:00 | ETA:0:00:03 | Loss:7.6459150314331055 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:02 | Loss:7.388745307922363 | top1:88.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 Data:0.009 | Batch:0.281 | Total:0:00:01 | ETA:0:00:01 | Loss:7.19892676671346 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.283 | Total:0:00:01 | ETA:0:00:00 | Loss:7.019128680229187 | top1:84.0\n",
      "\n",
      "Epoch: [97 | 4000] LR: 0.039988\n",
      "1/4 Data:0.532 | Batch:0.823 | Total:0:00:00 | ETA:0:00:03 | Loss:6.059637069702148 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:02 | Loss:5.801584482192993 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.285 | Total:0:00:01 | ETA:0:00:01 | Loss:5.613518397013347 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:00 | Loss:5.42866837978363 | top1:82.25\n",
      "\n",
      "Epoch: [98 | 4000] LR: 0.039987\n",
      "1/4 Data:0.549 | Batch:0.864 | Total:0:00:00 | ETA:0:00:03 | Loss:4.527224540710449 | top1:85.0\n",
      "2/4 Data:0.010 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:4.376650333404541 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:4.244332869847615 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:4.09643816947937 | top1:84.0\n",
      "\n",
      "Epoch: [99 | 4000] LR: 0.039986\n",
      "1/4 Data:0.542 | Batch:0.809 | Total:0:00:00 | ETA:0:00:03 | Loss:3.3552565574645996 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:02 | Loss:3.272726058959961 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:3.6269984245300293 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:4.708593368530273 | top1:87.25\n",
      "\n",
      "Epoch: [100 | 4000] LR: 0.039986\n",
      "1/4 Data:0.535 | Batch:0.813 | Total:0:00:00 | ETA:0:00:03 | Loss:13.137593269348145 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:02 | Loss:16.29156255722046 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:19.613075574239094 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:23.031115770339966 | top1:84.5\n",
      "\n",
      "Epoch: [101 | 4000] LR: 0.039985\n",
      "1/4 Data:0.542 | Batch:0.841 | Total:0:00:00 | ETA:0:00:03 | Loss:45.655521392822266 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.289 | Total:0:00:01 | ETA:0:00:02 | Loss:56.299482345581055 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:01 | Loss:68.81958643595378 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.287 | Total:0:00:01 | ETA:0:00:00 | Loss:82.53450298309326 | top1:83.25\n",
      "153/153 Data:0.000 | Batch:0.090 | Total:0:00:20 | ETA:0:00:00 | Loss:155.06666886915855 | top1:54.374183654785156\n",
      "39/39 Data:0.000 | Batch:0.132 | Total:0:00:08 | ETA:0:00:00 | Loss:154.39673438439002 | top1:93.1153793334961\n",
      "\n",
      "Epoch: [102 | 4000] LR: 0.039985\n",
      "1/4 Data:0.496 | Batch:0.786 | Total:0:00:00 | ETA:0:00:03 | Loss:154.60829162597656 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:169.77994537353516 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:184.5307820638021 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.281 | Total:0:00:01 | ETA:0:00:00 | Loss:198.5206756591797 | top1:83.75\n",
      "\n",
      "Epoch: [103 | 4000] LR: 0.039984\n",
      "1/4 Data:0.483 | Batch:0.783 | Total:0:00:00 | ETA:0:00:03 | Loss:264.0013122558594 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:02 | Loss:274.08343505859375 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:282.9676005045573 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:290.70308685302734 | top1:82.75\n",
      "\n",
      "Epoch: [104 | 4000] LR: 0.039983\n",
      "1/4 Data:0.529 | Batch:0.798 | Total:0:00:00 | ETA:0:00:03 | Loss:323.2948303222656 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:326.5333709716797 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:328.9083658854167 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:00 | Loss:330.43309783935547 | top1:83.75\n",
      "\n",
      "Epoch: [105 | 4000] LR: 0.039983\n",
      "1/4 Data:0.539 | Batch:0.842 | Total:0:00:00 | ETA:0:00:03 | Loss:333.7274169921875 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:332.02838134765625 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:329.6443176269531 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:326.6657409667969 | top1:84.5\n",
      "\n",
      "Epoch: [106 | 4000] LR: 0.039982\n",
      "1/4 Data:0.520 | Batch:0.797 | Total:0:00:00 | ETA:0:00:03 | Loss:309.1652526855469 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:304.2288513183594 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:298.91968790690106 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:293.31907653808594 | top1:84.75\n",
      "\n",
      "Epoch: [107 | 4000] LR: 0.039981\n",
      "1/4 Data:0.510 | Batch:0.798 | Total:0:00:00 | ETA:0:00:03 | Loss:263.985595703125 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:257.5573425292969 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:250.9837188720703 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:00 | Loss:244.36105346679688 | top1:86.25\n",
      "\n",
      "Epoch: [108 | 4000] LR: 0.039981\n",
      "1/4 Data:0.523 | Batch:0.806 | Total:0:00:00 | ETA:0:00:03 | Loss:211.15786743164062 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:204.4867706298828 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:197.92941284179688 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:00 | Loss:191.47246170043945 | top1:85.25\n",
      "\n",
      "Epoch: [109 | 4000] LR: 0.039980\n",
      "1/4 Data:0.530 | Batch:0.844 | Total:0:00:00 | ETA:0:00:03 | Loss:159.60580444335938 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:153.64778900146484 | top1:89.0\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:147.8387705485026 | top1:89.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:142.27354621887207 | top1:86.5\n",
      "\n",
      "Epoch: [110 | 4000] LR: 0.039979\n",
      "1/4 Data:0.521 | Batch:0.809 | Total:0:00:00 | ETA:0:00:03 | Loss:115.0489273071289 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:02 | Loss:110.15448760986328 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:01 | Loss:105.4889144897461 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:00 | Loss:100.9756908416748 | top1:84.75\n",
      "\n",
      "Epoch: [111 | 4000] LR: 0.039979\n",
      "1/4 Data:0.501 | Batch:0.786 | Total:0:00:00 | ETA:0:00:03 | Loss:79.39059448242188 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:75.71899795532227 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:72.17886098225911 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:68.87714290618896 | top1:84.0\n",
      "\n",
      "Epoch: [112 | 4000] LR: 0.039978\n",
      "1/4 Data:0.514 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:52.9957160949707 | top1:87.0\n",
      "2/4 Data:0.011 | Batch:0.264 | Total:0:00:01 | ETA:0:00:02 | Loss:50.395809173583984 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.257 | Total:0:00:01 | ETA:0:00:01 | Loss:47.90223058064779 | top1:81.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:45.56393337249756 | top1:80.5\n",
      "\n",
      "Epoch: [113 | 4000] LR: 0.039977\n",
      "1/4 Data:0.511 | Batch:0.797 | Total:0:00:00 | ETA:0:00:03 | Loss:34.43368911743164 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:32.64473628997803 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:30.948942184448242 | top1:81.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:00 | Loss:29.350114822387695 | top1:81.5\n",
      "\n",
      "Epoch: [114 | 4000] LR: 0.039976\n",
      "1/4 Data:0.476 | Batch:0.766 | Total:0:00:00 | ETA:0:00:03 | Loss:22.07668113708496 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:21.387622833251953 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:20.86656316121419 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:20.50728416442871 | top1:82.5\n",
      "\n",
      "Epoch: [115 | 4000] LR: 0.039976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 Data:0.530 | Batch:0.802 | Total:0:00:00 | ETA:0:00:03 | Loss:19.3432674407959 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:19.44587993621826 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:19.56677182515462 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:19.706826210021973 | top1:85.5\n",
      "\n",
      "Epoch: [116 | 4000] LR: 0.039975\n",
      "1/4 Data:0.504 | Batch:0.786 | Total:0:00:00 | ETA:0:00:03 | Loss:20.455869674682617 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:20.610754013061523 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:20.70205561319987 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:20.792211055755615 | top1:84.5\n",
      "\n",
      "Epoch: [117 | 4000] LR: 0.039974\n",
      "1/4 Data:0.504 | Batch:0.798 | Total:0:00:00 | ETA:0:00:03 | Loss:21.24135971069336 | top1:76.0\n",
      "2/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:02 | Loss:21.19561004638672 | top1:80.5\n",
      "3/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:01 | Loss:21.186016082763672 | top1:79.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:21.110726356506348 | top1:82.0\n",
      "\n",
      "Epoch: [118 | 4000] LR: 0.039973\n",
      "1/4 Data:0.512 | Batch:0.780 | Total:0:00:00 | ETA:0:00:03 | Loss:20.753360748291016 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:20.641130447387695 | top1:81.0\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:20.46523666381836 | top1:81.0\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:20.22109079360962 | top1:83.25\n",
      "\n",
      "Epoch: [119 | 4000] LR: 0.039972\n",
      "1/4 Data:0.519 | Batch:0.799 | Total:0:00:00 | ETA:0:00:03 | Loss:20.414512634277344 | top1:87.0\n",
      "2/4 Data:0.010 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:21.936220169067383 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.303 | Total:0:00:01 | ETA:0:00:01 | Loss:23.9302241007487 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:26.227227210998535 | top1:86.5\n",
      "\n",
      "Epoch: [120 | 4000] LR: 0.039971\n",
      "1/4 Data:0.513 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:38.9534912109375 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:02 | Loss:41.87372970581055 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.285 | Total:0:00:01 | ETA:0:00:01 | Loss:44.68146006266276 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:47.37413692474365 | top1:82.75\n",
      "\n",
      "Epoch: [121 | 4000] LR: 0.039971\n",
      "1/4 Data:0.560 | Batch:0.832 | Total:0:00:00 | ETA:0:00:03 | Loss:59.976715087890625 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.290 | Total:0:00:01 | ETA:0:00:02 | Loss:61.99818801879883 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:01 | Loss:63.70259348551432 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.292 | Total:0:00:01 | ETA:0:00:00 | Loss:66.16642379760742 | top1:86.0\n",
      "\n",
      "Epoch: [122 | 4000] LR: 0.039970\n",
      "1/4 Data:0.522 | Batch:0.813 | Total:0:00:00 | ETA:0:00:03 | Loss:85.63818359375 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:93.40311050415039 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:101.84794616699219 | top1:87.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:110.6374740600586 | top1:86.75\n",
      "\n",
      "Epoch: [123 | 4000] LR: 0.039969\n",
      "1/4 Data:0.525 | Batch:0.797 | Total:0:00:00 | ETA:0:00:03 | Loss:155.16427612304688 | top1:90.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:163.9660186767578 | top1:87.5\n",
      "3/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:01 | Loss:172.32498677571616 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.283 | Total:0:00:01 | ETA:0:00:00 | Loss:180.11885833740234 | top1:87.0\n",
      "\n",
      "Epoch: [124 | 4000] LR: 0.039968\n",
      "1/4 Data:0.573 | Batch:0.873 | Total:0:00:00 | ETA:0:00:03 | Loss:216.42637634277344 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.286 | Total:0:00:01 | ETA:0:00:02 | Loss:222.01958465576172 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.295 | Total:0:00:01 | ETA:0:00:01 | Loss:227.02596028645834 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:231.3798065185547 | top1:82.75\n",
      "\n",
      "Epoch: [125 | 4000] LR: 0.039967\n",
      "1/4 Data:0.492 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:249.6668701171875 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:251.23719787597656 | top1:78.0\n",
      "3/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:01 | Loss:252.16520182291666 | top1:77.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:252.4288444519043 | top1:77.75\n",
      "\n",
      "Epoch: [126 | 4000] LR: 0.039966\n",
      "1/4 Data:0.515 | Batch:0.812 | Total:0:00:00 | ETA:0:00:03 | Loss:250.76132202148438 | top1:87.0\n",
      "2/4 Data:0.011 | Batch:0.294 | Total:0:00:01 | ETA:0:00:02 | Loss:248.78770446777344 | top1:84.0\n",
      "3/4 Data:0.010 | Batch:0.291 | Total:0:00:01 | ETA:0:00:01 | Loss:246.36180623372397 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.293 | Total:0:00:01 | ETA:0:00:00 | Loss:243.48444747924805 | top1:81.75\n",
      "\n",
      "Epoch: [127 | 4000] LR: 0.039965\n",
      "1/4 Data:0.497 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:227.22232055664062 | top1:83.0\n",
      "2/4 Data:0.011 | Batch:0.279 | Total:0:00:01 | ETA:0:00:02 | Loss:223.05775451660156 | top1:81.5\n",
      "3/4 Data:0.010 | Batch:0.288 | Total:0:00:01 | ETA:0:00:01 | Loss:218.62947591145834 | top1:81.0\n",
      "4/4 Data:0.010 | Batch:0.292 | Total:0:00:01 | ETA:0:00:00 | Loss:214.01983261108398 | top1:81.5\n",
      "\n",
      "Epoch: [128 | 4000] LR: 0.039964\n",
      "1/4 Data:0.484 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:190.2357635498047 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.291 | Total:0:00:01 | ETA:0:00:02 | Loss:185.19503784179688 | top1:81.5\n",
      "3/4 Data:0.010 | Batch:0.281 | Total:0:00:01 | ETA:0:00:01 | Loss:180.061279296875 | top1:83.0\n",
      "4/4 Data:0.010 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:174.92917251586914 | top1:84.0\n",
      "\n",
      "Epoch: [129 | 4000] LR: 0.039963\n",
      "1/4 Data:0.520 | Batch:0.805 | Total:0:00:00 | ETA:0:00:03 | Loss:149.399169921875 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:02 | Loss:144.41998291015625 | top1:86.0\n",
      "3/4 Data:0.010 | Batch:0.296 | Total:0:00:01 | ETA:0:00:01 | Loss:139.53700256347656 | top1:85.66667175292969\n",
      "4/4 Data:0.010 | Batch:0.299 | Total:0:00:01 | ETA:0:00:00 | Loss:134.74459838867188 | top1:85.25\n",
      "\n",
      "Epoch: [130 | 4000] LR: 0.039962\n",
      "1/4 Data:0.495 | Batch:0.913 | Total:0:00:00 | ETA:0:00:03 | Loss:111.35160827636719 | top1:90.0\n",
      "2/4 Data:0.019 | Batch:0.494 | Total:0:00:01 | ETA:0:00:02 | Loss:107.0849838256836 | top1:85.5\n",
      "3/4 Data:0.012 | Batch:0.455 | Total:0:00:01 | ETA:0:00:01 | Loss:102.8781026204427 | top1:88.33333587646484\n",
      "4/4 Data:0.021 | Batch:0.484 | Total:0:00:02 | ETA:0:00:00 | Loss:100.62846565246582 | top1:87.0\n",
      "\n",
      "Epoch: [131 | 4000] LR: 0.039962\n",
      "1/4 Data:0.547 | Batch:1.011 | Total:0:00:01 | ETA:0:00:04 | Loss:104.83653259277344 | top1:76.0\n",
      "2/4 Data:0.021 | Batch:0.479 | Total:0:00:01 | ETA:0:00:02 | Loss:114.1553726196289 | top1:80.0\n",
      "3/4 Data:0.021 | Batch:0.476 | Total:0:00:01 | ETA:0:00:01 | Loss:125.18115234375 | top1:81.33333587646484\n",
      "4/4 Data:0.021 | Batch:0.498 | Total:0:00:02 | ETA:0:00:00 | Loss:137.31156539916992 | top1:81.0\n",
      "\n",
      "Epoch: [132 | 4000] LR: 0.039961\n",
      "1/4 Data:0.503 | Batch:0.981 | Total:0:00:01 | ETA:0:00:04 | Loss:202.40484619140625 | top1:83.0\n",
      "2/4 Data:0.021 | Batch:0.459 | Total:0:00:01 | ETA:0:00:02 | Loss:217.7335968017578 | top1:84.5\n",
      "3/4 Data:0.021 | Batch:0.488 | Total:0:00:01 | ETA:0:00:01 | Loss:233.04004923502603 | top1:85.33333587646484\n",
      "4/4 Data:0.021 | Batch:0.474 | Total:0:00:02 | ETA:0:00:00 | Loss:248.00411224365234 | top1:86.0\n",
      "\n",
      "Epoch: [133 | 4000] LR: 0.039960\n",
      "1/4 Data:0.533 | Batch:1.024 | Total:0:00:01 | ETA:0:00:04 | Loss:319.8023681640625 | top1:78.0\n",
      "2/4 Data:0.021 | Batch:0.473 | Total:0:00:01 | ETA:0:00:02 | Loss:331.6287536621094 | top1:80.5\n",
      "3/4 Data:0.019 | Batch:0.494 | Total:0:00:02 | ETA:0:00:01 | Loss:342.2908121744792 | top1:81.66667175292969\n",
      "4/4 Data:0.010 | Batch:0.455 | Total:0:00:02 | ETA:0:00:00 | Loss:351.70250701904297 | top1:82.5\n",
      "\n",
      "Epoch: [134 | 4000] LR: 0.039959\n",
      "1/4 Data:0.551 | Batch:1.015 | Total:0:00:01 | ETA:0:00:04 | Loss:392.3111877441406 | top1:92.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/4 Data:0.021 | Batch:0.497 | Total:0:00:01 | ETA:0:00:02 | Loss:396.7054901123047 | top1:87.5\n",
      "3/4 Data:0.019 | Batch:0.469 | Total:0:00:02 | ETA:0:00:01 | Loss:399.8188171386719 | top1:86.66667175292969\n",
      "4/4 Data:0.021 | Batch:0.495 | Total:0:00:02 | ETA:0:00:00 | Loss:401.746826171875 | top1:86.0\n",
      "\n",
      "Epoch: [135 | 4000] LR: 0.039958\n",
      "1/4 Data:0.515 | Batch:0.973 | Total:0:00:00 | ETA:0:00:03 | Loss:405.928466796875 | top1:87.0\n",
      "2/4 Data:0.022 | Batch:0.497 | Total:0:00:01 | ETA:0:00:02 | Loss:403.7155303955078 | top1:89.5\n",
      "3/4 Data:0.021 | Batch:0.467 | Total:0:00:01 | ETA:0:00:01 | Loss:400.7393086751302 | top1:86.0\n",
      "4/4 Data:0.021 | Batch:0.497 | Total:0:00:02 | ETA:0:00:00 | Loss:396.97764587402344 | top1:85.75\n",
      "\n",
      "Epoch: [136 | 4000] LR: 0.039956\n",
      "1/4 Data:0.537 | Batch:0.974 | Total:0:00:00 | ETA:0:00:03 | Loss:417.10516357421875 | top1:91.0\n",
      "2/4 Data:0.019 | Batch:0.485 | Total:0:00:01 | ETA:0:00:02 | Loss:465.8824157714844 | top1:89.5\n",
      "3/4 Data:0.019 | Batch:0.485 | Total:0:00:01 | ETA:0:00:01 | Loss:529.3879597981771 | top1:86.66667175292969\n",
      "4/4 Data:0.021 | Batch:0.512 | Total:0:00:02 | ETA:0:00:00 | Loss:603.1900939941406 | top1:86.0\n",
      "\n",
      "Epoch: [137 | 4000] LR: 0.039955\n",
      "1/4 Data:0.528 | Batch:0.984 | Total:0:00:01 | ETA:0:00:04 | Loss:1005.434814453125 | top1:89.0\n",
      "2/4 Data:0.019 | Batch:0.496 | Total:0:00:01 | ETA:0:00:02 | Loss:1096.7528076171875 | top1:87.5\n",
      "3/4 Data:0.021 | Batch:0.473 | Total:0:00:01 | ETA:0:00:01 | Loss:1185.9647623697917 | top1:85.0\n",
      "4/4 Data:0.019 | Batch:0.490 | Total:0:00:02 | ETA:0:00:00 | Loss:1271.4907531738281 | top1:84.5\n",
      "\n",
      "Epoch: [138 | 4000] LR: 0.039954\n",
      "1/4 Data:0.533 | Batch:0.983 | Total:0:00:01 | ETA:0:00:04 | Loss:1674.8311767578125 | top1:84.0\n",
      "2/4 Data:0.019 | Batch:0.494 | Total:0:00:01 | ETA:0:00:02 | Loss:1738.5558471679688 | top1:85.5\n",
      "3/4 Data:0.019 | Batch:0.475 | Total:0:00:01 | ETA:0:00:01 | Loss:1795.3356119791667 | top1:84.66667175292969\n",
      "4/4 Data:0.021 | Batch:0.492 | Total:0:00:02 | ETA:0:00:00 | Loss:1844.9723815917969 | top1:85.0\n",
      "\n",
      "Epoch: [139 | 4000] LR: 0.039953\n",
      "1/4 Data:0.520 | Batch:0.981 | Total:0:00:01 | ETA:0:00:04 | Loss:2057.476318359375 | top1:81.0\n",
      "2/4 Data:0.021 | Batch:0.485 | Total:0:00:01 | ETA:0:00:02 | Loss:2078.781982421875 | top1:85.0\n",
      "3/4 Data:0.019 | Batch:0.485 | Total:0:00:01 | ETA:0:00:01 | Loss:2093.5846354166665 | top1:81.66667175292969\n",
      "4/4 Data:0.019 | Batch:0.469 | Total:0:00:02 | ETA:0:00:00 | Loss:2102.1051635742188 | top1:81.75\n",
      "\n",
      "Epoch: [140 | 4000] LR: 0.039952\n",
      "1/4 Data:0.531 | Batch:1.004 | Total:0:00:01 | ETA:0:00:04 | Loss:2115.458984375 | top1:85.0\n",
      "2/4 Data:0.019 | Batch:0.481 | Total:0:00:01 | ETA:0:00:02 | Loss:2101.81689453125 | top1:86.0\n",
      "3/4 Data:0.021 | Batch:0.471 | Total:0:00:01 | ETA:0:00:01 | Loss:2083.7698974609375 | top1:87.33333587646484\n",
      "4/4 Data:0.019 | Batch:0.480 | Total:0:00:02 | ETA:0:00:00 | Loss:2061.7947998046875 | top1:85.25\n",
      "\n",
      "Epoch: [141 | 4000] LR: 0.039951\n",
      "1/4 Data:0.510 | Batch:0.986 | Total:0:00:01 | ETA:0:00:04 | Loss:1934.2679443359375 | top1:82.0\n",
      "2/4 Data:0.019 | Batch:0.483 | Total:0:00:01 | ETA:0:00:02 | Loss:1899.5202026367188 | top1:82.0\n",
      "3/4 Data:0.019 | Batch:0.479 | Total:0:00:01 | ETA:0:00:01 | Loss:1862.5884195963542 | top1:84.66667175292969\n",
      "4/4 Data:0.019 | Batch:0.485 | Total:0:00:02 | ETA:0:00:00 | Loss:1823.974853515625 | top1:83.25\n",
      "\n",
      "Epoch: [142 | 4000] LR: 0.039950\n",
      "1/4 Data:0.548 | Batch:1.028 | Total:0:00:01 | ETA:0:00:04 | Loss:1623.716796875 | top1:88.0\n",
      "2/4 Data:0.019 | Batch:0.470 | Total:0:00:01 | ETA:0:00:02 | Loss:1580.4840698242188 | top1:85.5\n",
      "3/4 Data:0.019 | Batch:0.502 | Total:0:00:02 | ETA:0:00:01 | Loss:1536.8621826171875 | top1:83.66667175292969\n",
      "4/4 Data:0.019 | Batch:0.483 | Total:0:00:02 | ETA:0:00:00 | Loss:1493.0782165527344 | top1:82.0\n",
      "\n",
      "Epoch: [143 | 4000] LR: 0.039949\n",
      "1/4 Data:0.534 | Batch:1.010 | Total:0:00:01 | ETA:0:00:04 | Loss:1274.6278076171875 | top1:78.0\n",
      "2/4 Data:0.012 | Batch:0.479 | Total:0:00:01 | ETA:0:00:02 | Loss:1231.7678833007812 | top1:78.0\n",
      "3/4 Data:0.019 | Batch:0.482 | Total:0:00:01 | ETA:0:00:01 | Loss:1192.721435546875 | top1:78.66667175292969\n",
      "4/4 Data:0.021 | Batch:0.476 | Total:0:00:02 | ETA:0:00:00 | Loss:1159.1011352539062 | top1:79.25\n",
      "\n",
      "Epoch: [144 | 4000] LR: 0.039948\n",
      "1/4 Data:0.541 | Batch:1.031 | Total:0:00:01 | ETA:0:00:04 | Loss:1014.9126586914062 | top1:83.0\n",
      "2/4 Data:0.019 | Batch:0.501 | Total:0:00:01 | ETA:0:00:02 | Loss:998.1345825195312 | top1:81.5\n",
      "3/4 Data:0.021 | Batch:0.481 | Total:0:00:02 | ETA:0:00:01 | Loss:983.6421305338541 | top1:81.66667175292969\n",
      "4/4 Data:0.019 | Batch:0.505 | Total:0:00:02 | ETA:0:00:00 | Loss:970.8256683349609 | top1:82.0\n",
      "\n",
      "Epoch: [145 | 4000] LR: 0.039947\n",
      "1/4 Data:0.560 | Batch:1.021 | Total:0:00:01 | ETA:0:00:04 | Loss:912.5421752929688 | top1:85.0\n",
      "2/4 Data:0.021 | Batch:0.483 | Total:0:00:01 | ETA:0:00:02 | Loss:903.2420654296875 | top1:84.0\n",
      "3/4 Data:0.019 | Batch:0.474 | Total:0:00:01 | ETA:0:00:01 | Loss:893.9679565429688 | top1:81.66667175292969\n",
      "4/4 Data:0.019 | Batch:0.474 | Total:0:00:02 | ETA:0:00:00 | Loss:884.5104370117188 | top1:82.25\n",
      "\n",
      "Epoch: [146 | 4000] LR: 0.039946\n",
      "1/4 Data:0.502 | Batch:0.953 | Total:0:00:00 | ETA:0:00:03 | Loss:835.7666015625 | top1:79.0\n",
      "2/4 Data:0.021 | Batch:0.473 | Total:0:00:01 | ETA:0:00:02 | Loss:824.7984619140625 | top1:84.0\n",
      "3/4 Data:0.019 | Batch:0.470 | Total:0:00:01 | ETA:0:00:01 | Loss:813.3705647786459 | top1:84.0\n",
      "4/4 Data:0.019 | Batch:0.496 | Total:0:00:02 | ETA:0:00:00 | Loss:801.5760498046875 | top1:84.5\n",
      "\n",
      "Epoch: [147 | 4000] LR: 0.039944\n",
      "1/4 Data:0.511 | Batch:0.978 | Total:0:00:00 | ETA:0:00:03 | Loss:741.7665405273438 | top1:76.0\n",
      "2/4 Data:0.020 | Batch:0.488 | Total:0:00:01 | ETA:0:00:02 | Loss:729.2010192871094 | top1:82.0\n",
      "3/4 Data:0.021 | Batch:0.461 | Total:0:00:01 | ETA:0:00:01 | Loss:716.6283162434896 | top1:83.0\n",
      "4/4 Data:0.019 | Batch:0.493 | Total:0:00:02 | ETA:0:00:00 | Loss:703.9450531005859 | top1:82.0\n",
      "\n",
      "Epoch: [148 | 4000] LR: 0.039943\n",
      "1/4 Data:0.546 | Batch:1.009 | Total:0:00:01 | ETA:0:00:04 | Loss:639.5974731445312 | top1:83.0\n",
      "2/4 Data:0.019 | Batch:0.475 | Total:0:00:01 | ETA:0:00:02 | Loss:626.330810546875 | top1:81.0\n",
      "3/4 Data:0.019 | Batch:0.472 | Total:0:00:01 | ETA:0:00:01 | Loss:612.8625691731771 | top1:82.0\n",
      "4/4 Data:0.019 | Batch:0.474 | Total:0:00:02 | ETA:0:00:00 | Loss:599.3043060302734 | top1:82.25\n",
      "\n",
      "Epoch: [149 | 4000] LR: 0.039942\n",
      "1/4 Data:0.511 | Batch:0.985 | Total:0:00:00 | ETA:0:00:03 | Loss:531.1815795898438 | top1:70.0\n",
      "2/4 Data:0.012 | Batch:0.476 | Total:0:00:01 | ETA:0:00:02 | Loss:517.3345794677734 | top1:77.0\n",
      "3/4 Data:0.019 | Batch:0.487 | Total:0:00:01 | ETA:0:00:01 | Loss:503.58973185221356 | top1:77.0\n",
      "4/4 Data:0.019 | Batch:0.484 | Total:0:00:02 | ETA:0:00:00 | Loss:489.91815185546875 | top1:78.5\n",
      "\n",
      "Epoch: [150 | 4000] LR: 0.039941\n",
      "1/4 Data:0.522 | Batch:0.996 | Total:0:00:01 | ETA:0:00:04 | Loss:422.18359375 | top1:73.0\n",
      "2/4 Data:0.019 | Batch:0.471 | Total:0:00:01 | ETA:0:00:02 | Loss:409.0373229980469 | top1:76.5\n",
      "3/4 Data:0.019 | Batch:0.481 | Total:0:00:01 | ETA:0:00:01 | Loss:396.14381917317706 | top1:75.66667175292969\n",
      "4/4 Data:0.019 | Batch:0.469 | Total:0:00:02 | ETA:0:00:00 | Loss:383.4809875488281 | top1:76.75\n",
      "\n",
      "Epoch: [151 | 4000] LR: 0.039940\n",
      "1/4 Data:0.538 | Batch:1.014 | Total:0:00:01 | ETA:0:00:04 | Loss:321.44091796875 | top1:80.0\n",
      "2/4 Data:0.019 | Batch:0.469 | Total:0:00:01 | ETA:0:00:02 | Loss:309.87469482421875 | top1:82.5\n",
      "3/4 Data:0.021 | Batch:0.484 | Total:0:00:01 | ETA:0:00:01 | Loss:298.6487528483073 | top1:82.0\n",
      "4/4 Data:0.019 | Batch:0.486 | Total:0:00:02 | ETA:0:00:00 | Loss:287.75023651123047 | top1:82.0\n",
      "153/153 Data:0.000 | Batch:0.090 | Total:0:00:38 | ETA:0:00:00 | Loss:235.36531508859457 | top1:55.9076042175293\n",
      "39/39 Data:0.000 | Batch:0.132 | Total:0:00:08 | ETA:0:00:00 | Loss:234.75419577574118 | top1:96.73076629638672\n",
      "\n",
      "Epoch: [152 | 4000] LR: 0.039938\n",
      "1/4 Data:0.518 | Batch:0.801 | Total:0:00:00 | ETA:0:00:03 | Loss:235.0081024169922 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:02 | Loss:225.50237274169922 | top1:80.0\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:216.3341267903646 | top1:80.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:207.51243209838867 | top1:81.75\n",
      "\n",
      "Epoch: [153 | 4000] LR: 0.039937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 Data:0.522 | Batch:0.799 | Total:0:00:00 | ETA:0:00:03 | Loss:165.2030487060547 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:157.8375473022461 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:150.77782185872397 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:00 | Loss:144.08459091186523 | top1:82.25\n",
      "\n",
      "Epoch: [154 | 4000] LR: 0.039936\n",
      "1/4 Data:0.527 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:111.98147583007812 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:02 | Loss:106.52997970581055 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:01 | Loss:101.42574564615886 | top1:79.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:00 | Loss:96.51642417907715 | top1:79.5\n",
      "\n",
      "Epoch: [155 | 4000] LR: 0.039935\n",
      "1/4 Data:0.510 | Batch:0.814 | Total:0:00:00 | ETA:0:00:03 | Loss:73.514892578125 | top1:71.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:69.50379943847656 | top1:77.0\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:65.84220886230469 | top1:79.0\n",
      "4/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:00 | Loss:62.44102668762207 | top1:77.5\n",
      "\n",
      "Epoch: [156 | 4000] LR: 0.039933\n",
      "1/4 Data:0.507 | Batch:0.783 | Total:0:00:00 | ETA:0:00:03 | Loss:46.35165786743164 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:02 | Loss:43.75327110290527 | top1:77.5\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:41.27672449747721 | top1:79.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:38.972105979919434 | top1:80.75\n",
      "\n",
      "Epoch: [157 | 4000] LR: 0.039932\n",
      "1/4 Data:0.512 | Batch:0.809 | Total:0:00:00 | ETA:0:00:03 | Loss:28.36661720275879 | top1:74.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:26.631023406982422 | top1:77.0\n",
      "3/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:01 | Loss:25.034138361612957 | top1:79.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:23.569252014160156 | top1:80.75\n",
      "\n",
      "Epoch: [158 | 4000] LR: 0.039931\n",
      "1/4 Data:0.522 | Batch:0.822 | Total:0:00:00 | ETA:0:00:03 | Loss:16.850833892822266 | top1:76.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:15.737898349761963 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:01 | Loss:14.757420539855957 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:00 | Loss:13.901532888412476 | top1:83.25\n",
      "\n",
      "Epoch: [159 | 4000] LR: 0.039929\n",
      "1/4 Data:0.493 | Batch:0.782 | Total:0:00:00 | ETA:0:00:03 | Loss:9.919160842895508 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:9.366791725158691 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:8.870869477589926 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:00 | Loss:16.136099457740784 | top1:82.5\n",
      "\n",
      "Epoch: [160 | 4000] LR: 0.039928\n",
      "1/4 Data:0.499 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:117.02719116210938 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:02 | Loss:173.0573501586914 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:01 | Loss:235.82819112141928 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:00 | Loss:302.72612380981445 | top1:83.25\n",
      "\n",
      "Epoch: [161 | 4000] LR: 0.039927\n",
      "1/4 Data:0.520 | Batch:0.804 | Total:0:00:00 | ETA:0:00:03 | Loss:647.1486206054688 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:716.7930603027344 | top1:81.0\n",
      "3/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:01 | Loss:783.4937133789062 | top1:81.0\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:846.3755035400391 | top1:81.25\n",
      "\n",
      "Epoch: [162 | 4000] LR: 0.039925\n",
      "1/4 Data:0.569 | Batch:0.841 | Total:0:00:00 | ETA:0:00:03 | Loss:1138.93603515625 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:1183.1605834960938 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.288 | Total:0:00:01 | ETA:0:00:01 | Loss:1222.1046142578125 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:1255.7190856933594 | top1:83.5\n",
      "\n",
      "Epoch: [163 | 4000] LR: 0.039924\n",
      "1/4 Data:0.549 | Batch:0.843 | Total:0:00:00 | ETA:0:00:03 | Loss:1397.681884765625 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:1410.8309326171875 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:01 | Loss:1419.3900960286458 | top1:89.0\n",
      "4/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:00 | Loss:1423.6722106933594 | top1:88.0\n",
      "\n",
      "Epoch: [164 | 4000] LR: 0.039923\n",
      "1/4 Data:0.508 | Batch:0.815 | Total:0:00:00 | ETA:0:00:03 | Loss:1425.0625 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:02 | Loss:1414.302001953125 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:01 | Loss:1400.6213785807292 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:1384.3466491699219 | top1:81.5\n",
      "\n",
      "Epoch: [165 | 4000] LR: 0.039921\n",
      "1/4 Data:0.543 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:1572.479248046875 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:1912.8336181640625 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:01 | Loss:2352.6806640625 | top1:79.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:00 | Loss:2862.8199462890625 | top1:81.25\n",
      "\n",
      "Epoch: [166 | 4000] LR: 0.039920\n",
      "1/4 Data:0.532 | Batch:0.827 | Total:0:00:00 | ETA:0:00:03 | Loss:5644.3349609375 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:6278.214111328125 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:6898.909016927083 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:7495.9500732421875 | top1:86.0\n",
      "\n",
      "Epoch: [167 | 4000] LR: 0.039918\n",
      "1/4 Data:0.532 | Batch:0.818 | Total:0:00:00 | ETA:0:00:03 | Loss:10323.0439453125 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:10775.94677734375 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:11181.676106770834 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:11539.021484375 | top1:82.5\n",
      "\n",
      "Epoch: [168 | 4000] LR: 0.039917\n",
      "1/4 Data:0.542 | Batch:0.819 | Total:0:00:00 | ETA:0:00:03 | Loss:13083.0673828125 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:13248.35791015625 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:01 | Loss:13368.824869791666 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:00 | Loss:13446.8505859375 | top1:81.75\n",
      "\n",
      "Epoch: [169 | 4000] LR: 0.039916\n",
      "1/4 Data:0.514 | Batch:0.810 | Total:0:00:00 | ETA:0:00:03 | Loss:13637.955078125 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:13565.2529296875 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:13462.4462890625 | top1:86.0\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:13332.517333984375 | top1:86.25\n",
      "\n",
      "Epoch: [170 | 4000] LR: 0.039914\n",
      "1/4 Data:0.529 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:12562.1923828125 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:02 | Loss:12344.4423828125 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.281 | Total:0:00:01 | ETA:0:00:01 | Loss:12111.955729166666 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:00 | Loss:11867.185302734375 | top1:85.25\n",
      "\n",
      "Epoch: [171 | 4000] LR: 0.039913\n",
      "1/4 Data:0.512 | Batch:0.806 | Total:0:00:00 | ETA:0:00:03 | Loss:10593.806640625 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:02 | Loss:10315.80908203125 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:01 | Loss:10034.80859375 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:00 | Loss:9752.3369140625 | top1:85.0\n",
      "\n",
      "Epoch: [172 | 4000] LR: 0.039911\n",
      "1/4 Data:0.533 | Batch:0.816 | Total:0:00:00 | ETA:0:00:03 | Loss:8341.2353515625 | top1:81.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:8064.005615234375 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.280 | Total:0:00:01 | ETA:0:00:01 | Loss:7791.223307291667 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:7523.60302734375 | top1:80.5\n",
      "\n",
      "Epoch: [173 | 4000] LR: 0.039910\n",
      "1/4 Data:0.537 | Batch:0.833 | Total:0:00:00 | ETA:0:00:03 | Loss:6214.6572265625 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:02 | Loss:5972.024169921875 | top1:80.0\n",
      "3/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:01 | Loss:5736.880533854167 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:00 | Loss:5509.4971923828125 | top1:83.75\n",
      "\n",
      "Epoch: [174 | 4000] LR: 0.039908\n",
      "1/4 Data:0.473 | Batch:0.776 | Total:0:00:00 | ETA:0:00:03 | Loss:4412.23876953125 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:02 | Loss:4216.91455078125 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:4029.7461751302085 | top1:81.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:00 | Loss:3850.6719360351562 | top1:83.75\n",
      "\n",
      "Epoch: [175 | 4000] LR: 0.039907\n",
      "1/4 Data:0.526 | Batch:0.796 | Total:0:00:00 | ETA:0:00:03 | Loss:2995.37109375 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:02 | Loss:2847.947998046875 | top1:88.0\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:2708.028564453125 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:00 | Loss:2575.4232788085938 | top1:86.0\n",
      "\n",
      "Epoch: [176 | 4000] LR: 0.039905\n",
      "1/4 Data:0.521 | Batch:0.799 | Total:0:00:00 | ETA:0:00:03 | Loss:1948.1097412109375 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:1843.32275390625 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:1744.7112223307292 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:00 | Loss:1652.0635375976562 | top1:84.0\n",
      "\n",
      "Epoch: [177 | 4000] LR: 0.039904\n",
      "1/4 Data:0.496 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:1217.2093505859375 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:1146.3272705078125 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:1080.1432495117188 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:1018.4409942626953 | top1:85.0\n",
      "\n",
      "Epoch: [178 | 4000] LR: 0.039902\n",
      "1/4 Data:0.500 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:730.996337890625 | top1:76.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:685.2279663085938 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:01 | Loss:642.8614095052084 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:603.615348815918 | top1:82.0\n",
      "\n",
      "Epoch: [179 | 4000] LR: 0.039901\n",
      "1/4 Data:0.530 | Batch:0.817 | Total:0:00:00 | ETA:0:00:03 | Loss:421.89202880859375 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:393.6805725097656 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:01 | Loss:369.45253499348956 | top1:81.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:349.85680389404297 | top1:80.75\n",
      "\n",
      "Epoch: [180 | 4000] LR: 0.039899\n",
      "1/4 Data:0.502 | Batch:0.810 | Total:0:00:00 | ETA:0:00:03 | Loss:272.3013916015625 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:02 | Loss:267.03932189941406 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:263.7908020019531 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:262.08056640625 | top1:83.25\n",
      "\n",
      "Epoch: [181 | 4000] LR: 0.039897\n",
      "1/4 Data:0.494 | Batch:0.789 | Total:0:00:00 | ETA:0:00:03 | Loss:259.2574462890625 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:02 | Loss:261.135009765625 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.251 | Total:0:00:01 | ETA:0:00:01 | Loss:263.2277119954427 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:265.3335876464844 | top1:82.25\n",
      "\n",
      "Epoch: [182 | 4000] LR: 0.039896\n",
      "1/4 Data:0.506 | Batch:0.804 | Total:0:00:00 | ETA:0:00:03 | Loss:275.1531066894531 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:276.41868591308594 | top1:80.5\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:277.2922058105469 | top1:81.0\n",
      "4/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:00 | Loss:277.6791305541992 | top1:83.5\n",
      "\n",
      "Epoch: [183 | 4000] LR: 0.039894\n",
      "1/4 Data:0.525 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:277.4989318847656 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:02 | Loss:276.05210876464844 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:01 | Loss:274.1765848795573 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:271.87870025634766 | top1:83.25\n",
      "\n",
      "Epoch: [184 | 4000] LR: 0.039893\n",
      "1/4 Data:0.515 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:260.0102233886719 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:02 | Loss:258.4666748046875 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:258.0421447753906 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:259.0696792602539 | top1:86.0\n",
      "\n",
      "Epoch: [185 | 4000] LR: 0.039891\n",
      "1/4 Data:0.502 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:270.001953125 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:02 | Loss:274.5767517089844 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:01 | Loss:279.3394470214844 | top1:78.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.250 | Total:0:00:01 | ETA:0:00:00 | Loss:283.9716262817383 | top1:81.5\n",
      "\n",
      "Epoch: [186 | 4000] LR: 0.039889\n",
      "1/4 Data:0.526 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:306.61004638671875 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:310.2163848876953 | top1:79.0\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:313.36468505859375 | top1:79.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:316.92359161376953 | top1:81.0\n",
      "\n",
      "Epoch: [187 | 4000] LR: 0.039888\n",
      "1/4 Data:0.508 | Batch:0.792 | Total:0:00:00 | ETA:0:00:03 | Loss:344.8800964355469 | top1:79.0\n",
      "2/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:02 | Loss:358.53782653808594 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:01 | Loss:374.1868591308594 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:390.99356842041016 | top1:83.25\n",
      "\n",
      "Epoch: [188 | 4000] LR: 0.039886\n",
      "1/4 Data:0.519 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:477.5601806640625 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:494.871826171875 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:01 | Loss:511.1423746744792 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:00 | Loss:526.1417236328125 | top1:84.75\n",
      "\n",
      "Epoch: [189 | 4000] LR: 0.039884\n",
      "1/4 Data:0.508 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:593.8626098632812 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:02 | Loss:602.6453552246094 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:609.7144775390625 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:00 | Loss:615.0846710205078 | top1:84.25\n",
      "\n",
      "Epoch: [190 | 4000] LR: 0.039883\n",
      "1/4 Data:0.512 | Batch:0.784 | Total:0:00:00 | ETA:0:00:03 | Loss:633.4273681640625 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:632.6955261230469 | top1:88.0\n",
      "3/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:01 | Loss:630.9759928385416 | top1:86.0\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:628.2907867431641 | top1:84.5\n",
      "\n",
      "Epoch: [191 | 4000] LR: 0.039881\n",
      "1/4 Data:0.500 | Batch:0.791 | Total:0:00:00 | ETA:0:00:03 | Loss:609.8241577148438 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:603.3674621582031 | top1:85.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:596.1337890625 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:00 | Loss:588.1134796142578 | top1:84.5\n",
      "\n",
      "Epoch: [192 | 4000] LR: 0.039879\n",
      "1/4 Data:0.528 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:544.7694702148438 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:534.4000854492188 | top1:75.5\n",
      "3/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:01 | Loss:523.5197245279948 | top1:77.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:512.3141632080078 | top1:80.0\n",
      "\n",
      "Epoch: [193 | 4000] LR: 0.039877\n",
      "1/4 Data:0.522 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:455.11572265625 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:443.007568359375 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:01 | Loss:430.9475504557292 | top1:80.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:418.8704528808594 | top1:79.75\n",
      "\n",
      "Epoch: [194 | 4000] LR: 0.039876\n",
      "1/4 Data:0.514 | Batch:0.778 | Total:0:00:00 | ETA:0:00:03 | Loss:358.7038879394531 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:346.89476013183594 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:01 | Loss:335.3928934733073 | top1:81.0\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:324.04600524902344 | top1:81.0\n",
      "\n",
      "Epoch: [195 | 4000] LR: 0.039874\n",
      "1/4 Data:0.498 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:268.5903015136719 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:258.33660888671875 | top1:79.5\n",
      "3/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:01 | Loss:248.33685302734375 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:00 | Loss:238.6806640625 | top1:82.25\n",
      "\n",
      "Epoch: [196 | 4000] LR: 0.039872\n",
      "1/4 Data:0.500 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:192.05030822753906 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:02 | Loss:183.73795318603516 | top1:86.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:175.8366241455078 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:168.39710998535156 | top1:86.0\n",
      "\n",
      "Epoch: [197 | 4000] LR: 0.039870\n",
      "1/4 Data:0.516 | Batch:0.800 | Total:0:00:00 | ETA:0:00:03 | Loss:133.37738037109375 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:127.61550521850586 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:01 | Loss:122.1832504272461 | top1:83.0\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:117.10181617736816 | top1:84.25\n",
      "\n",
      "Epoch: [198 | 4000] LR: 0.039869\n",
      "1/4 Data:0.541 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:93.72159576416016 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:90.17720031738281 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:01 | Loss:86.85092163085938 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:00 | Loss:83.81059074401855 | top1:84.25\n",
      "\n",
      "Epoch: [199 | 4000] LR: 0.039867\n",
      "1/4 Data:0.491 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:69.61832427978516 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:67.28105926513672 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:65.12435150146484 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:63.06448745727539 | top1:84.75\n",
      "\n",
      "Epoch: [200 | 4000] LR: 0.039865\n",
      "1/4 Data:0.567 | Batch:0.841 | Total:0:00:00 | ETA:0:00:03 | Loss:53.32490921020508 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:51.67173194885254 | top1:88.0\n",
      "3/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:01 | Loss:50.10566329956055 | top1:86.0\n",
      "4/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:00 | Loss:48.585185050964355 | top1:86.5\n",
      "\n",
      "Epoch: [201 | 4000] LR: 0.039863\n",
      "1/4 Data:0.514 | Batch:0.788 | Total:0:00:00 | ETA:0:00:03 | Loss:41.230857849121094 | top1:90.0\n",
      "2/4 Data:0.009 | Batch:0.260 | Total:0:00:01 | ETA:0:00:02 | Loss:39.98349380493164 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:38.72808074951172 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:37.5436954498291 | top1:83.5\n",
      "153/153 Data:0.000 | Batch:0.091 | Total:0:00:20 | ETA:0:00:00 | Loss:32.23414913292481 | top1:54.1513786315918\n",
      "39/39 Data:0.000 | Batch:0.132 | Total:0:00:08 | ETA:0:00:00 | Loss:31.499721918350613 | top1:93.6153793334961\n",
      "\n",
      "Epoch: [202 | 4000] LR: 0.039861\n",
      "1/4 Data:0.487 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:31.75509262084961 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:30.62708568572998 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:29.599892298380535 | top1:85.0\n",
      "4/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:00 | Loss:28.651499271392822 | top1:82.75\n",
      "\n",
      "Epoch: [203 | 4000] LR: 0.039860\n",
      "1/4 Data:0.520 | Batch:0.798 | Total:0:00:00 | ETA:0:00:03 | Loss:23.845258712768555 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:22.954893112182617 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:01 | Loss:22.14723078409831 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:21.36562442779541 | top1:85.25\n",
      "\n",
      "Epoch: [204 | 4000] LR: 0.039858\n",
      "1/4 Data:0.533 | Batch:0.805 | Total:0:00:00 | ETA:0:00:03 | Loss:17.691726684570312 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:17.089346885681152 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.250 | Total:0:00:01 | ETA:0:00:01 | Loss:16.50624879201253 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:00 | Loss:15.948776960372925 | top1:85.25\n",
      "\n",
      "Epoch: [205 | 4000] LR: 0.039856\n",
      "1/4 Data:0.489 | Batch:0.782 | Total:0:00:00 | ETA:0:00:03 | Loss:13.331788063049316 | top1:88.0\n",
      "2/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:02 | Loss:12.849913120269775 | top1:92.0\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:19.007847785949707 | top1:89.0\n",
      "4/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:00 | Loss:34.6165235042572 | top1:88.5\n",
      "\n",
      "Epoch: [206 | 4000] LR: 0.039854\n",
      "1/4 Data:0.489 | Batch:0.772 | Total:0:00:00 | ETA:0:00:03 | Loss:163.87655639648438 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:02 | Loss:225.91830444335938 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:299.88318888346356 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:381.90137481689453 | top1:80.0\n",
      "\n",
      "Epoch: [207 | 4000] LR: 0.039852\n",
      "1/4 Data:0.512 | Batch:0.787 | Total:0:00:00 | ETA:0:00:03 | Loss:815.6555786132812 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:909.5957336425781 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:1001.2409057617188 | top1:84.0\n",
      "4/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:00 | Loss:1089.2190399169922 | top1:84.5\n",
      "\n",
      "Epoch: [208 | 4000] LR: 0.039850\n",
      "1/4 Data:0.506 | Batch:0.780 | Total:0:00:00 | ETA:0:00:03 | Loss:1505.198486328125 | top1:81.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:1571.4601440429688 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:01 | Loss:1630.7773030598958 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:1682.9599304199219 | top1:84.0\n",
      "\n",
      "Epoch: [209 | 4000] LR: 0.039848\n",
      "1/4 Data:0.513 | Batch:0.784 | Total:0:00:00 | ETA:0:00:03 | Loss:1908.240478515625 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.258 | Total:0:00:01 | ETA:0:00:02 | Loss:1932.2142944335938 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:1949.6666259765625 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.251 | Total:0:00:01 | ETA:0:00:00 | Loss:1960.8929138183594 | top1:85.25\n",
      "\n",
      "Epoch: [210 | 4000] LR: 0.039846\n",
      "1/4 Data:0.485 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:1988.088623046875 | top1:81.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:02 | Loss:1977.29833984375 | top1:84.0\n",
      "3/4 Data:0.009 | Batch:0.251 | Total:0:00:01 | ETA:0:00:01 | Loss:1962.1908772786458 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:00 | Loss:1943.1628723144531 | top1:82.0\n",
      "\n",
      "Epoch: [211 | 4000] LR: 0.039844\n",
      "1/4 Data:0.493 | Batch:0.763 | Total:0:00:00 | ETA:0:00:03 | Loss:1830.245849609375 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.254 | Total:0:00:01 | ETA:0:00:02 | Loss:1798.4346923828125 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:01 | Loss:1764.4986979166667 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:1728.7530212402344 | top1:84.0\n",
      "\n",
      "Epoch: [212 | 4000] LR: 0.039842\n",
      "1/4 Data:0.490 | Batch:0.775 | Total:0:00:00 | ETA:0:00:03 | Loss:1542.8916015625 | top1:89.0\n",
      "2/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:02 | Loss:1502.4141235351562 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:01 | Loss:1461.451416015625 | top1:84.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:1420.2933044433594 | top1:83.75\n",
      "\n",
      "Epoch: [213 | 4000] LR: 0.039840\n",
      "1/4 Data:0.542 | Batch:0.834 | Total:0:00:00 | ETA:0:00:03 | Loss:1214.7275390625 | top1:76.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:1174.1962890625 | top1:75.5\n",
      "3/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:01 | Loss:1134.2591145833333 | top1:77.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:00 | Loss:1095.0636596679688 | top1:79.25\n",
      "\n",
      "Epoch: [214 | 4000] LR: 0.039838\n",
      "1/4 Data:0.541 | Batch:0.819 | Total:0:00:00 | ETA:0:00:03 | Loss:903.2947998046875 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:02 | Loss:867.7511291503906 | top1:80.0\n",
      "3/4 Data:0.009 | Batch:0.281 | Total:0:00:01 | ETA:0:00:01 | Loss:833.3046875 | top1:80.0\n",
      "4/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:00 | Loss:799.9773559570312 | top1:80.0\n",
      "\n",
      "Epoch: [215 | 4000] LR: 0.039836\n",
      "1/4 Data:0.488 | Batch:0.786 | Total:0:00:00 | ETA:0:00:03 | Loss:639.2052001953125 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:610.5741271972656 | top1:79.5\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:583.1389770507812 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:00 | Loss:556.9344482421875 | top1:82.75\n",
      "\n",
      "Epoch: [216 | 4000] LR: 0.039834\n",
      "1/4 Data:0.498 | Batch:0.799 | Total:0:00:00 | ETA:0:00:03 | Loss:431.78790283203125 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:410.24530029296875 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.284 | Total:0:00:01 | ETA:0:00:01 | Loss:389.83750406901044 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:370.48463439941406 | top1:83.75\n",
      "\n",
      "Epoch: [217 | 4000] LR: 0.039832\n",
      "1/4 Data:0.530 | Batch:0.816 | Total:0:00:00 | ETA:0:00:03 | Loss:279.00518798828125 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:263.8441848754883 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:01 | Loss:249.63741048177084 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:236.32742309570312 | top1:86.75\n",
      "\n",
      "Epoch: [218 | 4000] LR: 0.039830\n",
      "1/4 Data:0.528 | Batch:0.803 | Total:0:00:00 | ETA:0:00:03 | Loss:173.99319458007812 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:163.88038635253906 | top1:87.0\n",
      "3/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:01 | Loss:154.60741678873697 | top1:86.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.273 | Total:0:00:01 | ETA:0:00:00 | Loss:146.09083366394043 | top1:84.75\n",
      "\n",
      "Epoch: [219 | 4000] LR: 0.039828\n",
      "1/4 Data:0.506 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:106.90728759765625 | top1:85.0\n",
      "2/4 Data:0.010 | Batch:0.267 | Total:0:00:01 | ETA:0:00:02 | Loss:100.9981689453125 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:95.57326253255208 | top1:84.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:00 | Loss:90.6396656036377 | top1:82.75\n",
      "\n",
      "Epoch: [220 | 4000] LR: 0.039826\n",
      "1/4 Data:0.524 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:67.90021514892578 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:64.45085144042969 | top1:85.0\n",
      "3/4 Data:0.009 | Batch:0.283 | Total:0:00:01 | ETA:0:00:01 | Loss:61.31950378417969 | top1:83.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:00 | Loss:58.43569374084473 | top1:81.75\n",
      "\n",
      "Epoch: [221 | 4000] LR: 0.039824\n",
      "1/4 Data:0.526 | Batch:0.800 | Total:0:00:00 | ETA:0:00:03 | Loss:45.0649299621582 | top1:86.0\n",
      "2/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:02 | Loss:43.050357818603516 | top1:84.5\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:41.191670735677086 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.251 | Total:0:00:01 | ETA:0:00:00 | Loss:39.463948249816895 | top1:87.25\n",
      "\n",
      "Epoch: [222 | 4000] LR: 0.039822\n",
      "1/4 Data:0.493 | Batch:0.783 | Total:0:00:00 | ETA:0:00:03 | Loss:31.627872467041016 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:02 | Loss:30.39631748199463 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:01 | Loss:29.2596378326416 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:00 | Loss:28.219877243041992 | top1:80.25\n",
      "\n",
      "Epoch: [223 | 4000] LR: 0.039820\n",
      "1/4 Data:0.491 | Batch:0.774 | Total:0:00:00 | ETA:0:00:03 | Loss:23.150257110595703 | top1:93.0\n",
      "2/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:02 | Loss:22.3778018951416 | top1:88.0\n",
      "3/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:01 | Loss:21.640357971191406 | top1:87.0\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:20.921635627746582 | top1:86.0\n",
      "\n",
      "Epoch: [224 | 4000] LR: 0.039818\n",
      "1/4 Data:0.492 | Batch:0.781 | Total:0:00:00 | ETA:0:00:03 | Loss:17.679780960083008 | top1:85.0\n",
      "2/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:02 | Loss:17.390931129455566 | top1:83.0\n",
      "3/4 Data:0.007 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:17.154888153076172 | top1:82.0\n",
      "4/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:00 | Loss:16.979520320892334 | top1:82.5\n",
      "\n",
      "Epoch: [225 | 4000] LR: 0.039816\n",
      "1/4 Data:0.528 | Batch:0.812 | Total:0:00:00 | ETA:0:00:03 | Loss:16.361961364746094 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.282 | Total:0:00:01 | ETA:0:00:02 | Loss:16.396503448486328 | top1:79.5\n",
      "3/4 Data:0.009 | Batch:0.274 | Total:0:00:01 | ETA:0:00:01 | Loss:16.412885665893555 | top1:80.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:00 | Loss:16.426912307739258 | top1:81.0\n",
      "\n",
      "Epoch: [226 | 4000] LR: 0.039814\n",
      "1/4 Data:0.522 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:17.198936462402344 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.256 | Total:0:00:01 | ETA:0:00:02 | Loss:18.057982444763184 | top1:85.5\n",
      "3/4 Data:0.009 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:19.670567830403645 | top1:85.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.248 | Total:0:00:01 | ETA:0:00:00 | Loss:22.725921630859375 | top1:83.5\n",
      "\n",
      "Epoch: [227 | 4000] LR: 0.039811\n",
      "1/4 Data:0.494 | Batch:0.748 | Total:0:00:00 | ETA:0:00:03 | Loss:45.98950958251953 | top1:91.0\n",
      "2/4 Data:0.009 | Batch:0.249 | Total:0:00:01 | ETA:0:00:02 | Loss:55.791072845458984 | top1:89.5\n",
      "3/4 Data:0.009 | Batch:0.252 | Total:0:00:01 | ETA:0:00:01 | Loss:67.16280364990234 | top1:85.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.249 | Total:0:00:01 | ETA:0:00:00 | Loss:79.53054618835449 | top1:84.5\n",
      "\n",
      "Epoch: [228 | 4000] LR: 0.039809\n",
      "1/4 Data:0.499 | Batch:0.767 | Total:0:00:00 | ETA:0:00:03 | Loss:144.2825927734375 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.250 | Total:0:00:01 | ETA:0:00:02 | Loss:157.78089904785156 | top1:79.5\n",
      "3/4 Data:0.010 | Batch:0.255 | Total:0:00:01 | ETA:0:00:01 | Loss:170.7525634765625 | top1:81.0\n",
      "4/4 Data:0.009 | Batch:0.249 | Total:0:00:01 | ETA:0:00:00 | Loss:183.0544776916504 | top1:82.0\n",
      "\n",
      "Epoch: [229 | 4000] LR: 0.039807\n",
      "1/4 Data:0.530 | Batch:0.780 | Total:0:00:00 | ETA:0:00:03 | Loss:240.52589416503906 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.250 | Total:0:00:01 | ETA:0:00:02 | Loss:249.28856658935547 | top1:87.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/4 Data:0.009 | Batch:0.251 | Total:0:00:01 | ETA:0:00:01 | Loss:256.9908498128255 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.253 | Total:0:00:01 | ETA:0:00:00 | Loss:263.66682052612305 | top1:84.25\n",
      "\n",
      "Epoch: [230 | 4000] LR: 0.039805\n",
      "1/4 Data:0.504 | Batch:0.784 | Total:0:00:00 | ETA:0:00:03 | Loss:292.5658264160156 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:296.2397003173828 | top1:86.0\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:299.2996419270833 | top1:87.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:00 | Loss:301.7266616821289 | top1:85.5\n",
      "\n",
      "Epoch: [231 | 4000] LR: 0.039803\n",
      "1/4 Data:0.500 | Batch:0.785 | Total:0:00:00 | ETA:0:00:03 | Loss:310.45684814453125 | top1:87.0\n",
      "2/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:02 | Loss:310.21482849121094 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:309.3359680175781 | top1:86.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:307.8626708984375 | top1:85.75\n",
      "\n",
      "Epoch: [232 | 4000] LR: 0.039800\n",
      "1/4 Data:0.515 | Batch:0.795 | Total:0:00:00 | ETA:0:00:03 | Loss:300.9966125488281 | top1:93.0\n",
      "2/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:02 | Loss:301.92120361328125 | top1:88.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:305.2044169108073 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:00 | Loss:312.87606048583984 | top1:80.0\n",
      "\n",
      "Epoch: [233 | 4000] LR: 0.039798\n",
      "1/4 Data:0.519 | Batch:0.790 | Total:0:00:00 | ETA:0:00:03 | Loss:546.6159057617188 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:793.4955139160156 | top1:77.5\n",
      "3/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:01 | Loss:1103.0619506835938 | top1:75.0\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:1456.5919342041016 | top1:75.75\n",
      "\n",
      "Epoch: [234 | 4000] LR: 0.039796\n",
      "1/4 Data:0.512 | Batch:0.788 | Total:0:00:00 | ETA:0:00:03 | Loss:3365.68212890625 | top1:78.0\n",
      "2/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:02 | Loss:3794.009765625 | top1:77.5\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:4213.503255208333 | top1:77.0\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:4617.2989501953125 | top1:76.5\n",
      "\n",
      "Epoch: [235 | 4000] LR: 0.039794\n",
      "1/4 Data:0.514 | Batch:0.798 | Total:0:00:00 | ETA:0:00:03 | Loss:6531.330078125 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:6839.48974609375 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:01 | Loss:7116.391764322917 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:00 | Loss:7361.2315673828125 | top1:82.25\n",
      "\n",
      "Epoch: [236 | 4000] LR: 0.039792\n",
      "1/4 Data:0.530 | Batch:0.801 | Total:0:00:00 | ETA:0:00:03 | Loss:8424.3505859375 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:02 | Loss:8541.69873046875 | top1:83.5\n",
      "3/4 Data:0.009 | Batch:0.270 | Total:0:00:01 | ETA:0:00:01 | Loss:8629.2666015625 | top1:83.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:8688.640380859375 | top1:82.25\n",
      "\n",
      "Epoch: [237 | 4000] LR: 0.039789\n",
      "1/4 Data:0.520 | Batch:0.794 | Total:0:00:00 | ETA:0:00:03 | Loss:8853.3486328125 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:8812.6845703125 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:8752.014973958334 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:00 | Loss:8673.33837890625 | top1:80.75\n",
      "\n",
      "Epoch: [238 | 4000] LR: 0.039787\n",
      "1/4 Data:0.516 | Batch:0.829 | Total:0:00:00 | ETA:0:00:03 | Loss:8199.2685546875 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:8061.9169921875 | top1:79.0\n",
      "3/4 Data:0.009 | Batch:0.275 | Total:0:00:01 | ETA:0:00:01 | Loss:7914.552734375 | top1:79.0\n",
      "4/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:00 | Loss:7758.8765869140625 | top1:78.5\n",
      "\n",
      "Epoch: [239 | 4000] LR: 0.039785\n",
      "1/4 Data:0.522 | Batch:0.797 | Total:0:00:00 | ETA:0:00:03 | Loss:6946.4375 | top1:83.0\n",
      "2/4 Data:0.009 | Batch:0.264 | Total:0:00:01 | ETA:0:00:02 | Loss:6767.85546875 | top1:81.5\n",
      "3/4 Data:0.009 | Batch:0.269 | Total:0:00:01 | ETA:0:00:01 | Loss:6586.9560546875 | top1:83.0\n",
      "4/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:00 | Loss:6404.825927734375 | top1:82.5\n",
      "\n",
      "Epoch: [240 | 4000] LR: 0.039782\n",
      "1/4 Data:0.514 | Batch:0.804 | Total:0:00:00 | ETA:0:00:03 | Loss:5492.9892578125 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:5312.4609375 | top1:82.0\n",
      "3/4 Data:0.009 | Batch:0.276 | Total:0:00:01 | ETA:0:00:01 | Loss:5134.405436197917 | top1:80.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.265 | Total:0:00:01 | ETA:0:00:00 | Loss:4959.3577880859375 | top1:81.0\n",
      "\n",
      "Epoch: [241 | 4000] LR: 0.039780\n",
      "1/4 Data:0.537 | Batch:0.825 | Total:0:00:00 | ETA:0:00:03 | Loss:4101.81298828125 | top1:76.0\n",
      "2/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:02 | Loss:3942.0216064453125 | top1:80.5\n",
      "3/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:01 | Loss:3787.0069173177085 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:00 | Loss:3636.9730834960938 | top1:82.0\n",
      "\n",
      "Epoch: [242 | 4000] LR: 0.039778\n",
      "1/4 Data:0.545 | Batch:0.830 | Total:0:00:00 | ETA:0:00:03 | Loss:2912.28662109375 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:02 | Loss:2782.9964599609375 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:01 | Loss:2659.13330078125 | top1:78.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:00 | Loss:2540.5142211914062 | top1:81.0\n",
      "\n",
      "Epoch: [243 | 4000] LR: 0.039775\n",
      "1/4 Data:0.519 | Batch:0.808 | Total:0:00:00 | ETA:0:00:03 | Loss:1974.156982421875 | top1:75.0\n",
      "2/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:02 | Loss:1876.47119140625 | top1:80.0\n",
      "3/4 Data:0.009 | Batch:0.267 | Total:0:00:01 | ETA:0:00:01 | Loss:1783.8087972005208 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.278 | Total:0:00:01 | ETA:0:00:00 | Loss:1696.0034790039062 | top1:80.5\n",
      "\n",
      "Epoch: [244 | 4000] LR: 0.039773\n",
      "1/4 Data:0.521 | Batch:0.811 | Total:0:00:00 | ETA:0:00:03 | Loss:1280.35107421875 | top1:77.0\n",
      "2/4 Data:0.009 | Batch:0.268 | Total:0:00:01 | ETA:0:00:02 | Loss:1210.735107421875 | top1:81.0\n",
      "3/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:01 | Loss:1145.2676595052083 | top1:80.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.261 | Total:0:00:01 | ETA:0:00:00 | Loss:1083.717300415039 | top1:80.75\n",
      "\n",
      "Epoch: [245 | 4000] LR: 0.039771\n",
      "1/4 Data:0.546 | Batch:0.834 | Total:0:00:00 | ETA:0:00:03 | Loss:794.5418090820312 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:02 | Loss:747.3560485839844 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.272 | Total:0:00:01 | ETA:0:00:01 | Loss:703.3263142903646 | top1:82.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.277 | Total:0:00:01 | ETA:0:00:00 | Loss:662.2397613525391 | top1:82.75\n",
      "\n",
      "Epoch: [246 | 4000] LR: 0.039768\n",
      "1/4 Data:0.522 | Batch:0.813 | Total:0:00:00 | ETA:0:00:03 | Loss:470.7283935546875 | top1:80.0\n",
      "2/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:02 | Loss:440.26853942871094 | top1:83.0\n",
      "3/4 Data:0.009 | Batch:0.271 | Total:0:00:01 | ETA:0:00:01 | Loss:412.0908610026042 | top1:81.33333587646484\n",
      "4/4 Data:0.009 | Batch:0.279 | Total:0:00:01 | ETA:0:00:00 | Loss:385.97667694091797 | top1:80.5\n",
      "\n",
      "Epoch: [247 | 4000] LR: 0.039766\n",
      "1/4 Data:0.528 | Batch:0.812 | Total:0:00:00 | ETA:0:00:03 | Loss:265.0645446777344 | top1:84.0\n",
      "2/4 Data:0.009 | Batch:0.263 | Total:0:00:01 | ETA:0:00:02 | Loss:246.6764907836914 | top1:82.5\n",
      "3/4 Data:0.009 | Batch:0.262 | Total:0:00:01 | ETA:0:00:01 | Loss:230.13329060872397 | top1:82.66667175292969\n",
      "4/4 Data:0.009 | Batch:0.266 | Total:0:00:01 | ETA:0:00:00 | Loss:215.96871185302734 | top1:82.5\n",
      "\n",
      "Epoch: [248 | 4000] LR: 0.039763\n",
      "1/4 Data:0.529 | Batch:0.780 | Total:0:00:00 | ETA:0:00:03 | Loss:159.55816650390625 | top1:82.0\n",
      "2/4 Data:0.009 | Batch:0.259 | Total:0:00:01 | ETA:0:00:02 | Loss:157.01038360595703 | top1:81.5\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    # teacher feedback\n",
    "    if epoch in iter_time:\n",
    "        print(\"iterative training: feedback {}\".format(epoch))\n",
    "        teacher_model.load_state_dict(student_model.state_dict())\n",
    "        teacher_model_weights = {}\n",
    "        for name, param in teacher_model.named_parameters():\n",
    "            teacher_model_weights[name] = param.detach()\n",
    "    \n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, source_train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        test_loss, test_acc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "        source_loss, source_acc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "    \n",
    "        logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc, source_loss, source_acc])\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict' : student_model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
