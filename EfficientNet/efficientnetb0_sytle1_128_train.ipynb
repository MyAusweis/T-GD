{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 400\n",
    "start_epoch = 0\n",
    "train_batch = 300\n",
    "test_batch = 300\n",
    "lr = 0.04\n",
    "schedule = [75, 150, 225]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "#     torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(val_loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,)\n",
    "            bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 400] LR: 0.040000\n",
      "1/230 Data:1.815 | Batch:6.516 | Total:0:00:06 | ETA:0:24:53 | Loss:0.7019357085227966 | top1:50.66666793823242\n",
      "11/230 Data:0.055 | Batch:0.695 | Total:0:00:13 | ETA:0:04:41 | Loss:0.7241052009842612 | top1:49.66666793823242\n",
      "21/230 Data:0.047 | Batch:0.691 | Total:0:00:20 | ETA:0:02:27 | Loss:0.7396161613010225 | top1:50.126983642578125\n",
      "31/230 Data:0.059 | Batch:0.709 | Total:0:00:27 | ETA:0:02:19 | Loss:0.7264233558408676 | top1:50.66666793823242\n",
      "41/230 Data:0.054 | Batch:0.695 | Total:0:00:34 | ETA:0:02:12 | Loss:0.7196704745292664 | top1:51.178863525390625\n",
      "51/230 Data:0.059 | Batch:0.704 | Total:0:00:41 | ETA:0:02:05 | Loss:0.7167434528762219 | top1:51.32025909423828\n",
      "61/230 Data:0.045 | Batch:0.690 | Total:0:00:48 | ETA:0:01:58 | Loss:0.7201931330024219 | top1:51.14754104614258\n",
      "71/230 Data:0.059 | Batch:0.712 | Total:0:00:55 | ETA:0:01:51 | Loss:0.7208243608474731 | top1:51.09859085083008\n",
      "81/230 Data:0.047 | Batch:0.691 | Total:0:01:02 | ETA:0:01:44 | Loss:0.7187617472660395 | top1:51.2716064453125\n",
      "91/230 Data:0.065 | Batch:0.665 | Total:0:01:08 | ETA:0:01:32 | Loss:0.7196496908481305 | top1:51.234432220458984\n",
      "101/230 Data:0.048 | Batch:0.636 | Total:0:01:14 | ETA:0:01:20 | Loss:0.7183767846315214 | top1:51.290428161621094\n",
      "111/230 Data:0.057 | Batch:0.701 | Total:0:01:21 | ETA:0:01:22 | Loss:0.7154034880904464 | top1:51.78378677368164\n",
      "121/230 Data:0.059 | Batch:0.693 | Total:0:01:28 | ETA:0:01:17 | Loss:0.7139412056316029 | top1:51.980716705322266\n",
      "131/230 Data:0.027 | Batch:0.345 | Total:0:01:34 | ETA:0:00:56 | Loss:0.7143598713037622 | top1:52.114505767822266\n",
      "141/230 Data:0.059 | Batch:0.616 | Total:0:01:39 | ETA:0:00:47 | Loss:0.7132665979946758 | top1:52.430259704589844\n",
      "151/230 Data:0.059 | Batch:0.608 | Total:0:01:46 | ETA:0:00:49 | Loss:0.7102179381231598 | top1:52.91170120239258\n",
      "161/230 Data:0.053 | Batch:0.693 | Total:0:01:50 | ETA:0:00:32 | Loss:0.7070851514798514 | top1:53.39751434326172\n",
      "171/230 Data:0.059 | Batch:0.702 | Total:0:01:57 | ETA:0:00:42 | Loss:0.7028393930161906 | top1:54.05458450317383\n",
      "181/230 Data:0.059 | Batch:0.693 | Total:0:02:04 | ETA:0:00:35 | Loss:0.6978170614874823 | top1:54.771636962890625\n",
      "191/230 Data:0.059 | Batch:0.702 | Total:0:02:11 | ETA:0:00:28 | Loss:0.6924028945842963 | top1:55.55671691894531\n",
      "201/230 Data:0.043 | Batch:0.687 | Total:0:02:18 | ETA:0:00:21 | Loss:0.6873486848612923 | top1:56.22719955444336\n",
      "211/230 Data:0.059 | Batch:0.704 | Total:0:02:25 | ETA:0:00:14 | Loss:0.6811404499397459 | top1:57.00157928466797\n",
      "221/230 Data:0.059 | Batch:0.704 | Total:0:02:32 | ETA:0:00:07 | Loss:0.6754024171721342 | top1:57.73152542114258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 Data:0.002 | Batch:0.200 | Total:0:00:06 | ETA:0:00:00 | Loss:0.48304608234992397 | top1:77.20512390136719\n",
      "\n",
      "Epoch: [2 | 400] LR: 0.068000\n",
      "1/230 Data:1.796 | Batch:2.437 | Total:0:00:02 | ETA:0:09:32 | Loss:0.49888381361961365 | top1:73.0\n",
      "11/230 Data:0.059 | Batch:0.702 | Total:0:00:09 | ETA:0:03:13 | Loss:0.4729139344258742 | top1:77.93939208984375\n",
      "21/230 Data:0.056 | Batch:0.703 | Total:0:00:16 | ETA:0:02:26 | Loss:0.4542351123832521 | top1:79.4285659790039\n",
      "31/230 Data:0.059 | Batch:0.690 | Total:0:00:23 | ETA:0:02:20 | Loss:0.44507852100556894 | top1:79.93548583984375\n",
      "41/230 Data:0.033 | Batch:0.607 | Total:0:00:30 | ETA:0:02:10 | Loss:0.4374533135716508 | top1:80.60975646972656\n",
      "51/230 Data:0.051 | Batch:0.612 | Total:0:00:36 | ETA:0:01:58 | Loss:0.43113452897352333 | top1:80.98039245605469\n",
      "61/230 Data:0.059 | Batch:0.532 | Total:0:00:43 | ETA:0:01:50 | Loss:0.42387275226780624 | top1:81.4699478149414\n",
      "71/230 Data:0.045 | Batch:0.647 | Total:0:00:49 | ETA:0:01:38 | Loss:0.41956071786477533 | top1:81.86384582519531\n",
      "81/230 Data:0.055 | Batch:0.632 | Total:0:00:55 | ETA:0:01:32 | Loss:0.4108800269939281 | top1:82.4115219116211\n",
      "91/230 Data:0.036 | Batch:0.685 | Total:0:01:02 | ETA:0:01:33 | Loss:0.40207366727210664 | top1:82.88278198242188\n",
      "101/230 Data:0.059 | Batch:0.697 | Total:0:01:09 | ETA:0:01:31 | Loss:0.39362308146929975 | top1:83.31683349609375\n",
      "111/230 Data:0.041 | Batch:0.567 | Total:0:01:15 | ETA:0:01:19 | Loss:0.38482473616127494 | top1:83.77177429199219\n",
      "121/230 Data:0.063 | Batch:0.699 | Total:0:01:22 | ETA:0:01:08 | Loss:0.3756332404849943 | top1:84.22038269042969\n",
      "131/230 Data:0.050 | Batch:0.671 | Total:0:01:28 | ETA:0:01:06 | Loss:0.36776099291466574 | top1:84.53689575195312\n",
      "141/230 Data:0.037 | Batch:0.622 | Total:0:01:34 | ETA:0:00:50 | Loss:0.3602339673972299 | top1:84.8699722290039\n",
      "151/230 Data:0.031 | Batch:0.480 | Total:0:01:40 | ETA:0:00:48 | Loss:0.353475751466309 | top1:85.21854400634766\n",
      "161/230 Data:0.044 | Batch:0.537 | Total:0:01:46 | ETA:0:00:41 | Loss:0.346345311829022 | top1:85.55900573730469\n",
      "171/230 Data:0.043 | Batch:0.598 | Total:0:01:52 | ETA:0:00:36 | Loss:0.34061722971542535 | top1:85.8265151977539\n",
      "181/230 Data:0.032 | Batch:0.508 | Total:0:01:57 | ETA:0:00:29 | Loss:0.3335618861637063 | top1:86.16021728515625\n",
      "191/230 Data:0.058 | Batch:0.653 | Total:0:02:03 | ETA:0:00:24 | Loss:0.32604639934307617 | top1:86.49388885498047\n",
      "201/230 Data:0.042 | Batch:0.607 | Total:0:02:09 | ETA:0:00:17 | Loss:0.3187607650584842 | top1:86.83250427246094\n",
      "211/230 Data:0.036 | Batch:0.600 | Total:0:02:16 | ETA:0:00:13 | Loss:0.31187941593016494 | top1:87.15323638916016\n",
      "221/230 Data:0.040 | Batch:0.597 | Total:0:02:22 | ETA:0:00:06 | Loss:0.3049962176052154 | top1:87.47209930419922\n",
      "26/26 Data:0.001 | Batch:0.137 | Total:0:00:20 | ETA:0:00:00 | Loss:0.13459626241372183 | top1:94.87178802490234\n",
      "\n",
      "Epoch: [3 | 400] LR: 0.096000\n",
      "1/230 Data:7.081 | Batch:7.585 | Total:0:00:07 | ETA:0:29:10 | Loss:0.1620403677225113 | top1:93.33333587646484\n",
      "11/230 Data:0.030 | Batch:0.648 | Total:0:00:14 | ETA:0:05:07 | Loss:0.15376928787339816 | top1:94.0\n",
      "21/230 Data:0.060 | Batch:0.618 | Total:0:00:20 | ETA:0:02:13 | Loss:0.1658018586181459 | top1:93.73015594482422\n",
      "31/230 Data:0.042 | Batch:0.547 | Total:0:00:27 | ETA:0:02:03 | Loss:0.159357235075966 | top1:94.27957153320312\n",
      "41/230 Data:0.062 | Batch:0.637 | Total:0:00:33 | ETA:0:02:03 | Loss:0.14711276559931477 | top1:94.73983764648438\n",
      "51/230 Data:0.042 | Batch:0.588 | Total:0:00:39 | ETA:0:01:51 | Loss:0.14121030088441044 | top1:94.87581634521484\n",
      "61/230 Data:0.039 | Batch:0.590 | Total:0:00:45 | ETA:0:01:44 | Loss:0.13614986623164083 | top1:95.04917907714844\n",
      "71/230 Data:0.034 | Batch:0.722 | Total:0:00:52 | ETA:0:01:44 | Loss:0.13171510214746837 | top1:95.18778991699219\n",
      "81/230 Data:0.060 | Batch:0.712 | Total:0:00:59 | ETA:0:01:48 | Loss:0.12851317176296387 | top1:95.33333587646484\n",
      "91/230 Data:0.040 | Batch:0.619 | Total:0:01:06 | ETA:0:01:29 | Loss:0.1244578735268378 | top1:95.4945068359375\n",
      "101/230 Data:0.057 | Batch:0.703 | Total:0:01:11 | ETA:0:01:14 | Loss:0.12193651397777076 | top1:95.57755279541016\n",
      "111/230 Data:0.046 | Batch:0.552 | Total:0:01:17 | ETA:0:01:07 | Loss:0.11816770395448616 | top1:95.6906967163086\n",
      "121/230 Data:0.072 | Batch:0.597 | Total:0:01:23 | ETA:0:01:10 | Loss:0.11560835627731213 | top1:95.78512573242188\n",
      "131/230 Data:0.062 | Batch:0.652 | Total:0:01:29 | ETA:0:00:58 | Loss:0.11260061824822244 | top1:95.88040924072266\n",
      "141/230 Data:0.079 | Batch:0.519 | Total:0:01:34 | ETA:0:00:50 | Loss:0.1101372129950963 | top1:95.9692611694336\n",
      "151/230 Data:0.081 | Batch:0.583 | Total:0:01:40 | ETA:0:00:46 | Loss:0.10806341619779732 | top1:96.03532409667969\n",
      "161/230 Data:0.034 | Batch:0.488 | Total:0:01:46 | ETA:0:00:42 | Loss:0.1055452405295757 | top1:96.13457489013672\n",
      "171/230 Data:0.065 | Batch:0.614 | Total:0:01:53 | ETA:0:00:39 | Loss:0.10345372196003708 | top1:96.21637725830078\n",
      "181/230 Data:0.042 | Batch:0.728 | Total:0:01:59 | ETA:0:00:32 | Loss:0.10091322727336739 | top1:96.31491088867188\n",
      "191/230 Data:0.036 | Batch:0.422 | Total:0:02:05 | ETA:0:00:22 | Loss:0.09937231663946082 | top1:96.37521362304688\n",
      "201/230 Data:0.001 | Batch:0.573 | Total:0:02:11 | ETA:0:00:19 | Loss:0.09704988293201472 | top1:96.47097778320312\n",
      "211/230 Data:0.044 | Batch:0.601 | Total:0:02:17 | ETA:0:00:12 | Loss:0.09457393010892856 | top1:96.56082153320312\n",
      "221/230 Data:0.061 | Batch:0.601 | Total:0:02:22 | ETA:0:00:05 | Loss:0.09246651810589689 | top1:96.64102935791016\n",
      "26/26 Data:0.000 | Batch:0.164 | Total:0:00:21 | ETA:0:00:00 | Loss:0.038966656662523746 | top1:98.75640869140625\n",
      "\n",
      "Epoch: [4 | 400] LR: 0.124000\n",
      "1/230 Data:5.048 | Batch:5.571 | Total:0:00:05 | ETA:0:21:26 | Loss:0.06807687133550644 | top1:97.33333587646484\n",
      "11/230 Data:0.063 | Batch:0.689 | Total:0:00:14 | ETA:0:05:09 | Loss:0.037465020336888054 | top1:98.7272720336914\n",
      "21/230 Data:0.049 | Batch:0.500 | Total:0:00:21 | ETA:0:02:23 | Loss:0.039294647407673654 | top1:98.53968048095703\n",
      "31/230 Data:0.043 | Batch:0.667 | Total:0:00:27 | ETA:0:01:59 | Loss:0.045798379567361644 | top1:98.33333587646484\n",
      "41/230 Data:0.054 | Batch:0.650 | Total:0:00:34 | ETA:0:02:06 | Loss:0.048186465189224335 | top1:98.23577880859375\n",
      "51/230 Data:0.055 | Batch:0.615 | Total:0:00:39 | ETA:0:01:46 | Loss:0.0476633236360024 | top1:98.313720703125\n",
      "61/230 Data:0.028 | Batch:0.508 | Total:0:00:46 | ETA:0:01:44 | Loss:0.04632598361702728 | top1:98.3497314453125\n",
      "71/230 Data:0.034 | Batch:0.608 | Total:0:00:52 | ETA:0:01:35 | Loss:0.045828480286602406 | top1:98.35680389404297\n",
      "81/230 Data:0.038 | Batch:0.600 | Total:0:00:57 | ETA:0:01:25 | Loss:0.0442441231743605 | top1:98.41975402832031\n",
      "91/230 Data:0.062 | Batch:0.661 | Total:0:01:04 | ETA:0:01:29 | Loss:0.04402429529639718 | top1:98.42124938964844\n",
      "101/230 Data:0.001 | Batch:0.490 | Total:0:01:09 | ETA:0:01:16 | Loss:0.043106803873387896 | top1:98.46204376220703\n",
      "111/230 Data:0.064 | Batch:0.505 | Total:0:01:15 | ETA:0:01:11 | Loss:0.0427935299975378 | top1:98.46846771240234\n",
      "121/230 Data:0.053 | Batch:0.611 | Total:0:01:21 | ETA:0:01:04 | Loss:0.04268718818741397 | top1:98.48484802246094\n",
      "131/230 Data:0.067 | Batch:0.618 | Total:0:01:27 | ETA:0:01:01 | Loss:0.041911001836184326 | top1:98.5216293334961\n",
      "141/230 Data:0.032 | Batch:0.526 | Total:0:01:33 | ETA:0:00:51 | Loss:0.042230374952580065 | top1:98.50590515136719\n",
      "151/230 Data:0.069 | Batch:0.577 | Total:0:01:39 | ETA:0:00:47 | Loss:0.04176536449138692 | top1:98.5209732055664\n",
      "161/230 Data:0.025 | Batch:0.356 | Total:0:01:43 | ETA:0:00:33 | Loss:0.04180225159579552 | top1:98.50724792480469\n",
      "171/230 Data:0.031 | Batch:0.394 | Total:0:01:48 | ETA:0:00:30 | Loss:0.04154806602637322 | top1:98.50487518310547\n",
      "181/230 Data:0.019 | Batch:0.417 | Total:0:01:55 | ETA:0:00:31 | Loss:0.04059482784684521 | top1:98.54143524169922\n",
      "191/230 Data:0.038 | Batch:0.474 | Total:0:02:02 | ETA:0:00:27 | Loss:0.04005278165681356 | top1:98.5602035522461\n",
      "201/230 Data:0.033 | Batch:0.384 | Total:0:02:07 | ETA:0:00:15 | Loss:0.03992611160548172 | top1:98.57379913330078\n",
      "211/230 Data:0.053 | Batch:0.571 | Total:0:02:12 | ETA:0:00:11 | Loss:0.03943820252696767 | top1:98.59083557128906\n",
      "221/230 Data:0.027 | Batch:0.340 | Total:0:02:16 | ETA:0:00:04 | Loss:0.039168887373430834 | top1:98.6108627319336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 Data:0.141 | Batch:0.340 | Total:0:00:12 | ETA:0:00:00 | Loss:0.027783757612968866 | top1:99.0\n",
      "\n",
      "Epoch: [5 | 400] LR: 0.152000\n",
      "1/230 Data:2.008 | Batch:2.649 | Total:0:00:02 | ETA:0:10:20 | Loss:0.00872308760881424 | top1:100.0\n",
      "11/230 Data:0.059 | Batch:0.703 | Total:0:00:09 | ETA:0:03:17 | Loss:0.016366861591284924 | top1:99.36363983154297\n",
      "21/230 Data:0.059 | Batch:0.701 | Total:0:00:16 | ETA:0:02:26 | Loss:0.015175748001118856 | top1:99.4285659790039\n",
      "31/230 Data:0.061 | Batch:0.682 | Total:0:00:23 | ETA:0:02:15 | Loss:0.015929729568832103 | top1:99.40859985351562\n",
      "41/230 Data:0.048 | Batch:0.712 | Total:0:00:30 | ETA:0:02:09 | Loss:0.019435636866724164 | top1:99.35772705078125\n",
      "51/230 Data:0.048 | Batch:0.647 | Total:0:00:36 | ETA:0:01:59 | Loss:0.019640898192757925 | top1:99.35294342041016\n",
      "61/230 Data:0.032 | Batch:0.596 | Total:0:00:43 | ETA:0:01:57 | Loss:0.0199364512361067 | top1:99.32787322998047\n",
      "71/230 Data:0.067 | Batch:0.710 | Total:0:00:50 | ETA:0:01:41 | Loss:0.02116798397488105 | top1:99.27699279785156\n",
      "81/230 Data:0.060 | Batch:0.706 | Total:0:00:56 | ETA:0:01:36 | Loss:0.021299206322622426 | top1:99.27571868896484\n",
      "91/230 Data:0.060 | Batch:0.628 | Total:0:01:02 | ETA:0:01:30 | Loss:0.021256776686481477 | top1:99.24908447265625\n",
      "101/230 Data:0.048 | Batch:0.664 | Total:0:01:09 | ETA:0:01:25 | Loss:0.021465516364487756 | top1:99.23432159423828\n",
      "111/230 Data:0.041 | Batch:0.613 | Total:0:01:15 | ETA:0:01:19 | Loss:0.021981556170938856 | top1:99.21321868896484\n",
      "121/230 Data:0.059 | Batch:0.681 | Total:0:01:22 | ETA:0:01:11 | Loss:0.022047006233579917 | top1:99.20661163330078\n",
      "131/230 Data:0.043 | Batch:0.685 | Total:0:01:29 | ETA:0:01:07 | Loss:0.022006765954402624 | top1:99.21119689941406\n",
      "141/230 Data:0.061 | Batch:0.705 | Total:0:01:35 | ETA:0:00:59 | Loss:0.02241910540033132 | top1:99.19621276855469\n",
      "151/230 Data:0.045 | Batch:0.688 | Total:0:01:42 | ETA:0:00:56 | Loss:0.022626652447846007 | top1:99.17218780517578\n",
      "161/230 Data:0.059 | Batch:0.711 | Total:0:01:49 | ETA:0:00:49 | Loss:0.02251574835343784 | top1:99.18219757080078\n",
      "171/230 Data:0.059 | Batch:0.690 | Total:0:01:56 | ETA:0:00:42 | Loss:0.022409208582534472 | top1:99.1890869140625\n",
      "181/230 Data:0.059 | Batch:0.691 | Total:0:02:03 | ETA:0:00:35 | Loss:0.022118654411455916 | top1:99.19705200195312\n",
      "191/230 Data:0.060 | Batch:0.667 | Total:0:02:10 | ETA:0:00:27 | Loss:0.02203340846033406 | top1:99.19720458984375\n",
      "201/230 Data:0.059 | Batch:0.707 | Total:0:02:17 | ETA:0:00:20 | Loss:0.022126228980075994 | top1:99.19900512695312\n",
      "211/230 Data:0.063 | Batch:0.677 | Total:0:02:23 | ETA:0:00:13 | Loss:0.022058809476780985 | top1:99.1943130493164\n",
      "221/230 Data:0.059 | Batch:0.602 | Total:0:02:30 | ETA:0:00:07 | Loss:0.021762871908811994 | top1:99.1990966796875\n",
      "26/26 Data:0.002 | Batch:0.181 | Total:0:00:12 | ETA:0:00:00 | Loss:0.020466331195516083 | top1:99.29486846923828\n",
      "\n",
      "Epoch: [6 | 400] LR: 0.180000\n",
      "1/230 Data:4.159 | Batch:4.725 | Total:0:00:04 | ETA:0:18:21 | Loss:0.008584968745708466 | top1:100.0\n",
      "11/230 Data:0.036 | Batch:0.613 | Total:0:00:11 | ETA:0:03:52 | Loss:0.018328259910710833 | top1:99.57575988769531\n",
      "21/230 Data:0.049 | Batch:0.630 | Total:0:00:17 | ETA:0:02:07 | Loss:0.014765273280707853 | top1:99.57142639160156\n",
      "31/230 Data:0.032 | Batch:0.572 | Total:0:00:23 | ETA:0:02:11 | Loss:0.013550518121900819 | top1:99.54838562011719\n",
      "41/230 Data:0.060 | Batch:0.676 | Total:0:00:30 | ETA:0:02:03 | Loss:0.015735913905678543 | top1:99.49594116210938\n",
      "51/230 Data:0.045 | Batch:0.634 | Total:0:00:36 | ETA:0:01:54 | Loss:0.015817773982645104 | top1:99.50980377197266\n",
      "61/230 Data:0.028 | Batch:0.642 | Total:0:00:42 | ETA:0:01:44 | Loss:0.0174665152396793 | top1:99.45355224609375\n",
      "71/230 Data:0.076 | Batch:0.572 | Total:0:00:48 | ETA:0:01:38 | Loss:0.01823660900363777 | top1:99.41314697265625\n",
      "81/230 Data:0.052 | Batch:0.707 | Total:0:00:55 | ETA:0:01:33 | Loss:0.01786515658094697 | top1:99.43621063232422\n",
      "91/230 Data:0.051 | Batch:0.554 | Total:0:01:01 | ETA:0:01:24 | Loss:0.017376938582766434 | top1:99.45787811279297\n",
      "101/230 Data:0.031 | Batch:0.669 | Total:0:01:07 | ETA:0:01:18 | Loss:0.017597420909903058 | top1:99.4389419555664\n",
      "111/230 Data:0.049 | Batch:0.562 | Total:0:01:13 | ETA:0:01:15 | Loss:0.01721321218143645 | top1:99.46246337890625\n",
      "121/230 Data:0.071 | Batch:0.613 | Total:0:01:19 | ETA:0:01:05 | Loss:0.017899074331926536 | top1:99.42699432373047\n",
      "131/230 Data:0.058 | Batch:0.646 | Total:0:01:25 | ETA:0:01:02 | Loss:0.017808190513160505 | top1:99.40967559814453\n",
      "141/230 Data:0.007 | Batch:0.678 | Total:0:01:32 | ETA:0:00:59 | Loss:0.018470517211058673 | top1:99.39006805419922\n",
      "151/230 Data:0.042 | Batch:0.554 | Total:0:01:38 | ETA:0:00:54 | Loss:0.018666326108431778 | top1:99.37086486816406\n",
      "161/230 Data:0.059 | Batch:0.558 | Total:0:01:44 | ETA:0:00:43 | Loss:0.01872537593120096 | top1:99.36646270751953\n",
      "171/230 Data:0.043 | Batch:0.572 | Total:0:01:50 | ETA:0:00:36 | Loss:0.018536858388620337 | top1:99.36842346191406\n",
      "181/230 Data:0.041 | Batch:0.591 | Total:0:01:56 | ETA:0:00:30 | Loss:0.018584577798925712 | top1:99.37568664550781\n",
      "191/230 Data:0.038 | Batch:0.580 | Total:0:02:02 | ETA:0:00:22 | Loss:0.0182788161407298 | top1:99.38394165039062\n",
      "201/230 Data:0.042 | Batch:0.567 | Total:0:02:08 | ETA:0:00:18 | Loss:0.018334689222285476 | top1:99.38143157958984\n",
      "211/230 Data:0.043 | Batch:0.691 | Total:0:02:14 | ETA:0:00:12 | Loss:0.018192780364671132 | top1:99.3838882446289\n",
      "221/230 Data:0.035 | Batch:0.588 | Total:0:02:20 | ETA:0:00:06 | Loss:0.017852249209204377 | top1:99.39215850830078\n",
      "26/26 Data:0.002 | Batch:0.199 | Total:0:00:23 | ETA:0:00:00 | Loss:0.016422875341959298 | top1:99.5128173828125\n",
      "\n",
      "Epoch: [7 | 400] LR: 0.208000\n",
      "1/230 Data:7.778 | Batch:8.387 | Total:0:00:08 | ETA:0:32:12 | Loss:0.0035076276399195194 | top1:99.66667938232422\n",
      "11/230 Data:0.042 | Batch:0.454 | Total:0:00:16 | ETA:0:05:43 | Loss:0.009572574456053024 | top1:99.66666412353516\n",
      "21/230 Data:0.042 | Batch:0.640 | Total:0:00:21 | ETA:0:01:59 | Loss:0.011490923579826597 | top1:99.58729553222656\n",
      "31/230 Data:0.056 | Batch:0.640 | Total:0:00:27 | ETA:0:02:01 | Loss:0.010497726280347354 | top1:99.6236572265625\n",
      "41/230 Data:0.053 | Batch:0.611 | Total:0:00:33 | ETA:0:01:53 | Loss:0.012345478499433162 | top1:99.55284881591797\n",
      "51/230 Data:0.051 | Batch:0.549 | Total:0:00:40 | ETA:0:01:53 | Loss:0.012214886952492902 | top1:99.56208801269531\n",
      "61/230 Data:0.035 | Batch:0.659 | Total:0:00:46 | ETA:0:01:46 | Loss:0.014128115590180835 | top1:99.49726867675781\n",
      "71/230 Data:0.040 | Batch:0.568 | Total:0:00:52 | ETA:0:01:41 | Loss:0.01483900759542938 | top1:99.47417449951172\n",
      "81/230 Data:0.043 | Batch:0.630 | Total:0:00:58 | ETA:0:01:32 | Loss:0.014189921106056621 | top1:99.51028442382812\n",
      "91/230 Data:0.043 | Batch:0.625 | Total:0:01:05 | ETA:0:01:28 | Loss:0.013979217436057158 | top1:99.5054931640625\n",
      "101/230 Data:0.054 | Batch:0.633 | Total:0:01:11 | ETA:0:01:25 | Loss:0.014081922840986328 | top1:99.50164794921875\n",
      "111/230 Data:0.081 | Batch:0.696 | Total:0:01:18 | ETA:0:01:19 | Loss:0.014657337891319682 | top1:99.46546936035156\n",
      "121/230 Data:0.059 | Batch:0.700 | Total:0:01:24 | ETA:0:01:13 | Loss:0.014510147925370089 | top1:99.48209381103516\n",
      "131/230 Data:0.059 | Batch:0.670 | Total:0:01:31 | ETA:0:01:05 | Loss:0.014410478620566 | top1:99.48346710205078\n",
      "141/230 Data:0.028 | Batch:0.473 | Total:0:01:35 | ETA:0:00:42 | Loss:0.013948987195795032 | top1:99.50118255615234\n",
      "151/230 Data:0.062 | Batch:0.598 | Total:0:01:40 | ETA:0:00:35 | Loss:0.013694234197575365 | top1:99.51213836669922\n",
      "161/230 Data:0.007 | Batch:0.476 | Total:0:01:45 | ETA:0:00:40 | Loss:0.013763182614503358 | top1:99.5155258178711\n",
      "171/230 Data:0.022 | Batch:0.618 | Total:0:01:52 | ETA:0:00:36 | Loss:0.01380920935063405 | top1:99.5126724243164\n",
      "181/230 Data:0.059 | Batch:0.705 | Total:0:01:57 | ETA:0:00:25 | Loss:0.01369636997568308 | top1:99.51933288574219\n",
      "191/230 Data:0.043 | Batch:0.687 | Total:0:02:04 | ETA:0:00:28 | Loss:0.016948689390443198 | top1:99.4188461303711\n",
      "201/230 Data:0.059 | Batch:0.694 | Total:0:02:11 | ETA:0:00:21 | Loss:0.01982218949242949 | top1:99.3233871459961\n",
      "211/230 Data:0.059 | Batch:0.695 | Total:0:02:18 | ETA:0:00:14 | Loss:0.02077184634360979 | top1:99.2922592163086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/230 Data:0.059 | Batch:0.699 | Total:0:02:25 | ETA:0:00:07 | Loss:0.021394318657241364 | top1:99.26395416259766\n",
      "26/26 Data:0.002 | Batch:0.185 | Total:0:00:06 | ETA:0:00:00 | Loss:0.030985599312071618 | top1:99.025634765625\n",
      "\n",
      "Epoch: [8 | 400] LR: 0.236000\n",
      "1/230 Data:1.835 | Batch:2.487 | Total:0:00:02 | ETA:0:09:43 | Loss:0.011803006753325462 | top1:99.66667938232422\n",
      "11/230 Data:0.059 | Batch:0.706 | Total:0:00:09 | ETA:0:03:15 | Loss:0.032245799475772816 | top1:99.1212158203125\n",
      "21/230 Data:0.059 | Batch:0.705 | Total:0:00:16 | ETA:0:02:27 | Loss:0.02603572021637644 | top1:99.20634460449219\n",
      "31/230 Data:0.047 | Batch:0.696 | Total:0:00:23 | ETA:0:02:20 | Loss:0.022578796724818887 | top1:99.30107116699219\n",
      "41/230 Data:0.059 | Batch:0.704 | Total:0:00:30 | ETA:0:02:12 | Loss:0.02183411644612689 | top1:99.32520294189453\n",
      "51/230 Data:0.059 | Batch:0.688 | Total:0:00:37 | ETA:0:02:06 | Loss:0.02161105865996112 | top1:99.35294342041016\n",
      "61/230 Data:0.059 | Batch:0.701 | Total:0:00:44 | ETA:0:01:59 | Loss:0.020964546499346367 | top1:99.35519409179688\n",
      "71/230 Data:0.059 | Batch:0.712 | Total:0:00:51 | ETA:0:01:51 | Loss:0.02072713441136752 | top1:99.34272003173828\n",
      "81/230 Data:0.053 | Batch:0.708 | Total:0:00:58 | ETA:0:01:46 | Loss:0.019921420622088105 | top1:99.3744888305664\n",
      "91/230 Data:0.049 | Batch:0.699 | Total:0:01:05 | ETA:0:01:39 | Loss:0.018779387233835123 | top1:99.4065933227539\n",
      "101/230 Data:0.059 | Batch:0.693 | Total:0:01:12 | ETA:0:01:32 | Loss:0.01783614907250048 | top1:99.44224548339844\n",
      "111/230 Data:0.050 | Batch:0.701 | Total:0:01:19 | ETA:0:01:25 | Loss:0.017830321857357515 | top1:99.43843841552734\n",
      "121/230 Data:0.060 | Batch:0.700 | Total:0:01:26 | ETA:0:01:17 | Loss:0.01765343627540302 | top1:99.42699432373047\n",
      "131/230 Data:0.059 | Batch:0.704 | Total:0:01:33 | ETA:0:01:10 | Loss:0.017689388587480794 | top1:99.42494201660156\n",
      "141/230 Data:0.060 | Batch:0.558 | Total:0:01:40 | ETA:0:01:02 | Loss:0.01762295878700357 | top1:99.43262481689453\n",
      "151/230 Data:0.062 | Batch:0.689 | Total:0:01:47 | ETA:0:00:50 | Loss:0.017479199833631366 | top1:99.43487548828125\n",
      "161/230 Data:0.058 | Batch:0.665 | Total:0:01:53 | ETA:0:00:46 | Loss:0.01681338053255238 | top1:99.44927215576172\n",
      "171/230 Data:0.059 | Batch:0.621 | Total:0:01:59 | ETA:0:00:38 | Loss:0.016891893269174205 | top1:99.4502944946289\n",
      "181/230 Data:0.059 | Batch:0.610 | Total:0:02:06 | ETA:0:00:33 | Loss:0.016598116408535715 | top1:99.4511947631836\n",
      "191/230 Data:0.050 | Batch:0.669 | Total:0:02:13 | ETA:0:00:26 | Loss:0.01640881696933544 | top1:99.46073150634766\n",
      "201/230 Data:0.061 | Batch:0.653 | Total:0:02:19 | ETA:0:00:20 | Loss:0.016447043300760373 | top1:99.45439910888672\n",
      "211/230 Data:0.059 | Batch:0.701 | Total:0:02:26 | ETA:0:00:13 | Loss:0.01605991464976102 | top1:99.46761322021484\n",
      "221/230 Data:0.051 | Batch:0.672 | Total:0:02:33 | ETA:0:00:06 | Loss:0.015930019784060293 | top1:99.466064453125\n",
      "26/26 Data:0.002 | Batch:0.210 | Total:0:00:09 | ETA:0:00:00 | Loss:0.013468409348906089 | top1:99.58973693847656\n",
      "\n",
      "Epoch: [9 | 400] LR: 0.264000\n",
      "1/230 Data:2.702 | Batch:3.283 | Total:0:00:03 | ETA:0:12:42 | Loss:0.009709355421364307 | top1:99.66667938232422\n",
      "11/230 Data:0.074 | Batch:0.582 | Total:0:00:10 | ETA:0:03:33 | Loss:0.0093808719036381 | top1:99.7272720336914\n",
      "21/230 Data:0.025 | Batch:0.543 | Total:0:00:16 | ETA:0:02:07 | Loss:0.009629852192509654 | top1:99.65078735351562\n",
      "31/230 Data:0.063 | Batch:0.595 | Total:0:00:22 | ETA:0:02:01 | Loss:0.009206867077380358 | top1:99.64515686035156\n",
      "41/230 Data:0.059 | Batch:0.608 | Total:0:00:28 | ETA:0:02:00 | Loss:0.010109724629551127 | top1:99.63414764404297\n",
      "51/230 Data:0.052 | Batch:0.593 | Total:0:00:34 | ETA:0:01:52 | Loss:0.010121144321815604 | top1:99.64051818847656\n",
      "61/230 Data:0.032 | Batch:0.529 | Total:0:00:41 | ETA:0:01:47 | Loss:0.00903755007359413 | top1:99.67759704589844\n",
      "71/230 Data:0.038 | Batch:0.515 | Total:0:00:47 | ETA:0:01:35 | Loss:0.008781763112982472 | top1:99.69013977050781\n",
      "81/230 Data:0.030 | Batch:0.633 | Total:0:00:52 | ETA:0:01:24 | Loss:0.008445080976579684 | top1:99.69547271728516\n",
      "91/230 Data:0.045 | Batch:0.620 | Total:0:00:58 | ETA:0:01:20 | Loss:0.008010083526452225 | top1:99.71062469482422\n",
      "101/230 Data:0.043 | Batch:0.619 | Total:0:01:04 | ETA:0:01:21 | Loss:0.007706992726687729 | top1:99.72277069091797\n",
      "111/230 Data:0.060 | Batch:0.600 | Total:0:01:10 | ETA:0:01:12 | Loss:0.007957636672103576 | top1:99.71772003173828\n",
      "121/230 Data:0.044 | Batch:0.584 | Total:0:01:16 | ETA:0:01:01 | Loss:0.0078020226005907365 | top1:99.72176361083984\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
