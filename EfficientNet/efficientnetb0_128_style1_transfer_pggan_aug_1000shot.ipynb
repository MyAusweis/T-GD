{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0/to_pggan/1000shot' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.2\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.2\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.01\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.5, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "#         auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "#         arc.update(auroc, inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 10/10 | Loss:1.6922 | MainLoss:1.6041 | SPLoss:0.4470 | CLSLoss:4.3440 | top1:49.6500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8995 | MainLoss:0.7736 | SPLoss:0.8697 | CLSLoss:3.9018 | top1:51.2804 | AUROC:0.5190\n",
      "Test | 39/10 | Loss:0.4312 | MainLoss:0.3053 | SPLoss:0.8697 | CLSLoss:3.9018 | top1:88.2051 | AUROC:0.9997\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 10/10 | Loss:0.8923 | MainLoss:0.7654 | SPLoss:0.8933 | CLSLoss:3.7581 | top1:50.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8526 | MainLoss:0.7259 | SPLoss:0.9045 | CLSLoss:3.6256 | top1:51.8100 | AUROC:0.5258\n",
      "Test | 39/10 | Loss:0.4501 | MainLoss:0.3234 | SPLoss:0.9045 | CLSLoss:3.6256 | top1:87.2308 | AUROC:0.9997\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 10/10 | Loss:0.8613 | MainLoss:0.7359 | SPLoss:0.9037 | CLSLoss:3.5058 | top1:50.8500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8341 | MainLoss:0.7129 | SPLoss:0.8739 | CLSLoss:3.3813 | top1:52.1589 | AUROC:0.5319\n",
      "Test | 39/10 | Loss:0.4279 | MainLoss:0.3067 | SPLoss:0.8739 | CLSLoss:3.3813 | top1:89.4231 | AUROC:0.9997\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 10/10 | Loss:0.8364 | MainLoss:0.7164 | SPLoss:0.8721 | CLSLoss:3.2756 | top1:51.9000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8195 | MainLoss:0.7019 | SPLoss:0.8606 | CLSLoss:3.1586 | top1:52.5016 | AUROC:0.5356\n",
      "Test | 39/10 | Loss:0.4483 | MainLoss:0.3306 | SPLoss:0.8606 | CLSLoss:3.1586 | top1:86.6154 | AUROC:0.9996\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 10/10 | Loss:0.8207 | MainLoss:0.7067 | SPLoss:0.8344 | CLSLoss:3.0588 | top1:53.2500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8161 | MainLoss:0.7114 | SPLoss:0.7532 | CLSLoss:2.9394 | top1:52.0530 | AUROC:0.5378\n",
      "Test | 39/10 | Loss:0.3872 | MainLoss:0.2825 | SPLoss:0.7532 | CLSLoss:2.9394 | top1:93.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 10/10 | Loss:0.8174 | MainLoss:0.7111 | SPLoss:0.7802 | CLSLoss:2.8285 | top1:51.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8017 | MainLoss:0.7020 | SPLoss:0.7270 | CLSLoss:2.7009 | top1:52.3396 | AUROC:0.5377\n",
      "Test | 39/10 | Loss:0.4039 | MainLoss:0.3042 | SPLoss:0.7270 | CLSLoss:2.7009 | top1:90.8974 | AUROC:0.9997\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 10/10 | Loss:0.8093 | MainLoss:0.7108 | SPLoss:0.7263 | CLSLoss:2.5898 | top1:51.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7928 | MainLoss:0.7005 | SPLoss:0.6774 | CLSLoss:2.4521 | top1:52.1994 | AUROC:0.5336\n",
      "Test | 39/10 | Loss:0.3985 | MainLoss:0.3063 | SPLoss:0.6774 | CLSLoss:2.4521 | top1:91.1282 | AUROC:0.9997\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 10/10 | Loss:0.7900 | MainLoss:0.7018 | SPLoss:0.6467 | CLSLoss:2.3530 | top1:53.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7890 | MainLoss:0.6968 | SPLoss:0.6998 | CLSLoss:2.2219 | top1:51.8131 | AUROC:0.5362\n",
      "Test | 39/10 | Loss:0.4900 | MainLoss:0.3978 | SPLoss:0.6998 | CLSLoss:2.2219 | top1:65.1538 | AUROC:0.9996\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 10/10 | Loss:0.7853 | MainLoss:0.7037 | SPLoss:0.6039 | CLSLoss:2.1202 | top1:52.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7721 | MainLoss:0.6944 | SPLoss:0.5788 | CLSLoss:1.9860 | top1:52.6199 | AUROC:0.5316\n",
      "Test | 39/10 | Loss:0.4190 | MainLoss:0.3413 | SPLoss:0.5788 | CLSLoss:1.9860 | top1:85.2051 | AUROC:0.9996\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 10/10 | Loss:0.7716 | MainLoss:0.6992 | SPLoss:0.5359 | CLSLoss:1.8808 | top1:51.9000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7637 | MainLoss:0.6990 | SPLoss:0.4700 | CLSLoss:1.7726 | top1:52.1838 | AUROC:0.5339\n",
      "Test | 39/10 | Loss:0.3778 | MainLoss:0.3131 | SPLoss:0.4700 | CLSLoss:1.7726 | top1:90.2949 | AUROC:0.9997\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.7743 | MainLoss:0.7105 | SPLoss:0.4724 | CLSLoss:1.6595 | top1:50.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7542 | MainLoss:0.6943 | SPLoss:0.4471 | CLSLoss:1.5167 | top1:52.4673 | AUROC:0.5291\n",
      "Test | 39/10 | Loss:0.3999 | MainLoss:0.3400 | SPLoss:0.4471 | CLSLoss:1.5167 | top1:86.5385 | AUROC:0.9997\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.7664 | MainLoss:0.7116 | SPLoss:0.4060 | CLSLoss:1.4280 | top1:51.2500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7474 | MainLoss:0.6933 | SPLoss:0.4121 | CLSLoss:1.2814 | top1:51.8910 | AUROC:0.5282\n",
      "Test | 39/10 | Loss:0.4223 | MainLoss:0.3683 | SPLoss:0.4120 | CLSLoss:1.2814 | top1:76.3077 | AUROC:0.9996\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 10/10 | Loss:0.7466 | MainLoss:0.6993 | SPLoss:0.3530 | CLSLoss:1.2020 | top1:52.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7389 | MainLoss:0.6999 | SPLoss:0.2781 | CLSLoss:1.1217 | top1:51.8255 | AUROC:0.5275\n",
      "Test | 39/10 | Loss:0.3482 | MainLoss:0.3092 | SPLoss:0.2781 | CLSLoss:1.1217 | top1:92.3462 | AUROC:0.9997\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 10/10 | Loss:0.7404 | MainLoss:0.7024 | SPLoss:0.2761 | CLSLoss:1.0473 | top1:51.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7302 | MainLoss:0.6982 | SPLoss:0.2228 | CLSLoss:0.9663 | top1:51.9533 | AUROC:0.5266\n",
      "Test | 39/10 | Loss:0.3488 | MainLoss:0.3169 | SPLoss:0.2228 | CLSLoss:0.9663 | top1:91.7436 | AUROC:0.9997\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 10/10 | Loss:0.7299 | MainLoss:0.6993 | SPLoss:0.2154 | CLSLoss:0.9059 | top1:52.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7236 | MainLoss:0.6984 | SPLoss:0.1685 | CLSLoss:0.8339 | top1:51.8287 | AUROC:0.5265\n",
      "Test | 39/10 | Loss:0.3418 | MainLoss:0.3166 | SPLoss:0.1685 | CLSLoss:0.8339 | top1:92.6923 | AUROC:0.9997\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 10/10 | Loss:0.7225 | MainLoss:0.6986 | SPLoss:0.1616 | CLSLoss:0.7771 | top1:50.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7154 | MainLoss:0.6955 | SPLoss:0.1266 | CLSLoss:0.7192 | top1:51.9502 | AUROC:0.5270\n",
      "Test | 39/10 | Loss:0.3540 | MainLoss:0.3341 | SPLoss:0.1266 | CLSLoss:0.7192 | top1:90.8846 | AUROC:0.9996\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 10/10 | Loss:0.7187 | MainLoss:0.7000 | SPLoss:0.1205 | CLSLoss:0.6687 | top1:50.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7091 | MainLoss:0.6928 | SPLoss:0.1016 | CLSLoss:0.6169 | top1:52.0312 | AUROC:0.5249\n",
      "Test | 39/10 | Loss:0.3800 | MainLoss:0.3637 | SPLoss:0.1016 | CLSLoss:0.6169 | top1:86.3333 | AUROC:0.9997\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 10/10 | Loss:0.7115 | MainLoss:0.6970 | SPLoss:0.0864 | CLSLoss:0.5840 | top1:51.8500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7053 | MainLoss:0.6926 | SPLoss:0.0732 | CLSLoss:0.5377 | top1:52.0498 | AUROC:0.5257\n",
      "Test | 39/10 | Loss:0.3858 | MainLoss:0.3731 | SPLoss:0.0732 | CLSLoss:0.5377 | top1:85.5385 | AUROC:0.9997\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 10/10 | Loss:0.7119 | MainLoss:0.7003 | SPLoss:0.0659 | CLSLoss:0.5011 | top1:51.4500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7030 | MainLoss:0.6924 | SPLoss:0.0610 | CLSLoss:0.4538 | top1:51.8879 | AUROC:0.5262\n",
      "Test | 39/10 | Loss:0.4047 | MainLoss:0.3941 | SPLoss:0.0610 | CLSLoss:0.4538 | top1:79.6795 | AUROC:0.9997\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 10/10 | Loss:0.7057 | MainLoss:0.6962 | SPLoss:0.0527 | CLSLoss:0.4254 | top1:49.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7008 | MainLoss:0.6922 | SPLoss:0.0464 | CLSLoss:0.3951 | top1:51.8910 | AUROC:0.5264\n",
      "Test | 39/10 | Loss:0.4083 | MainLoss:0.3997 | SPLoss:0.0464 | CLSLoss:0.3951 | top1:81.5513 | AUROC:0.9997\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 10/10 | Loss:0.7048 | MainLoss:0.6969 | SPLoss:0.0413 | CLSLoss:0.3688 | top1:51.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7009 | MainLoss:0.6938 | SPLoss:0.0364 | CLSLoss:0.3453 | top1:50.0374 | AUROC:0.5247\n",
      "Test | 39/10 | Loss:0.4409 | MainLoss:0.4339 | SPLoss:0.0364 | CLSLoss:0.3453 | top1:50.6154 | AUROC:0.9997\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 10/10 | Loss:0.7030 | MainLoss:0.6968 | SPLoss:0.0300 | CLSLoss:0.3253 | top1:49.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6992 | MainLoss:0.6934 | SPLoss:0.0283 | CLSLoss:0.2985 | top1:50.1371 | AUROC:0.5224\n",
      "Test | 39/10 | Loss:0.4435 | MainLoss:0.4377 | SPLoss:0.0283 | CLSLoss:0.2985 | top1:52.2308 | AUROC:0.9997\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 10/10 | Loss:0.7028 | MainLoss:0.6977 | SPLoss:0.0241 | CLSLoss:0.2766 | top1:51.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6976 | MainLoss:0.6929 | SPLoss:0.0212 | CLSLoss:0.2543 | top1:50.9190 | AUROC:0.5193\n",
      "Test | 39/10 | Loss:0.4445 | MainLoss:0.4398 | SPLoss:0.0212 | CLSLoss:0.2543 | top1:67.7949 | AUROC:0.9997\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 10/10 | Loss:0.7022 | MainLoss:0.6979 | SPLoss:0.0195 | CLSLoss:0.2351 | top1:49.9000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6984 | MainLoss:0.6945 | SPLoss:0.0177 | CLSLoss:0.2118 | top1:51.1215 | AUROC:0.5165\n",
      "Test | 39/10 | Loss:0.4105 | MainLoss:0.4067 | SPLoss:0.0177 | CLSLoss:0.2118 | top1:94.4103 | AUROC:0.9996\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 10/10 | Loss:0.6991 | MainLoss:0.6955 | SPLoss:0.0163 | CLSLoss:0.2011 | top1:50.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6959 | MainLoss:0.6926 | SPLoss:0.0141 | CLSLoss:0.1861 | top1:51.2056 | AUROC:0.5153\n",
      "Test | 39/10 | Loss:0.4529 | MainLoss:0.4497 | SPLoss:0.0141 | CLSLoss:0.1861 | top1:85.7564 | AUROC:0.9996\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 10/10 | Loss:0.7008 | MainLoss:0.6977 | SPLoss:0.0135 | CLSLoss:0.1728 | top1:49.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6955 | MainLoss:0.6927 | SPLoss:0.0124 | CLSLoss:0.1567 | top1:51.1215 | AUROC:0.5137\n",
      "Test | 39/10 | Loss:0.4649 | MainLoss:0.4621 | SPLoss:0.0124 | CLSLoss:0.1567 | top1:87.6410 | AUROC:0.9997\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 10/10 | Loss:0.6975 | MainLoss:0.6948 | SPLoss:0.0115 | CLSLoss:0.1473 | top1:50.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6965 | MainLoss:0.6941 | SPLoss:0.0096 | CLSLoss:0.1413 | top1:50.0000 | AUROC:0.5124\n",
      "Test | 39/10 | Loss:0.5130 | MainLoss:0.5107 | SPLoss:0.0096 | CLSLoss:0.1413 | top1:50.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 10/10 | Loss:0.6985 | MainLoss:0.6963 | SPLoss:0.0086 | CLSLoss:0.1334 | top1:49.8500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6968 | MainLoss:0.6948 | SPLoss:0.0080 | CLSLoss:0.1205 | top1:50.7321 | AUROC:0.5118\n",
      "Test | 39/10 | Loss:0.4574 | MainLoss:0.4554 | SPLoss:0.0080 | CLSLoss:0.1205 | top1:95.9615 | AUROC:0.9997\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 10/10 | Loss:0.6992 | MainLoss:0.6973 | SPLoss:0.0078 | CLSLoss:0.1146 | top1:50.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6946 | MainLoss:0.6928 | SPLoss:0.0070 | CLSLoss:0.1058 | top1:50.7664 | AUROC:0.5115\n",
      "Test | 39/10 | Loss:0.5147 | MainLoss:0.5130 | SPLoss:0.0070 | CLSLoss:0.1058 | top1:76.0256 | AUROC:0.9997\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 10/10 | Loss:0.6987 | MainLoss:0.6971 | SPLoss:0.0064 | CLSLoss:0.1012 | top1:49.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6944 | MainLoss:0.6929 | SPLoss:0.0056 | CLSLoss:0.0954 | top1:50.5950 | AUROC:0.5098\n",
      "Test | 39/10 | Loss:0.5274 | MainLoss:0.5258 | SPLoss:0.0056 | CLSLoss:0.0954 | top1:71.1026 | AUROC:0.9997\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 10/10 | Loss:0.6971 | MainLoss:0.6956 | SPLoss:0.0050 | CLSLoss:0.0912 | top1:49.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6943 | MainLoss:0.6930 | SPLoss:0.0046 | CLSLoss:0.0840 | top1:49.9502 | AUROC:0.5085\n",
      "Test | 39/10 | Loss:0.5440 | MainLoss:0.5427 | SPLoss:0.0046 | CLSLoss:0.0840 | top1:53.8205 | AUROC:0.9997\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 10/10 | Loss:0.6948 | MainLoss:0.6936 | SPLoss:0.0040 | CLSLoss:0.0816 | top1:50.4000 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_ac