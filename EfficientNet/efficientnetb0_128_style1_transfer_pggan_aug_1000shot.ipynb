{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0/to_pggan/1000shot' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.2\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.2\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.01\n",
    "sp_beta = 0.01\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "#         auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "#         arc.update(auroc, inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 10/10 | Loss:1.5755 | MainLoss:1.5271 | SPLoss:0.5112 | CLSLoss:4.3294 | top1:49.6500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.8168 | MainLoss:0.7687 | SPLoss:0.9394 | CLSLoss:3.8758 | top1:51.3676 | AUROC:0.5204\n",
      "Test | 39/10 | Loss:0.3638 | MainLoss:0.3156 | SPLoss:0.9394 | CLSLoss:3.8758 | top1:87.2821 | AUROC:0.9997\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 10/10 | Loss:0.8132 | MainLoss:0.7657 | SPLoss:1.0242 | CLSLoss:3.7228 | top1:51.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7632 | MainLoss:0.7169 | SPLoss:1.0611 | CLSLoss:3.5767 | top1:52.2243 | AUROC:0.5324\n",
      "Test | 39/10 | Loss:0.3721 | MainLoss:0.3257 | SPLoss:1.0611 | CLSLoss:3.5767 | top1:86.8846 | AUROC:0.9997\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 10/10 | Loss:0.7652 | MainLoss:0.7195 | SPLoss:1.1082 | CLSLoss:3.4631 | top1:52.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7440 | MainLoss:0.6987 | SPLoss:1.1934 | CLSLoss:3.3408 | top1:53.0187 | AUROC:0.5411\n",
      "Test | 39/10 | Loss:0.4144 | MainLoss:0.3691 | SPLoss:1.1934 | CLSLoss:3.3408 | top1:79.3718 | AUROC:0.9997\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 10/10 | Loss:0.7519 | MainLoss:0.7075 | SPLoss:1.1999 | CLSLoss:3.2418 | top1:51.8000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7364 | MainLoss:0.6929 | SPLoss:1.2232 | CLSLoss:3.1290 | top1:53.4019 | AUROC:0.5498\n",
      "Test | 39/10 | Loss:0.3819 | MainLoss:0.3384 | SPLoss:1.2232 | CLSLoss:3.1290 | top1:85.8077 | AUROC:0.9996\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 10/10 | Loss:0.7430 | MainLoss:0.7002 | SPLoss:1.2465 | CLSLoss:3.0359 | top1:51.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7303 | MainLoss:0.6882 | SPLoss:1.2926 | CLSLoss:2.9250 | top1:54.1433 | AUROC:0.5601\n",
      "Test | 39/10 | Loss:0.3974 | MainLoss:0.3552 | SPLoss:1.2926 | CLSLoss:2.9250 | top1:81.2821 | AUROC:0.9997\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 10/10 | Loss:0.7325 | MainLoss:0.6912 | SPLoss:1.2879 | CLSLoss:2.8422 | top1:55.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7259 | MainLoss:0.6853 | SPLoss:1.3155 | CLSLoss:2.7419 | top1:55.0810 | AUROC:0.5732\n",
      "Test | 39/10 | Loss:0.4001 | MainLoss:0.3595 | SPLoss:1.3155 | CLSLoss:2.7419 | top1:79.7436 | AUROC:0.9997\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 10/10 | Loss:0.7339 | MainLoss:0.6943 | SPLoss:1.3101 | CLSLoss:2.6525 | top1:53.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7220 | MainLoss:0.6833 | SPLoss:1.3356 | CLSLoss:2.5364 | top1:55.5296 | AUROC:0.5845\n",
      "Test | 39/10 | Loss:0.3833 | MainLoss:0.3446 | SPLoss:1.3356 | CLSLoss:2.5364 | top1:84.5513 | AUROC:0.9997\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 10/10 | Loss:0.7308 | MainLoss:0.6928 | SPLoss:1.3532 | CLSLoss:2.4445 | top1:55.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7208 | MainLoss:0.6841 | SPLoss:1.3335 | CLSLoss:2.3297 | top1:54.7726 | AUROC:0.5965\n",
      "Test | 39/10 | Loss:0.3566 | MainLoss:0.3199 | SPLoss:1.3335 | CLSLoss:2.3297 | top1:90.9103 | AUROC:0.9996\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 10/10 | Loss:0.7239 | MainLoss:0.6879 | SPLoss:1.3612 | CLSLoss:2.2476 | top1:54.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7135 | MainLoss:0.6779 | SPLoss:1.4148 | CLSLoss:2.1427 | top1:58.0685 | AUROC:0.6129\n",
      "Test | 39/10 | Loss:0.4091 | MainLoss:0.3736 | SPLoss:1.4148 | CLSLoss:2.1427 | top1:73.7436 | AUROC:0.9997\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 10/10 | Loss:0.7235 | MainLoss:0.6890 | SPLoss:1.3943 | CLSLoss:2.0573 | top1:54.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7080 | MainLoss:0.6744 | SPLoss:1.4124 | CLSLoss:1.9547 | top1:58.4891 | AUROC:0.6291\n",
      "Test | 39/10 | Loss:0.3857 | MainLoss:0.3520 | SPLoss:1.4124 | CLSLoss:1.9547 | top1:82.7051 | AUROC:0.9997\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.7173 | MainLoss:0.6844 | SPLoss:1.4177 | CLSLoss:1.8767 | top1:55.8500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7045 | MainLoss:0.6716 | SPLoss:1.5074 | CLSLoss:1.7830 | top1:59.6355 | AUROC:0.6520\n",
      "Test | 39/10 | Loss:0.4335 | MainLoss:0.4006 | SPLoss:1.5074 | CLSLoss:1.7830 | top1:64.0641 | AUROC:0.9997\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.7110 | MainLoss:0.6793 | SPLoss:1.4594 | CLSLoss:1.7129 | top1:57.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6917 | MainLoss:0.6605 | SPLoss:1.4895 | CLSLoss:1.6313 | top1:62.0872 | AUROC:0.6748\n",
      "Test | 39/10 | Loss:0.3933 | MainLoss:0.3621 | SPLoss:1.4895 | CLSLoss:1.6313 | top1:79.5128 | AUROC:0.9996\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 10/10 | Loss:0.7073 | MainLoss:0.6766 | SPLoss:1.4952 | CLSLoss:1.5701 | top1:57.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6834 | MainLoss:0.6523 | SPLoss:1.6028 | CLSLoss:1.5006 | top1:64.0561 | AUROC:0.7003\n",
      "Test | 39/10 | Loss:0.4362 | MainLoss:0.4051 | SPLoss:1.6028 | CLSLoss:1.5006 | top1:67.3590 | AUROC:0.9997\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 10/10 | Loss:0.6992 | MainLoss:0.6689 | SPLoss:1.5804 | CLSLoss:1.4469 | top1:59.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6651 | MainLoss:0.6348 | SPLoss:1.6400 | CLSLoss:1.3934 | top1:65.9315 | AUROC:0.7290\n",
      "Test | 39/10 | Loss:0.3925 | MainLoss:0.3621 | SPLoss:1.6400 | CLSLoss:1.3934 | top1:78.9359 | AUROC:0.9996\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 10/10 | Loss:0.6957 | MainLoss:0.6651 | SPLoss:1.7092 | CLSLoss:1.3510 | top1:59.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6497 | MainLoss:0.6190 | SPLoss:1.7705 | CLSLoss:1.2967 | top1:66.7446 | AUROC:0.7617\n",
      "Test | 39/10 | Loss:0.3672 | MainLoss:0.3365 | SPLoss:1.7705 | CLSLoss:1.2967 | top1:83.8333 | AUROC:0.9995\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 10/10 | Loss:0.6856 | MainLoss:0.6539 | SPLoss:1.8970 | CLSLoss:1.2698 | top1:61.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6196 | MainLoss:0.5865 | SPLoss:2.0952 | CLSLoss:1.2168 | top1:72.4642 | AUROC:0.8020\n",
      "Test | 39/10 | Loss:0.4182 | MainLoss:0.3851 | SPLoss:2.0952 | CLSLoss:1.2168 | top1:74.4872 | AUROC:0.9994\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 10/10 | Loss:0.7569 | MainLoss:0.6180 | SPLoss:12.6609 | CLSLoss:1.2301 | top1:66.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7575 | MainLoss:0.5467 | SPLoss:19.8666 | CLSLoss:1.2064 | top1:74.0810 | AUROC:0.8418\n",
      "Test | 39/10 | Loss:0.5618 | MainLoss:0.3511 | SPLoss:19.8666 | CLSLoss:1.2064 | top1:80.1923 | AUROC:0.9992\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 10/10 | Loss:0.8143 | MainLoss:0.6086 | SPLoss:19.3938 | CLSLoss:1.1743 | top1:66.8000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.7009 | MainLoss:0.5016 | SPLoss:18.7657 | CLSLoss:1.1601 | top1:76.9065 | AUROC:0.8828\n",
      "Test | 39/10 | Loss:0.5336 | MainLoss:0.3343 | SPLoss:18.7657 | CLSLoss:1.1601 | top1:80.9103 | AUROC:0.9990\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 10/10 | Loss:0.7905 | MainLoss:0.5951 | SPLoss:18.4459 | CLSLoss:1.1002 | top1:68.9000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.6473 | MainLoss:0.4575 | SPLoss:17.9330 | CLSLoss:1.0492 | top1:80.1963 | AUROC:0.9227\n",
      "Test | 39/10 | Loss:0.5280 | MainLoss:0.3382 | SPLoss:17.9330 | CLSLoss:1.0492 | top1:80.4103 | AUROC:0.9983\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 10/10 | Loss:0.7475 | MainLoss:0.5602 | SPLoss:17.6983 | CLSLoss:1.0306 | top1:71.5500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.5808 | MainLoss:0.3989 | SPLoss:17.2138 | CLSLoss:0.9841 | top1:84.7913 | AUROC:0.9537\n",
      "Test | 39/10 | Loss:0.5497 | MainLoss:0.3678 | SPLoss:17.2138 | CLSLoss:0.9841 | top1:77.1667 | AUROC:0.9971\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 10/10 | Loss:0.7056 | MainLoss:0.5261 | SPLoss:16.9894 | CLSLoss:0.9560 | top1:73.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.4930 | MainLoss:0.3171 | SPLoss:16.6782 | CLSLoss:0.9106 | top1:91.4174 | AUROC:0.9719\n",
      "Test | 39/10 | Loss:0.7042 | MainLoss:0.5284 | SPLoss:16.6782 | CLSLoss:0.9106 | top1:62.0256 | AUROC:0.9959\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 10/10 | Loss:0.6784 | MainLoss:0.5062 | SPLoss:16.3326 | CLSLoss:0.8886 | top1:74.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.4309 | MainLoss:0.2613 | SPLoss:16.0865 | CLSLoss:0.8791 | top1:93.5732 | AUROC:0.9834\n",
      "Test | 39/10 | Loss:0.7132 | MainLoss:0.5436 | SPLoss:16.0865 | CLSLoss:0.8791 | top1:62.1154 | AUROC:0.9938\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 10/10 | Loss:0.6561 | MainLoss:0.4896 | SPLoss:15.8016 | CLSLoss:0.8494 | top1:76.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.4289 | MainLoss:0.2657 | SPLoss:15.5326 | CLSLoss:0.7875 | top1:94.2087 | AUROC:0.9882\n",
      "Test | 39/10 | Loss:0.6745 | MainLoss:0.5113 | SPLoss:15.5326 | CLSLoss:0.7875 | top1:63.3590 | AUROC:0.9923\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 10/10 | Loss:0.6278 | MainLoss:0.4673 | SPLoss:15.2810 | CLSLoss:0.7720 | top1:77.6500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3877 | MainLoss:0.2301 | SPLoss:14.9930 | CLSLoss:0.7707 | top1:95.6137 | AUROC:0.9909\n",
      "Test | 39/10 | Loss:0.7670 | MainLoss:0.6094 | SPLoss:14.9930 | CLSLoss:0.7707 | top1:56.7436 | AUROC:0.9903\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 10/10 | Loss:0.6056 | MainLoss:0.4521 | SPLoss:14.5880 | CLSLoss:0.7649 | top1:78.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3722 | MainLoss:0.2213 | SPLoss:14.3280 | CLSLoss:0.7540 | top1:95.7446 | AUROC:0.9919\n",
      "Test | 39/10 | Loss:0.7921 | MainLoss:0.6413 | SPLoss:14.3279 | CLSLoss:0.7540 | top1:55.3974 | AUROC:0.9895\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 10/10 | Loss:0.6100 | MainLoss:0.4632 | SPLoss:13.9443 | CLSLoss:0.7404 | top1:77.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3862 | MainLoss:0.2431 | SPLoss:13.5777 | CLSLoss:0.7254 | top1:95.4455 | AUROC:0.9926\n",
      "Test | 39/10 | Loss:0.6760 | MainLoss:0.5330 | SPLoss:13.5777 | CLSLoss:0.7254 | top1:60.9103 | AUROC:0.9903\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 10/10 | Loss:0.6063 | MainLoss:0.4657 | SPLoss:13.3431 | CLSLoss:0.7223 | top1:77.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3652 | MainLoss:0.2283 | SPLoss:13.0362 | CLSLoss:0.6549 | top1:96.2461 | AUROC:0.9930\n",
      "Test | 39/10 | Loss:0.7456 | MainLoss:0.6087 | SPLoss:13.0362 | CLSLoss:0.6549 | top1:56.3974 | AUROC:0.9900\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 10/10 | Loss:0.5839 | MainLoss:0.4503 | SPLoss:12.6978 | CLSLoss:0.6683 | top1:78.6500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3578 | MainLoss:0.2274 | SPLoss:12.3770 | CLSLoss:0.6623 | top1:95.6698 | AUROC:0.9930\n",
      "Test | 39/10 | Loss:0.6985 | MainLoss:0.5681 | SPLoss:12.3770 | CLSLoss:0.6623 | top1:59.4744 | AUROC:0.9895\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 10/10 | Loss:0.5850 | MainLoss:0.4564 | SPLoss:12.2228 | CLSLoss:0.6450 | top1:78.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3514 | MainLoss:0.2252 | SPLoss:12.0337 | CLSLoss:0.5897 | top1:96.5140 | AUROC:0.9944\n",
      "Test | 39/10 | Loss:0.7151 | MainLoss:0.5889 | SPLoss:12.0337 | CLSLoss:0.5897 | top1:57.8462 | AUROC:0.9882\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 10/10 | Loss:0.5726 | MainLoss:0.4488 | SPLoss:11.7762 | CLSLoss:0.6028 | top1:78.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3534 | MainLoss:0.2328 | SPLoss:11.4594 | CLSLoss:0.6065 | top1:95.7788 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6722 | MainLoss:0.5516 | SPLoss:11.4594 | CLSLoss:0.6065 | top1:60.5641 | AUROC:0.9886\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 10/10 | Loss:0.5694 | MainLoss:0.4508 | SPLoss:11.2713 | CLSLoss:0.5883 | top1:78.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3397 | MainLoss:0.2239 | SPLoss:11.0005 | CLSLoss:0.5882 | top1:96.2586 | AUROC:0.9944\n",
      "Test | 39/10 | Loss:0.6869 | MainLoss:0.5710 | SPLoss:11.0005 | CLSLoss:0.5882 | top1:59.2436 | AUROC:0.9896\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 10/10 | Loss:0.5445 | MainLoss:0.4303 | SPLoss:10.8136 | CLSLoss:0.6026 | top1:79.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3043 | MainLoss:0.1914 | SPLoss:10.6758 | CLSLoss:0.6177 | top1:96.9875 | AUROC:0.9948\n",
      "Test | 39/10 | Loss:0.7598 | MainLoss:0.6469 | SPLoss:10.6758 | CLSLoss:0.6177 | top1:56.2308 | AUROC:0.9885\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 10/10 | Loss:0.5528 | MainLoss:0.4422 | SPLoss:10.4546 | CLSLoss:0.6060 | top1:78.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3220 | MainLoss:0.2134 | SPLoss:10.2701 | CLSLoss:0.5874 | top1:96.7196 | AUROC:0.9946\n",
      "Test | 39/10 | Loss:0.7199 | MainLoss:0.6113 | SPLoss:10.2701 | CLSLoss:0.5874 | top1:56.8590 | AUROC:0.9892\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 10/10 | Loss:0.5445 | MainLoss:0.4376 | SPLoss:10.1119 | CLSLoss:0.5861 | top1:80.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3295 | MainLoss:0.2252 | SPLoss:9.8604 | CLSLoss:0.5625 | top1:96.1713 | AUROC:0.9948\n",
      "Test | 39/10 | Loss:0.6656 | MainLoss:0.5613 | SPLoss:9.8604 | CLSLoss:0.5625 | top1:60.0641 | AUROC:0.9893\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 10/10 | Loss:0.5348 | MainLoss:0.4313 | SPLoss:9.7868 | CLSLoss:0.5629 | top1:80.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3086 | MainLoss:0.2066 | SPLoss:9.6328 | CLSLoss:0.5648 | top1:96.6978 | AUROC:0.9948\n",
      "Test | 39/10 | Loss:0.7092 | MainLoss:0.6072 | SPLoss:9.6328 | CLSLoss:0.5648 | top1:57.8590 | AUROC:0.9888\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 10/10 | Loss:0.5336 | MainLoss:0.4332 | SPLoss:9.4902 | CLSLoss:0.5549 | top1:79.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2867 | MainLoss:0.1871 | SPLoss:9.3907 | CLSLoss:0.5666 | top1:96.9751 | AUROC:0.9947\n",
      "Test | 39/10 | Loss:0.7640 | MainLoss:0.6644 | SPLoss:9.3907 | CLSLoss:0.5666 | top1:56.1795 | AUROC:0.9878\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 10/10 | Loss:0.5388 | MainLoss:0.4411 | SPLoss:9.2260 | CLSLoss:0.5420 | top1:79.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3328 | MainLoss:0.2376 | SPLoss:8.9966 | CLSLoss:0.5208 | top1:96.2492 | AUROC:0.9946\n",
      "Test | 39/10 | Loss:0.6511 | MainLoss:0.5559 | SPLoss:8.9966 | CLSLoss:0.5208 | top1:59.4744 | AUROC:0.9885\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 10/10 | Loss:0.5292 | MainLoss:0.4349 | SPLoss:8.9082 | CLSLoss:0.5244 | top1:79.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2854 | MainLoss:0.1915 | SPLoss:8.8491 | CLSLoss:0.5413 | top1:97.0343 | AUROC:0.9948\n",
      "Test | 39/10 | Loss:0.7528 | MainLoss:0.6589 | SPLoss:8.8491 | CLSLoss:0.5413 | top1:55.3718 | AUROC:0.9878\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 10/10 | Loss:0.5131 | MainLoss:0.4216 | SPLoss:8.6170 | CLSLoss:0.5392 | top1:79.9500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3086 | MainLoss:0.2189 | SPLoss:8.4182 | CLSLoss:0.5575 | top1:95.9408 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6720 | MainLoss:0.5822 | SPLoss:8.4182 | CLSLoss:0.5575 | top1:59.7436 | AUROC:0.9881\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 10/10 | Loss:0.5304 | MainLoss:0.4414 | SPLoss:8.3887 | CLSLoss:0.5111 | top1:79.1500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3230 | MainLoss:0.2359 | SPLoss:8.1770 | CLSLoss:0.5382 | top1:95.1340 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6428 | MainLoss:0.5557 | SPLoss:8.1770 | CLSLoss:0.5382 | top1:62.0128 | AUROC:0.9875\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 10/10 | Loss:0.5230 | MainLoss:0.4365 | SPLoss:8.1482 | CLSLoss:0.5049 | top1:79.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2971 | MainLoss:0.2118 | SPLoss:8.0619 | CLSLoss:0.4753 | top1:96.7788 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.7170 | MainLoss:0.6316 | SPLoss:8.0619 | CLSLoss:0.4753 | top1:56.1026 | AUROC:0.9878\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 10/10 | Loss:0.5209 | MainLoss:0.4376 | SPLoss:7.8455 | CLSLoss:0.4826 | top1:79.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2870 | MainLoss:0.2037 | SPLoss:7.8147 | CLSLoss:0.5068 | top1:96.7103 | AUROC:0.9943\n",
      "Test | 39/10 | Loss:0.7093 | MainLoss:0.6261 | SPLoss:7.8147 | CLSLoss:0.5068 | top1:57.0641 | AUROC:0.9897\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 10/10 | Loss:0.5151 | MainLoss:0.4322 | SPLoss:7.7928 | CLSLoss:0.4974 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2737 | MainLoss:0.1902 | SPLoss:7.8621 | CLSLoss:0.4868 | top1:97.0935 | AUROC:0.9943\n",
      "Test | 39/10 | Loss:0.7590 | MainLoss:0.6755 | SPLoss:7.8621 | CLSLoss:0.4868 | top1:55.5641 | AUROC:0.9867\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 10/10 | Loss:0.5148 | MainLoss:0.4324 | SPLoss:7.7680 | CLSLoss:0.4767 | top1:79.4500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2919 | MainLoss:0.2096 | SPLoss:7.7519 | CLSLoss:0.4753 | top1:96.9502 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.7250 | MainLoss:0.6427 | SPLoss:7.7519 | CLSLoss:0.4753 | top1:55.5256 | AUROC:0.9874\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 10/10 | Loss:0.5232 | MainLoss:0.4414 | SPLoss:7.7082 | CLSLoss:0.4714 | top1:78.8000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3151 | MainLoss:0.2349 | SPLoss:7.5470 | CLSLoss:0.4710 | top1:96.0810 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6540 | MainLoss:0.5739 | SPLoss:7.5470 | CLSLoss:0.4710 | top1:59.0256 | AUROC:0.9889\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 10/10 | Loss:0.5254 | MainLoss:0.4464 | SPLoss:7.4324 | CLSLoss:0.4712 | top1:79.2500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2918 | MainLoss:0.2135 | SPLoss:7.3784 | CLSLoss:0.4466 | top1:96.7009 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.7136 | MainLoss:0.6354 | SPLoss:7.3784 | CLSLoss:0.4466 | top1:56.2308 | AUROC:0.9898\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 10/10 | Loss:0.5044 | MainLoss:0.4268 | SPLoss:7.2945 | CLSLoss:0.4662 | top1:80.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2745 | MainLoss:0.1976 | SPLoss:7.2009 | CLSLoss:0.4874 | top1:96.5857 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.7234 | MainLoss:0.6466 | SPLoss:7.2008 | CLSLoss:0.4874 | top1:57.2308 | AUROC:0.9893\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 10/10 | Loss:0.5110 | MainLoss:0.4352 | SPLoss:7.1070 | CLSLoss:0.4724 | top1:79.9000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2953 | MainLoss:0.2197 | SPLoss:7.1085 | CLSLoss:0.4487 | top1:96.3146 | AUROC:0.9937\n",
      "Test | 39/10 | Loss:0.6785 | MainLoss:0.6029 | SPLoss:7.1085 | CLSLoss:0.4487 | top1:57.8974 | AUROC:0.9904\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 10/10 | Loss:0.4961 | MainLoss:0.4207 | SPLoss:7.0589 | CLSLoss:0.4745 | top1:80.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3074 | MainLoss:0.2329 | SPLoss:6.9778 | CLSLoss:0.4642 | top1:95.9097 | AUROC:0.9935\n",
      "Test | 39/10 | Loss:0.6489 | MainLoss:0.5744 | SPLoss:6.9778 | CLSLoss:0.4642 | top1:59.1154 | AUROC:0.9907\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 10/10 | Loss:0.5059 | MainLoss:0.4313 | SPLoss:6.9882 | CLSLoss:0.4713 | top1:80.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.3043 | MainLoss:0.2309 | SPLoss:6.9229 | CLSLoss:0.4151 | top1:96.5078 | AUROC:0.9926\n",
      "Test | 39/10 | Loss:0.7052 | MainLoss:0.6318 | SPLoss:6.9229 | CLSLoss:0.4151 | top1:54.6795 | AUROC:0.9904\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 10/10 | Loss:0.4997 | MainLoss:0.4267 | SPLoss:6.8534 | CLSLoss:0.4535 | top1:80.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2792 | MainLoss:0.2050 | SPLoss:6.9330 | CLSLoss:0.4830 | top1:96.5421 | AUROC:0.9945\n",
      "Test | 39/10 | Loss:0.6930 | MainLoss:0.6189 | SPLoss:6.9330 | CLSLoss:0.4830 | top1:58.3462 | AUROC:0.9876\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 10/10 | Loss:0.4793 | MainLoss:0.4052 | SPLoss:6.9292 | CLSLoss:0.4845 | top1:81.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2848 | MainLoss:0.2109 | SPLoss:6.9124 | CLSLoss:0.4836 | top1:96.3240 | AUROC:0.9943\n",
      "Test | 39/10 | Loss:0.6813 | MainLoss:0.6073 | SPLoss:6.9124 | CLSLoss:0.4836 | top1:58.8590 | AUROC:0.9878\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 10/10 | Loss:0.4966 | MainLoss:0.4227 | SPLoss:6.9067 | CLSLoss:0.4825 | top1:80.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2890 | MainLoss:0.2153 | SPLoss:6.8898 | CLSLoss:0.4803 | top1:96.1682 | AUROC:0.9943\n",
      "Test | 39/10 | Loss:0.6715 | MainLoss:0.5978 | SPLoss:6.8898 | CLSLoss:0.4803 | top1:59.1667 | AUROC:0.9876\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 10/10 | Loss:0.4978 | MainLoss:0.4241 | SPLoss:6.8889 | CLSLoss:0.4785 | top1:80.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2860 | MainLoss:0.2124 | SPLoss:6.8829 | CLSLoss:0.4741 | top1:96.3115 | AUROC:0.9944\n",
      "Test | 39/10 | Loss:0.6783 | MainLoss:0.6047 | SPLoss:6.8829 | CLSLoss:0.4741 | top1:58.7179 | AUROC:0.9877\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 10/10 | Loss:0.4920 | MainLoss:0.4185 | SPLoss:6.8713 | CLSLoss:0.4751 | top1:80.7000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2861 | MainLoss:0.2128 | SPLoss:6.8559 | CLSLoss:0.4724 | top1:96.3146 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6783 | MainLoss:0.6050 | SPLoss:6.8559 | CLSLoss:0.4724 | top1:58.6154 | AUROC:0.9878\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 10/10 | Loss:0.4824 | MainLoss:0.4092 | SPLoss:6.8454 | CLSLoss:0.4734 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2849 | MainLoss:0.2118 | SPLoss:6.8338 | CLSLoss:0.4755 | top1:96.3240 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6800 | MainLoss:0.6069 | SPLoss:6.8338 | CLSLoss:0.4755 | top1:58.5128 | AUROC:0.9881\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 10/10 | Loss:0.4966 | MainLoss:0.4235 | SPLoss:6.8311 | CLSLoss:0.4742 | top1:80.4500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2857 | MainLoss:0.2127 | SPLoss:6.8308 | CLSLoss:0.4726 | top1:96.2928 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6782 | MainLoss:0.6052 | SPLoss:6.8308 | CLSLoss:0.4726 | top1:58.6538 | AUROC:0.9880\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 10/10 | Loss:0.4903 | MainLoss:0.4172 | SPLoss:6.8313 | CLSLoss:0.4713 | top1:80.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2855 | MainLoss:0.2125 | SPLoss:6.8289 | CLSLoss:0.4741 | top1:96.3364 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6789 | MainLoss:0.6059 | SPLoss:6.8289 | CLSLoss:0.4741 | top1:58.4744 | AUROC:0.9876\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 10/10 | Loss:0.4972 | MainLoss:0.4242 | SPLoss:6.8273 | CLSLoss:0.4746 | top1:79.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2832 | MainLoss:0.2101 | SPLoss:6.8306 | CLSLoss:0.4732 | top1:96.4548 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6844 | MainLoss:0.6114 | SPLoss:6.8306 | CLSLoss:0.4732 | top1:58.1410 | AUROC:0.9880\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 10/10 | Loss:0.4975 | MainLoss:0.4245 | SPLoss:6.8218 | CLSLoss:0.4704 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2845 | MainLoss:0.2117 | SPLoss:6.8128 | CLSLoss:0.4701 | top1:96.3770 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6801 | MainLoss:0.6072 | SPLoss:6.8128 | CLSLoss:0.4701 | top1:58.2949 | AUROC:0.9881\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 10/10 | Loss:0.4784 | MainLoss:0.4057 | SPLoss:6.8019 | CLSLoss:0.4703 | top1:81.2500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2840 | MainLoss:0.2113 | SPLoss:6.7982 | CLSLoss:0.4754 | top1:96.3364 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6803 | MainLoss:0.6076 | SPLoss:6.7982 | CLSLoss:0.4754 | top1:58.4872 | AUROC:0.9882\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 10/10 | Loss:0.5140 | MainLoss:0.4414 | SPLoss:6.7930 | CLSLoss:0.4695 | top1:79.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2814 | MainLoss:0.2089 | SPLoss:6.7890 | CLSLoss:0.4650 | top1:96.5389 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6870 | MainLoss:0.6145 | SPLoss:6.7890 | CLSLoss:0.4650 | top1:57.7949 | AUROC:0.9882\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 10/10 | Loss:0.5105 | MainLoss:0.4380 | SPLoss:6.7834 | CLSLoss:0.4654 | top1:80.4500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2840 | MainLoss:0.2116 | SPLoss:6.7774 | CLSLoss:0.4584 | top1:96.4984 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6833 | MainLoss:0.6109 | SPLoss:6.7774 | CLSLoss:0.4584 | top1:58.0000 | AUROC:0.9884\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 10/10 | Loss:0.5163 | MainLoss:0.4441 | SPLoss:6.7704 | CLSLoss:0.4524 | top1:78.9500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2832 | MainLoss:0.2111 | SPLoss:6.7620 | CLSLoss:0.4505 | top1:96.6137 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6872 | MainLoss:0.6151 | SPLoss:6.7620 | CLSLoss:0.4505 | top1:57.4872 | AUROC:0.9885\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 10/10 | Loss:0.4951 | MainLoss:0.4230 | SPLoss:6.7566 | CLSLoss:0.4537 | top1:80.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2908 | MainLoss:0.2188 | SPLoss:6.7382 | CLSLoss:0.4545 | top1:96.2835 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6676 | MainLoss:0.5957 | SPLoss:6.7382 | CLSLoss:0.4545 | top1:58.5641 | AUROC:0.9888\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 10/10 | Loss:0.4920 | MainLoss:0.4202 | SPLoss:6.7250 | CLSLoss:0.4572 | top1:80.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2862 | MainLoss:0.2143 | SPLoss:6.7337 | CLSLoss:0.4588 | top1:96.3956 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6763 | MainLoss:0.6043 | SPLoss:6.7337 | CLSLoss:0.4588 | top1:58.2820 | AUROC:0.9887\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 10/10 | Loss:0.5090 | MainLoss:0.4371 | SPLoss:6.7349 | CLSLoss:0.4599 | top1:79.7000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2881 | MainLoss:0.2163 | SPLoss:6.7253 | CLSLoss:0.4567 | top1:96.3396 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6717 | MainLoss:0.5999 | SPLoss:6.7253 | CLSLoss:0.4567 | top1:58.4231 | AUROC:0.9884\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 10/10 | Loss:0.4959 | MainLoss:0.4240 | SPLoss:6.7250 | CLSLoss:0.4574 | top1:79.8000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2866 | MainLoss:0.2148 | SPLoss:6.7293 | CLSLoss:0.4587 | top1:96.3427 | AUROC:0.9942\n",
      "Test | 39/10 | Loss:0.6741 | MainLoss:0.6022 | SPLoss:6.7293 | CLSLoss:0.4587 | top1:58.3974 | AUROC:0.9887\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 10/10 | Loss:0.4928 | MainLoss:0.4208 | SPLoss:6.7361 | CLSLoss:0.4628 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2897 | MainLoss:0.2179 | SPLoss:6.7152 | CLSLoss:0.4600 | top1:96.2679 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6681 | MainLoss:0.5964 | SPLoss:6.7152 | CLSLoss:0.4600 | top1:58.6795 | AUROC:0.9882\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 10/10 | Loss:0.4955 | MainLoss:0.4237 | SPLoss:6.7148 | CLSLoss:0.4613 | top1:80.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2854 | MainLoss:0.2137 | SPLoss:6.7146 | CLSLoss:0.4590 | top1:96.4393 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6783 | MainLoss:0.6065 | SPLoss:6.7146 | CLSLoss:0.4590 | top1:58.2436 | AUROC:0.9881\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 10/10 | Loss:0.4975 | MainLoss:0.4258 | SPLoss:6.7146 | CLSLoss:0.4573 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2799 | MainLoss:0.2080 | SPLoss:6.7247 | CLSLoss:0.4586 | top1:96.6449 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6908 | MainLoss:0.6189 | SPLoss:6.7247 | CLSLoss:0.4586 | top1:57.5000 | AUROC:0.9884\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 10/10 | Loss:0.4791 | MainLoss:0.4074 | SPLoss:6.7187 | CLSLoss:0.4595 | top1:81.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2819 | MainLoss:0.2102 | SPLoss:6.7074 | CLSLoss:0.4637 | top1:96.5203 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6851 | MainLoss:0.6134 | SPLoss:6.7074 | CLSLoss:0.4637 | top1:57.8590 | AUROC:0.9886\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 10/10 | Loss:0.4835 | MainLoss:0.4118 | SPLoss:6.7092 | CLSLoss:0.4638 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2816 | MainLoss:0.2098 | SPLoss:6.7079 | CLSLoss:0.4656 | top1:96.5389 | AUROC:0.9941\n",
      "Test | 39/10 | Loss:0.6865 | MainLoss:0.6148 | SPLoss:6.7079 | CLSLoss:0.4656 | top1:57.9359 | AUROC:0.9882\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 10/10 | Loss:0.5077 | MainLoss:0.4361 | SPLoss:6.6992 | CLSLoss:0.4611 | top1:79.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2827 | MainLoss:0.2112 | SPLoss:6.6888 | CLSLoss:0.4598 | top1:96.5670 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6840 | MainLoss:0.6125 | SPLoss:6.6888 | CLSLoss:0.4598 | top1:57.7051 | AUROC:0.9884\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 10/10 | Loss:0.4862 | MainLoss:0.4148 | SPLoss:6.6820 | CLSLoss:0.4610 | top1:80.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2820 | MainLoss:0.2106 | SPLoss:6.6723 | CLSLoss:0.4631 | top1:96.5701 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6846 | MainLoss:0.6132 | SPLoss:6.6723 | CLSLoss:0.4631 | top1:57.7308 | AUROC:0.9888\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 10/10 | Loss:0.4879 | MainLoss:0.4166 | SPLoss:6.6624 | CLSLoss:0.4645 | top1:80.6500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2834 | MainLoss:0.2122 | SPLoss:6.6543 | CLSLoss:0.4664 | top1:96.4455 | AUROC:0.9940\n",
      "Test | 39/10 | Loss:0.6812 | MainLoss:0.6100 | SPLoss:6.6543 | CLSLoss:0.4664 | top1:58.1282 | AUROC:0.9888\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 10/10 | Loss:0.4945 | MainLoss:0.4233 | SPLoss:6.6503 | CLSLoss:0.4644 | top1:80.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2824 | MainLoss:0.2112 | SPLoss:6.6577 | CLSLoss:0.4639 | top1:96.5265 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6843 | MainLoss:0.6131 | SPLoss:6.6577 | CLSLoss:0.4639 | top1:57.8333 | AUROC:0.9891\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 10/10 | Loss:0.4836 | MainLoss:0.4125 | SPLoss:6.6436 | CLSLoss:0.4656 | top1:81.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2840 | MainLoss:0.2130 | SPLoss:6.6406 | CLSLoss:0.4658 | top1:96.3801 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6797 | MainLoss:0.6086 | SPLoss:6.6406 | CLSLoss:0.4658 | top1:58.3205 | AUROC:0.9886\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 10/10 | Loss:0.4935 | MainLoss:0.4224 | SPLoss:6.6431 | CLSLoss:0.4668 | top1:80.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2862 | MainLoss:0.2153 | SPLoss:6.6267 | CLSLoss:0.4652 | top1:96.2960 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6735 | MainLoss:0.6026 | SPLoss:6.6267 | CLSLoss:0.4652 | top1:58.5256 | AUROC:0.9889\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 10/10 | Loss:0.4999 | MainLoss:0.4290 | SPLoss:6.6265 | CLSLoss:0.4626 | top1:80.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2823 | MainLoss:0.2113 | SPLoss:6.6328 | CLSLoss:0.4625 | top1:96.4424 | AUROC:0.9937\n",
      "Test | 39/10 | Loss:0.6827 | MainLoss:0.6117 | SPLoss:6.6328 | CLSLoss:0.4625 | top1:58.0641 | AUROC:0.9888\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 10/10 | Loss:0.4816 | MainLoss:0.4107 | SPLoss:6.6197 | CLSLoss:0.4621 | top1:80.8000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2816 | MainLoss:0.2107 | SPLoss:6.6183 | CLSLoss:0.4653 | top1:96.4361 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6836 | MainLoss:0.6127 | SPLoss:6.6183 | CLSLoss:0.4653 | top1:58.0641 | AUROC:0.9889\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 10/10 | Loss:0.4954 | MainLoss:0.4245 | SPLoss:6.6239 | CLSLoss:0.4651 | top1:80.4500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2796 | MainLoss:0.2087 | SPLoss:6.6237 | CLSLoss:0.4620 | top1:96.5670 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6898 | MainLoss:0.6189 | SPLoss:6.6237 | CLSLoss:0.4620 | top1:57.6923 | AUROC:0.9886\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 10/10 | Loss:0.5028 | MainLoss:0.4320 | SPLoss:6.6171 | CLSLoss:0.4628 | top1:79.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2850 | MainLoss:0.2144 | SPLoss:6.6011 | CLSLoss:0.4579 | top1:96.3801 | AUROC:0.9936\n",
      "Test | 39/10 | Loss:0.6771 | MainLoss:0.6065 | SPLoss:6.6011 | CLSLoss:0.4579 | top1:58.2179 | AUROC:0.9888\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 10/10 | Loss:0.4770 | MainLoss:0.4064 | SPLoss:6.6018 | CLSLoss:0.4604 | top1:81.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2826 | MainLoss:0.2120 | SPLoss:6.5933 | CLSLoss:0.4621 | top1:96.3863 | AUROC:0.9937\n",
      "Test | 39/10 | Loss:0.6823 | MainLoss:0.6118 | SPLoss:6.5932 | CLSLoss:0.4621 | top1:58.1410 | AUROC:0.9886\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 10/10 | Loss:0.4859 | MainLoss:0.4153 | SPLoss:6.5929 | CLSLoss:0.4629 | top1:80.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2789 | MainLoss:0.2082 | SPLoss:6.5985 | CLSLoss:0.4639 | top1:96.5078 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6902 | MainLoss:0.6196 | SPLoss:6.5985 | CLSLoss:0.4639 | top1:57.8205 | AUROC:0.9883\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 10/10 | Loss:0.4944 | MainLoss:0.4238 | SPLoss:6.5929 | CLSLoss:0.4619 | top1:80.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2802 | MainLoss:0.2097 | SPLoss:6.5883 | CLSLoss:0.4606 | top1:96.5389 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6865 | MainLoss:0.6160 | SPLoss:6.5883 | CLSLoss:0.4606 | top1:57.8205 | AUROC:0.9887\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 10/10 | Loss:0.4828 | MainLoss:0.4124 | SPLoss:6.5775 | CLSLoss:0.4631 | top1:80.9500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2851 | MainLoss:0.2148 | SPLoss:6.5688 | CLSLoss:0.4636 | top1:96.1682 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6737 | MainLoss:0.6034 | SPLoss:6.5688 | CLSLoss:0.4636 | top1:58.7179 | AUROC:0.9885\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 10/10 | Loss:0.4874 | MainLoss:0.4172 | SPLoss:6.5623 | CLSLoss:0.4621 | top1:80.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2805 | MainLoss:0.2101 | SPLoss:6.5717 | CLSLoss:0.4640 | top1:96.2866 | AUROC:0.9938\n",
      "Test | 39/10 | Loss:0.6833 | MainLoss:0.6129 | SPLoss:6.5717 | CLSLoss:0.4640 | top1:58.3974 | AUROC:0.9884\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n",
      "Train | 10/10 | Loss:0.4927 | MainLoss:0.4223 | SPLoss:6.5760 | CLSLoss:0.4639 | top1:80.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2797 | MainLoss:0.2092 | SPLoss:6.5897 | CLSLoss:0.4606 | top1:96.4361 | AUROC:0.9939\n",
      "Test | 39/10 | Loss:0.6876 | MainLoss:0.6171 | SPLoss:6.5897 | CLSLoss:0.4606 | top1:58.1410 | AUROC:0.9881\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 10/10 | Loss:0.4862 | MainLoss:0.4158 | SPLoss:6.5833 | CLSLoss:0.4590 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2823 | MainLoss:0.2120 | SPLoss:6.5758 | CLSLoss:0.4597 | top1:96.3364 | AUROC:0.9937\n",
      "Test | 39/10 | Loss:0.6822 | MainLoss:0.6118 | SPLoss:6.5758 | CLSLoss:0.4597 | top1:58.3462 | AUROC:0.9882\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 10/10 | Loss:0.4876 | MainLoss:0.4173 | SPLoss:6.5726 | CLSLoss:0.4590 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2814 | MainLoss:0.2111 | SPLoss:6.5694 | CLSLoss:0.4603 | top1:96.3863 | AUROC:0.9937\n",
      "Test | 39/10 | Loss:0.6851 | MainLoss:0.6148 | SPLoss:6.5694 | CLSLoss:0.4603 | top1:58.1154 | AUROC:0.9882\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 10/10 | Loss:0.4764 | MainLoss:0.4061 | SPLoss:6.5706 | CLSLoss:0.4624 | top1:81.2500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2867 | MainLoss:0.2166 | SPLoss:6.5481 | CLSLoss:0.4629 | top1:96.1122 | AUROC:0.9935\n",
      "Test | 39/10 | Loss:0.6739 | MainLoss:0.6038 | SPLoss:6.5481 | CLSLoss:0.4629 | top1:58.8205 | AUROC:0.9883\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 10/10 | Loss:0.4952 | MainLoss:0.4252 | SPLoss:6.5377 | CLSLoss:0.4604 | top1:79.9000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2823 | MainLoss:0.2123 | SPLoss:6.5461 | CLSLoss:0.4601 | top1:96.2710 | AUROC:0.9936\n",
      "Test | 39/10 | Loss:0.6825 | MainLoss:0.6125 | SPLoss:6.5461 | CLSLoss:0.4601 | top1:58.3462 | AUROC:0.9882\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 10/10 | Loss:0.4906 | MainLoss:0.4206 | SPLoss:6.5450 | CLSLoss:0.4607 | top1:80.0000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2854 | MainLoss:0.2155 | SPLoss:6.5329 | CLSLoss:0.4571 | top1:96.1994 | AUROC:0.9935\n",
      "Test | 39/10 | Loss:0.6755 | MainLoss:0.6056 | SPLoss:6.5329 | CLSLoss:0.4571 | top1:58.5385 | AUROC:0.9889\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 10/10 | Loss:0.4952 | MainLoss:0.4252 | SPLoss:6.5377 | CLSLoss:0.4577 | top1:80.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2856 | MainLoss:0.2158 | SPLoss:6.5246 | CLSLoss:0.4540 | top1:96.2243 | AUROC:0.9934\n",
      "Test | 39/10 | Loss:0.6755 | MainLoss:0.6057 | SPLoss:6.5246 | CLSLoss:0.4540 | top1:58.5000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.039308\n",
      "Train | 10/10 | Loss:0.4877 | MainLoss:0.4179 | SPLoss:6.5256 | CLSLoss:0.4525 | top1:80.4500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2839 | MainLoss:0.2142 | SPLoss:6.5199 | CLSLoss:0.4533 | top1:96.2399 | AUROC:0.9934\n",
      "Test | 39/10 | Loss:0.6795 | MainLoss:0.6098 | SPLoss:6.5199 | CLSLoss:0.4533 | top1:58.4231 | AUROC:0.9886\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.039291\n",
      "Train | 10/10 | Loss:0.4856 | MainLoss:0.4159 | SPLoss:6.5153 | CLSLoss:0.4553 | top1:81.1500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2863 | MainLoss:0.2168 | SPLoss:6.4989 | CLSLoss:0.4528 | top1:96.1090 | AUROC:0.9933\n",
      "Test | 39/10 | Loss:0.6731 | MainLoss:0.6035 | SPLoss:6.4989 | CLSLoss:0.4528 | top1:58.8333 | AUROC:0.9888\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.039274\n",
      "Train | 10/10 | Loss:0.4830 | MainLoss:0.4136 | SPLoss:6.4915 | CLSLoss:0.4544 | top1:80.6000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2855 | MainLoss:0.2160 | SPLoss:6.4945 | CLSLoss:0.4548 | top1:96.1028 | AUROC:0.9933\n",
      "Test | 39/10 | Loss:0.6752 | MainLoss:0.6057 | SPLoss:6.4945 | CLSLoss:0.4548 | top1:58.7436 | AUROC:0.9886\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.039258\n",
      "Train | 10/10 | Loss:0.4876 | MainLoss:0.4183 | SPLoss:6.4789 | CLSLoss:0.4539 | top1:80.8000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2873 | MainLoss:0.2181 | SPLoss:6.4630 | CLSLoss:0.4546 | top1:95.9969 | AUROC:0.9931\n",
      "Test | 39/10 | Loss:0.6714 | MainLoss:0.6022 | SPLoss:6.4630 | CLSLoss:0.4546 | top1:58.8974 | AUROC:0.9892\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.039241\n",
      "Train | 10/10 | Loss:0.4785 | MainLoss:0.4091 | SPLoss:6.4798 | CLSLoss:0.4552 | top1:81.1500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2835 | MainLoss:0.2141 | SPLoss:6.4847 | CLSLoss:0.4568 | top1:96.1464 | AUROC:0.9931\n",
      "Test | 39/10 | Loss:0.6798 | MainLoss:0.6104 | SPLoss:6.4847 | CLSLoss:0.4568 | top1:58.4487 | AUROC:0.9890\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.039223\n",
      "Train | 10/10 | Loss:0.4801 | MainLoss:0.4106 | SPLoss:6.4856 | CLSLoss:0.4586 | top1:81.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2835 | MainLoss:0.2141 | SPLoss:6.4846 | CLSLoss:0.4579 | top1:96.1464 | AUROC:0.9929\n",
      "Test | 39/10 | Loss:0.6806 | MainLoss:0.6111 | SPLoss:6.4846 | CLSLoss:0.4579 | top1:58.5513 | AUROC:0.9886\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.039206\n",
      "Train | 10/10 | Loss:0.4861 | MainLoss:0.4166 | SPLoss:6.4853 | CLSLoss:0.4597 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2865 | MainLoss:0.2171 | SPLoss:6.4855 | CLSLoss:0.4591 | top1:96.0031 | AUROC:0.9931\n",
      "Test | 39/10 | Loss:0.6730 | MainLoss:0.6035 | SPLoss:6.4855 | CLSLoss:0.4591 | top1:58.9615 | AUROC:0.9889\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.039188\n",
      "Train | 10/10 | Loss:0.4819 | MainLoss:0.4124 | SPLoss:6.4889 | CLSLoss:0.4606 | top1:80.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2867 | MainLoss:0.2173 | SPLoss:6.4878 | CLSLoss:0.4588 | top1:96.0561 | AUROC:0.9932\n",
      "Test | 39/10 | Loss:0.6738 | MainLoss:0.6044 | SPLoss:6.4878 | CLSLoss:0.4588 | top1:58.9744 | AUROC:0.9887\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.039170\n",
      "Train | 10/10 | Loss:0.4780 | MainLoss:0.4086 | SPLoss:6.4790 | CLSLoss:0.4616 | top1:81.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2878 | MainLoss:0.2185 | SPLoss:6.4773 | CLSLoss:0.4598 | top1:95.9065 | AUROC:0.9933\n",
      "Test | 39/10 | Loss:0.6701 | MainLoss:0.6008 | SPLoss:6.4773 | CLSLoss:0.4598 | top1:59.2436 | AUROC:0.9884\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.039152\n",
      "Train | 10/10 | Loss:0.4904 | MainLoss:0.4210 | SPLoss:6.4771 | CLSLoss:0.4580 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2882 | MainLoss:0.2189 | SPLoss:6.4717 | CLSLoss:0.4575 | top1:95.9377 | AUROC:0.9932\n",
      "Test | 39/10 | Loss:0.6700 | MainLoss:0.6007 | SPLoss:6.4717 | CLSLoss:0.4575 | top1:59.2051 | AUROC:0.9888\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.039134\n",
      "Train | 10/10 | Loss:0.4963 | MainLoss:0.4272 | SPLoss:6.4572 | CLSLoss:0.4535 | top1:80.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2856 | MainLoss:0.2165 | SPLoss:6.4575 | CLSLoss:0.4526 | top1:96.0654 | AUROC:0.9933\n",
      "Test | 39/10 | Loss:0.6747 | MainLoss:0.6056 | SPLoss:6.4575 | CLSLoss:0.4526 | top1:58.9615 | AUROC:0.9884\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.039116\n",
      "Train | 10/10 | Loss:0.5011 | MainLoss:0.4320 | SPLoss:6.4637 | CLSLoss:0.4496 | top1:80.4000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2822 | MainLoss:0.2131 | SPLoss:6.4660 | CLSLoss:0.4458 | top1:96.2617 | AUROC:0.9933\n",
      "Test | 39/10 | Loss:0.6818 | MainLoss:0.6126 | SPLoss:6.4661 | CLSLoss:0.4458 | top1:58.4487 | AUROC:0.9886\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.039097\n",
      "Train | 10/10 | Loss:0.4758 | MainLoss:0.4067 | SPLoss:6.4617 | CLSLoss:0.4500 | top1:80.9500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2823 | MainLoss:0.2132 | SPLoss:6.4601 | CLSLoss:0.4494 | top1:96.2150 | AUROC:0.9932\n",
      "Test | 39/10 | Loss:0.6825 | MainLoss:0.6134 | SPLoss:6.4601 | CLSLoss:0.4494 | top1:58.4359 | AUROC:0.9887\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.039079\n",
      "Train | 10/10 | Loss:0.4747 | MainLoss:0.4057 | SPLoss:6.4507 | CLSLoss:0.4517 | top1:81.5000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2856 | MainLoss:0.2168 | SPLoss:6.4292 | CLSLoss:0.4528 | top1:95.9626 | AUROC:0.9931\n",
      "Test | 39/10 | Loss:0.6752 | MainLoss:0.6064 | SPLoss:6.4292 | CLSLoss:0.4528 | top1:59.0256 | AUROC:0.9885\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.039060\n",
      "Train | 10/10 | Loss:0.4843 | MainLoss:0.4154 | SPLoss:6.4379 | CLSLoss:0.4521 | top1:80.3000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2816 | MainLoss:0.2126 | SPLoss:6.4402 | CLSLoss:0.4531 | top1:96.1682 | AUROC:0.9930\n",
      "Test | 39/10 | Loss:0.6847 | MainLoss:0.6157 | SPLoss:6.4402 | CLSLoss:0.4531 | top1:58.5128 | AUROC:0.9885\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.039040\n",
      "Train | 10/10 | Loss:0.4780 | MainLoss:0.4091 | SPLoss:6.4400 | CLSLoss:0.4531 | top1:81.0500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2832 | MainLoss:0.2144 | SPLoss:6.4313 | CLSLoss:0.4535 | top1:96.0436 | AUROC:0.9929\n",
      "Test | 39/10 | Loss:0.6804 | MainLoss:0.6115 | SPLoss:6.4313 | CLSLoss:0.4535 | top1:58.7949 | AUROC:0.9883\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.039021\n",
      "Train | 10/10 | Loss:0.4830 | MainLoss:0.4143 | SPLoss:6.4222 | CLSLoss:0.4529 | top1:80.5500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2825 | MainLoss:0.2138 | SPLoss:6.4224 | CLSLoss:0.4526 | top1:96.0685 | AUROC:0.9929\n",
      "Test | 39/10 | Loss:0.6812 | MainLoss:0.6124 | SPLoss:6.4224 | CLSLoss:0.4526 | top1:58.7564 | AUROC:0.9891\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.039002\n",
      "Train | 10/10 | Loss:0.4829 | MainLoss:0.4141 | SPLoss:6.4288 | CLSLoss:0.4553 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2836 | MainLoss:0.2148 | SPLoss:6.4261 | CLSLoss:0.4543 | top1:95.9938 | AUROC:0.9930\n",
      "Test | 39/10 | Loss:0.6790 | MainLoss:0.6102 | SPLoss:6.4261 | CLSLoss:0.4543 | top1:58.9615 | AUROC:0.9885\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.038982\n",
      "Train | 10/10 | Loss:0.4943 | MainLoss:0.4255 | SPLoss:6.4241 | CLSLoss:0.4535 | top1:80.1000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2883 | MainLoss:0.2197 | SPLoss:6.4146 | CLSLoss:0.4495 | top1:95.8941 | AUROC:0.9931\n",
      "Test | 39/10 | Loss:0.6676 | MainLoss:0.5990 | SPLoss:6.4146 | CLSLoss:0.4495 | top1:59.2820 | AUROC:0.9888\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.038962\n",
      "Train | 10/10 | Loss:0.4758 | MainLoss:0.4070 | SPLoss:6.4232 | CLSLoss:0.4490 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2813 | MainLoss:0.2126 | SPLoss:6.4231 | CLSLoss:0.4513 | top1:96.1838 | AUROC:0.9930\n",
      "Test | 39/10 | Loss:0.6833 | MainLoss:0.6145 | SPLoss:6.4231 | CLSLoss:0.4513 | top1:58.4231 | AUROC:0.9890\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.038942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 10/10 | Loss:0.4907 | MainLoss:0.4219 | SPLoss:6.4323 | CLSLoss:0.4500 | top1:80.2000 | AUROC:0.0000\n",
      "Test | 161/10 | Loss:0.2815 | MainLoss:0.2127 | SPLoss:6.4312 | CLSLoss:0.4492 | top1:96.2679 | AUROC:0.9929\n",
      "Test | 39/10 | Loss:0.6844 | MainLoss:0.6156 | SPLoss:6.4312 | CLSLoss:0.4492 | top1:58.1154 | AUROC:0.9888\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.038922\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
