{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b1' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 180\n",
    "test_batch = 180\n",
    "lr = 0.1\n",
    "schedule = [20, 125, 225, 275]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/b1' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 4\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 6.52M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model, input_size=(3,64,64), device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 300] LR: 0.100000\n",
      "1/431 Data:7.456 | Batch:11.075 | Total:0:00:11 | ETA:1:19:23 | Loss:0.6812984347343445 | top1:60.55555725097656\n",
      "11/431 Data:0.001 | Batch:0.335 | Total:0:00:21 | ETA:0:14:50 | Loss:4.196884913878008 | top1:51.46464920043945\n",
      "21/431 Data:3.968 | Batch:4.607 | Total:0:00:39 | ETA:0:12:33 | Loss:3.4299332925251553 | top1:49.92063522338867\n",
      "31/431 Data:0.741 | Batch:1.281 | Total:0:00:53 | ETA:0:09:09 | Loss:2.5805559312143633 | top1:50.913978576660156\n",
      "41/431 Data:2.096 | Batch:2.556 | Total:0:01:08 | ETA:0:09:58 | Loss:2.126017201237562 | top1:50.4742546081543\n",
      "51/431 Data:0.533 | Batch:0.977 | Total:0:01:23 | ETA:0:10:02 | Loss:1.8458262167724908 | top1:51.05664825439453\n",
      "61/431 Data:2.563 | Batch:3.024 | Total:0:01:39 | ETA:0:09:55 | Loss:1.6586601753703882 | top1:51.029144287109375\n",
      "71/431 Data:0.001 | Batch:0.666 | Total:0:01:53 | ETA:0:10:09 | Loss:1.5253013364026244 | top1:50.860721588134766\n",
      "81/431 Data:2.166 | Batch:2.679 | Total:0:02:08 | ETA:0:08:49 | Loss:1.4254476508976501 | top1:50.74074172973633\n",
      "91/431 Data:1.180 | Batch:1.587 | Total:0:02:23 | ETA:0:08:31 | Loss:1.3460606035295424 | top1:50.726497650146484\n",
      "101/431 Data:2.559 | Batch:3.141 | Total:0:02:39 | ETA:0:08:24 | Loss:1.2817039413027245 | top1:50.92409133911133\n",
      "111/431 Data:0.001 | Batch:0.475 | Total:0:02:53 | ETA:0:08:57 | Loss:1.230560797291833 | top1:50.81581497192383\n",
      "121/431 Data:3.508 | Batch:4.460 | Total:0:03:11 | ETA:0:09:21 | Loss:1.1867277602518886 | top1:51.01469421386719\n",
      "131/431 Data:0.011 | Batch:0.572 | Total:0:03:25 | ETA:0:08:59 | Loss:1.1498811604412458 | top1:51.05173873901367\n",
      "141/431 Data:1.551 | Batch:1.994 | Total:0:03:40 | ETA:0:07:24 | Loss:1.1177150742382023 | top1:51.12687301635742\n",
      "151/431 Data:0.001 | Batch:0.422 | Total:0:03:56 | ETA:0:08:07 | Loss:1.0900892824526653 | top1:51.070640563964844\n",
      "161/431 Data:0.485 | Batch:1.241 | Total:0:04:12 | ETA:0:07:07 | Loss:1.0661186224185162 | top1:50.91787338256836\n",
      "171/431 Data:0.704 | Batch:1.468 | Total:0:04:28 | ETA:0:07:05 | Loss:1.0446538855457863 | top1:51.007144927978516\n",
      "181/431 Data:0.467 | Batch:0.984 | Total:0:04:42 | ETA:0:05:46 | Loss:1.0259257897487661 | top1:50.951507568359375\n",
      "191/431 Data:0.966 | Batch:1.688 | Total:0:05:00 | ETA:0:07:10 | Loss:1.0089806745813779 | top1:50.983131408691406\n",
      "201/431 Data:0.007 | Batch:0.634 | Total:0:05:15 | ETA:0:06:04 | Loss:0.9934556074996492 | top1:51.11111068725586\n",
      "211/431 Data:3.526 | Batch:4.139 | Total:0:05:33 | ETA:0:06:37 | Loss:0.9799799913478688 | top1:50.9847297668457\n",
      "221/431 Data:0.001 | Batch:0.430 | Total:0:05:46 | ETA:0:04:51 | Loss:0.9674168655235843 | top1:51.02815628051758\n",
      "231/431 Data:3.003 | Batch:3.382 | Total:0:06:04 | ETA:0:05:41 | Loss:0.9561224770752382 | top1:51.04136657714844\n",
      "241/431 Data:0.001 | Batch:0.497 | Total:0:06:16 | ETA:0:04:07 | Loss:0.9455197075095909 | top1:51.08805847167969\n",
      "251/431 Data:2.499 | Batch:2.969 | Total:0:06:34 | ETA:0:05:11 | Loss:0.9356005322410765 | top1:51.188575744628906\n",
      "261/431 Data:0.001 | Batch:0.330 | Total:0:06:48 | ETA:0:04:04 | Loss:0.9262143585873747 | top1:51.272884368896484\n",
      "271/431 Data:3.081 | Batch:3.505 | Total:0:07:06 | ETA:0:04:41 | Loss:0.9176356279981972 | top1:51.32636260986328\n",
      "281/431 Data:0.012 | Batch:0.573 | Total:0:07:20 | ETA:0:03:36 | Loss:0.909861524970506 | top1:51.29695129394531\n",
      "291/431 Data:2.733 | Batch:3.121 | Total:0:07:37 | ETA:0:03:59 | Loss:0.9025252461433411 | top1:51.27720642089844\n",
      "301/431 Data:0.001 | Batch:0.501 | Total:0:07:51 | ETA:0:03:54 | Loss:0.8956130036087924 | top1:51.297523498535156\n",
      "311/431 Data:4.156 | Batch:4.774 | Total:0:08:09 | ETA:0:03:43 | Loss:0.8892578931100116 | top1:51.34869384765625\n",
      "321/431 Data:0.001 | Batch:0.385 | Total:0:08:23 | ETA:0:02:37 | Loss:0.8832299466073699 | top1:51.417449951171875\n",
      "331/431 Data:3.531 | Batch:4.034 | Total:0:08:40 | ETA:0:02:48 | Loss:0.8776781180834122 | top1:51.40986633300781\n",
      "341/431 Data:0.001 | Batch:0.406 | Total:0:08:53 | ETA:0:02:35 | Loss:0.8722859059960262 | top1:51.53470230102539\n",
      "351/431 Data:1.847 | Batch:2.673 | Total:0:09:11 | ETA:0:02:25 | Loss:0.8672223996233057 | top1:51.57645034790039\n",
      "361/431 Data:0.026 | Batch:1.215 | Total:0:09:26 | ETA:0:01:46 | Loss:0.8623483973526889 | top1:51.68205642700195\n",
      "371/431 Data:1.763 | Batch:3.144 | Total:0:09:44 | ETA:0:01:46 | Loss:0.8577330130129811 | top1:51.76849365234375\n",
      "381/431 Data:0.001 | Batch:1.104 | Total:0:09:58 | ETA:0:01:10 | Loss:0.8535352679375275 | top1:51.755615234375\n",
      "391/431 Data:2.176 | Batch:2.710 | Total:0:10:15 | ETA:0:01:08 | Loss:0.8494002854122835 | top1:51.84711456298828\n",
      "401/431 Data:0.001 | Batch:0.583 | Total:0:10:29 | ETA:0:00:42 | Loss:0.8453889147301862 | top1:51.936824798583984\n",
      "411/431 Data:2.778 | Batch:3.240 | Total:0:10:46 | ETA:0:00:35 | Loss:0.841715580065465 | top1:51.98567199707031\n",
      "421/431 Data:0.001 | Batch:0.376 | Total:0:11:00 | ETA:0:00:19 | Loss:0.8382223060182041 | top1:51.966217041015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 Data:0.422 | Batch:1.063 | Total:0:00:36 | ETA:0:00:00 | Loss:0.6953139231755183 | top1:50.0\n",
      "\n",
      "Epoch: [2 | 300] LR: 0.170000\n",
      "1/431 Data:2.018 | Batch:2.588 | Total:0:00:02 | ETA:0:18:33 | Loss:0.6833078265190125 | top1:58.333335876464844\n",
      "11/431 Data:0.877 | Batch:1.462 | Total:0:00:09 | ETA:0:04:44 | Loss:0.6920161193067377 | top1:53.53535461425781\n",
      "21/431 Data:0.001 | Batch:0.566 | Total:0:00:16 | ETA:0:05:12 | Loss:0.6968885631788344 | top1:52.619049072265625\n",
      "31/431 Data:0.315 | Batch:0.916 | Total:0:00:24 | ETA:0:05:21 | Loss:0.6986123227304027 | top1:52.455196380615234\n",
      "41/431 Data:0.005 | Batch:0.421 | Total:0:00:30 | ETA:0:03:58 | Loss:0.6987550665692586 | top1:51.97831726074219\n",
      "51/431 Data:0.001 | Batch:0.400 | Total:0:00:35 | ETA:0:03:24 | Loss:0.6973796767346999 | top1:52.200435638427734\n",
      "61/431 Data:1.312 | Batch:1.763 | Total:0:00:43 | ETA:0:04:41 | Loss:0.6956978274173424 | top1:52.841529846191406\n",
      "71/431 Data:0.001 | Batch:0.400 | Total:0:00:49 | ETA:0:04:32 | Loss:0.6950158334114183 | top1:53.14554214477539\n",
      "81/431 Data:0.946 | Batch:1.456 | Total:0:00:56 | ETA:0:04:17 | Loss:0.6945939373087 | top1:53.31961441040039\n",
      "91/431 Data:0.003 | Batch:0.459 | Total:0:01:02 | ETA:0:03:52 | Loss:0.6948294652687325 | top1:53.05250549316406\n",
      "101/431 Data:0.002 | Batch:0.456 | Total:0:01:08 | ETA:0:03:23 | Loss:0.695375642564037 | top1:52.84378433227539\n",
      "111/431 Data:0.001 | Batch:0.601 | Total:0:01:13 | ETA:0:02:40 | Loss:0.6955453952153524 | top1:52.74774932861328\n",
      "121/431 Data:0.001 | Batch:0.504 | Total:0:01:19 | ETA:0:02:58 | Loss:0.6959294780226778 | top1:52.497703552246094\n",
      "131/431 Data:0.183 | Batch:0.640 | Total:0:01:25 | ETA:0:03:02 | Loss:0.6959740310224868 | top1:52.54452896118164\n",
      "141/431 Data:0.383 | Batch:0.971 | Total:0:01:33 | ETA:0:03:48 | Loss:0.695709762843788 | top1:52.57683181762695\n",
      "151/431 Data:0.001 | Batch:0.679 | Total:0:01:40 | ETA:0:03:37 | Loss:0.6957404534548324 | top1:52.59382247924805\n",
      "161/431 Data:0.797 | Batch:1.345 | Total:0:01:48 | ETA:0:03:23 | Loss:0.6954207931246076 | top1:52.7191162109375\n",
      "171/431 Data:0.008 | Batch:0.708 | Total:0:01:56 | ETA:0:03:35 | Loss:0.6950627126191792 | top1:52.9012336730957\n",
      "181/431 Data:0.402 | Batch:0.826 | Total:0:02:03 | ETA:0:02:41 | Loss:0.6951387814395336 | top1:52.845306396484375\n",
      "191/431 Data:0.006 | Batch:0.396 | Total:0:02:08 | ETA:0:02:22 | Loss:0.6951908902972156 | top1:52.77777862548828\n",
      "201/431 Data:1.573 | Batch:2.074 | Total:0:02:16 | ETA:0:03:01 | Loss:0.695358743715049 | top1:52.6893310546875\n",
      "211/431 Data:0.000 | Batch:0.600 | Total:0:02:23 | ETA:0:02:27 | Loss:0.6951879791173889 | top1:52.79884338378906\n",
      "221/431 Data:0.004 | Batch:0.547 | Total:0:02:30 | ETA:0:02:33 | Loss:0.695116813635934 | top1:52.782806396484375\n",
      "231/431 Data:0.001 | Batch:0.585 | Total:0:02:39 | ETA:0:02:56 | Loss:0.6950391107823426 | top1:52.90283966064453\n",
      "241/431 Data:1.043 | Batch:1.568 | Total:0:02:46 | ETA:0:02:16 | Loss:0.6950722576671616 | top1:52.81005096435547\n",
      "251/431 Data:0.000 | Batch:0.411 | Total:0:02:53 | ETA:0:02:29 | Loss:0.6949586151130646 | top1:52.83089828491211\n",
      "261/431 Data:0.755 | Batch:1.158 | Total:0:03:00 | ETA:0:01:57 | Loss:0.6950001785124855 | top1:52.79267883300781\n",
      "271/431 Data:0.001 | Batch:0.603 | Total:0:03:06 | ETA:0:01:58 | Loss:0.6949225996253235 | top1:52.81877899169922\n",
      "281/431 Data:0.488 | Batch:1.042 | Total:0:03:14 | ETA:0:02:04 | Loss:0.6947994013698076 | top1:52.83708572387695\n",
      "291/431 Data:0.001 | Batch:0.795 | Total:0:03:21 | ETA:0:01:43 | Loss:0.6949202236031339 | top1:52.79686737060547\n",
      "301/431 Data:0.834 | Batch:1.440 | Total:0:03:29 | ETA:0:01:40 | Loss:0.6949922523625269 | top1:52.718711853027344\n",
      "311/431 Data:0.000 | Batch:0.433 | Total:0:03:36 | ETA:0:01:39 | Loss:0.6948507888140786 | top1:52.77241516113281\n",
      "321/431 Data:1.191 | Batch:1.886 | Total:0:03:45 | ETA:0:01:32 | Loss:0.6948443339249798 | top1:52.75701141357422\n",
      "331/431 Data:0.000 | Batch:0.480 | Total:0:03:51 | ETA:0:01:21 | Loss:0.6948195247491681 | top1:52.759315490722656\n",
      "341/431 Data:0.002 | Batch:0.494 | Total:0:03:56 | ETA:0:00:43 | Loss:0.6948247891017768 | top1:52.74030685424805\n",
      "351/431 Data:0.009 | Batch:0.582 | Total:0:04:01 | ETA:0:00:40 | Loss:0.6947853988052434 | top1:52.700225830078125\n",
      "361/431 Data:0.021 | Batch:0.595 | Total:0:04:06 | ETA:0:00:36 | Loss:0.6946036333522638 | top1:52.805477142333984\n",
      "371/431 Data:0.007 | Batch:0.470 | Total:0:04:12 | ETA:0:00:34 | Loss:0.6945958070035251 | top1:52.834678649902344\n",
      "381/431 Data:0.001 | Batch:0.374 | Total:0:04:18 | ETA:0:00:35 | Loss:0.6945736333141177 | top1:52.83319091796875\n",
      "391/431 Data:0.011 | Batch:0.529 | Total:0:04:24 | ETA:0:00:24 | Loss:0.6945091926533243 | top1:52.84171676635742\n",
      "401/431 Data:0.003 | Batch:0.538 | Total:0:04:30 | ETA:0:00:18 | Loss:0.6944959330142585 | top1:52.822113037109375\n",
      "411/431 Data:0.004 | Batch:0.578 | Total:0:04:36 | ETA:0:00:13 | Loss:0.6945591529790502 | top1:52.822383880615234\n",
      "421/431 Data:0.001 | Batch:0.486 | Total:0:04:42 | ETA:0:00:06 | Loss:0.6945848660344466 | top1:52.78437805175781\n",
      "44/44 Data:0.000 | Batch:0.084 | Total:0:00:28 | ETA:0:00:00 | Loss:0.6932842250053699 | top1:50.000003814697266\n",
      "\n",
      "Epoch: [3 | 300] LR: 0.240000\n",
      "1/431 Data:3.109 | Batch:3.544 | Total:0:00:03 | ETA:0:25:24 | Loss:0.6913228631019592 | top1:57.22222900390625\n",
      "11/431 Data:0.005 | Batch:0.603 | Total:0:00:09 | ETA:0:06:04 | Loss:0.6914604089476846 | top1:54.242427825927734\n",
      "21/431 Data:0.589 | Batch:1.041 | Total:0:00:16 | ETA:0:04:54 | Loss:0.6952076043401446 | top1:53.35979080200195\n",
      "31/431 Data:0.001 | Batch:0.588 | Total:0:00:21 | ETA:0:03:55 | Loss:0.694962045838756 | top1:52.79569625854492\n",
      "41/431 Data:0.072 | Batch:0.547 | Total:0:00:27 | ETA:0:03:34 | Loss:0.696766478259389 | top1:51.97831726074219\n",
      "51/431 Data:0.010 | Batch:0.688 | Total:0:00:33 | ETA:0:04:05 | Loss:0.6983086945963841 | top1:51.350765228271484\n",
      "61/431 Data:0.218 | Batch:0.786 | Total:0:00:40 | ETA:0:04:19 | Loss:0.6966318048414637 | top1:51.93989181518555\n",
      "71/431 Data:0.001 | Batch:0.650 | Total:0:00:46 | ETA:0:03:17 | Loss:0.695904126469518 | top1:52.12050247192383\n",
      "81/431 Data:0.001 | Batch:0.531 | Total:0:00:51 | ETA:0:03:32 | Loss:0.6953156921598647 | top1:52.28395080566406\n",
      "91/431 Data:0.000 | Batch:0.383 | Total:0:00:57 | ETA:0:03:09 | Loss:0.6949675600607317 | top1:52.40537643432617\n",
      "101/431 Data:0.010 | Batch:0.409 | Total:0:01:02 | ETA:0:03:08 | Loss:0.6948551553310734 | top1:52.249725341796875\n",
      "111/431 Data:0.980 | Batch:1.529 | Total:0:01:10 | ETA:0:04:16 | Loss:0.6950015635103792 | top1:52.25225067138672\n",
      "121/431 Data:0.002 | Batch:0.486 | Total:0:01:16 | ETA:0:03:35 | Loss:0.6948243325406854 | top1:52.20845031738281\n",
      "131/431 Data:0.432 | Batch:1.059 | Total:0:01:22 | ETA:0:02:50 | Loss:0.6947070528532713 | top1:52.23494338989258\n",
      "141/431 Data:0.006 | Batch:0.645 | Total:0:01:30 | ETA:0:03:58 | Loss:0.6943229458010789 | top1:52.478328704833984\n",
      "151/431 Data:0.100 | Batch:0.808 | Total:0:01:38 | ETA:0:03:40 | Loss:0.694134190382547 | top1:52.557029724121094\n",
      "161/431 Data:0.975 | Batch:1.638 | Total:0:01:47 | ETA:0:04:23 | Loss:0.6938575857914753 | top1:52.674259185791016\n",
      "171/431 Data:0.001 | Batch:0.801 | Total:0:01:57 | ETA:0:04:04 | Loss:0.693857733269184 | top1:52.628326416015625\n",
      "181/431 Data:1.384 | Batch:2.063 | Total:0:02:05 | ETA:0:03:37 | Loss:0.6939029433450646 | top1:52.50767517089844\n",
      "191/431 Data:0.000 | Batch:0.549 | Total:0:02:12 | ETA:0:03:19 | Loss:0.6939984091913513 | top1:52.446189880371094\n",
      "201/431 Data:1.088 | Batch:1.770 | Total:0:02:20 | ETA:0:03:03 | Loss:0.6938877772929063 | top1:52.44886779785156\n",
      "211/431 Data:0.008 | Batch:0.672 | Total:0:02:29 | ETA:0:03:21 | Loss:0.6937815222130003 | top1:52.556610107421875\n",
      "221/431 Data:0.007 | Batch:0.859 | Total:0:02:38 | ETA:0:03:05 | Loss:0.6936780395011557 | top1:52.682254791259766\n",
      "231/431 Data:0.017 | Batch:0.480 | Total:0:02:44 | ETA:0:02:09 | Loss:0.6934654124371418 | top1:52.77777862548828\n",
      "241/431 Data:0.005 | Batch:0.575 | Total:0:02:51 | ETA:0:02:18 | Loss:0.693130716743311 | top1:52.934532165527344\n",
      "251/431 Data:0.010 | Batch:0.435 | Total:0:03:00 | ETA:0:02:48 | Loss:0.6930597787834258 | top1:52.99689865112305\n",
      "261/431 Data:0.001 | Batch:0.724 | Total:0:03:06 | ETA:0:01:52 | Loss:0.6928061265141571 | top1:53.11409378051758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/431 Data:0.001 | Batch:0.394 | Total:0:03:14 | ETA:0:02:08 | Loss:0.6927873784765546 | top1:53.058631896972656\n",
      "281/431 Data:0.747 | Batch:1.258 | Total:0:03:20 | ETA:0:01:35 | Loss:0.6927767873234596 | top1:53.034793853759766\n",
      "291/431 Data:0.029 | Batch:0.610 | Total:0:03:26 | ETA:0:01:37 | Loss:0.692734172254084 | top1:53.05078125\n",
      "301/431 Data:0.934 | Batch:1.788 | Total:0:03:35 | ETA:0:01:59 | Loss:0.6926169650895255 | top1:53.108154296875\n",
      "311/431 Data:0.001 | Batch:0.522 | Total:0:03:40 | ETA:0:01:17 | Loss:0.6926162254388692 | top1:53.0975341796875\n",
      "321/431 Data:0.669 | Batch:1.161 | Total:0:03:48 | ETA:0:01:18 | Loss:0.6925445440402284 | top1:53.139495849609375\n",
      "331/431 Data:0.006 | Batch:0.558 | Total:0:03:54 | ETA:0:01:09 | Loss:0.6925136967367996 | top1:53.17052459716797\n",
      "341/431 Data:0.536 | Batch:1.018 | Total:0:04:02 | ETA:0:01:07 | Loss:0.6925067751288764 | top1:53.13457107543945\n",
      "351/431 Data:0.007 | Batch:0.708 | Total:0:04:08 | ETA:0:00:53 | Loss:0.6923955310443868 | top1:53.216209411621094\n",
      "361/431 Data:0.584 | Batch:1.133 | Total:0:04:14 | ETA:0:00:43 | Loss:0.6923038924830112 | top1:53.28254699707031\n",
      "371/431 Data:0.000 | Batch:0.376 | Total:0:04:22 | ETA:0:00:52 | Loss:0.6921665090113637 | top1:53.343814849853516\n",
      "381/431 Data:0.061 | Batch:0.620 | Total:0:04:27 | ETA:0:00:27 | Loss:0.6921848983276547 | top1:53.34208297729492\n",
      "391/431 Data:0.000 | Batch:0.343 | Total:0:04:34 | ETA:0:00:29 | Loss:0.6921875475312743 | top1:53.360328674316406\n",
      "401/431 Data:0.011 | Batch:0.608 | Total:0:04:40 | ETA:0:00:19 | Loss:0.6922639730267989 | top1:53.31809616088867\n",
      "411/431 Data:0.007 | Batch:0.789 | Total:0:04:46 | ETA:0:00:12 | Loss:0.6922410082643049 | top1:53.330631256103516\n",
      "421/431 Data:0.001 | Batch:0.426 | Total:0:04:52 | ETA:0:00:07 | Loss:0.6922053251583616 | top1:53.32805633544922\n",
      "44/44 Data:0.000 | Batch:0.086 | Total:0:00:29 | ETA:0:00:00 | Loss:0.6940588822731605 | top1:50.0\n",
      "\n",
      "Epoch: [4 | 300] LR: 0.310000\n",
      "1/431 Data:4.816 | Batch:5.393 | Total:0:00:05 | ETA:0:38:48 | Loss:0.6880471110343933 | top1:60.55555725097656\n",
      "11/431 Data:0.000 | Batch:0.610 | Total:0:00:10 | ETA:0:06:41 | Loss:0.69170757857236 | top1:53.939395904541016\n",
      "21/431 Data:0.496 | Batch:0.901 | Total:0:00:15 | ETA:0:03:42 | Loss:0.6914363815670922 | top1:53.650794982910156\n",
      "31/431 Data:0.001 | Batch:0.461 | Total:0:00:21 | ETA:0:03:44 | Loss:0.6923111907897457 | top1:53.727596282958984\n",
      "41/431 Data:0.853 | Batch:1.262 | Total:0:00:27 | ETA:0:03:57 | Loss:0.6924225571678906 | top1:53.428184509277344\n",
      "51/431 Data:0.000 | Batch:0.472 | Total:0:00:33 | ETA:0:04:11 | Loss:0.6921175157322603 | top1:53.48583984375\n",
      "61/431 Data:0.024 | Batch:0.351 | Total:0:00:38 | ETA:0:03:10 | Loss:0.6908959892929577 | top1:53.91621017456055\n",
      "71/431 Data:0.001 | Batch:0.439 | Total:0:00:43 | ETA:0:03:11 | Loss:0.6917089635217694 | top1:53.49765396118164\n",
      "81/431 Data:0.002 | Batch:0.487 | Total:0:00:48 | ETA:0:02:50 | Loss:0.6918821312763073 | top1:53.347049713134766\n",
      "91/431 Data:0.106 | Batch:0.750 | Total:0:00:54 | ETA:0:03:17 | Loss:0.691830921304095 | top1:53.29059982299805\n",
      "101/431 Data:0.001 | Batch:0.396 | Total:0:01:01 | ETA:0:03:57 | Loss:0.6918834382944768 | top1:53.3718376159668\n",
      "111/431 Data:0.005 | Batch:0.512 | Total:0:01:06 | ETA:0:03:11 | Loss:0.6917963774354609 | top1:53.438438415527344\n",
      "121/431 Data:0.001 | Batch:0.630 | Total:0:01:14 | ETA:0:03:41 | Loss:0.691563507742133 | top1:53.613407135009766\n",
      "131/431 Data:0.001 | Batch:0.773 | Total:0:01:21 | ETA:0:03:23 | Loss:0.6916506313185655 | top1:53.502967834472656\n",
      "141/431 Data:0.096 | Batch:0.595 | Total:0:01:27 | ETA:0:03:02 | Loss:0.6916856465610206 | top1:53.534278869628906\n",
      "151/431 Data:0.000 | Batch:0.453 | Total:0:01:34 | ETA:0:03:44 | Loss:0.6917499197239907 | top1:53.5283317565918\n",
      "161/431 Data:2.092 | Batch:2.462 | Total:0:01:44 | ETA:0:04:21 | Loss:0.6914883492896275 | top1:53.595584869384766\n",
      "171/431 Data:0.005 | Batch:0.681 | Total:0:01:50 | ETA:0:02:39 | Loss:0.6915238510098374 | top1:53.55100631713867\n",
      "181/431 Data:0.246 | Batch:0.658 | Total:0:01:59 | ETA:0:03:53 | Loss:0.69138579177593 | top1:53.615718841552734\n",
      "191/431 Data:0.001 | Batch:0.731 | Total:0:02:07 | ETA:0:02:57 | Loss:0.6913724906781581 | top1:53.61256790161133\n",
      "201/431 Data:0.001 | Batch:0.426 | Total:0:02:15 | ETA:0:03:03 | Loss:0.6913864304177204 | top1:53.64289855957031\n",
      "211/431 Data:0.246 | Batch:0.808 | Total:0:02:22 | ETA:0:02:44 | Loss:0.6912512058895346 | top1:53.73881149291992\n",
      "221/431 Data:0.001 | Batch:0.779 | Total:0:02:30 | ETA:0:02:43 | Loss:0.6912071610468006 | top1:53.793365478515625\n",
      "231/431 Data:0.588 | Batch:1.165 | Total:0:02:38 | ETA:0:02:41 | Loss:0.6911655584455052 | top1:53.85041046142578\n",
      "241/431 Data:0.005 | Batch:0.688 | Total:0:02:45 | ETA:0:02:21 | Loss:0.6909799595591438 | top1:53.95574188232422\n",
      "251/431 Data:0.389 | Batch:1.061 | Total:0:02:52 | ETA:0:02:09 | Loss:0.6909407645582678 | top1:53.97078323364258\n",
      "261/431 Data:0.002 | Batch:0.445 | Total:0:02:59 | ETA:0:01:59 | Loss:0.690941182361252 | top1:53.96977615356445\n",
      "271/431 Data:0.018 | Batch:0.438 | Total:0:03:06 | ETA:0:01:39 | Loss:0.6909563708569291 | top1:53.97499084472656\n",
      "281/431 Data:0.278 | Batch:0.869 | Total:0:03:12 | ETA:0:01:27 | Loss:0.6907629118271146 | top1:54.05495834350586\n",
      "291/431 Data:0.204 | Batch:0.676 | Total:0:03:19 | ETA:0:01:44 | Loss:0.6909926044162606 | top1:53.98243713378906\n",
      "301/431 Data:0.220 | Batch:0.804 | Total:0:03:26 | ETA:0:01:26 | Loss:0.6910254911726892 | top1:53.977481842041016\n",
      "311/431 Data:0.671 | Batch:1.240 | Total:0:03:33 | ETA:0:01:33 | Loss:0.6911017252701271 | top1:53.901390075683594\n",
      "321/431 Data:0.047 | Batch:0.794 | Total:0:03:41 | ETA:0:01:22 | Loss:0.6911665630117755 | top1:53.8335075378418\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
