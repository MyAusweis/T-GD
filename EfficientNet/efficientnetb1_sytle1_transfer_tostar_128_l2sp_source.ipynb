{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b1/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b1' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 5000\n",
    "start_epoch = 0\n",
    "train_batch = 32\n",
    "test_batch = 300\n",
    "lr = 0.03\n",
    "schedule = [200, 1500, 3000]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b1/to_star/l2sp_style1' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 4\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# sp\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "fc_name = 'fc.'\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(target_dir, '100_shot_style1')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b1/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "source_model = EfficientNet.from_name(model_name, num_classes=num_classes)\n",
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    source_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 6.52M\n"
     ]
    }
   ],
   "source": [
    "source_model.to('cuda')\n",
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_bn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        16, 16, kernel_size=(3, 3), stride=(1, 1), groups=16, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        16, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(4, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(24, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(24, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(6, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(36, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(10, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(10, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(10, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(60, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(20, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(120, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(28, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(168, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(48, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(288, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(80, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): GroupNorm(480, 1920, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1920, 1920, kernel_size=(3, 3), stride=(1, 1), groups=1920, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): GroupNorm(480, 1920, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1920, 80, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        80, 1920, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): GroupNorm(80, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): GroupNorm(320, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in source_model.parameters():\n",
    "    param.requires_grad = False\n",
    "source_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model_weights = {}\n",
    "for name, param in source_model.named_parameters():\n",
    "    source_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - source_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1          [-1, 3, 129, 129]               0\n",
      "Conv2dStaticSamePadding-2           [-1, 32, 64, 64]             864\n",
      "         GroupNorm-3           [-1, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-4           [-1, 32, 64, 64]               0\n",
      "         ZeroPad2d-5           [-1, 32, 66, 66]               0\n",
      "Conv2dStaticSamePadding-6           [-1, 32, 64, 64]             288\n",
      "         GroupNorm-7           [-1, 32, 64, 64]              64\n",
      "MemoryEfficientSwish-8           [-1, 32, 64, 64]               0\n",
      "          Identity-9             [-1, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11              [-1, 8, 1, 1]               0\n",
      "         Identity-12              [-1, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288\n",
      "         Identity-14           [-1, 32, 64, 64]               0\n",
      "Conv2dStaticSamePadding-15           [-1, 16, 64, 64]             512\n",
      "        GroupNorm-16           [-1, 16, 64, 64]              32\n",
      "      MBConvBlock-17           [-1, 16, 64, 64]               0\n",
      "        ZeroPad2d-18           [-1, 16, 66, 66]               0\n",
      "Conv2dStaticSamePadding-19           [-1, 16, 64, 64]             144\n",
      "        GroupNorm-20           [-1, 16, 64, 64]              32\n",
      "MemoryEfficientSwish-21           [-1, 16, 64, 64]               0\n",
      "         Identity-22             [-1, 16, 1, 1]               0\n",
      "Conv2dStaticSamePadding-23              [-1, 4, 1, 1]              68\n",
      "MemoryEfficientSwish-24              [-1, 4, 1, 1]               0\n",
      "         Identity-25              [-1, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-26             [-1, 16, 1, 1]              80\n",
      "         Identity-27           [-1, 16, 64, 64]               0\n",
      "Conv2dStaticSamePadding-28           [-1, 16, 64, 64]             256\n",
      "        GroupNorm-29           [-1, 16, 64, 64]              32\n",
      "      MBConvBlock-30           [-1, 16, 64, 64]               0\n",
      "         Identity-31           [-1, 16, 64, 64]               0\n",
      "Conv2dStaticSamePadding-32           [-1, 96, 64, 64]           1,536\n",
      "        GroupNorm-33           [-1, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-34           [-1, 96, 64, 64]               0\n",
      "        ZeroPad2d-35           [-1, 96, 65, 65]               0\n",
      "Conv2dStaticSamePadding-36           [-1, 96, 32, 32]             864\n",
      "        GroupNorm-37           [-1, 96, 32, 32]             192\n",
      "MemoryEfficientSwish-38           [-1, 96, 32, 32]               0\n",
      "         Identity-39             [-1, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-40              [-1, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-41              [-1, 4, 1, 1]               0\n",
      "         Identity-42              [-1, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-43             [-1, 96, 1, 1]             480\n",
      "         Identity-44           [-1, 96, 32, 32]               0\n",
      "Conv2dStaticSamePadding-45           [-1, 24, 32, 32]           2,304\n",
      "        GroupNorm-46           [-1, 24, 32, 32]              48\n",
      "      MBConvBlock-47           [-1, 24, 32, 32]               0\n",
      "         Identity-48           [-1, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-49          [-1, 144, 32, 32]           3,456\n",
      "        GroupNorm-50          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-51          [-1, 144, 32, 32]               0\n",
      "        ZeroPad2d-52          [-1, 144, 34, 34]               0\n",
      "Conv2dStaticSamePadding-53          [-1, 144, 32, 32]           1,296\n",
      "        GroupNorm-54          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-55          [-1, 144, 32, 32]               0\n",
      "         Identity-56            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-57              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-58              [-1, 6, 1, 1]               0\n",
      "         Identity-59              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-60            [-1, 144, 1, 1]           1,008\n",
      "         Identity-61          [-1, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-62           [-1, 24, 32, 32]           3,456\n",
      "        GroupNorm-63           [-1, 24, 32, 32]              48\n",
      "      MBConvBlock-64           [-1, 24, 32, 32]               0\n",
      "         Identity-65           [-1, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-66          [-1, 144, 32, 32]           3,456\n",
      "        GroupNorm-67          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-68          [-1, 144, 32, 32]               0\n",
      "        ZeroPad2d-69          [-1, 144, 34, 34]               0\n",
      "Conv2dStaticSamePadding-70          [-1, 144, 32, 32]           1,296\n",
      "        GroupNorm-71          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-72          [-1, 144, 32, 32]               0\n",
      "         Identity-73            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-74              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-75              [-1, 6, 1, 1]               0\n",
      "         Identity-76              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-77            [-1, 144, 1, 1]           1,008\n",
      "         Identity-78          [-1, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-79           [-1, 24, 32, 32]           3,456\n",
      "        GroupNorm-80           [-1, 24, 32, 32]              48\n",
      "      MBConvBlock-81           [-1, 24, 32, 32]               0\n",
      "         Identity-82           [-1, 24, 32, 32]               0\n",
      "Conv2dStaticSamePadding-83          [-1, 144, 32, 32]           3,456\n",
      "        GroupNorm-84          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-85          [-1, 144, 32, 32]               0\n",
      "        ZeroPad2d-86          [-1, 144, 35, 35]               0\n",
      "Conv2dStaticSamePadding-87          [-1, 144, 16, 16]           3,600\n",
      "        GroupNorm-88          [-1, 144, 16, 16]             288\n",
      "MemoryEfficientSwish-89          [-1, 144, 16, 16]               0\n",
      "         Identity-90            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-91              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-92              [-1, 6, 1, 1]               0\n",
      "         Identity-93              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-94            [-1, 144, 1, 1]           1,008\n",
      "         Identity-95          [-1, 144, 16, 16]               0\n",
      "Conv2dStaticSamePadding-96           [-1, 40, 16, 16]           5,760\n",
      "        GroupNorm-97           [-1, 40, 16, 16]              80\n",
      "      MBConvBlock-98           [-1, 40, 16, 16]               0\n",
      "         Identity-99           [-1, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-100          [-1, 240, 16, 16]           9,600\n",
      "       GroupNorm-101          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-102          [-1, 240, 16, 16]               0\n",
      "       ZeroPad2d-103          [-1, 240, 20, 20]               0\n",
      "Conv2dStaticSamePadding-104          [-1, 240, 16, 16]           6,000\n",
      "       GroupNorm-105          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-106          [-1, 240, 16, 16]               0\n",
      "        Identity-107            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-108             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-109             [-1, 10, 1, 1]               0\n",
      "        Identity-110             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-111            [-1, 240, 1, 1]           2,640\n",
      "        Identity-112          [-1, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-113           [-1, 40, 16, 16]           9,600\n",
      "       GroupNorm-114           [-1, 40, 16, 16]              80\n",
      "     MBConvBlock-115           [-1, 40, 16, 16]               0\n",
      "        Identity-116           [-1, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-117          [-1, 240, 16, 16]           9,600\n",
      "       GroupNorm-118          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-119          [-1, 240, 16, 16]               0\n",
      "       ZeroPad2d-120          [-1, 240, 20, 20]               0\n",
      "Conv2dStaticSamePadding-121          [-1, 240, 16, 16]           6,000\n",
      "       GroupNorm-122          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-123          [-1, 240, 16, 16]               0\n",
      "        Identity-124            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-125             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-126             [-1, 10, 1, 1]               0\n",
      "        Identity-127             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-128            [-1, 240, 1, 1]           2,640\n",
      "        Identity-129          [-1, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-130           [-1, 40, 16, 16]           9,600\n",
      "       GroupNorm-131           [-1, 40, 16, 16]              80\n",
      "     MBConvBlock-132           [-1, 40, 16, 16]               0\n",
      "        Identity-133           [-1, 40, 16, 16]               0\n",
      "Conv2dStaticSamePadding-134          [-1, 240, 16, 16]           9,600\n",
      "       GroupNorm-135          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-136          [-1, 240, 16, 16]               0\n",
      "       ZeroPad2d-137          [-1, 240, 17, 17]               0\n",
      "Conv2dStaticSamePadding-138            [-1, 240, 8, 8]           2,160\n",
      "       GroupNorm-139            [-1, 240, 8, 8]             480\n",
      "MemoryEfficientSwish-140            [-1, 240, 8, 8]               0\n",
      "        Identity-141            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-142             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-143             [-1, 10, 1, 1]               0\n",
      "        Identity-144             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-145            [-1, 240, 1, 1]           2,640\n",
      "        Identity-146            [-1, 240, 8, 8]               0\n",
      "Conv2dStaticSamePadding-147             [-1, 80, 8, 8]          19,200\n",
      "       GroupNorm-148             [-1, 80, 8, 8]             160\n",
      "     MBConvBlock-149             [-1, 80, 8, 8]               0\n",
      "        Identity-150             [-1, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-151            [-1, 480, 8, 8]          38,400\n",
      "       GroupNorm-152            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-153            [-1, 480, 8, 8]               0\n",
      "       ZeroPad2d-154          [-1, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-155            [-1, 480, 8, 8]           4,320\n",
      "       GroupNorm-156            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-157            [-1, 480, 8, 8]               0\n",
      "        Identity-158            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-159             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-160             [-1, 20, 1, 1]               0\n",
      "        Identity-161             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-162            [-1, 480, 1, 1]          10,080\n",
      "        Identity-163            [-1, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-164             [-1, 80, 8, 8]          38,400\n",
      "       GroupNorm-165             [-1, 80, 8, 8]             160\n",
      "     MBConvBlock-166             [-1, 80, 8, 8]               0\n",
      "        Identity-167             [-1, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-168            [-1, 480, 8, 8]          38,400\n",
      "       GroupNorm-169            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-170            [-1, 480, 8, 8]               0\n",
      "       ZeroPad2d-171          [-1, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-172            [-1, 480, 8, 8]           4,320\n",
      "       GroupNorm-173            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-174            [-1, 480, 8, 8]               0\n",
      "        Identity-175            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-176             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-177             [-1, 20, 1, 1]               0\n",
      "        Identity-178             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-179            [-1, 480, 1, 1]          10,080\n",
      "        Identity-180            [-1, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-181             [-1, 80, 8, 8]          38,400\n",
      "       GroupNorm-182             [-1, 80, 8, 8]             160\n",
      "     MBConvBlock-183             [-1, 80, 8, 8]               0\n",
      "        Identity-184             [-1, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-185            [-1, 480, 8, 8]          38,400\n",
      "       GroupNorm-186            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-187            [-1, 480, 8, 8]               0\n",
      "       ZeroPad2d-188          [-1, 480, 10, 10]               0\n",
      "Conv2dStaticSamePadding-189            [-1, 480, 8, 8]           4,320\n",
      "       GroupNorm-190            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-191            [-1, 480, 8, 8]               0\n",
      "        Identity-192            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-193             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-194             [-1, 20, 1, 1]               0\n",
      "        Identity-195             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-196            [-1, 480, 1, 1]          10,080\n",
      "        Identity-197            [-1, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-198             [-1, 80, 8, 8]          38,400\n",
      "       GroupNorm-199             [-1, 80, 8, 8]             160\n",
      "     MBConvBlock-200             [-1, 80, 8, 8]               0\n",
      "        Identity-201             [-1, 80, 8, 8]               0\n",
      "Conv2dStaticSamePadding-202            [-1, 480, 8, 8]          38,400\n",
      "       GroupNorm-203            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-204            [-1, 480, 8, 8]               0\n",
      "       ZeroPad2d-205          [-1, 480, 12, 12]               0\n",
      "Conv2dStaticSamePadding-206            [-1, 480, 8, 8]          12,000\n",
      "       GroupNorm-207            [-1, 480, 8, 8]             960\n",
      "MemoryEfficientSwish-208            [-1, 480, 8, 8]               0\n",
      "        Identity-209            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-210             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-211             [-1, 20, 1, 1]               0\n",
      "        Identity-212             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-213            [-1, 480, 1, 1]          10,080\n",
      "        Identity-214            [-1, 480, 8, 8]               0\n",
      "Conv2dStaticSamePadding-215            [-1, 112, 8, 8]          53,760\n",
      "       GroupNorm-216            [-1, 112, 8, 8]             224\n",
      "     MBConvBlock-217            [-1, 112, 8, 8]               0\n",
      "        Identity-218            [-1, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-219            [-1, 672, 8, 8]          75,264\n",
      "       GroupNorm-220            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-221            [-1, 672, 8, 8]               0\n",
      "       ZeroPad2d-222          [-1, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-223            [-1, 672, 8, 8]          16,800\n",
      "       GroupNorm-224            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-225            [-1, 672, 8, 8]               0\n",
      "        Identity-226            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-227             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-228             [-1, 28, 1, 1]               0\n",
      "        Identity-229             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-230            [-1, 672, 1, 1]          19,488\n",
      "        Identity-231            [-1, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-232            [-1, 112, 8, 8]          75,264\n",
      "       GroupNorm-233            [-1, 112, 8, 8]             224\n",
      "     MBConvBlock-234            [-1, 112, 8, 8]               0\n",
      "        Identity-235            [-1, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-236            [-1, 672, 8, 8]          75,264\n",
      "       GroupNorm-237            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-238            [-1, 672, 8, 8]               0\n",
      "       ZeroPad2d-239          [-1, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-240            [-1, 672, 8, 8]          16,800\n",
      "       GroupNorm-241            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-242            [-1, 672, 8, 8]               0\n",
      "        Identity-243            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-244             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-245             [-1, 28, 1, 1]               0\n",
      "        Identity-246             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-247            [-1, 672, 1, 1]          19,488\n",
      "        Identity-248            [-1, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-249            [-1, 112, 8, 8]          75,264\n",
      "       GroupNorm-250            [-1, 112, 8, 8]             224\n",
      "     MBConvBlock-251            [-1, 112, 8, 8]               0\n",
      "        Identity-252            [-1, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-253            [-1, 672, 8, 8]          75,264\n",
      "       GroupNorm-254            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-255            [-1, 672, 8, 8]               0\n",
      "       ZeroPad2d-256          [-1, 672, 12, 12]               0\n",
      "Conv2dStaticSamePadding-257            [-1, 672, 8, 8]          16,800\n",
      "       GroupNorm-258            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-259            [-1, 672, 8, 8]               0\n",
      "        Identity-260            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-261             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-262             [-1, 28, 1, 1]               0\n",
      "        Identity-263             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-264            [-1, 672, 1, 1]          19,488\n",
      "        Identity-265            [-1, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-266            [-1, 112, 8, 8]          75,264\n",
      "       GroupNorm-267            [-1, 112, 8, 8]             224\n",
      "     MBConvBlock-268            [-1, 112, 8, 8]               0\n",
      "        Identity-269            [-1, 112, 8, 8]               0\n",
      "Conv2dStaticSamePadding-270            [-1, 672, 8, 8]          75,264\n",
      "       GroupNorm-271            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-272            [-1, 672, 8, 8]               0\n",
      "       ZeroPad2d-273          [-1, 672, 11, 11]               0\n",
      "Conv2dStaticSamePadding-274            [-1, 672, 4, 4]          16,800\n",
      "       GroupNorm-275            [-1, 672, 4, 4]           1,344\n",
      "MemoryEfficientSwish-276            [-1, 672, 4, 4]               0\n",
      "        Identity-277            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-278             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-279             [-1, 28, 1, 1]               0\n",
      "        Identity-280             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-281            [-1, 672, 1, 1]          19,488\n",
      "        Identity-282            [-1, 672, 4, 4]               0\n",
      "Conv2dStaticSamePadding-283            [-1, 192, 4, 4]         129,024\n",
      "       GroupNorm-284            [-1, 192, 4, 4]             384\n",
      "     MBConvBlock-285            [-1, 192, 4, 4]               0\n",
      "        Identity-286            [-1, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-287           [-1, 1152, 4, 4]         221,184\n",
      "       GroupNorm-288           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-289           [-1, 1152, 4, 4]               0\n",
      "       ZeroPad2d-290           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-291           [-1, 1152, 4, 4]          28,800\n",
      "       GroupNorm-292           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-293           [-1, 1152, 4, 4]               0\n",
      "        Identity-294           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-295             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-296             [-1, 48, 1, 1]               0\n",
      "        Identity-297             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-298           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-299           [-1, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-300            [-1, 192, 4, 4]         221,184\n",
      "       GroupNorm-301            [-1, 192, 4, 4]             384\n",
      "     MBConvBlock-302            [-1, 192, 4, 4]               0\n",
      "        Identity-303            [-1, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-304           [-1, 1152, 4, 4]         221,184\n",
      "       GroupNorm-305           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-306           [-1, 1152, 4, 4]               0\n",
      "       ZeroPad2d-307           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-308           [-1, 1152, 4, 4]          28,800\n",
      "       GroupNorm-309           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-310           [-1, 1152, 4, 4]               0\n",
      "        Identity-311           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-312             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-313             [-1, 48, 1, 1]               0\n",
      "        Identity-314             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-315           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-316           [-1, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-317            [-1, 192, 4, 4]         221,184\n",
      "       GroupNorm-318            [-1, 192, 4, 4]             384\n",
      "     MBConvBlock-319            [-1, 192, 4, 4]               0\n",
      "        Identity-320            [-1, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-321           [-1, 1152, 4, 4]         221,184\n",
      "       GroupNorm-322           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-323           [-1, 1152, 4, 4]               0\n",
      "       ZeroPad2d-324           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-325           [-1, 1152, 4, 4]          28,800\n",
      "       GroupNorm-326           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-327           [-1, 1152, 4, 4]               0\n",
      "        Identity-328           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-329             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-330             [-1, 48, 1, 1]               0\n",
      "        Identity-331             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-332           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-333           [-1, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-334            [-1, 192, 4, 4]         221,184\n",
      "       GroupNorm-335            [-1, 192, 4, 4]             384\n",
      "     MBConvBlock-336            [-1, 192, 4, 4]               0\n",
      "        Identity-337            [-1, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-338           [-1, 1152, 4, 4]         221,184\n",
      "       GroupNorm-339           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-340           [-1, 1152, 4, 4]               0\n",
      "       ZeroPad2d-341           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-342           [-1, 1152, 4, 4]          28,800\n",
      "       GroupNorm-343           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-344           [-1, 1152, 4, 4]               0\n",
      "        Identity-345           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-346             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-347             [-1, 48, 1, 1]               0\n",
      "        Identity-348             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-349           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-350           [-1, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-351            [-1, 192, 4, 4]         221,184\n",
      "       GroupNorm-352            [-1, 192, 4, 4]             384\n",
      "     MBConvBlock-353            [-1, 192, 4, 4]               0\n",
      "        Identity-354            [-1, 192, 4, 4]               0\n",
      "Conv2dStaticSamePadding-355           [-1, 1152, 4, 4]         221,184\n",
      "       GroupNorm-356           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-357           [-1, 1152, 4, 4]               0\n",
      "       ZeroPad2d-358           [-1, 1152, 6, 6]               0\n",
      "Conv2dStaticSamePadding-359           [-1, 1152, 4, 4]          10,368\n",
      "       GroupNorm-360           [-1, 1152, 4, 4]           2,304\n",
      "MemoryEfficientSwish-361           [-1, 1152, 4, 4]               0\n",
      "        Identity-362           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-363             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-364             [-1, 48, 1, 1]               0\n",
      "        Identity-365             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-366           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-367           [-1, 1152, 4, 4]               0\n",
      "Conv2dStaticSamePadding-368            [-1, 320, 4, 4]         368,640\n",
      "       GroupNorm-369            [-1, 320, 4, 4]             640\n",
      "     MBConvBlock-370            [-1, 320, 4, 4]               0\n",
      "        Identity-371            [-1, 320, 4, 4]               0\n",
      "Conv2dStaticSamePadding-372           [-1, 1920, 4, 4]         614,400\n",
      "       GroupNorm-373           [-1, 1920, 4, 4]           3,840\n",
      "MemoryEfficientSwish-374           [-1, 1920, 4, 4]               0\n",
      "       ZeroPad2d-375           [-1, 1920, 6, 6]               0\n",
      "Conv2dStaticSamePadding-376           [-1, 1920, 4, 4]          17,280\n",
      "       GroupNorm-377           [-1, 1920, 4, 4]           3,840\n",
      "MemoryEfficientSwish-378           [-1, 1920, 4, 4]               0\n",
      "        Identity-379           [-1, 1920, 1, 1]               0\n",
      "Conv2dStaticSamePadding-380             [-1, 80, 1, 1]         153,680\n",
      "MemoryEfficientSwish-381             [-1, 80, 1, 1]               0\n",
      "        Identity-382             [-1, 80, 1, 1]               0\n",
      "Conv2dStaticSamePadding-383           [-1, 1920, 1, 1]         155,520\n",
      "        Identity-384           [-1, 1920, 4, 4]               0\n",
      "Conv2dStaticSamePadding-385            [-1, 320, 4, 4]         614,400\n",
      "       GroupNorm-386            [-1, 320, 4, 4]             640\n",
      "     MBConvBlock-387            [-1, 320, 4, 4]               0\n",
      "        Identity-388            [-1, 320, 4, 4]               0\n",
      "Conv2dStaticSamePadding-389           [-1, 1280, 4, 4]         409,600\n",
      "       GroupNorm-390           [-1, 1280, 4, 4]           2,560\n",
      "MemoryEfficientSwish-391           [-1, 1280, 4, 4]               0\n",
      "AdaptiveAvgPool2d-392           [-1, 1280, 1, 1]               0\n",
      "         Dropout-393                 [-1, 1280]               0\n",
      "          Linear-394                    [-1, 2]           2,562\n",
      "================================================================\n",
      "Total params: 6,515,746\n",
      "Trainable params: 6,515,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 99.20\n",
      "Params size (MB): 24.86\n",
      "Estimated Total Size (MB): 124.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3,128,128),device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.', 'Source Loss', 'Source ACC.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss = loss_main + alpha*loss_sp + beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    print('{batch}/{size} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(model)\n",
    "        loss_sp = reg_l2sp(model)\n",
    "        loss = loss_main + alpha*loss_sp + beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 5000] LR: 0.030000\n",
      "1/13 | Total:0:00:03 | ETA:0:00:40 | Loss:2.171658515930176 | top1:71.875\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:2.0552837415175005 | top1:59.090911865234375\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:2.1971473693847656 | top1:59.114585876464844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 | Total:0:00:48 | ETA:0:00:00 | Loss:4.203340092305274 | top1:50.000003814697266\n",
      "26/26 | Total:0:00:31 | ETA:0:00:00 | Loss:4.179152983885545 | top1:50.0\n",
      "\n",
      "Epoch: [2 | 5000] LR: 0.051000\n",
      "1/13 | Total:0:00:02 | ETA:0:00:31 | Loss:4.208789825439453 | top1:46.875\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:6.309647430073131 | top1:52.272727966308594\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:6.376481254895528 | top1:53.125\n",
      "\n",
      "Epoch: [3 | 5000] LR: 0.072000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:6.97813081741333 | top1:43.75\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:5.131925387815996 | top1:55.397727966308594\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:4.940211872259776 | top1:55.72916793823242\n",
      "\n",
      "Epoch: [4 | 5000] LR: 0.093000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:22 | Loss:2.4845900535583496 | top1:53.125\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:28.710534886880353 | top1:65.05682373046875\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:52.70183023810387 | top1:64.32292175292969\n",
      "\n",
      "Epoch: [5 | 5000] LR: 0.114000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:24 | Loss:471.7242431640625 | top1:68.75\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:1134.0569014115767 | top1:55.11363983154297\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:1144.142562866211 | top1:53.645835876464844\n",
      "\n",
      "Epoch: [6 | 5000] LR: 0.135000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:20 | Loss:1158.3367919921875 | top1:59.375\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:587.8750915527344 | top1:59.375\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:562.1282475789388 | top1:59.375\n",
      "\n",
      "Epoch: [7 | 5000] LR: 0.156000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:307.9631652832031 | top1:53.125\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:399.5488974831321 | top1:50.85227584838867\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:393.1985855102539 | top1:50.78125\n",
      "\n",
      "Epoch: [8 | 5000] LR: 0.177000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:283.3994140625 | top1:50.0\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:119.206787109375 | top1:54.54545593261719\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:115.03020159403484 | top1:54.6875\n",
      "\n",
      "Epoch: [9 | 5000] LR: 0.198000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:148.2314910888672 | top1:46.875\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:1017.0755878795277 | top1:47.44318389892578\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:1031.9917538960774 | top1:47.13541793823242\n",
      "\n",
      "Epoch: [10 | 5000] LR: 0.219000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:20 | Loss:1039.527587890625 | top1:46.875\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:352.61012216047806 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:338.4195818901062 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [11 | 5000] LR: 0.240000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:540.6032104492188 | top1:28.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:2159.6454467773438 | top1:45.17045593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:2123.977238972982 | top1:45.57291793823242\n",
      "\n",
      "Epoch: [12 | 5000] LR: 0.240000\n",
      "1/13 | Total:0:00:00 | ETA:0:00:11 | Loss:1349.4498291015625 | top1:62.5\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:405.9533580433239 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:05 | ETA:0:00:01 | Loss:405.4769007364909 | top1:52.083335876464844\n",
      "\n",
      "Epoch: [13 | 5000] LR: 0.240000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:478.5860900878906 | top1:40.625\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:410.537954157049 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:05 | ETA:0:00:01 | Loss:384.0529677073161 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [14 | 5000] LR: 0.240000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:53.65057373046875 | top1:46.875\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:93.00668863816695 | top1:48.57954788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:102.32693076133728 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [15 | 5000] LR: 0.240000\n",
      "1/13 | Total:0:00:00 | ETA:0:00:12 | Loss:195.87440490722656 | top1:46.875\n",
      "11/13 | Total:0:00:04 | ETA:0:00:01 | Loss:88.39053522456776 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:05 | ETA:0:00:01 | Loss:81.2828030983607 | top1:53.90625\n",
      "\n",
      "Epoch: [16 | 5000] LR: 0.240000\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:75.45337677001953 | top1:43.75\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:1245.9377004450018 | top1:47.44318389892578\n",
      "13/13 | Total:0:00:05 | ETA:0:00:01 | Loss:1299.8874212900798 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [17 | 5000] LR: 0.239999\n",
      "1/13 | Total:0:00:00 | ETA:0:00:11 | Loss:1701.46337890625 | top1:50.0\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:676.3610340465199 | top1:50.28409194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:632.326613108317 | top1:50.520835876464844\n",
      "\n",
      "Epoch: [18 | 5000] LR: 0.239999\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:214.19775390625 | top1:56.25\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:361.5997786088423 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:349.1456654866536 | top1:53.90625\n",
      "\n",
      "Epoch: [19 | 5000] LR: 0.239999\n",
      "1/13 | Total:0:00:00 | ETA:0:00:11 | Loss:159.34605407714844 | top1:37.5\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:121.1878617026589 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:149.16333866119385 | top1:51.5625\n",
      "\n",
      "Epoch: [20 | 5000] LR: 0.239998\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:521.6290893554688 | top1:31.25\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:390.9769349531694 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:363.62879212697345 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [21 | 5000] LR: 0.239998\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:31.68441390991211 | top1:71.875\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:43.93808451565829 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:48.28317372004191 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [22 | 5000] LR: 0.239998\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:93.19300842285156 | top1:50.0\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:46.80162438479337 | top1:54.82954788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:43.37984037399292 | top1:55.72916793823242\n",
      "\n",
      "Epoch: [23 | 5000] LR: 0.239997\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:7.582751274108887 | top1:59.375\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:64.61900277571245 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:66.41237020492554 | top1:51.5625\n",
      "\n",
      "Epoch: [24 | 5000] LR: 0.239997\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:74.60757446289062 | top1:43.75\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:26.83937738158486 | top1:53.97727584838867\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:26.28446791569392 | top1:54.6875\n",
      "\n",
      "Epoch: [25 | 5000] LR: 0.239996\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:32.5427360534668 | top1:53.125\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:53.32802963256836 | top1:47.44318389892578\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:51.15865103403727 | top1:48.4375\n",
      "\n",
      "Epoch: [26 | 5000] LR: 0.239995\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:19.605533599853516 | top1:59.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:15.512646241621537 | top1:53.69318389892578\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:18.783230940500896 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [27 | 5000] LR: 0.239995\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:61.33321762084961 | top1:56.25\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:44.635856021534316 | top1:54.2613639831543\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:41.5464813709259 | top1:53.125\n",
      "\n",
      "Epoch: [28 | 5000] LR: 0.239994\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:4.307722091674805 | top1:53.125\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:25.878225781700827 | top1:45.7386360168457\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:27.987856805324554 | top1:45.3125\n",
      "\n",
      "Epoch: [29 | 5000] LR: 0.239993\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:47.72887420654297 | top1:37.5\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:21.870027282021262 | top1:53.69318389892578\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:20.499740560849506 | top1:54.16666793823242\n",
      "\n",
      "Epoch: [30 | 5000] LR: 0.239992\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:7.007508754730225 | top1:46.875\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:14.007827802137895 | top1:47.72727584838867\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:13.89368760585785 | top1:48.4375\n",
      "\n",
      "Epoch: [31 | 5000] LR: 0.239991\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:10.72705078125 | top1:40.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:8.501330440694636 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:10.442703862984976 | top1:47.13541793823242\n",
      "\n",
      "Epoch: [32 | 5000] LR: 0.239991\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:38.66907501220703 | top1:40.625\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:35.17785289070823 | top1:47.15909194946289\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:33.04655639330546 | top1:46.09375\n",
      "\n",
      "Epoch: [33 | 5000] LR: 0.239990\n",
      "1/13 | Total:0:00:02 | ETA:0:00:27 | Loss:6.1430816650390625 | top1:50.0\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:113549.831669504 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:131900.14517621198 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [34 | 5000] LR: 0.239989\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:329045.125 | top1:40.625\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:160551.4405184659 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:147341.77611287436 | top1:51.82291793823242\n",
      "\n",
      "Epoch: [35 | 5000] LR: 0.239987\n",
      "1/13 | Total:0:00:02 | ETA:0:00:30 | Loss:22.067638397216797 | top1:40.625\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:23550.830101359974 | top1:50.0\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:24256.879415829975 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [36 | 5000] LR: 0.239986\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:27437.57421875 | top1:43.75\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:8910.675351576372 | top1:52.272727966308594\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:8244.555748303732 | top1:50.78125\n",
      "\n",
      "Epoch: [37 | 5000] LR: 0.239985\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:1699.3062744140625 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:3407.63808371804 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:3288.5293986002603 | top1:53.38541793823242\n",
      "\n",
      "Epoch: [38 | 5000] LR: 0.239984\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:1450.031005859375 | top1:43.75\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:403.75557656721634 | top1:53.125\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:410.6536434491475 | top1:53.125\n",
      "\n",
      "Epoch: [39 | 5000] LR: 0.239983\n",
      "1/13 | Total:0:00:00 | ETA:0:00:12 | Loss:572.2207641601562 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:465.6213392777876 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:434.5007680257161 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [40 | 5000] LR: 0.239981\n",
      "1/13 | Total:0:00:01 | ETA:0:00:20 | Loss:50.71824645996094 | top1:50.0\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:45.72277238152244 | top1:50.56818389892578\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:50.97171747684479 | top1:50.0\n",
      "\n",
      "Epoch: [41 | 5000] LR: 0.239980\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:107.52772521972656 | top1:56.25\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:57.276884599165484 | top1:55.681819915771484\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:53.337671756744385 | top1:54.6875\n",
      "\n",
      "Epoch: [42 | 5000] LR: 0.239979\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:9.086444854736328 | top1:46.875\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:45.79512466083873 | top1:43.181819915771484\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:46.86941965421041 | top1:43.489585876464844\n",
      "\n",
      "Epoch: [43 | 5000] LR: 0.239977\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:50.938350677490234 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:18.943664810874246 | top1:47.44318389892578\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:18.19620442390442 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [44 | 5000] LR: 0.239976\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:18.84008026123047 | top1:59.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:179.3876041065563 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:183.40356604258218 | top1:51.5625\n",
      "\n",
      "Epoch: [45 | 5000] LR: 0.239974\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:194.6463165283203 | top1:65.625\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:80.95292420820756 | top1:54.54545593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:78.08736006418864 | top1:55.208335876464844\n",
      "\n",
      "Epoch: [46 | 5000] LR: 0.239973\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:53.16848373413086 | top1:53.125\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:52.063758156516336 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:49.53841304779053 | top1:50.78125\n",
      "\n",
      "Epoch: [47 | 5000] LR: 0.239971\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:16.412185668945312 | top1:59.375\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:18.557667298750445 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:21.440852642059326 | top1:48.4375\n",
      "\n",
      "Epoch: [48 | 5000] LR: 0.239969\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:57.95799255371094 | top1:43.75\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:40.43804584849965 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:37.76045854886373 | top1:51.82291793823242\n",
      "\n",
      "Epoch: [49 | 5000] LR: 0.239968\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:6.529300212860107 | top1:46.875\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:9.145696033130992 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:9.23170804977417 | top1:51.5625\n",
      "\n",
      "Epoch: [50 | 5000] LR: 0.239966\n",
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:9.179798126220703 | top1:43.75\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:42.43184796246615 | top1:55.965911865234375\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:55.72063251336416 | top1:55.72916793823242\n",
      "\n",
      "Epoch: [51 | 5000] LR: 0.239964\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:225.67076110839844 | top1:46.875\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:161.82499105280095 | top1:45.7386360168457\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:150.4292686780294 | top1:47.13541793823242\n",
      "102/102 | Total:0:00:51 | ETA:0:00:00 | Loss:13.323566169913756 | top1:50.000003814697266\n",
      "26/26 | Total:0:00:32 | ETA:0:00:00 | Loss:13.310348143944374 | top1:50.0\n",
      "\n",
      "Epoch: [52 | 5000] LR: 0.239962\n",
      "1/13 | Total:0:00:02 | ETA:0:00:28 | Loss:13.252192497253418 | top1:56.25\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:86.37609161030163 | top1:57.10227584838867\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:99.90214149157207 | top1:55.989585876464844\n",
      "\n",
      "Epoch: [53 | 5000] LR: 0.239960\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:247.21788024902344 | top1:46.875\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:126.30130065571178 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:116.24420086542766 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [54 | 5000] LR: 0.239958\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:3.4592771530151367 | top1:59.375\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:215.89105038209394 | top1:44.03409194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:254.7750379641851 | top1:45.57291793823242\n",
      "\n",
      "Epoch: [55 | 5000] LR: 0.239956\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:706.86474609375 | top1:46.875\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:426.85819868607956 | top1:46.022727966308594\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:433.52717844645184 | top1:45.57291793823242\n",
      "\n",
      "Epoch: [56 | 5000] LR: 0.239954\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:930.0711669921875 | top1:68.75\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:1956.0490500710227 | top1:56.53409194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:1892.0131123860676 | top1:55.72916793823242\n",
      "\n",
      "Epoch: [57 | 5000] LR: 0.239952\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:879.6014404296875 | top1:71.875\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:292266.3221771067 | top1:50.85227584838867\n",
      "13/13 | Total:0:00:05 | ETA:0:00:01 | Loss:435585.5557456811 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [58 | 5000] LR: 0.239950\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:2390919.0 | top1:59.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:1965164.0681818181 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:1832715.7630208333 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [59 | 5000] LR: 0.239948\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:197617.421875 | top1:62.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:132073.35837069424 | top1:47.72727584838867\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:146365.02902730307 | top1:48.177085876464844\n",
      "\n",
      "Epoch: [60 | 5000] LR: 0.239945\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:298175.90625 | top1:43.75\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:143917.14897017044 | top1:47.44318389892578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:132056.5778503418 | top1:48.4375\n",
      "\n",
      "Epoch: [61 | 5000] LR: 0.239943\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:87.14213562011719 | top1:68.75\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:21312.327685269443 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:21882.751061757404 | top1:52.083335876464844\n",
      "\n",
      "Epoch: [62 | 5000] LR: 0.239941\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:23997.009765625 | top1:56.25\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:7677.793341029774 | top1:48.57954788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:7111.341090361278 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [63 | 5000] LR: 0.239938\n",
      "1/13 | Total:0:00:01 | ETA:0:00:20 | Loss:1588.6661376953125 | top1:43.75\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:2995.2240656072445 | top1:50.0\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:2881.3588256835938 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [64 | 5000] LR: 0.239936\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:1171.1781005859375 | top1:50.0\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:356.088780186393 | top1:46.306819915771484\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:385.44573120276135 | top1:47.65625\n",
      "\n",
      "Epoch: [65 | 5000] LR: 0.239933\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:838.0252685546875 | top1:40.625\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:684.1072776100852 | top1:44.60227584838867\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:638.0128059387207 | top1:45.57291793823242\n",
      "\n",
      "Epoch: [66 | 5000] LR: 0.239931\n",
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:69.60216522216797 | top1:53.125\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:133.16511526974764 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:157.85778419176737 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [67 | 5000] LR: 0.239928\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:440.94677734375 | top1:56.25\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:245.4055054404519 | top1:49.147727966308594\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:226.07551956176758 | top1:48.958335876464844\n",
      "\n",
      "Epoch: [68 | 5000] LR: 0.239926\n",
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:4.970930099487305 | top1:37.5\n",
      "11/13 | Total:0:00:11 | ETA:0:00:03 | Loss:2776.5150130878797 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:2932.1825445493064 | top1:48.4375\n",
      "\n",
      "Epoch: [69 | 5000] LR: 0.239923\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:4132.0185546875 | top1:56.25\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:1493.7898868214 | top1:52.272727966308594\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:1388.2241125106812 | top1:51.5625\n",
      "\n",
      "Epoch: [70 | 5000] LR: 0.239920\n",
      "1/13 | Total:0:00:01 | ETA:0:00:22 | Loss:539.4132080078125 | top1:62.5\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:1528.4428988370028 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:1490.095962524414 | top1:50.78125\n",
      "\n",
      "Epoch: [71 | 5000] LR: 0.239918\n",
      "1/13 | Total:0:00:02 | ETA:0:00:27 | Loss:811.1407470703125 | top1:37.5\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:4278.2925409403715 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:7340.228448987007 | top1:52.083335876464844\n",
      "\n",
      "Epoch: [72 | 5000] LR: 0.239915\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:53783.47265625 | top1:53.125\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:55909.44691051136 | top1:50.56818389892578\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:52618.619791666664 | top1:50.78125\n",
      "\n",
      "Epoch: [73 | 5000] LR: 0.239912\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:10043.4814453125 | top1:50.0\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:4488.889235756614 | top1:45.45454788208008\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:4963.0206991831465 | top1:45.3125\n",
      "\n",
      "Epoch: [74 | 5000] LR: 0.239909\n",
      "1/13 | Total:0:00:02 | ETA:0:00:26 | Loss:10438.2802734375 | top1:65.625\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:5736.6171542080965 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:5279.023701985677 | top1:53.38541793823242\n",
      "\n",
      "Epoch: [75 | 5000] LR: 0.239906\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:56.62404251098633 | top1:40.625\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:3557.3998846574264 | top1:46.590911865234375\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:3684.2820068995156 | top1:46.614585876464844\n",
      "\n",
      "Epoch: [76 | 5000] LR: 0.239903\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:4396.787109375 | top1:56.25\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:1561.5693423531272 | top1:52.556819915771484\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:1731.256414572398 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [77 | 5000] LR: 0.239900\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:7023.68603515625 | top1:50.0\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:14833.92049893466 | top1:50.85227584838867\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:14322.838419596354 | top1:50.26041793823242\n",
      "\n",
      "Epoch: [78 | 5000] LR: 0.239897\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:6359.26806640625 | top1:43.75\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:1680.8644303408537 | top1:47.44318389892578\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:1700.22900946935 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [79 | 5000] LR: 0.239894\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:2305.15087890625 | top1:40.625\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:2019.1912175958807 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:1890.533198038737 | top1:50.78125\n",
      "\n",
      "Epoch: [80 | 5000] LR: 0.239890\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:278.6595764160156 | top1:53.125\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:895.6451294638894 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:993.0630483627319 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [81 | 5000] LR: 0.239887\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:1970.1055908203125 | top1:53.125\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:882.5748450539329 | top1:55.11363983154297\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:810.2657287915548 | top1:56.51041793823242\n",
      "\n",
      "Epoch: [82 | 5000] LR: 0.239884\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:30.613826751708984 | top1:40.625\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:6370.752163626931 | top1:48.29545593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:6564.586455980937 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [83 | 5000] LR: 0.239881\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:7457.8828125 | top1:37.5\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:2435.744763461026 | top1:53.69318389892578\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:2272.170642852783 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [84 | 5000] LR: 0.239877\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:880.3307495117188 | top1:37.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:1764.5507202148438 | top1:47.72727584838867\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:1701.8734842936199 | top1:47.395835876464844\n",
      "\n",
      "Epoch: [85 | 5000] LR: 0.239874\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:738.1690063476562 | top1:59.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:281.9354645989158 | top1:54.82954788208008\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:335.8784188429515 | top1:55.46875\n",
      "\n",
      "Epoch: [86 | 5000] LR: 0.239870\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:1158.7711181640625 | top1:56.25\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:1090.6093472567472 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:1023.2942199707031 | top1:51.82291793823242\n",
      "\n",
      "Epoch: [87 | 5000] LR: 0.239867\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:168.73118591308594 | top1:50.0\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:272.92871353843 | top1:47.15909194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:312.3028537432353 | top1:46.875\n",
      "\n",
      "Epoch: [88 | 5000] LR: 0.239863\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:741.7719116210938 | top1:46.875\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:374.4062860662287 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:344.04457720120746 | top1:49.21875\n",
      "\n",
      "Epoch: [89 | 5000] LR: 0.239860\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:3.3844902515411377 | top1:56.25\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:57.53596598451788 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:59.546201368172966 | top1:51.04166793823242\n",
      "\n",
      "Epoch: [90 | 5000] LR: 0.239856\n",
      "1/13 | Total:0:00:02 | ETA:0:00:28 | Loss:71.06077575683594 | top1:53.125\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:25.64173184741627 | top1:44.8863639831543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:25.997770845890045 | top1:44.53125\n",
      "\n",
      "Epoch: [91 | 5000] LR: 0.239852\n",
      "1/13 | Total:0:00:01 | ETA:0:00:22 | Loss:58.2197380065918 | top1:43.75\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:126.62116692282937 | top1:49.147727966308594\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:122.49841276804607 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [92 | 5000] LR: 0.239848\n",
      "1/13 | Total:0:00:02 | ETA:0:00:32 | Loss:57.268306732177734 | top1:56.25\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:51.8427472114563 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:68.85248323281606 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [93 | 5000] LR: 0.239845\n",
      "1/13 | Total:0:00:01 | ETA:0:00:24 | Loss:317.16534423828125 | top1:56.25\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:304.2242542613636 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:286.1299680074056 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [94 | 5000] LR: 0.239841\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:55.02151107788086 | top1:34.375\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:265.6685231815685 | top1:49.431819915771484\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:347.8827917575836 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [95 | 5000] LR: 0.239837\n",
      "1/13 | Total:0:00:02 | ETA:0:00:31 | Loss:1377.13330078125 | top1:56.25\n",
      "11/13 | Total:0:00:11 | ETA:0:00:02 | Loss:924.5065862482244 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:855.9979031880697 | top1:47.65625\n",
      "\n",
      "Epoch: [96 | 5000] LR: 0.239833\n",
      "1/13 | Total:0:00:02 | ETA:0:00:31 | Loss:43.07644271850586 | top1:59.375\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:137.35896908153188 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:148.32092968622842 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [97 | 5000] LR: 0.239829\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:249.00567626953125 | top1:56.25\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:102.78133335980502 | top1:50.56818389892578\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:94.72803509235382 | top1:51.5625\n",
      "\n",
      "Epoch: [98 | 5000] LR: 0.239825\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:29.249467849731445 | top1:59.375\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:773.2286399494518 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:790.1025055249532 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [99 | 5000] LR: 0.239821\n",
      "1/13 | Total:0:00:02 | ETA:0:00:25 | Loss:827.7205810546875 | top1:50.0\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:347.06406506625086 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:379.301397005717 | top1:50.78125\n",
      "\n",
      "Epoch: [100 | 5000] LR: 0.239817\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:955.3631591796875 | top1:40.625\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:1010.4839089133523 | top1:48.29545593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:953.0219141642252 | top1:48.958335876464844\n",
      "\n",
      "Epoch: [101 | 5000] LR: 0.239812\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:204.3753662109375 | top1:62.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:244.84520417993718 | top1:54.82954788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:306.8743697007497 | top1:54.427085876464844\n",
      "102/102 | Total:0:00:46 | ETA:0:00:00 | Loss:1083.648373665066 | top1:50.000003814697266\n",
      "26/26 | Total:0:00:17 | ETA:0:00:00 | Loss:1083.6475078876201 | top1:50.0\n",
      "\n",
      "Epoch: [102 | 5000] LR: 0.239808\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:1083.5894775390625 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:722.8814239501953 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:669.424747467041 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [103 | 5000] LR: 0.239804\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:35.14998245239258 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:01 | Loss:1033.7258128252897 | top1:49.431819915771484\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:1174.706567843755 | top1:48.958335876464844\n",
      "\n",
      "Epoch: [104 | 5000] LR: 0.239800\n",
      "1/13 | Total:0:00:01 | ETA:0:00:23 | Loss:2641.493408203125 | top1:53.125\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:1230.531377618963 | top1:52.272727966308594\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:1129.1334013938904 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [105 | 5000] LR: 0.239795\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:10.18665885925293 | top1:68.75\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:351.87019365484065 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:360.6231271425883 | top1:50.520835876464844\n",
      "\n",
      "Epoch: [106 | 5000] LR: 0.239791\n",
      "1/13 | Total:0:00:01 | ETA:0:00:24 | Loss:389.295166015625 | top1:56.25\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:129.22185702757403 | top1:57.67045593261719\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:128.75701820850372 | top1:57.29166793823242\n",
      "\n",
      "Epoch: [107 | 5000] LR: 0.239786\n",
      "1/13 | Total:0:00:02 | ETA:0:00:26 | Loss:242.48545837402344 | top1:46.875\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:540.9041318026456 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:523.7428067525228 | top1:52.34375\n",
      "\n",
      "Epoch: [108 | 5000] LR: 0.239782\n",
      "1/13 | Total:0:00:02 | ETA:0:00:26 | Loss:248.56155395507812 | top1:43.75\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:1371.2466895363548 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:1809.5266284942627 | top1:49.47916793823242\n",
      "\n",
      "Epoch: [109 | 5000] LR: 0.239777\n",
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:7321.6728515625 | top1:50.0\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:4963.9079922762785 | top1:47.15909194946289\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:4597.146977742513 | top1:46.614585876464844\n",
      "\n",
      "Epoch: [110 | 5000] LR: 0.239773\n",
      "1/13 | Total:0:00:02 | ETA:0:00:27 | Loss:231.70883178710938 | top1:46.875\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:417.11994249170476 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:458.1021653016408 | top1:48.958335876464844\n",
      "\n",
      "Epoch: [111 | 5000] LR: 0.239768\n",
      "1/13 | Total:0:00:02 | ETA:0:00:28 | Loss:859.907470703125 | top1:50.0\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:373.8932783820412 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:343.5656904379527 | top1:50.0\n",
      "\n",
      "Epoch: [112 | 5000] LR: 0.239763\n",
      "1/13 | Total:0:00:02 | ETA:0:00:28 | Loss:52.06448745727539 | top1:46.875\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:456.9260562549938 | top1:47.15909194946289\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:457.700706799825 | top1:48.177085876464844\n",
      "\n",
      "Epoch: [113 | 5000] LR: 0.239758\n",
      "1/13 | Total:0:00:02 | ETA:0:00:30 | Loss:380.6197509765625 | top1:56.25\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:115.6135649247603 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:110.34847259521484 | top1:50.0\n",
      "\n",
      "Epoch: [114 | 5000] LR: 0.239754\n",
      "1/13 | Total:0:00:01 | ETA:0:00:22 | Loss:73.29370880126953 | top1:50.0\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:128.296743219549 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:124.3662618001302 | top1:48.69791793823242\n",
      "\n",
      "Epoch: [115 | 5000] LR: 0.239749\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:62.23520278930664 | top1:46.875\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:99.78397646817294 | top1:54.2613639831543\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:121.83272488911946 | top1:53.645835876464844\n",
      "\n",
      "Epoch: [116 | 5000] LR: 0.239744\n",
      "1/13 | Total:0:00:01 | ETA:0:00:20 | Loss:406.9166564941406 | top1:43.75\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:306.7571175315163 | top1:47.72727584838867\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:286.4418083826701 | top1:48.4375\n",
      "\n",
      "Epoch: [117 | 5000] LR: 0.239739\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:38.54762268066406 | top1:50.0\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:50.1721435026689 | top1:50.0\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:55.633647998174034 | top1:49.47916793823242\n",
      "\n",
      "Epoch: [118 | 5000] LR: 0.239734\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:115.65097045898438 | top1:37.5\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:63.87947307933461 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:61.669414361317955 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [119 | 5000] LR: 0.239729\n",
      "1/13 | Total:0:00:00 | ETA:0:00:12 | Loss:73.2231674194336 | top1:53.125\n",
      "11/13 | Total:0:00:04 | ETA:0:00:01 | Loss:317.7726384943182 | top1:55.965911865234375\n",
      "13/13 | Total:0:00:05 | ETA:0:00:01 | Loss:322.4959996541341 | top1:54.427085876464844\n",
      "\n",
      "Epoch: [120 | 5000] LR: 0.239724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:322.6327209472656 | top1:53.125\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:361.2745369997892 | top1:54.54545593261719\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:674.4021716117859 | top1:53.90625\n",
      "\n",
      "Epoch: [121 | 5000] LR: 0.239719\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:6242.12158203125 | top1:34.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:8753.02783203125 | top1:46.875\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:8333.01719156901 | top1:46.09375\n",
      "\n",
      "Epoch: [122 | 5000] LR: 0.239714\n",
      "1/13 | Total:0:00:01 | ETA:0:00:18 | Loss:2517.58251953125 | top1:62.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:864.7425778128884 | top1:54.2613639831543\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:971.5639564196268 | top1:54.6875\n",
      "\n",
      "Epoch: [123 | 5000] LR: 0.239708\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:2442.05615234375 | top1:46.875\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:1807.1110146262429 | top1:49.431819915771484\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:1679.5158793131511 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [124 | 5000] LR: 0.239703\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:134.99526977539062 | top1:43.75\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:234.10934899070045 | top1:50.56818389892578\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:265.09716447194415 | top1:50.0\n",
      "\n",
      "Epoch: [125 | 5000] LR: 0.239698\n",
      "1/13 | Total:0:00:01 | ETA:0:00:15 | Loss:598.0460815429688 | top1:46.875\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:297.6869583129883 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:273.7595838705699 | top1:52.864585876464844\n",
      "\n",
      "Epoch: [126 | 5000] LR: 0.239692\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:10.577469825744629 | top1:37.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:105.3734924143011 | top1:47.72727584838867\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:107.03957962989807 | top1:47.65625\n",
      "\n",
      "Epoch: [127 | 5000] LR: 0.239687\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:106.07311248779297 | top1:56.25\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:98.38666222312234 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:144.25555324554443 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [128 | 5000] LR: 0.239681\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:862.44580078125 | top1:62.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:951.5401777787642 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:898.7190704345703 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [129 | 5000] LR: 0.239676\n",
      "1/13 | Total:0:00:00 | ETA:0:00:11 | Loss:204.623046875 | top1:40.625\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:109.65254367481579 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:125.10562912623088 | top1:53.38541793823242\n",
      "\n",
      "Epoch: [130 | 5000] LR: 0.239670\n",
      "1/13 | Total:0:00:02 | ETA:0:00:28 | Loss:313.1840515136719 | top1:43.75\n",
      "11/13 | Total:0:00:11 | ETA:0:00:02 | Loss:192.06464663418856 | top1:52.556819915771484\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:177.47954765955606 | top1:53.38541793823242\n",
      "\n",
      "Epoch: [131 | 5000] LR: 0.239665\n",
      "1/13 | Total:0:00:01 | ETA:0:00:24 | Loss:7.601754188537598 | top1:56.25\n",
      "11/13 | Total:0:00:11 | ETA:0:00:02 | Loss:433.53984559666026 | top1:50.0\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:479.3932196696599 | top1:49.21875\n",
      "\n",
      "Epoch: [132 | 5000] LR: 0.239659\n",
      "1/13 | Total:0:00:01 | ETA:0:00:21 | Loss:925.5302734375 | top1:68.75\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:394.8066922101107 | top1:52.272727966308594\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:362.36852471033734 | top1:51.82291793823242\n",
      "\n",
      "Epoch: [133 | 5000] LR: 0.239653\n",
      "1/13 | Total:0:00:02 | ETA:0:00:27 | Loss:21.119462966918945 | top1:50.0\n",
      "11/13 | Total:0:00:08 | ETA:0:00:02 | Loss:177.13344417918813 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:09 | ETA:0:00:01 | Loss:177.42607418696085 | top1:51.82291793823242\n",
      "\n",
      "Epoch: [134 | 5000] LR: 0.239648\n",
      "1/13 | Total:0:00:02 | ETA:0:00:30 | Loss:147.73580932617188 | top1:43.75\n",
      "11/13 | Total:0:00:11 | ETA:0:00:03 | Loss:46.49502017281272 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:44.68526943524679 | top1:48.177085876464844\n",
      "\n",
      "Epoch: [135 | 5000] LR: 0.239642\n",
      "1/13 | Total:0:00:02 | ETA:0:00:26 | Loss:32.1148567199707 | top1:50.0\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:39.38398499922319 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:37.60016632080078 | top1:47.65625\n",
      "\n",
      "Epoch: [136 | 5000] LR: 0.239636\n",
      "1/13 | Total:0:00:02 | ETA:0:00:28 | Loss:12.985052108764648 | top1:59.375\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:247.28263542868874 | top1:50.0\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:266.28894392649335 | top1:48.958335876464844\n",
      "\n",
      "Epoch: [137 | 5000] LR: 0.239630\n",
      "1/13 | Total:0:00:02 | ETA:0:00:26 | Loss:435.82025146484375 | top1:43.75\n",
      "11/13 | Total:0:00:09 | ETA:0:00:02 | Loss:173.4373048652302 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:10 | ETA:0:00:01 | Loss:159.4798473318418 | top1:47.395835876464844\n",
      "\n",
      "Epoch: [138 | 5000] LR: 0.239624\n",
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:123.30973052978516 | top1:46.875\n",
      "11/13 | Total:0:00:11 | ETA:0:00:03 | Loss:1496.460710698908 | top1:49.431819915771484\n",
      "13/13 | Total:0:00:12 | ETA:0:00:01 | Loss:1503.0529740651448 | top1:49.21875\n",
      "\n",
      "Epoch: [139 | 5000] LR: 0.239618\n",
      "1/13 | Total:0:00:02 | ETA:0:00:27 | Loss:1291.06787109375 | top1:46.875\n",
      "11/13 | Total:0:00:10 | ETA:0:00:02 | Loss:378.6992909691551 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:11 | ETA:0:00:01 | Loss:356.46927014986676 | top1:49.47916793823242\n",
      "\n",
      "Epoch: [140 | 5000] LR: 0.239612\n",
      "1/13 | Total:0:00:02 | ETA:0:00:27 | Loss:183.78067016601562 | top1:37.5\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:331.20979725230825 | top1:45.45454788208008\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:319.45848846435547 | top1:44.53125\n",
      "\n",
      "Epoch: [141 | 5000] LR: 0.239606\n",
      "1/13 | Total:0:00:01 | ETA:0:00:20 | Loss:140.95053100585938 | top1:43.75\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:53.68760871887207 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:55.17999887466431 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [142 | 5000] LR: 0.239600\n",
      "1/13 | Total:0:00:01 | ETA:0:00:13 | Loss:79.06937408447266 | top1:53.125\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:60.552939501675695 | top1:47.72727584838867\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:56.75093563397726 | top1:46.875\n",
      "\n",
      "Epoch: [143 | 5000] LR: 0.239594\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:10.045961380004883 | top1:59.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:57.51674305308949 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:61.2972297668457 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [144 | 5000] LR: 0.239588\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:94.0184097290039 | top1:56.25\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:38.568637349388815 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:35.6155593196551 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [145 | 5000] LR: 0.239581\n",
      "1/13 | Total:0:00:00 | ETA:0:00:12 | Loss:70.09490203857422 | top1:59.375\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:842.2685165405273 | top1:46.022727966308594\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:845.8534399668375 | top1:46.875\n",
      "\n",
      "Epoch: [146 | 5000] LR: 0.239575\n",
      "1/13 | Total:0:00:01 | ETA:0:00:22 | Loss:725.2222290039062 | top1:53.125\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:228.5570664839311 | top1:52.840911865234375\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:223.8516082763672 | top1:53.125\n",
      "\n",
      "Epoch: [147 | 5000] LR: 0.239569\n",
      "1/13 | Total:0:00:01 | ETA:0:00:16 | Loss:225.07371520996094 | top1:62.5\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:243.2400921908292 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:229.71373176574707 | top1:49.21875\n",
      "\n",
      "Epoch: [148 | 5000] LR: 0.239562\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:52.466644287109375 | top1:59.375\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:5630.97841869701 | top1:54.2613639831543\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:9812.57624578476 | top1:53.645835876464844\n",
      "\n",
      "Epoch: [149 | 5000] LR: 0.239556\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:73086.171875 | top1:40.625\n",
      "11/13 | Total:0:00:06 | ETA:0:00:02 | Loss:75893.25497159091 | top1:46.875\n",
      "13/13 | Total:0:00:07 | ETA:0:00:01 | Loss:71430.09261067708 | top1:46.614585876464844\n",
      "\n",
      "Epoch: [150 | 5000] LR: 0.239549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:13684.1787109375 | top1:62.5\n",
      "11/13 | Total:0:00:43 | ETA:0:00:09 | Loss:5072.125779932196 | top1:46.590911865234375\n",
      "13/13 | Total:0:00:48 | ETA:0:00:04 | Loss:5533.024966239929 | top1:48.177085876464844\n",
      "\n",
      "Epoch: [151 | 5000] LR: 0.239543\n",
      "1/13 | Total:0:00:08 | ETA:0:01:43 | Loss:10968.876953125 | top1:40.625\n",
      "11/13 | Total:0:00:32 | ETA:0:00:05 | Loss:6182.1398370916195 | top1:49.147727966308594\n",
      "13/13 | Total:0:00:33 | ETA:0:00:03 | Loss:5692.274594624837 | top1:48.958335876464844\n",
      "102/102 | Total:0:02:11 | ETA:0:00:00 | Loss:65.46404971301477 | top1:50.000003814697266\n",
      "26/26 | Total:0:02:03 | ETA:0:00:00 | Loss:65.4648821904109 | top1:50.0\n",
      "\n",
      "Epoch: [152 | 5000] LR: 0.239536\n",
      "1/13 | Total:0:00:05 | ETA:0:01:12 | Loss:65.4752197265625 | top1:50.0\n",
      "11/13 | Total:0:00:28 | ETA:0:00:05 | Loss:677.6513641530818 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:32 | ETA:0:00:03 | Loss:717.3408989111582 | top1:48.4375\n",
      "\n",
      "Epoch: [153 | 5000] LR: 0.239529\n",
      "1/13 | Total:0:00:08 | ETA:0:01:37 | Loss:1031.9571533203125 | top1:31.25\n",
      "11/13 | Total:0:00:43 | ETA:0:00:08 | Loss:380.0939330187711 | top1:46.875\n",
      "13/13 | Total:0:00:50 | ETA:0:00:04 | Loss:350.5900046825409 | top1:46.35416793823242\n",
      "\n",
      "Epoch: [154 | 5000] LR: 0.239523\n",
      "1/13 | Total:0:00:12 | ETA:0:02:26 | Loss:49.94595718383789 | top1:56.25\n",
      "11/13 | Total:0:00:47 | ETA:0:00:07 | Loss:123.86397517811169 | top1:50.28409194946289\n",
      "13/13 | Total:0:00:50 | ETA:0:00:04 | Loss:120.66237735748291 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [155 | 5000] LR: 0.239516\n",
      "1/13 | Total:0:00:11 | ETA:0:02:20 | Loss:65.34532165527344 | top1:50.0\n",
      "11/13 | Total:0:00:58 | ETA:0:00:10 | Loss:26.796667445789684 | top1:48.0113639831543\n",
      "13/13 | Total:0:01:04 | ETA:0:00:05 | Loss:28.0392902692159 | top1:48.177085876464844\n",
      "\n",
      "Epoch: [156 | 5000] LR: 0.239509\n",
      "1/13 | Total:0:00:09 | ETA:0:01:52 | Loss:47.94816970825195 | top1:40.625\n",
      "11/13 | Total:0:00:53 | ETA:0:00:09 | Loss:40.50758656588468 | top1:49.147727966308594\n",
      "13/13 | Total:0:00:56 | ETA:0:00:05 | Loss:38.063645919164024 | top1:50.78125\n",
      "\n",
      "Epoch: [157 | 5000] LR: 0.239502\n",
      "1/13 | Total:0:00:06 | ETA:0:01:14 | Loss:8.59592342376709 | top1:46.875\n",
      "11/13 | Total:0:00:42 | ETA:0:00:08 | Loss:989.7785658402877 | top1:42.6136360168457\n",
      "13/13 | Total:0:00:48 | ETA:0:00:04 | Loss:1092.154603322347 | top1:43.75\n",
      "\n",
      "Epoch: [158 | 5000] LR: 0.239495\n",
      "1/13 | Total:0:00:09 | ETA:0:01:53 | Loss:2081.559814453125 | top1:37.5\n",
      "11/13 | Total:0:00:46 | ETA:0:00:08 | Loss:879.0423427928578 | top1:47.44318389892578\n",
      "13/13 | Total:0:00:51 | ETA:0:00:04 | Loss:807.0767339070638 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [159 | 5000] LR: 0.239489\n",
      "1/13 | Total:0:00:09 | ETA:0:01:57 | Loss:40.52231979370117 | top1:43.75\n",
      "11/13 | Total:0:00:43 | ETA:0:00:07 | Loss:221.97377326271751 | top1:50.28409194946289\n",
      "13/13 | Total:0:00:47 | ETA:0:00:04 | Loss:221.00438721974692 | top1:51.302085876464844\n",
      "\n",
      "Epoch: [160 | 5000] LR: 0.239482\n",
      "1/13 | Total:0:00:05 | ETA:0:01:04 | Loss:170.224853515625 | top1:46.875\n",
      "11/13 | Total:0:00:24 | ETA:0:00:05 | Loss:94.64662846651945 | top1:41.19318389892578\n",
      "13/13 | Total:0:00:24 | ETA:0:00:03 | Loss:113.89408349990845 | top1:41.145835876464844\n",
      "\n",
      "Epoch: [161 | 5000] LR: 0.239475\n",
      "1/13 | Total:0:00:00 | ETA:0:00:11 | Loss:390.7563171386719 | top1:53.125\n",
      "11/13 | Total:0:00:11 | ETA:0:00:03 | Loss:332.8992857499556 | top1:48.29545593261719\n",
      "13/13 | Total:0:00:15 | ETA:0:00:02 | Loss:311.1024735768636 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [162 | 5000] LR: 0.239467\n",
      "1/13 | Total:0:00:02 | ETA:0:00:29 | Loss:39.94236373901367 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:28.95190564068881 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:33.02698894341787 | top1:49.47916793823242\n",
      "\n",
      "Epoch: [163 | 5000] LR: 0.239460\n",
      "1/13 | Total:0:00:05 | ETA:0:01:05 | Loss:83.08502197265625 | top1:59.375\n",
      "11/13 | Total:0:00:15 | ETA:0:00:04 | Loss:56.04431438446045 | top1:50.85227584838867\n",
      "13/13 | Total:0:00:16 | ETA:0:00:02 | Loss:52.317962090174355 | top1:51.5625\n",
      "\n",
      "Epoch: [164 | 5000] LR: 0.239453\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:9.1044340133667 | top1:46.875\n",
      "11/13 | Total:0:00:28 | ETA:0:00:06 | Loss:18.690563808787953 | top1:45.7386360168457\n",
      "13/13 | Total:0:00:32 | ETA:0:00:03 | Loss:19.79913322130839 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [165 | 5000] LR: 0.239446\n",
      "1/13 | Total:0:00:01 | ETA:0:00:17 | Loss:32.407100677490234 | top1:53.125\n",
      "11/13 | Total:0:00:05 | ETA:0:00:02 | Loss:21.777500065890226 | top1:50.56818389892578\n",
      "13/13 | Total:0:00:06 | ETA:0:00:01 | Loss:25.344321807225544 | top1:50.520835876464844\n",
      "\n",
      "Epoch: [166 | 5000] LR: 0.239439\n",
      "1/13 | Total:0:00:00 | ETA:0:00:10 | Loss:144.53797912597656 | top1:68.75\n",
      "11/13 | Total:0:00:07 | ETA:0:00:02 | Loss:555.6491948908025 | top1:45.7386360168457\n",
      "13/13 | Total:0:00:08 | ETA:0:00:01 | Loss:560.8969141642252 | top1:46.09375\n",
      "\n",
      "Epoch: [167 | 5000] LR: 0.239431\n",
      "1/13 | Total:0:00:00 | ETA:0:00:10 | Loss:529.6686401367188 | top1:37.5\n",
      "11/13 | Total:0:00:12 | ETA:0:00:03 | Loss:189.5539876764471 | top1:53.125\n",
      "13/13 | Total:0:00:17 | ETA:0:00:02 | Loss:177.81451193491617 | top1:53.645835876464844\n",
      "\n",
      "Epoch: [168 | 5000] LR: 0.239424\n",
      "1/13 | Total:0:00:01 | ETA:0:00:14 | Loss:71.37393188476562 | top1:56.25\n",
      "11/13 | Total:0:00:18 | ETA:0:00:04 | Loss:129.83025221391156 | top1:46.590911865234375\n",
      "13/13 | Total:0:00:22 | ETA:0:00:02 | Loss:125.9235471089681 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [169 | 5000] LR: 0.239417\n",
      "1/13 | Total:0:00:11 | ETA:0:02:22 | Loss:64.16938018798828 | top1:56.25\n",
      "11/13 | Total:0:00:31 | ETA:0:00:05 | Loss:283.3951481905851 | top1:57.3863639831543\n",
      "13/13 | Total:0:00:35 | ETA:0:00:02 | Loss:372.99883619944256 | top1:56.51041793823242\n",
      "\n",
      "Epoch: [170 | 5000] LR: 0.239409\n",
      "1/13 | Total:0:00:07 | ETA:0:01:30 | Loss:1510.5806884765625 | top1:53.125\n",
      "11/13 | Total:0:00:30 | ETA:0:00:05 | Loss:1049.5086683793502 | top1:48.29545593261719\n",
      "13/13 | Total:0:00:35 | ETA:0:00:03 | Loss:1016.0319735209147 | top1:48.4375\n",
      "\n",
      "Epoch: [171 | 5000] LR: 0.239402\n",
      "1/13 | Total:0:00:08 | ETA:0:01:37 | Loss:1871.3662109375 | top1:59.375\n",
      "11/13 | Total:0:00:37 | ETA:0:00:06 | Loss:7458.716708096591 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:43 | ETA:0:00:04 | Loss:7371.6166585286455 | top1:52.083335876464844\n",
      "\n",
      "Epoch: [172 | 5000] LR: 0.239394\n",
      "1/13 | Total:0:00:11 | ETA:0:02:24 | Loss:5117.96435546875 | top1:71.875\n",
      "11/13 | Total:0:00:48 | ETA:0:00:08 | Loss:1491.2360139326615 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:52 | ETA:0:00:04 | Loss:1428.724443435669 | top1:52.083335876464844\n",
      "\n",
      "Epoch: [173 | 5000] LR: 0.239387\n",
      "1/13 | Total:0:00:07 | ETA:0:01:26 | Loss:1297.1075439453125 | top1:56.25\n",
      "11/13 | Total:0:00:28 | ETA:0:00:05 | Loss:3157.2183726917615 | top1:48.29545593261719\n",
      "13/13 | Total:0:00:32 | ETA:0:00:03 | Loss:3080.2828165690103 | top1:48.177085876464844\n",
      "\n",
      "Epoch: [174 | 5000] LR: 0.239379\n",
      "1/13 | Total:0:00:07 | ETA:0:01:33 | Loss:1721.197509765625 | top1:43.75\n",
      "11/13 | Total:0:00:43 | ETA:0:00:09 | Loss:486.8301121104847 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:47 | ETA:0:00:04 | Loss:471.43285433451337 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [175 | 5000] LR: 0.239371\n",
      "1/13 | Total:0:00:12 | ETA:0:02:34 | Loss:370.35052490234375 | top1:43.75\n",
      "11/13 | Total:0:00:37 | ETA:0:00:05 | Loss:350.0932520086115 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:39 | ETA:0:00:03 | Loss:329.3297799428304 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [176 | 5000] LR: 0.239363\n",
      "1/13 | Total:0:00:05 | ETA:0:01:04 | Loss:64.26984405517578 | top1:50.0\n",
      "11/13 | Total:0:00:35 | ETA:0:00:06 | Loss:98.84795275601473 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:39 | ETA:0:00:04 | Loss:117.71970589955647 | top1:50.0\n",
      "\n",
      "Epoch: [177 | 5000] LR: 0.239356\n",
      "1/13 | Total:0:00:10 | ETA:0:02:09 | Loss:340.63458251953125 | top1:50.0\n",
      "11/13 | Total:0:00:43 | ETA:0:00:07 | Loss:202.6703952442516 | top1:51.70454788208008\n",
      "13/13 | Total:0:00:45 | ETA:0:00:04 | Loss:187.1917026837667 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [178 | 5000] LR: 0.239348\n",
      "1/13 | Total:0:00:06 | ETA:0:01:24 | Loss:7.439262866973877 | top1:40.625\n",
      "11/13 | Total:0:00:26 | ETA:0:00:05 | Loss:2613.995191487399 | top1:50.28409194946289\n",
      "13/13 | Total:0:00:30 | ETA:0:00:02 | Loss:2999.3738474051156 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [179 | 5000] LR: 0.239340\n",
      "1/13 | Total:0:00:12 | ETA:0:02:36 | Loss:7069.14208984375 | top1:53.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/13 | Total:0:00:45 | ETA:0:00:07 | Loss:3365.977096557617 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:51 | ETA:0:00:04 | Loss:3088.81373723348 | top1:47.91666793823242\n",
      "\n",
      "Epoch: [180 | 5000] LR: 0.239332\n",
      "1/13 | Total:0:00:09 | ETA:0:02:00 | Loss:41.621856689453125 | top1:56.25\n",
      "11/13 | Total:0:00:34 | ETA:0:00:05 | Loss:856.8150648637252 | top1:49.715911865234375\n",
      "13/13 | Total:0:00:37 | ETA:0:00:03 | Loss:870.0888226826986 | top1:50.0\n",
      "\n",
      "Epoch: [181 | 5000] LR: 0.239324\n",
      "1/13 | Total:0:00:07 | ETA:0:01:26 | Loss:851.843994140625 | top1:53.125\n",
      "11/13 | Total:0:00:38 | ETA:0:00:07 | Loss:270.27677544680506 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:44 | ETA:0:00:04 | Loss:257.0092872778575 | top1:49.21875\n",
      "\n",
      "Epoch: [182 | 5000] LR: 0.239316\n",
      "1/13 | Total:0:00:10 | ETA:0:02:06 | Loss:162.2753143310547 | top1:56.25\n",
      "11/13 | Total:0:00:42 | ETA:0:00:07 | Loss:241.12837080522016 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:45 | ETA:0:00:04 | Loss:230.9761823018392 | top1:52.34375\n",
      "\n",
      "Epoch: [183 | 5000] LR: 0.239308\n",
      "1/13 | Total:0:00:05 | ETA:0:01:09 | Loss:85.68579864501953 | top1:68.75\n",
      "11/13 | Total:0:00:39 | ETA:0:00:07 | Loss:50.99551569331776 | top1:48.8636360168457\n",
      "13/13 | Total:0:00:46 | ETA:0:00:04 | Loss:63.44881602128347 | top1:48.4375\n",
      "\n",
      "Epoch: [184 | 5000] LR: 0.239300\n",
      "1/13 | Total:0:00:12 | ETA:0:02:33 | Loss:241.7054901123047 | top1:50.0\n",
      "11/13 | Total:0:00:43 | ETA:0:00:07 | Loss:213.56010228937322 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:47 | ETA:0:00:03 | Loss:200.21173286437988 | top1:52.34375\n",
      "\n",
      "Epoch: [185 | 5000] LR: 0.239292\n",
      "1/13 | Total:0:00:10 | ETA:0:02:02 | Loss:32.64268493652344 | top1:56.25\n",
      "11/13 | Total:0:00:48 | ETA:0:00:08 | Loss:101.65694600885564 | top1:55.11363983154297\n",
      "13/13 | Total:0:00:52 | ETA:0:00:04 | Loss:110.98380629221599 | top1:54.6875\n",
      "\n",
      "Epoch: [186 | 5000] LR: 0.239284\n",
      "1/13 | Total:0:00:01 | ETA:0:00:19 | Loss:201.27809143066406 | top1:53.125\n",
      "11/13 | Total:0:00:40 | ETA:0:00:08 | Loss:87.64278871362859 | top1:51.42045593261719\n",
      "13/13 | Total:0:00:46 | ETA:0:00:04 | Loss:80.93021909395854 | top1:49.47916793823242\n",
      "\n",
      "Epoch: [187 | 5000] LR: 0.239275\n",
      "1/13 | Total:0:00:13 | ETA:0:02:38 | Loss:68.60855865478516 | top1:59.375\n",
      "11/13 | Total:0:00:52 | ETA:0:00:08 | Loss:782.254074790261 | top1:54.54545593261719\n",
      "13/13 | Total:0:00:57 | ETA:0:00:04 | Loss:785.1767648061117 | top1:53.38541793823242\n",
      "\n",
      "Epoch: [188 | 5000] LR: 0.239267\n",
      "1/13 | Total:0:00:10 | ETA:0:02:06 | Loss:669.0609130859375 | top1:40.625\n",
      "11/13 | Total:0:00:39 | ETA:0:00:06 | Loss:203.99694496935064 | top1:48.0113639831543\n",
      "13/13 | Total:0:00:42 | ETA:0:00:03 | Loss:203.21649124224982 | top1:49.21875\n",
      "\n",
      "Epoch: [189 | 5000] LR: 0.239259\n",
      "1/13 | Total:0:00:07 | ETA:0:01:34 | Loss:297.48724365234375 | top1:62.5\n",
      "11/13 | Total:0:00:45 | ETA:0:00:08 | Loss:427.7035189541903 | top1:51.1363639831543\n",
      "13/13 | Total:0:00:48 | ETA:0:00:04 | Loss:407.7339909871419 | top1:50.520835876464844\n",
      "\n",
      "Epoch: [190 | 5000] LR: 0.239250\n",
      "1/13 | Total:0:00:05 | ETA:0:01:09 | Loss:129.02818298339844 | top1:40.625\n",
      "11/13 | Total:0:00:39 | ETA:0:00:07 | Loss:49.54021774638783 | top1:53.40909194946289\n",
      "13/13 | Total:0:00:44 | ETA:0:00:04 | Loss:55.12987049420675 | top1:53.125\n",
      "\n",
      "Epoch: [191 | 5000] LR: 0.239242\n",
      "1/13 | Total:0:00:12 | ETA:0:02:28 | Loss:128.24118041992188 | top1:46.875\n",
      "11/13 | Total:0:00:53 | ETA:0:00:09 | Loss:86.8358589519154 | top1:49.431819915771484\n",
      "13/13 | Total:0:00:56 | ETA:0:00:04 | Loss:80.5031243165334 | top1:49.739585876464844\n",
      "\n",
      "Epoch: [192 | 5000] LR: 0.239233\n",
      "1/13 | Total:0:00:06 | ETA:0:01:16 | Loss:5.488997459411621 | top1:37.5\n",
      "11/13 | Total:0:00:33 | ETA:0:00:06 | Loss:367.47829625823283 | top1:43.75\n",
      "13/13 | Total:0:00:36 | ETA:0:00:03 | Loss:391.46535474061966 | top1:44.53125\n",
      "\n",
      "Epoch: [193 | 5000] LR: 0.239225\n",
      "1/13 | Total:0:00:08 | ETA:0:01:44 | Loss:589.784912109375 | top1:46.875\n",
      "11/13 | Total:0:00:35 | ETA:0:00:06 | Loss:221.08884633671153 | top1:51.9886360168457\n",
      "13/13 | Total:0:00:38 | ETA:0:00:03 | Loss:205.00786983966827 | top1:52.60416793823242\n",
      "\n",
      "Epoch: [194 | 5000] LR: 0.239216\n",
      "1/13 | Total:0:00:09 | ETA:0:01:53 | Loss:57.451011657714844 | top1:53.125\n",
      "11/13 | Total:0:00:43 | ETA:0:00:07 | Loss:169.35138563676313 | top1:50.56818389892578\n",
      "13/13 | Total:0:00:49 | ETA:0:00:04 | Loss:166.341822942098 | top1:50.78125\n",
      "\n",
      "Epoch: [195 | 5000] LR: 0.239208\n",
      "1/13 | Total:0:00:12 | ETA:0:02:30 | Loss:104.86932373046875 | top1:59.375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        test_loss, test_acc = test(val_target_loader, model, criterion, epoch, use_cuda)\n",
    "        source_loss, source_acc = test(val_source_loader, model, criterion, epoch, use_cuda)\n",
    "\n",
    "        logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc, source_loss, source_acc])\n",
    "        \n",
    "\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict' : model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
