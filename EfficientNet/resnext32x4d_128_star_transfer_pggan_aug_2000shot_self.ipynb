{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import resnext50_32x4d\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 2: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/star/128/32x4d/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'resnext32x4d' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 400\n",
    "test_batch = 400\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/star/128/32x4d/to_pggan/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = 'fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/star/128/32x4d/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "student_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 22.98M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + (sp_alpha*loss_sp) + (sp_alpha*loss_cls)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 10/10 | Loss:1.1491 | MainLoss:0.7641 | Alpha:0.0763 | SPLoss:0.2434 | CLSLoss:4.7964 | top1:70.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.3376 | MainLoss:0.3376 | SPLoss:0.3609 | CLSLoss:4.3039 | top1:85.4268 | AUROC:0.9489\n",
      "Test | 77/10 | Loss:0.2595 | MainLoss:0.2595 | SPLoss:0.3609 | CLSLoss:4.3038 | top1:88.7353 | AUROC:0.9975\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 10/10 | Loss:1.4995 | MainLoss:0.4430 | Alpha:0.3820 | SPLoss:0.0214 | CLSLoss:2.7339 | top1:79.6750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2861 | MainLoss:0.2861 | SPLoss:0.0451 | CLSLoss:1.4100 | top1:91.7352 | AUROC:0.9891\n",
      "Test | 77/10 | Loss:0.2875 | MainLoss:0.2875 | SPLoss:0.0451 | CLSLoss:1.4100 | top1:86.8644 | AUROC:0.9962\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 10/10 | Loss:0.7568 | MainLoss:0.4163 | Alpha:0.3967 | SPLoss:0.0151 | CLSLoss:0.8416 | top1:82.6250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2807 | MainLoss:0.2807 | SPLoss:0.0392 | CLSLoss:0.4199 | top1:95.6386 | AUROC:0.9976\n",
      "Test | 77/10 | Loss:0.3740 | MainLoss:0.3740 | SPLoss:0.0392 | CLSLoss:0.4199 | top1:81.5924 | AUROC:0.9926\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 10/10 | Loss:0.5383 | MainLoss:0.4195 | Alpha:0.3965 | SPLoss:0.0134 | CLSLoss:0.2864 | top1:82.4250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2679 | MainLoss:0.2679 | SPLoss:0.0334 | CLSLoss:0.1959 | top1:96.6667 | AUROC:0.9991\n",
      "Test | 77/10 | Loss:0.4375 | MainLoss:0.4375 | SPLoss:0.0334 | CLSLoss:0.1959 | top1:76.9758 | AUROC:0.9854\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 10/10 | Loss:0.4732 | MainLoss:0.4007 | Alpha:0.3982 | SPLoss:0.0131 | CLSLoss:0.1691 | top1:82.8000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2363 | MainLoss:0.2363 | SPLoss:0.0304 | CLSLoss:0.1388 | top1:96.7632 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.4671 | MainLoss:0.4671 | SPLoss:0.0304 | CLSLoss:0.1388 | top1:74.3480 | AUROC:0.9764\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 10/10 | Loss:0.4396 | MainLoss:0.3843 | Alpha:0.4027 | SPLoss:0.0122 | CLSLoss:0.1251 | top1:82.6750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1859 | MainLoss:0.1859 | SPLoss:0.0268 | CLSLoss:0.1132 | top1:97.5732 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.5026 | MainLoss:0.5026 | SPLoss:0.0268 | CLSLoss:0.1132 | top1:69.2693 | AUROC:0.9702\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 10/10 | Loss:0.4058 | MainLoss:0.3572 | Alpha:0.4098 | SPLoss:0.0143 | CLSLoss:0.1042 | top1:84.1750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1718 | MainLoss:0.1718 | SPLoss:0.0233 | CLSLoss:0.0852 | top1:97.9408 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.5181 | MainLoss:0.5181 | SPLoss:0.0233 | CLSLoss:0.0852 | top1:68.0243 | AUROC:0.9687\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 10/10 | Loss:0.4072 | MainLoss:0.3662 | Alpha:0.4115 | SPLoss:0.0141 | CLSLoss:0.0853 | top1:84.0500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1372 | MainLoss:0.1372 | SPLoss:0.0282 | CLSLoss:0.0825 | top1:99.3738 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.6914 | MainLoss:0.6914 | SPLoss:0.0282 | CLSLoss:0.0825 | top1:56.2189 | AUROC:0.9671\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 10/10 | Loss:0.3974 | MainLoss:0.3589 | Alpha:0.3983 | SPLoss:0.0190 | CLSLoss:0.0778 | top1:83.6000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1402 | MainLoss:0.1402 | SPLoss:0.0296 | CLSLoss:0.0810 | top1:98.1153 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.4937 | MainLoss:0.4937 | SPLoss:0.0296 | CLSLoss:0.0810 | top1:70.3211 | AUROC:0.9729\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 10/10 | Loss:0.4242 | MainLoss:0.3787 | Alpha:0.4165 | SPLoss:0.0295 | CLSLoss:0.0797 | top1:83.7750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1646 | MainLoss:0.1646 | SPLoss:0.0400 | CLSLoss:0.0996 | top1:97.4548 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.4167 | MainLoss:0.4167 | SPLoss:0.0400 | CLSLoss:0.0996 | top1:77.1822 | AUROC:0.9802\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.5083 | MainLoss:0.4153 | Alpha:0.4206 | SPLoss:0.1145 | CLSLoss:0.1074 | top1:80.0750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1523 | MainLoss:0.1523 | SPLoss:0.0432 | CLSLoss:0.1068 | top1:97.6262 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4226 | MainLoss:0.4226 | SPLoss:0.0432 | CLSLoss:0.1068 | top1:76.8676 | AUROC:0.9806\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 10/10 | Loss:0.3870 | MainLoss:0.3366 | Alpha:0.4241 | SPLoss:0.0256 | CLSLoss:0.0934 | top1:86.3000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1568 | MainLoss:0.1568 | SPLoss:0.0367 | CLSLoss:0.0972 | top1:96.1932 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.3687 | MainLoss:0.3687 | SPLoss:0.0367 | CLSLoss:0.0972 | top1:81.5662 | AUROC:0.9803\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 10/10 | Loss:0.4592 | MainLoss:0.3968 | Alpha:0.4288 | SPLoss:0.0542 | CLSLoss:0.0909 | top1:83.7000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.3090 | MainLoss:0.3090 | SPLoss:0.1064 | CLSLoss:0.2788 | top1:95.9938 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.5510 | MainLoss:0.5510 | SPLoss:0.1064 | CLSLoss:0.2788 | top1:84.1284 | AUROC:0.9730\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 10/10 | Loss:0.3958 | MainLoss:0.3183 | Alpha:0.3966 | SPLoss:0.0314 | CLSLoss:0.1638 | top1:86.4250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1642 | MainLoss:0.1642 | SPLoss:0.0476 | CLSLoss:0.0797 | top1:97.7788 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.3972 | MainLoss:0.3972 | SPLoss:0.0476 | CLSLoss:0.0797 | top1:80.1376 | AUROC:0.9842\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 10/10 | Loss:0.3261 | MainLoss:0.2747 | Alpha:0.4263 | SPLoss:0.0301 | CLSLoss:0.0905 | top1:88.9000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2650 | MainLoss:0.2650 | SPLoss:0.0482 | CLSLoss:0.0632 | top1:90.8162 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2904 | MainLoss:0.2904 | SPLoss:0.0482 | CLSLoss:0.0632 | top1:91.3467 | AUROC:0.9840\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 10/10 | Loss:0.3579 | MainLoss:0.3093 | Alpha:0.4206 | SPLoss:0.0367 | CLSLoss:0.0789 | top1:87.3000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1119 | MainLoss:0.1119 | SPLoss:0.0532 | CLSLoss:0.0691 | top1:99.2835 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4784 | MainLoss:0.4784 | SPLoss:0.0532 | CLSLoss:0.0691 | top1:70.3408 | AUROC:0.9838\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 10/10 | Loss:0.4738 | MainLoss:0.3842 | Alpha:0.4276 | SPLoss:0.1257 | CLSLoss:0.0840 | top1:81.5000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1170 | MainLoss:0.1170 | SPLoss:0.1560 | CLSLoss:0.1177 | top1:99.6573 | AUROC:0.9999\n",
      "Test | 77/10 | Loss:0.5916 | MainLoss:0.5916 | SPLoss:0.1560 | CLSLoss:0.1177 | top1:58.0472 | AUROC:0.9780\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 10/10 | Loss:0.3481 | MainLoss:0.2870 | Alpha:0.4158 | SPLoss:0.0435 | CLSLoss:0.1035 | top1:87.5750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0850 | MainLoss:0.0850 | SPLoss:0.0835 | CLSLoss:0.1165 | top1:98.9284 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4553 | MainLoss:0.4553 | SPLoss:0.0835 | CLSLoss:0.1165 | top1:74.4889 | AUROC:0.9779\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 10/10 | Loss:0.3274 | MainLoss:0.2763 | Alpha:0.4341 | SPLoss:0.0286 | CLSLoss:0.0892 | top1:89.0000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1193 | MainLoss:0.1193 | SPLoss:0.0452 | CLSLoss:0.0728 | top1:99.5452 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.5929 | MainLoss:0.5929 | SPLoss:0.0452 | CLSLoss:0.0728 | top1:63.7582 | AUROC:0.9780\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 10/10 | Loss:0.3087 | MainLoss:0.2519 | Alpha:0.4128 | SPLoss:0.0403 | CLSLoss:0.0973 | top1:90.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0676 | MainLoss:0.0676 | SPLoss:0.0497 | CLSLoss:0.0861 | top1:99.2523 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4763 | MainLoss:0.4763 | SPLoss:0.0497 | CLSLoss:0.0861 | top1:75.0557 | AUROC:0.9759\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 10/10 | Loss:0.3649 | MainLoss:0.3003 | Alpha:0.4353 | SPLoss:0.0466 | CLSLoss:0.1018 | top1:84.7750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.7747 | MainLoss:0.7747 | SPLoss:0.1184 | CLSLoss:0.0682 | top1:65.8006 | AUROC:0.9992\n",
      "Test | 77/10 | Loss:0.3763 | MainLoss:0.3763 | SPLoss:0.1184 | CLSLoss:0.0682 | top1:87.6671 | AUROC:0.9727\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 10/10 | Loss:0.5562 | MainLoss:0.4009 | Alpha:0.3329 | SPLoss:0.3154 | CLSLoss:0.1509 | top1:79.8250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0640 | MainLoss:0.0640 | SPLoss:0.1026 | CLSLoss:0.1204 | top1:98.7819 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4134 | MainLoss:0.4134 | SPLoss:0.1026 | CLSLoss:0.1204 | top1:79.3676 | AUROC:0.9785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 10/10 | Loss:0.3552 | MainLoss:0.2922 | Alpha:0.4451 | SPLoss:0.0422 | CLSLoss:0.0994 | top1:88.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1354 | MainLoss:0.1354 | SPLoss:0.0425 | CLSLoss:0.0985 | top1:96.0935 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2739 | MainLoss:0.2739 | SPLoss:0.0425 | CLSLoss:0.0985 | top1:88.1619 | AUROC:0.9805\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 10/10 | Loss:0.4517 | MainLoss:0.3013 | Alpha:0.4534 | SPLoss:0.0800 | CLSLoss:0.2527 | top1:86.1750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2608 | MainLoss:0.2608 | SPLoss:0.0466 | CLSLoss:0.1397 | top1:90.5421 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2299 | MainLoss:0.2299 | SPLoss:0.0466 | CLSLoss:0.1397 | top1:91.5072 | AUROC:0.9749\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 10/10 | Loss:0.3281 | MainLoss:0.2576 | Alpha:0.4380 | SPLoss:0.0549 | CLSLoss:0.1056 | top1:90.0000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0721 | MainLoss:0.0721 | SPLoss:0.1890 | CLSLoss:0.0897 | top1:99.3520 | AUROC:0.9999\n",
      "Test | 77/10 | Loss:0.8281 | MainLoss:0.8281 | SPLoss:0.1890 | CLSLoss:0.0897 | top1:57.9358 | AUROC:0.9616\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 10/10 | Loss:0.3630 | MainLoss:0.2953 | Alpha:0.3939 | SPLoss:0.0792 | CLSLoss:0.0925 | top1:89.4500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0800 | MainLoss:0.0800 | SPLoss:0.0884 | CLSLoss:0.1144 | top1:98.8411 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.3360 | MainLoss:0.3360 | SPLoss:0.0884 | CLSLoss:0.1144 | top1:83.2012 | AUROC:0.9804\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 10/10 | Loss:0.2936 | MainLoss:0.2313 | Alpha:0.4507 | SPLoss:0.0380 | CLSLoss:0.1000 | top1:90.6500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2714 | MainLoss:0.2714 | SPLoss:0.0644 | CLSLoss:0.0884 | top1:90.9284 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2487 | MainLoss:0.2487 | SPLoss:0.0644 | CLSLoss:0.0884 | top1:92.6474 | AUROC:0.9788\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 10/10 | Loss:0.2729 | MainLoss:0.2145 | Alpha:0.4343 | SPLoss:0.0445 | CLSLoss:0.0901 | top1:91.7500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0993 | MainLoss:0.0993 | SPLoss:0.0560 | CLSLoss:0.0813 | top1:97.7009 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2856 | MainLoss:0.2856 | SPLoss:0.0560 | CLSLoss:0.0813 | top1:86.5727 | AUROC:0.9761\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 10/10 | Loss:0.4827 | MainLoss:0.3368 | Alpha:0.4601 | SPLoss:0.1246 | CLSLoss:0.1930 | top1:82.2000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1353 | MainLoss:0.1353 | SPLoss:0.1516 | CLSLoss:0.4951 | top1:95.9034 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2033 | MainLoss:0.2033 | SPLoss:0.1516 | CLSLoss:0.4951 | top1:91.6055 | AUROC:0.9876\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 10/10 | Loss:0.2941 | MainLoss:0.1865 | Alpha:0.4565 | SPLoss:0.0420 | CLSLoss:0.1944 | top1:93.0500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1447 | MainLoss:0.1447 | SPLoss:0.0687 | CLSLoss:0.1046 | top1:95.4829 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2339 | MainLoss:0.2339 | SPLoss:0.0687 | CLSLoss:0.1046 | top1:90.7045 | AUROC:0.9781\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 10/10 | Loss:0.2610 | MainLoss:0.1986 | Alpha:0.4585 | SPLoss:0.0445 | CLSLoss:0.0914 | top1:92.7250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0463 | MainLoss:0.0463 | SPLoss:0.0670 | CLSLoss:0.0982 | top1:99.2461 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4649 | MainLoss:0.4649 | SPLoss:0.0670 | CLSLoss:0.0982 | top1:77.4902 | AUROC:0.9713\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 10/10 | Loss:0.3063 | MainLoss:0.2340 | Alpha:0.4500 | SPLoss:0.0564 | CLSLoss:0.1044 | top1:91.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0292 | MainLoss:0.0292 | SPLoss:0.0541 | CLSLoss:0.1132 | top1:99.5203 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.6005 | MainLoss:0.6005 | SPLoss:0.0541 | CLSLoss:0.1132 | top1:72.3067 | AUROC:0.9678\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 10/10 | Loss:0.4858 | MainLoss:0.3039 | Alpha:0.4382 | SPLoss:0.1301 | CLSLoss:0.2821 | top1:84.7500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0935 | MainLoss:0.0935 | SPLoss:0.0724 | CLSLoss:0.1025 | top1:97.4081 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2604 | MainLoss:0.2604 | SPLoss:0.0724 | CLSLoss:0.1025 | top1:88.3978 | AUROC:0.9804\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 10/10 | Loss:0.2812 | MainLoss:0.2132 | Alpha:0.4671 | SPLoss:0.0618 | CLSLoss:0.0837 | top1:91.8750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1406 | MainLoss:0.1406 | SPLoss:0.1487 | CLSLoss:0.0708 | top1:99.3707 | AUROC:0.9999\n",
      "Test | 77/10 | Loss:0.6891 | MainLoss:0.6891 | SPLoss:0.1487 | CLSLoss:0.0708 | top1:59.6363 | AUROC:0.9736\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 10/10 | Loss:0.2916 | MainLoss:0.2184 | Alpha:0.4086 | SPLoss:0.0682 | CLSLoss:0.1109 | top1:91.6250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0527 | MainLoss:0.0527 | SPLoss:0.0728 | CLSLoss:0.0947 | top1:99.1620 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.3728 | MainLoss:0.3728 | SPLoss:0.0728 | CLSLoss:0.0947 | top1:81.4024 | AUROC:0.9778\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 10/10 | Loss:0.2979 | MainLoss:0.2159 | Alpha:0.4630 | SPLoss:0.0557 | CLSLoss:0.1213 | top1:92.8750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0506 | MainLoss:0.0506 | SPLoss:0.0473 | CLSLoss:0.1447 | top1:98.9065 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.3308 | MainLoss:0.3308 | SPLoss:0.0473 | CLSLoss:0.1447 | top1:84.0039 | AUROC:0.9799\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 10/10 | Loss:0.2169 | MainLoss:0.1563 | Alpha:0.4674 | SPLoss:0.0423 | CLSLoss:0.0873 | top1:93.9250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0280 | MainLoss:0.0280 | SPLoss:0.0716 | CLSLoss:0.0888 | top1:99.6075 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.6525 | MainLoss:0.6525 | SPLoss:0.0716 | CLSLoss:0.0888 | top1:70.6062 | AUROC:0.9692\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 10/10 | Loss:0.2659 | MainLoss:0.2014 | Alpha:0.4387 | SPLoss:0.0704 | CLSLoss:0.0766 | top1:92.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.6658 | MainLoss:0.6658 | SPLoss:0.1975 | CLSLoss:0.0515 | top1:67.9969 | AUROC:0.9992\n",
      "Test | 77/10 | Loss:0.4962 | MainLoss:0.4962 | SPLoss:0.1975 | CLSLoss:0.0515 | top1:80.7110 | AUROC:0.9697\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 10/10 | Loss:0.3793 | MainLoss:0.2498 | Alpha:0.3652 | SPLoss:0.1763 | CLSLoss:0.1760 | top1:89.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0530 | MainLoss:0.0530 | SPLoss:0.0925 | CLSLoss:0.0998 | top1:98.8255 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.3352 | MainLoss:0.3352 | SPLoss:0.0925 | CLSLoss:0.0998 | top1:83.8729 | AUROC:0.9711\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 10/10 | Loss:0.3308 | MainLoss:0.2357 | Alpha:0.4705 | SPLoss:0.0830 | CLSLoss:0.1191 | top1:88.5750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0465 | MainLoss:0.0465 | SPLoss:0.0948 | CLSLoss:0.1162 | top1:99.4735 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4240 | MainLoss:0.4240 | SPLoss:0.0948 | CLSLoss:0.1162 | top1:77.1232 | AUROC:0.9786\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 10/10 | Loss:0.1718 | MainLoss:0.1132 | Alpha:0.4594 | SPLoss:0.0404 | CLSLoss:0.0871 | top1:96.3750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0630 | MainLoss:0.0630 | SPLoss:0.0604 | CLSLoss:0.0757 | top1:98.6137 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.3179 | MainLoss:0.3179 | SPLoss:0.0604 | CLSLoss:0.0757 | top1:84.5184 | AUROC:0.9724\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 10/10 | Loss:0.3210 | MainLoss:0.2172 | Alpha:0.4729 | SPLoss:0.1056 | CLSLoss:0.1136 | top1:88.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2795 | MainLoss:0.2795 | SPLoss:0.3166 | CLSLoss:0.0774 | top1:94.7601 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.9361 | MainLoss:0.9361 | SPLoss:0.3166 | CLSLoss:0.0774 | top1:53.0668 | AUROC:0.9600\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 10/10 | Loss:0.2907 | MainLoss:0.1740 | Alpha:0.3688 | SPLoss:0.0992 | CLSLoss:0.2175 | top1:94.0000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0522 | MainLoss:0.0522 | SPLoss:0.1288 | CLSLoss:0.1027 | top1:98.7446 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.3095 | MainLoss:0.3095 | SPLoss:0.1288 | CLSLoss:0.1027 | top1:85.7765 | AUROC:0.9718\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 10/10 | Loss:0.1785 | MainLoss:0.1225 | Alpha:0.4745 | SPLoss:0.0405 | CLSLoss:0.0773 | top1:95.6250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0301 | MainLoss:0.0301 | SPLoss:0.1076 | CLSLoss:0.1108 | top1:99.4642 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.8748 | MainLoss:0.8748 | SPLoss:0.1076 | CLSLoss:0.1108 | top1:66.2779 | AUROC:0.9496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 10/10 | Loss:0.2649 | MainLoss:0.1861 | Alpha:0.4198 | SPLoss:0.0960 | CLSLoss:0.0915 | top1:92.7000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1704 | MainLoss:0.1704 | SPLoss:0.1457 | CLSLoss:0.0639 | top1:94.4735 | AUROC:0.9993\n",
      "Test | 77/10 | Loss:0.2387 | MainLoss:0.2387 | SPLoss:0.1457 | CLSLoss:0.0639 | top1:90.8847 | AUROC:0.9678\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 10/10 | Loss:0.2605 | MainLoss:0.1767 | Alpha:0.4677 | SPLoss:0.0821 | CLSLoss:0.0973 | top1:93.9000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1402 | MainLoss:0.1402 | SPLoss:0.0556 | CLSLoss:0.0924 | top1:95.4081 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2372 | MainLoss:0.2372 | SPLoss:0.0556 | CLSLoss:0.0924 | top1:90.3408 | AUROC:0.9671\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 10/10 | Loss:0.2360 | MainLoss:0.1681 | Alpha:0.4746 | SPLoss:0.0686 | CLSLoss:0.0745 | top1:93.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.9493 | MainLoss:0.9493 | SPLoss:0.1817 | CLSLoss:0.0543 | top1:58.7477 | AUROC:0.9983\n",
      "Test | 77/10 | Loss:0.6843 | MainLoss:0.6843 | SPLoss:0.1817 | CLSLoss:0.0543 | top1:71.8512 | AUROC:0.9577\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 10/10 | Loss:0.5826 | MainLoss:0.3442 | Alpha:0.3184 | SPLoss:0.5069 | CLSLoss:0.2366 | top1:82.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0376 | MainLoss:0.0376 | SPLoss:0.1943 | CLSLoss:0.1533 | top1:99.0249 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.3462 | MainLoss:0.3462 | SPLoss:0.1943 | CLSLoss:0.1533 | top1:84.7477 | AUROC:0.9698\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 10/10 | Loss:0.1472 | MainLoss:0.0879 | Alpha:0.4754 | SPLoss:0.0357 | CLSLoss:0.0890 | top1:97.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1080 | MainLoss:0.1080 | SPLoss:0.0553 | CLSLoss:0.0700 | top1:96.5794 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2388 | MainLoss:0.2388 | SPLoss:0.0553 | CLSLoss:0.0700 | top1:90.1769 | AUROC:0.9688\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 10/10 | Loss:0.1515 | MainLoss:0.0990 | Alpha:0.4791 | SPLoss:0.0414 | CLSLoss:0.0681 | top1:96.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1457 | MainLoss:0.1457 | SPLoss:0.0558 | CLSLoss:0.0621 | top1:95.3863 | AUROC:0.9992\n",
      "Test | 77/10 | Loss:0.2424 | MainLoss:0.2424 | SPLoss:0.0558 | CLSLoss:0.0621 | top1:90.3735 | AUROC:0.9658\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 10/10 | Loss:0.6603 | MainLoss:0.4047 | Alpha:0.4771 | SPLoss:0.4217 | CLSLoss:0.1152 | top1:80.4750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.4263 | MainLoss:0.4263 | SPLoss:1.3331 | CLSLoss:0.1008 | top1:91.6947 | AUROC:0.9989\n",
      "Test | 77/10 | Loss:0.5628 | MainLoss:0.5628 | SPLoss:1.3331 | CLSLoss:0.1008 | top1:53.1815 | AUROC:0.9843\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 10/10 | Loss:0.4666 | MainLoss:0.4207 | Alpha:0.3802 | SPLoss:0.0061 | CLSLoss:0.1144 | top1:80.8250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.2418 | MainLoss:0.2418 | SPLoss:0.0190 | CLSLoss:0.1316 | top1:98.7477 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.5291 | MainLoss:0.5291 | SPLoss:0.0190 | CLSLoss:0.1316 | top1:64.2923 | AUROC:0.9832\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 10/10 | Loss:0.3747 | MainLoss:0.3147 | Alpha:0.4127 | SPLoss:0.0039 | CLSLoss:0.1416 | top1:87.4250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1550 | MainLoss:0.1550 | SPLoss:0.0120 | CLSLoss:0.1518 | top1:99.3614 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.4888 | MainLoss:0.4888 | SPLoss:0.0120 | CLSLoss:0.1518 | top1:71.9201 | AUROC:0.9825\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 10/10 | Loss:0.3261 | MainLoss:0.2577 | Alpha:0.4301 | SPLoss:0.0029 | CLSLoss:0.1560 | top1:90.7500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1165 | MainLoss:0.1165 | SPLoss:0.0088 | CLSLoss:0.1587 | top1:99.4019 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4444 | MainLoss:0.4444 | SPLoss:0.0088 | CLSLoss:0.1587 | top1:77.4935 | AUROC:0.9821\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 10/10 | Loss:0.2822 | MainLoss:0.2100 | Alpha:0.4433 | SPLoss:0.0024 | CLSLoss:0.1606 | top1:93.6000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0930 | MainLoss:0.0930 | SPLoss:0.0073 | CLSLoss:0.1603 | top1:99.2243 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.4089 | MainLoss:0.4089 | SPLoss:0.0073 | CLSLoss:0.1603 | top1:81.0387 | AUROC:0.9810\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 10/10 | Loss:0.2527 | MainLoss:0.1805 | Alpha:0.4516 | SPLoss:0.0021 | CLSLoss:0.1580 | top1:94.9000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0825 | MainLoss:0.0825 | SPLoss:0.0067 | CLSLoss:0.1561 | top1:98.9346 | AUROC:0.9999\n",
      "Test | 77/10 | Loss:0.3686 | MainLoss:0.3686 | SPLoss:0.0067 | CLSLoss:0.1561 | top1:84.4954 | AUROC:0.9800\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 10/10 | Loss:0.2212 | MainLoss:0.1500 | Alpha:0.4597 | SPLoss:0.0019 | CLSLoss:0.1531 | top1:96.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0767 | MainLoss:0.0767 | SPLoss:0.0059 | CLSLoss:0.1486 | top1:98.5514 | AUROC:0.9999\n",
      "Test | 77/10 | Loss:0.3355 | MainLoss:0.3355 | SPLoss:0.0059 | CLSLoss:0.1486 | top1:86.5596 | AUROC:0.9786\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 10/10 | Loss:0.1902 | MainLoss:0.1216 | Alpha:0.4671 | SPLoss:0.0017 | CLSLoss:0.1451 | top1:97.3250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0693 | MainLoss:0.0693 | SPLoss:0.0053 | CLSLoss:0.1414 | top1:98.3645 | AUROC:0.9999\n",
      "Test | 77/10 | Loss:0.3163 | MainLoss:0.3163 | SPLoss:0.0053 | CLSLoss:0.1414 | top1:87.3132 | AUROC:0.9764\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 10/10 | Loss:0.1775 | MainLoss:0.1130 | Alpha:0.4701 | SPLoss:0.0015 | CLSLoss:0.1357 | top1:97.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0740 | MainLoss:0.0740 | SPLoss:0.0042 | CLSLoss:0.1289 | top1:97.8972 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.2879 | MainLoss:0.2879 | SPLoss:0.0042 | CLSLoss:0.1289 | top1:88.6337 | AUROC:0.9745\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 10/10 | Loss:0.1559 | MainLoss:0.0965 | Alpha:0.4746 | SPLoss:0.0012 | CLSLoss:0.1239 | top1:97.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0706 | MainLoss:0.0706 | SPLoss:0.0035 | CLSLoss:0.1189 | top1:97.8006 | AUROC:0.9998\n",
      "Test | 77/10 | Loss:0.2819 | MainLoss:0.2819 | SPLoss:0.0035 | CLSLoss:0.1189 | top1:88.6894 | AUROC:0.9726\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 10/10 | Loss:0.1428 | MainLoss:0.0879 | Alpha:0.4770 | SPLoss:0.0012 | CLSLoss:0.1139 | top1:97.4500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0742 | MainLoss:0.0742 | SPLoss:0.0031 | CLSLoss:0.1069 | top1:97.5701 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2695 | MainLoss:0.2695 | SPLoss:0.0031 | CLSLoss:0.1069 | top1:89.0498 | AUROC:0.9709\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 10/10 | Loss:0.1302 | MainLoss:0.0804 | Alpha:0.4791 | SPLoss:0.0010 | CLSLoss:0.1029 | top1:97.8750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0780 | MainLoss:0.0780 | SPLoss:0.0029 | CLSLoss:0.0970 | top1:97.3832 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2593 | MainLoss:0.2593 | SPLoss:0.0029 | CLSLoss:0.0970 | top1:89.4397 | AUROC:0.9694\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 10/10 | Loss:0.1225 | MainLoss:0.0774 | Alpha:0.4804 | SPLoss:0.0009 | CLSLoss:0.0931 | top1:97.7000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0707 | MainLoss:0.0707 | SPLoss:0.0024 | CLSLoss:0.0856 | top1:97.7196 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.2682 | MainLoss:0.2682 | SPLoss:0.0024 | CLSLoss:0.0856 | top1:89.0727 | AUROC:0.9686\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 10/10 | Loss:0.1075 | MainLoss:0.0670 | Alpha:0.4825 | SPLoss:0.0011 | CLSLoss:0.0830 | top1:98.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0689 | MainLoss:0.0689 | SPLoss:0.0027 | CLSLoss:0.0793 | top1:97.8972 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2688 | MainLoss:0.2688 | SPLoss:0.0027 | CLSLoss:0.0793 | top1:88.7222 | AUROC:0.9679\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 10/10 | Loss:0.1052 | MainLoss:0.0682 | Alpha:0.4828 | SPLoss:0.0011 | CLSLoss:0.0755 | top1:98.0000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0778 | MainLoss:0.0778 | SPLoss:0.0024 | CLSLoss:0.0694 | top1:97.5389 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2562 | MainLoss:0.2562 | SPLoss:0.0024 | CLSLoss:0.0694 | top1:89.4364 | AUROC:0.9680\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 10/10 | Loss:0.0876 | MainLoss:0.0542 | Alpha:0.4862 | SPLoss:0.0012 | CLSLoss:0.0674 | top1:98.4750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0929 | MainLoss:0.0929 | SPLoss:0.0025 | CLSLoss:0.0646 | top1:96.7383 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2428 | MainLoss:0.2428 | SPLoss:0.0025 | CLSLoss:0.0646 | top1:90.6094 | AUROC:0.9691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 10/10 | Loss:0.0885 | MainLoss:0.0578 | Alpha:0.4855 | SPLoss:0.0012 | CLSLoss:0.0619 | top1:98.1500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0760 | MainLoss:0.0760 | SPLoss:0.0025 | CLSLoss:0.0580 | top1:97.6885 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2603 | MainLoss:0.2603 | SPLoss:0.0025 | CLSLoss:0.0580 | top1:89.5249 | AUROC:0.9690\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 10/10 | Loss:0.0832 | MainLoss:0.0553 | Alpha:0.4863 | SPLoss:0.0011 | CLSLoss:0.0564 | top1:98.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0577 | MainLoss:0.0577 | SPLoss:0.0026 | CLSLoss:0.0549 | top1:98.3053 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2945 | MainLoss:0.2945 | SPLoss:0.0026 | CLSLoss:0.0549 | top1:87.9325 | AUROC:0.9659\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 10/10 | Loss:0.0872 | MainLoss:0.0608 | Alpha:0.4833 | SPLoss:0.0014 | CLSLoss:0.0533 | top1:98.1500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0669 | MainLoss:0.0669 | SPLoss:0.0027 | CLSLoss:0.0511 | top1:98.0592 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2737 | MainLoss:0.2737 | SPLoss:0.0027 | CLSLoss:0.0511 | top1:88.9187 | AUROC:0.9674\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 10/10 | Loss:0.0815 | MainLoss:0.0567 | Alpha:0.4858 | SPLoss:0.0008 | CLSLoss:0.0502 | top1:98.1500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0758 | MainLoss:0.0758 | SPLoss:0.0019 | CLSLoss:0.0477 | top1:97.6231 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2603 | MainLoss:0.2603 | SPLoss:0.0019 | CLSLoss:0.0477 | top1:89.7739 | AUROC:0.9676\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 10/10 | Loss:0.0860 | MainLoss:0.0623 | Alpha:0.4846 | SPLoss:0.0011 | CLSLoss:0.0477 | top1:98.0250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0831 | MainLoss:0.0831 | SPLoss:0.0025 | CLSLoss:0.0454 | top1:97.1776 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2530 | MainLoss:0.2530 | SPLoss:0.0025 | CLSLoss:0.0454 | top1:90.2785 | AUROC:0.9673\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 10/10 | Loss:0.0779 | MainLoss:0.0552 | Alpha:0.4863 | SPLoss:0.0010 | CLSLoss:0.0455 | top1:98.3000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0784 | MainLoss:0.0784 | SPLoss:0.0020 | CLSLoss:0.0453 | top1:97.4548 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2617 | MainLoss:0.2617 | SPLoss:0.0020 | CLSLoss:0.0453 | top1:89.9476 | AUROC:0.9670\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 10/10 | Loss:0.0802 | MainLoss:0.0581 | Alpha:0.4858 | SPLoss:0.0012 | CLSLoss:0.0442 | top1:98.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0779 | MainLoss:0.0779 | SPLoss:0.0020 | CLSLoss:0.0440 | top1:97.4798 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2641 | MainLoss:0.2641 | SPLoss:0.0020 | CLSLoss:0.0440 | top1:89.9050 | AUROC:0.9675\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 10/10 | Loss:0.0802 | MainLoss:0.0586 | Alpha:0.4856 | SPLoss:0.0011 | CLSLoss:0.0434 | top1:98.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0672 | MainLoss:0.0672 | SPLoss:0.0019 | CLSLoss:0.0428 | top1:97.9284 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2818 | MainLoss:0.2818 | SPLoss:0.0019 | CLSLoss:0.0428 | top1:89.2890 | AUROC:0.9667\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 10/10 | Loss:0.0747 | MainLoss:0.0534 | Alpha:0.4864 | SPLoss:0.0012 | CLSLoss:0.0425 | top1:98.2000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0795 | MainLoss:0.0795 | SPLoss:0.0027 | CLSLoss:0.0422 | top1:97.2866 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2600 | MainLoss:0.2600 | SPLoss:0.0027 | CLSLoss:0.0422 | top1:90.2228 | AUROC:0.9677\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 10/10 | Loss:0.0719 | MainLoss:0.0511 | Alpha:0.4874 | SPLoss:0.0010 | CLSLoss:0.0418 | top1:98.4250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0866 | MainLoss:0.0866 | SPLoss:0.0021 | CLSLoss:0.0411 | top1:97.0499 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2593 | MainLoss:0.2593 | SPLoss:0.0021 | CLSLoss:0.0411 | top1:90.3473 | AUROC:0.9667\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 10/10 | Loss:0.0822 | MainLoss:0.0623 | Alpha:0.4846 | SPLoss:0.0014 | CLSLoss:0.0398 | top1:98.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0888 | MainLoss:0.0888 | SPLoss:0.0028 | CLSLoss:0.0391 | top1:96.9938 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2574 | MainLoss:0.2574 | SPLoss:0.0028 | CLSLoss:0.0391 | top1:90.3735 | AUROC:0.9658\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 10/10 | Loss:0.0788 | MainLoss:0.0596 | Alpha:0.4853 | SPLoss:0.0010 | CLSLoss:0.0386 | top1:98.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0839 | MainLoss:0.0839 | SPLoss:0.0021 | CLSLoss:0.0370 | top1:97.3458 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2622 | MainLoss:0.2622 | SPLoss:0.0021 | CLSLoss:0.0370 | top1:90.0197 | AUROC:0.9656\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 10/10 | Loss:0.0663 | MainLoss:0.0472 | Alpha:0.4880 | SPLoss:0.0009 | CLSLoss:0.0381 | top1:98.7000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0759 | MainLoss:0.0759 | SPLoss:0.0017 | CLSLoss:0.0392 | top1:97.6355 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2762 | MainLoss:0.2762 | SPLoss:0.0017 | CLSLoss:0.0392 | top1:89.7215 | AUROC:0.9654\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 10/10 | Loss:0.0874 | MainLoss:0.0689 | Alpha:0.4831 | SPLoss:0.0015 | CLSLoss:0.0368 | top1:97.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0742 | MainLoss:0.0742 | SPLoss:0.0028 | CLSLoss:0.0357 | top1:97.6417 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2707 | MainLoss:0.2707 | SPLoss:0.0028 | CLSLoss:0.0357 | top1:89.7772 | AUROC:0.9656\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 10/10 | Loss:0.0722 | MainLoss:0.0540 | Alpha:0.4869 | SPLoss:0.0010 | CLSLoss:0.0362 | top1:98.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0847 | MainLoss:0.0847 | SPLoss:0.0023 | CLSLoss:0.0361 | top1:96.9626 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2589 | MainLoss:0.2589 | SPLoss:0.0023 | CLSLoss:0.0361 | top1:90.3342 | AUROC:0.9658\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 10/10 | Loss:0.0693 | MainLoss:0.0508 | Alpha:0.4873 | SPLoss:0.0010 | CLSLoss:0.0368 | top1:98.4250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0637 | MainLoss:0.0637 | SPLoss:0.0031 | CLSLoss:0.0361 | top1:98.0249 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2857 | MainLoss:0.2857 | SPLoss:0.0031 | CLSLoss:0.0361 | top1:89.2169 | AUROC:0.9652\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 10/10 | Loss:0.0746 | MainLoss:0.0564 | Alpha:0.4856 | SPLoss:0.0013 | CLSLoss:0.0360 | top1:98.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0732 | MainLoss:0.0732 | SPLoss:0.0024 | CLSLoss:0.0349 | top1:97.6137 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2764 | MainLoss:0.2764 | SPLoss:0.0024 | CLSLoss:0.0349 | top1:89.7969 | AUROC:0.9646\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 10/10 | Loss:0.0638 | MainLoss:0.0460 | Alpha:0.4885 | SPLoss:0.0011 | CLSLoss:0.0355 | top1:98.6000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0914 | MainLoss:0.0914 | SPLoss:0.0024 | CLSLoss:0.0353 | top1:96.9657 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2586 | MainLoss:0.2586 | SPLoss:0.0024 | CLSLoss:0.0353 | top1:90.4882 | AUROC:0.9664\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 10/10 | Loss:0.0734 | MainLoss:0.0562 | Alpha:0.4860 | SPLoss:0.0013 | CLSLoss:0.0341 | top1:98.2750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0739 | MainLoss:0.0739 | SPLoss:0.0024 | CLSLoss:0.0339 | top1:97.6885 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2718 | MainLoss:0.2718 | SPLoss:0.0024 | CLSLoss:0.0339 | top1:89.8526 | AUROC:0.9653\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 10/10 | Loss:0.0624 | MainLoss:0.0452 | Alpha:0.4886 | SPLoss:0.0011 | CLSLoss:0.0340 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0835 | MainLoss:0.0835 | SPLoss:0.0020 | CLSLoss:0.0358 | top1:97.2679 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2728 | MainLoss:0.2728 | SPLoss:0.0020 | CLSLoss:0.0358 | top1:90.1638 | AUROC:0.9646\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 10/10 | Loss:0.0680 | MainLoss:0.0507 | Alpha:0.4875 | SPLoss:0.0009 | CLSLoss:0.0346 | top1:98.6250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0765 | MainLoss:0.0765 | SPLoss:0.0018 | CLSLoss:0.0310 | top1:97.4766 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2719 | MainLoss:0.2719 | SPLoss:0.0018 | CLSLoss:0.0310 | top1:89.7543 | AUROC:0.9646\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 10/10 | Loss:0.0737 | MainLoss:0.0582 | Alpha:0.4854 | SPLoss:0.0010 | CLSLoss:0.0308 | top1:98.4000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0787 | MainLoss:0.0787 | SPLoss:0.0022 | CLSLoss:0.0316 | top1:97.4860 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2704 | MainLoss:0.2704 | SPLoss:0.0022 | CLSLoss:0.0316 | top1:89.8788 | AUROC:0.9646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n",
      "Train | 10/10 | Loss:0.0669 | MainLoss:0.0510 | Alpha:0.4876 | SPLoss:0.0012 | CLSLoss:0.0315 | top1:98.3750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0905 | MainLoss:0.0905 | SPLoss:0.0022 | CLSLoss:0.0337 | top1:96.9564 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2705 | MainLoss:0.2705 | SPLoss:0.0022 | CLSLoss:0.0337 | top1:90.2949 | AUROC:0.9643\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 10/10 | Loss:0.0690 | MainLoss:0.0525 | Alpha:0.4873 | SPLoss:0.0014 | CLSLoss:0.0325 | top1:98.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0674 | MainLoss:0.0674 | SPLoss:0.0028 | CLSLoss:0.0329 | top1:97.8661 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2908 | MainLoss:0.2908 | SPLoss:0.0028 | CLSLoss:0.0329 | top1:89.4397 | AUROC:0.9637\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 10/10 | Loss:0.0654 | MainLoss:0.0491 | Alpha:0.4877 | SPLoss:0.0011 | CLSLoss:0.0323 | top1:98.4500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0752 | MainLoss:0.0752 | SPLoss:0.0023 | CLSLoss:0.0316 | top1:97.3707 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2693 | MainLoss:0.2693 | SPLoss:0.0023 | CLSLoss:0.0316 | top1:90.0786 | AUROC:0.9650\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 10/10 | Loss:0.0633 | MainLoss:0.0475 | Alpha:0.4885 | SPLoss:0.0010 | CLSLoss:0.0312 | top1:98.7000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0830 | MainLoss:0.0830 | SPLoss:0.0019 | CLSLoss:0.0327 | top1:97.1900 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2701 | MainLoss:0.2701 | SPLoss:0.0019 | CLSLoss:0.0327 | top1:90.2851 | AUROC:0.9642\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n",
      "Train | 10/10 | Loss:0.0627 | MainLoss:0.0465 | Alpha:0.4887 | SPLoss:0.0012 | CLSLoss:0.0320 | top1:98.4500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0728 | MainLoss:0.0728 | SPLoss:0.0021 | CLSLoss:0.0317 | top1:97.5763 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2767 | MainLoss:0.2767 | SPLoss:0.0021 | CLSLoss:0.0317 | top1:89.9803 | AUROC:0.9640\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 10/10 | Loss:0.0559 | MainLoss:0.0399 | Alpha:0.4904 | SPLoss:0.0011 | CLSLoss:0.0317 | top1:98.7500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0789 | MainLoss:0.0789 | SPLoss:0.0022 | CLSLoss:0.0320 | top1:97.4174 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2823 | MainLoss:0.2823 | SPLoss:0.0022 | CLSLoss:0.0320 | top1:89.9574 | AUROC:0.9628\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 10/10 | Loss:0.0528 | MainLoss:0.0367 | Alpha:0.4909 | SPLoss:0.0008 | CLSLoss:0.0319 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0846 | MainLoss:0.0846 | SPLoss:0.0020 | CLSLoss:0.0321 | top1:97.1246 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2756 | MainLoss:0.2756 | SPLoss:0.0020 | CLSLoss:0.0321 | top1:90.2883 | AUROC:0.9641\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.039308\n",
      "Train | 10/10 | Loss:0.0639 | MainLoss:0.0480 | Alpha:0.4887 | SPLoss:0.0015 | CLSLoss:0.0309 | top1:98.6250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0796 | MainLoss:0.0796 | SPLoss:0.0024 | CLSLoss:0.0302 | top1:97.2555 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2661 | MainLoss:0.2661 | SPLoss:0.0024 | CLSLoss:0.0302 | top1:90.4128 | AUROC:0.9654\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.039291\n",
      "Train | 10/10 | Loss:0.0570 | MainLoss:0.0413 | Alpha:0.4897 | SPLoss:0.0013 | CLSLoss:0.0308 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0822 | MainLoss:0.0822 | SPLoss:0.0021 | CLSLoss:0.0309 | top1:97.2586 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2708 | MainLoss:0.2708 | SPLoss:0.0021 | CLSLoss:0.0309 | top1:90.5505 | AUROC:0.9654\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.039274\n",
      "Train | 10/10 | Loss:0.0574 | MainLoss:0.0421 | Alpha:0.4898 | SPLoss:0.0011 | CLSLoss:0.0302 | top1:98.7000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0806 | MainLoss:0.0806 | SPLoss:0.0021 | CLSLoss:0.0295 | top1:97.3115 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2681 | MainLoss:0.2681 | SPLoss:0.0021 | CLSLoss:0.0295 | top1:90.4456 | AUROC:0.9655\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.039258\n",
      "Train | 10/10 | Loss:0.0569 | MainLoss:0.0416 | Alpha:0.4901 | SPLoss:0.0014 | CLSLoss:0.0297 | top1:98.6000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0797 | MainLoss:0.0797 | SPLoss:0.0025 | CLSLoss:0.0307 | top1:97.3583 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2722 | MainLoss:0.2722 | SPLoss:0.0025 | CLSLoss:0.0307 | top1:90.3277 | AUROC:0.9649\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.039241\n",
      "Train | 10/10 | Loss:0.0506 | MainLoss:0.0349 | Alpha:0.4915 | SPLoss:0.0011 | CLSLoss:0.0307 | top1:99.1000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0721 | MainLoss:0.0721 | SPLoss:0.0023 | CLSLoss:0.0311 | top1:97.6978 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2883 | MainLoss:0.2883 | SPLoss:0.0023 | CLSLoss:0.0311 | top1:89.9803 | AUROC:0.9639\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.039223\n",
      "Train | 10/10 | Loss:0.0568 | MainLoss:0.0418 | Alpha:0.4896 | SPLoss:0.0011 | CLSLoss:0.0296 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0727 | MainLoss:0.0727 | SPLoss:0.0024 | CLSLoss:0.0294 | top1:97.6324 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2803 | MainLoss:0.2803 | SPLoss:0.0024 | CLSLoss:0.0294 | top1:90.1049 | AUROC:0.9646\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.039206\n",
      "Train | 10/10 | Loss:0.0577 | MainLoss:0.0434 | Alpha:0.4891 | SPLoss:0.0011 | CLSLoss:0.0283 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0879 | MainLoss:0.0879 | SPLoss:0.0025 | CLSLoss:0.0290 | top1:97.0654 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2649 | MainLoss:0.2649 | SPLoss:0.0025 | CLSLoss:0.0290 | top1:90.6357 | AUROC:0.9657\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.039188\n",
      "Train | 10/10 | Loss:0.0534 | MainLoss:0.0388 | Alpha:0.4903 | SPLoss:0.0010 | CLSLoss:0.0289 | top1:98.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0634 | MainLoss:0.0634 | SPLoss:0.0022 | CLSLoss:0.0286 | top1:97.9720 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2906 | MainLoss:0.2906 | SPLoss:0.0022 | CLSLoss:0.0286 | top1:89.7936 | AUROC:0.9647\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.039170\n",
      "Train | 10/10 | Loss:0.0600 | MainLoss:0.0458 | Alpha:0.4887 | SPLoss:0.0010 | CLSLoss:0.0282 | top1:98.6000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0627 | MainLoss:0.0627 | SPLoss:0.0026 | CLSLoss:0.0260 | top1:97.8567 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2880 | MainLoss:0.2880 | SPLoss:0.0026 | CLSLoss:0.0260 | top1:89.5839 | AUROC:0.9641\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.039152\n",
      "Train | 10/10 | Loss:0.0634 | MainLoss:0.0498 | Alpha:0.4869 | SPLoss:0.0014 | CLSLoss:0.0266 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0774 | MainLoss:0.0774 | SPLoss:0.0028 | CLSLoss:0.0266 | top1:97.3894 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2740 | MainLoss:0.2740 | SPLoss:0.0028 | CLSLoss:0.0266 | top1:90.0885 | AUROC:0.9643\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.039134\n",
      "Train | 10/10 | Loss:0.0514 | MainLoss:0.0374 | Alpha:0.4907 | SPLoss:0.0011 | CLSLoss:0.0274 | top1:98.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0793 | MainLoss:0.0793 | SPLoss:0.0021 | CLSLoss:0.0285 | top1:97.2773 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2770 | MainLoss:0.2770 | SPLoss:0.0021 | CLSLoss:0.0285 | top1:90.1638 | AUROC:0.9643\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.039116\n",
      "Train | 10/10 | Loss:0.0519 | MainLoss:0.0378 | Alpha:0.4907 | SPLoss:0.0009 | CLSLoss:0.0279 | top1:98.9000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0620 | MainLoss:0.0620 | SPLoss:0.0021 | CLSLoss:0.0281 | top1:97.9408 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.3013 | MainLoss:0.3013 | SPLoss:0.0021 | CLSLoss:0.0281 | top1:89.3906 | AUROC:0.9627\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.039097\n",
      "Train | 10/10 | Loss:0.0570 | MainLoss:0.0432 | Alpha:0.4893 | SPLoss:0.0009 | CLSLoss:0.0275 | top1:98.7250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0821 | MainLoss:0.0821 | SPLoss:0.0020 | CLSLoss:0.0254 | top1:97.3115 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2648 | MainLoss:0.2648 | SPLoss:0.0020 | CLSLoss:0.0254 | top1:90.1835 | AUROC:0.9643\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.039079\n",
      "Train | 10/10 | Loss:0.0613 | MainLoss:0.0481 | Alpha:0.4885 | SPLoss:0.0011 | CLSLoss:0.0260 | top1:98.4750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0660 | MainLoss:0.0660 | SPLoss:0.0025 | CLSLoss:0.0268 | top1:97.8255 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2856 | MainLoss:0.2856 | SPLoss:0.0025 | CLSLoss:0.0268 | top1:89.5184 | AUROC:0.9624\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.039060\n",
      "Train | 10/10 | Loss:0.0485 | MainLoss:0.0344 | Alpha:0.4910 | SPLoss:0.0010 | CLSLoss:0.0277 | top1:98.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0835 | MainLoss:0.0835 | SPLoss:0.0019 | CLSLoss:0.0276 | top1:97.2181 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2732 | MainLoss:0.2732 | SPLoss:0.0019 | CLSLoss:0.0276 | top1:90.3408 | AUROC:0.9635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [111 | 1000] LR: 0.039040\n",
      "Train | 10/10 | Loss:0.0467 | MainLoss:0.0327 | Alpha:0.4920 | SPLoss:0.0008 | CLSLoss:0.0277 | top1:99.1500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0774 | MainLoss:0.0774 | SPLoss:0.0017 | CLSLoss:0.0273 | top1:97.3925 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2754 | MainLoss:0.2754 | SPLoss:0.0017 | CLSLoss:0.0273 | top1:90.3571 | AUROC:0.9645\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.039021\n",
      "Train | 10/10 | Loss:0.0553 | MainLoss:0.0412 | Alpha:0.4902 | SPLoss:0.0018 | CLSLoss:0.0270 | top1:98.8000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0833 | MainLoss:0.0833 | SPLoss:0.0028 | CLSLoss:0.0269 | top1:97.2835 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2675 | MainLoss:0.2675 | SPLoss:0.0028 | CLSLoss:0.0269 | top1:90.6258 | AUROC:0.9658\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.039002\n",
      "Train | 10/10 | Loss:0.0487 | MainLoss:0.0347 | Alpha:0.4915 | SPLoss:0.0010 | CLSLoss:0.0275 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0683 | MainLoss:0.0683 | SPLoss:0.0024 | CLSLoss:0.0265 | top1:97.7601 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2814 | MainLoss:0.2814 | SPLoss:0.0024 | CLSLoss:0.0265 | top1:90.0459 | AUROC:0.9651\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.038982\n",
      "Train | 10/10 | Loss:0.0574 | MainLoss:0.0443 | Alpha:0.4889 | SPLoss:0.0012 | CLSLoss:0.0256 | top1:98.6750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0856 | MainLoss:0.0856 | SPLoss:0.0021 | CLSLoss:0.0261 | top1:97.0280 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2634 | MainLoss:0.2634 | SPLoss:0.0021 | CLSLoss:0.0261 | top1:90.6848 | AUROC:0.9658\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.038962\n",
      "Train | 10/10 | Loss:0.0476 | MainLoss:0.0342 | Alpha:0.4916 | SPLoss:0.0009 | CLSLoss:0.0264 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0972 | MainLoss:0.0972 | SPLoss:0.0019 | CLSLoss:0.0264 | top1:96.6137 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2506 | MainLoss:0.2506 | SPLoss:0.0019 | CLSLoss:0.0264 | top1:91.1337 | AUROC:0.9678\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.038942\n",
      "Train | 10/10 | Loss:0.0460 | MainLoss:0.0325 | Alpha:0.4917 | SPLoss:0.0010 | CLSLoss:0.0263 | top1:99.1750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0678 | MainLoss:0.0678 | SPLoss:0.0020 | CLSLoss:0.0274 | top1:97.7632 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2859 | MainLoss:0.2859 | SPLoss:0.0020 | CLSLoss:0.0274 | top1:90.2589 | AUROC:0.9656\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.038922\n",
      "Train | 10/10 | Loss:0.0514 | MainLoss:0.0380 | Alpha:0.4908 | SPLoss:0.0011 | CLSLoss:0.0261 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0678 | MainLoss:0.0678 | SPLoss:0.0021 | CLSLoss:0.0250 | top1:97.8193 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2761 | MainLoss:0.2761 | SPLoss:0.0021 | CLSLoss:0.0250 | top1:90.2621 | AUROC:0.9656\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.038901\n",
      "Train | 10/10 | Loss:0.0510 | MainLoss:0.0382 | Alpha:0.4906 | SPLoss:0.0011 | CLSLoss:0.0250 | top1:98.9000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0524 | MainLoss:0.0524 | SPLoss:0.0024 | CLSLoss:0.0254 | top1:98.2991 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.3040 | MainLoss:0.3040 | SPLoss:0.0024 | CLSLoss:0.0254 | top1:89.4463 | AUROC:0.9662\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.038881\n",
      "Train | 10/10 | Loss:0.0562 | MainLoss:0.0432 | Alpha:0.4885 | SPLoss:0.0014 | CLSLoss:0.0252 | top1:98.5750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1109 | MainLoss:0.1109 | SPLoss:0.0034 | CLSLoss:0.0244 | top1:96.2368 | AUROC:0.9993\n",
      "Test | 77/10 | Loss:0.2462 | MainLoss:0.2462 | SPLoss:0.0034 | CLSLoss:0.0244 | top1:91.3368 | AUROC:0.9683\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.038860\n",
      "Train | 10/10 | Loss:0.0479 | MainLoss:0.0355 | Alpha:0.4907 | SPLoss:0.0010 | CLSLoss:0.0244 | top1:98.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0861 | MainLoss:0.0861 | SPLoss:0.0015 | CLSLoss:0.0252 | top1:97.1869 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2580 | MainLoss:0.2580 | SPLoss:0.0015 | CLSLoss:0.0252 | top1:90.9600 | AUROC:0.9679\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.038839\n",
      "Train | 10/10 | Loss:0.0505 | MainLoss:0.0379 | Alpha:0.4907 | SPLoss:0.0011 | CLSLoss:0.0246 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0647 | MainLoss:0.0647 | SPLoss:0.0022 | CLSLoss:0.0246 | top1:97.9159 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2776 | MainLoss:0.2776 | SPLoss:0.0022 | CLSLoss:0.0246 | top1:90.2490 | AUROC:0.9671\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.038818\n",
      "Train | 10/10 | Loss:0.0585 | MainLoss:0.0462 | Alpha:0.4887 | SPLoss:0.0013 | CLSLoss:0.0239 | top1:98.5250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0608 | MainLoss:0.0608 | SPLoss:0.0028 | CLSLoss:0.0235 | top1:97.9564 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2755 | MainLoss:0.2755 | SPLoss:0.0028 | CLSLoss:0.0235 | top1:90.0033 | AUROC:0.9674\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.038796\n",
      "Train | 10/10 | Loss:0.0493 | MainLoss:0.0368 | Alpha:0.4904 | SPLoss:0.0010 | CLSLoss:0.0244 | top1:98.8250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0766 | MainLoss:0.0766 | SPLoss:0.0021 | CLSLoss:0.0245 | top1:97.5171 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2647 | MainLoss:0.2647 | SPLoss:0.0021 | CLSLoss:0.0245 | top1:90.5898 | AUROC:0.9666\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.038775\n",
      "Train | 10/10 | Loss:0.0589 | MainLoss:0.0467 | Alpha:0.4888 | SPLoss:0.0012 | CLSLoss:0.0239 | top1:98.6250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0621 | MainLoss:0.0621 | SPLoss:0.0022 | CLSLoss:0.0229 | top1:97.9470 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2799 | MainLoss:0.2799 | SPLoss:0.0022 | CLSLoss:0.0229 | top1:89.8132 | AUROC:0.9654\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.038753\n",
      "Train | 10/10 | Loss:0.0537 | MainLoss:0.0417 | Alpha:0.4893 | SPLoss:0.0010 | CLSLoss:0.0236 | top1:98.7750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0671 | MainLoss:0.0671 | SPLoss:0.0022 | CLSLoss:0.0236 | top1:97.7383 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2770 | MainLoss:0.2770 | SPLoss:0.0022 | CLSLoss:0.0236 | top1:90.1704 | AUROC:0.9661\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.038731\n",
      "Train | 10/10 | Loss:0.0487 | MainLoss:0.0364 | Alpha:0.4909 | SPLoss:0.0008 | CLSLoss:0.0240 | top1:98.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0779 | MainLoss:0.0779 | SPLoss:0.0017 | CLSLoss:0.0230 | top1:97.4735 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2645 | MainLoss:0.2645 | SPLoss:0.0017 | CLSLoss:0.0230 | top1:90.5079 | AUROC:0.9668\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.038709\n",
      "Train | 10/10 | Loss:0.0447 | MainLoss:0.0327 | Alpha:0.4918 | SPLoss:0.0007 | CLSLoss:0.0237 | top1:99.0750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0867 | MainLoss:0.0867 | SPLoss:0.0018 | CLSLoss:0.0238 | top1:97.0903 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2558 | MainLoss:0.2558 | SPLoss:0.0018 | CLSLoss:0.0238 | top1:90.9502 | AUROC:0.9679\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.038687\n",
      "Train | 10/10 | Loss:0.0476 | MainLoss:0.0354 | Alpha:0.4912 | SPLoss:0.0009 | CLSLoss:0.0240 | top1:98.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0952 | MainLoss:0.0952 | SPLoss:0.0020 | CLSLoss:0.0242 | top1:96.7819 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2516 | MainLoss:0.2516 | SPLoss:0.0020 | CLSLoss:0.0242 | top1:91.1763 | AUROC:0.9682\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.038664\n",
      "Train | 10/10 | Loss:0.0459 | MainLoss:0.0335 | Alpha:0.4915 | SPLoss:0.0006 | CLSLoss:0.0246 | top1:98.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0831 | MainLoss:0.0831 | SPLoss:0.0022 | CLSLoss:0.0229 | top1:97.0966 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2471 | MainLoss:0.2471 | SPLoss:0.0022 | CLSLoss:0.0229 | top1:91.0419 | AUROC:0.9690\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.038641\n",
      "Train | 10/10 | Loss:0.0449 | MainLoss:0.0329 | Alpha:0.4917 | SPLoss:0.0007 | CLSLoss:0.0238 | top1:99.1500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0775 | MainLoss:0.0775 | SPLoss:0.0017 | CLSLoss:0.0238 | top1:97.3770 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2667 | MainLoss:0.2667 | SPLoss:0.0017 | CLSLoss:0.0238 | top1:90.7372 | AUROC:0.9670\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.038619\n",
      "Train | 10/10 | Loss:0.0447 | MainLoss:0.0327 | Alpha:0.4919 | SPLoss:0.0009 | CLSLoss:0.0235 | top1:99.1000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0834 | MainLoss:0.0834 | SPLoss:0.0016 | CLSLoss:0.0239 | top1:97.3333 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2631 | MainLoss:0.2631 | SPLoss:0.0016 | CLSLoss:0.0239 | top1:90.7962 | AUROC:0.9671\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.038596\n",
      "Train | 10/10 | Loss:0.0401 | MainLoss:0.0277 | Alpha:0.4933 | SPLoss:0.0009 | CLSLoss:0.0242 | top1:99.0500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1098 | MainLoss:0.1098 | SPLoss:0.0018 | CLSLoss:0.0239 | top1:96.2399 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2501 | MainLoss:0.2501 | SPLoss:0.0018 | CLSLoss:0.0239 | top1:91.2549 | AUROC:0.9677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [133 | 1000] LR: 0.038572\n",
      "Train | 10/10 | Loss:0.0416 | MainLoss:0.0292 | Alpha:0.4921 | SPLoss:0.0011 | CLSLoss:0.0241 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0706 | MainLoss:0.0706 | SPLoss:0.0021 | CLSLoss:0.0241 | top1:97.7726 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2830 | MainLoss:0.2830 | SPLoss:0.0021 | CLSLoss:0.0241 | top1:90.3768 | AUROC:0.9655\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.038549\n",
      "Train | 10/10 | Loss:0.0422 | MainLoss:0.0302 | Alpha:0.4927 | SPLoss:0.0008 | CLSLoss:0.0237 | top1:99.1500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0818 | MainLoss:0.0818 | SPLoss:0.0017 | CLSLoss:0.0227 | top1:97.2586 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2611 | MainLoss:0.2611 | SPLoss:0.0017 | CLSLoss:0.0227 | top1:90.7438 | AUROC:0.9668\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.038525\n",
      "Train | 10/10 | Loss:0.0424 | MainLoss:0.0305 | Alpha:0.4924 | SPLoss:0.0009 | CLSLoss:0.0233 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0778 | MainLoss:0.0778 | SPLoss:0.0018 | CLSLoss:0.0227 | top1:97.3832 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2672 | MainLoss:0.2672 | SPLoss:0.0018 | CLSLoss:0.0227 | top1:90.6914 | AUROC:0.9664\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.038502\n",
      "Train | 10/10 | Loss:0.0447 | MainLoss:0.0328 | Alpha:0.4917 | SPLoss:0.0009 | CLSLoss:0.0233 | top1:98.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0666 | MainLoss:0.0666 | SPLoss:0.0026 | CLSLoss:0.0215 | top1:97.7664 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2647 | MainLoss:0.2647 | SPLoss:0.0026 | CLSLoss:0.0215 | top1:90.7765 | AUROC:0.9684\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.038478\n",
      "Train | 10/10 | Loss:0.0372 | MainLoss:0.0257 | Alpha:0.4935 | SPLoss:0.0007 | CLSLoss:0.0226 | top1:99.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0896 | MainLoss:0.0896 | SPLoss:0.0017 | CLSLoss:0.0235 | top1:97.0530 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2563 | MainLoss:0.2563 | SPLoss:0.0017 | CLSLoss:0.0235 | top1:91.1665 | AUROC:0.9680\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.038453\n",
      "Train | 10/10 | Loss:0.0479 | MainLoss:0.0363 | Alpha:0.4910 | SPLoss:0.0008 | CLSLoss:0.0229 | top1:99.0250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0851 | MainLoss:0.0851 | SPLoss:0.0021 | CLSLoss:0.0208 | top1:97.1184 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2538 | MainLoss:0.2538 | SPLoss:0.0021 | CLSLoss:0.0208 | top1:91.0583 | AUROC:0.9675\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.038429\n",
      "Train | 10/10 | Loss:0.0378 | MainLoss:0.0266 | Alpha:0.4930 | SPLoss:0.0008 | CLSLoss:0.0220 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0684 | MainLoss:0.0684 | SPLoss:0.0015 | CLSLoss:0.0227 | top1:97.7165 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2853 | MainLoss:0.2853 | SPLoss:0.0015 | CLSLoss:0.0227 | top1:90.4817 | AUROC:0.9660\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.038405\n",
      "Train | 10/10 | Loss:0.0432 | MainLoss:0.0316 | Alpha:0.4922 | SPLoss:0.0013 | CLSLoss:0.0222 | top1:99.0500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0773 | MainLoss:0.0773 | SPLoss:0.0024 | CLSLoss:0.0223 | top1:97.3956 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2776 | MainLoss:0.2776 | SPLoss:0.0024 | CLSLoss:0.0223 | top1:90.5931 | AUROC:0.9656\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.038380\n",
      "Train | 10/10 | Loss:0.0391 | MainLoss:0.0276 | Alpha:0.4933 | SPLoss:0.0008 | CLSLoss:0.0225 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0815 | MainLoss:0.0815 | SPLoss:0.0019 | CLSLoss:0.0223 | top1:97.2710 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2747 | MainLoss:0.2747 | SPLoss:0.0019 | CLSLoss:0.0223 | top1:90.6094 | AUROC:0.9652\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.038355\n",
      "Train | 10/10 | Loss:0.0350 | MainLoss:0.0234 | Alpha:0.4942 | SPLoss:0.0009 | CLSLoss:0.0227 | top1:99.4000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0681 | MainLoss:0.0681 | SPLoss:0.0015 | CLSLoss:0.0230 | top1:97.8318 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.3039 | MainLoss:0.3039 | SPLoss:0.0015 | CLSLoss:0.0230 | top1:90.1737 | AUROC:0.9637\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.038330\n",
      "Train | 10/10 | Loss:0.0413 | MainLoss:0.0300 | Alpha:0.4926 | SPLoss:0.0011 | CLSLoss:0.0218 | top1:99.0500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0863 | MainLoss:0.0863 | SPLoss:0.0022 | CLSLoss:0.0209 | top1:97.0872 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2689 | MainLoss:0.2689 | SPLoss:0.0022 | CLSLoss:0.0209 | top1:90.7700 | AUROC:0.9656\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.038305\n",
      "Train | 10/10 | Loss:0.0337 | MainLoss:0.0226 | Alpha:0.4941 | SPLoss:0.0008 | CLSLoss:0.0217 | top1:99.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0639 | MainLoss:0.0639 | SPLoss:0.0018 | CLSLoss:0.0221 | top1:97.9252 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:0.0018 | CLSLoss:0.0221 | top1:89.8984 | AUROC:0.9637\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.038279\n",
      "Train | 10/10 | Loss:0.0391 | MainLoss:0.0281 | Alpha:0.4933 | SPLoss:0.0010 | CLSLoss:0.0214 | top1:99.2750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0845 | MainLoss:0.0845 | SPLoss:0.0019 | CLSLoss:0.0217 | top1:97.2555 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2737 | MainLoss:0.2737 | SPLoss:0.0019 | CLSLoss:0.0217 | top1:90.6226 | AUROC:0.9656\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.038254\n",
      "Train | 10/10 | Loss:0.0464 | MainLoss:0.0353 | Alpha:0.4906 | SPLoss:0.0011 | CLSLoss:0.0216 | top1:98.8750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0619 | MainLoss:0.0619 | SPLoss:0.0031 | CLSLoss:0.0193 | top1:97.9720 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2875 | MainLoss:0.2875 | SPLoss:0.0031 | CLSLoss:0.0193 | top1:89.8493 | AUROC:0.9638\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.038228\n",
      "Train | 10/10 | Loss:0.0412 | MainLoss:0.0308 | Alpha:0.4921 | SPLoss:0.0009 | CLSLoss:0.0201 | top1:99.1750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0922 | MainLoss:0.0922 | SPLoss:0.0019 | CLSLoss:0.0207 | top1:96.9533 | AUROC:0.9993\n",
      "Test | 77/10 | Loss:0.2689 | MainLoss:0.2689 | SPLoss:0.0019 | CLSLoss:0.0207 | top1:90.7208 | AUROC:0.9652\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.038202\n",
      "Train | 10/10 | Loss:0.0409 | MainLoss:0.0299 | Alpha:0.4927 | SPLoss:0.0011 | CLSLoss:0.0211 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0715 | MainLoss:0.0715 | SPLoss:0.0024 | CLSLoss:0.0207 | top1:97.6417 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2821 | MainLoss:0.2821 | SPLoss:0.0024 | CLSLoss:0.0207 | top1:90.3834 | AUROC:0.9647\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.038176\n",
      "Train | 10/10 | Loss:0.0377 | MainLoss:0.0266 | Alpha:0.4932 | SPLoss:0.0008 | CLSLoss:0.0217 | top1:99.1750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0783 | MainLoss:0.0783 | SPLoss:0.0020 | CLSLoss:0.0207 | top1:97.4798 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2858 | MainLoss:0.2858 | SPLoss:0.0020 | CLSLoss:0.0207 | top1:90.5210 | AUROC:0.9640\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.038150\n",
      "Train | 10/10 | Loss:0.0466 | MainLoss:0.0361 | Alpha:0.4912 | SPLoss:0.0012 | CLSLoss:0.0202 | top1:98.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0662 | MainLoss:0.0662 | SPLoss:0.0025 | CLSLoss:0.0184 | top1:97.8069 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2830 | MainLoss:0.2830 | SPLoss:0.0025 | CLSLoss:0.0184 | top1:90.1147 | AUROC:0.9650\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.038123\n",
      "Train | 10/10 | Loss:0.0401 | MainLoss:0.0299 | Alpha:0.4922 | SPLoss:0.0008 | CLSLoss:0.0199 | top1:99.1000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0543 | MainLoss:0.0543 | SPLoss:0.0022 | CLSLoss:0.0199 | top1:98.2461 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.3076 | MainLoss:0.3076 | SPLoss:0.0022 | CLSLoss:0.0199 | top1:89.5184 | AUROC:0.9634\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.038097\n",
      "Train | 10/10 | Loss:0.0313 | MainLoss:0.0206 | Alpha:0.4945 | SPLoss:0.0008 | CLSLoss:0.0209 | top1:99.4250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0663 | MainLoss:0.0663 | SPLoss:0.0015 | CLSLoss:0.0220 | top1:97.9844 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.3094 | MainLoss:0.3094 | SPLoss:0.0015 | CLSLoss:0.0220 | top1:89.9115 | AUROC:0.9631\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.038070\n",
      "Train | 10/10 | Loss:0.0498 | MainLoss:0.0389 | Alpha:0.4908 | SPLoss:0.0012 | CLSLoss:0.0210 | top1:98.8750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0675 | MainLoss:0.0675 | SPLoss:0.0029 | CLSLoss:0.0185 | top1:97.8474 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2880 | MainLoss:0.2880 | SPLoss:0.0029 | CLSLoss:0.0185 | top1:89.8952 | AUROC:0.9628\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.038043\n",
      "Train | 10/10 | Loss:0.0301 | MainLoss:0.0200 | Alpha:0.4947 | SPLoss:0.0006 | CLSLoss:0.0198 | top1:99.5500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0826 | MainLoss:0.0826 | SPLoss:0.0012 | CLSLoss:0.0215 | top1:97.4548 | AUROC:0.9993\n",
      "Test | 77/10 | Loss:0.2982 | MainLoss:0.2982 | SPLoss:0.0012 | CLSLoss:0.0215 | top1:90.2883 | AUROC:0.9631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [155 | 1000] LR: 0.038015\n",
      "Train | 10/10 | Loss:0.0346 | MainLoss:0.0232 | Alpha:0.4947 | SPLoss:0.0016 | CLSLoss:0.0213 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0730 | MainLoss:0.0730 | SPLoss:0.0025 | CLSLoss:0.0217 | top1:97.7477 | AUROC:0.9993\n",
      "Test | 77/10 | Loss:0.3125 | MainLoss:0.3125 | SPLoss:0.0025 | CLSLoss:0.0217 | top1:90.0459 | AUROC:0.9625\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.037988\n",
      "Train | 10/10 | Loss:0.0448 | MainLoss:0.0343 | Alpha:0.4917 | SPLoss:0.0014 | CLSLoss:0.0201 | top1:98.9500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0967 | MainLoss:0.0967 | SPLoss:0.0027 | CLSLoss:0.0191 | top1:96.8474 | AUROC:0.9992\n",
      "Test | 77/10 | Loss:0.2786 | MainLoss:0.2786 | SPLoss:0.0027 | CLSLoss:0.0191 | top1:90.4751 | AUROC:0.9634\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.037961\n",
      "Train | 10/10 | Loss:0.0366 | MainLoss:0.0263 | Alpha:0.4930 | SPLoss:0.0010 | CLSLoss:0.0198 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0823 | MainLoss:0.0823 | SPLoss:0.0023 | CLSLoss:0.0194 | top1:97.2991 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2884 | MainLoss:0.2884 | SPLoss:0.0023 | CLSLoss:0.0194 | top1:90.0786 | AUROC:0.9619\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.037933\n",
      "Train | 10/10 | Loss:0.0401 | MainLoss:0.0301 | Alpha:0.4925 | SPLoss:0.0008 | CLSLoss:0.0196 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0739 | MainLoss:0.0739 | SPLoss:0.0020 | CLSLoss:0.0193 | top1:97.6137 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2917 | MainLoss:0.2917 | SPLoss:0.0020 | CLSLoss:0.0193 | top1:89.9017 | AUROC:0.9623\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.037905\n",
      "Train | 10/10 | Loss:0.0470 | MainLoss:0.0372 | Alpha:0.4912 | SPLoss:0.0011 | CLSLoss:0.0189 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0723 | MainLoss:0.0723 | SPLoss:0.0022 | CLSLoss:0.0188 | top1:97.6698 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2976 | MainLoss:0.2976 | SPLoss:0.0022 | CLSLoss:0.0188 | top1:89.8362 | AUROC:0.9615\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.037877\n",
      "Train | 10/10 | Loss:0.0378 | MainLoss:0.0279 | Alpha:0.4932 | SPLoss:0.0009 | CLSLoss:0.0193 | top1:99.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0811 | MainLoss:0.0811 | SPLoss:0.0020 | CLSLoss:0.0195 | top1:97.3863 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2866 | MainLoss:0.2866 | SPLoss:0.0020 | CLSLoss:0.0195 | top1:90.3244 | AUROC:0.9635\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.037849\n",
      "Train | 10/10 | Loss:0.0298 | MainLoss:0.0197 | Alpha:0.4951 | SPLoss:0.0007 | CLSLoss:0.0199 | top1:99.4750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0764 | MainLoss:0.0764 | SPLoss:0.0013 | CLSLoss:0.0206 | top1:97.5857 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2985 | MainLoss:0.2985 | SPLoss:0.0013 | CLSLoss:0.0206 | top1:90.3768 | AUROC:0.9637\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.037820\n",
      "Train | 10/10 | Loss:0.0331 | MainLoss:0.0227 | Alpha:0.4944 | SPLoss:0.0008 | CLSLoss:0.0203 | top1:99.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0845 | MainLoss:0.0845 | SPLoss:0.0018 | CLSLoss:0.0197 | top1:97.3146 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2865 | MainLoss:0.2865 | SPLoss:0.0018 | CLSLoss:0.0197 | top1:90.7471 | AUROC:0.9646\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.037792\n",
      "Train | 10/10 | Loss:0.0350 | MainLoss:0.0251 | Alpha:0.4940 | SPLoss:0.0011 | CLSLoss:0.0190 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0899 | MainLoss:0.0899 | SPLoss:0.0019 | CLSLoss:0.0196 | top1:97.1558 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2911 | MainLoss:0.2911 | SPLoss:0.0019 | CLSLoss:0.0196 | top1:90.5472 | AUROC:0.9637\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.037763\n",
      "Train | 10/10 | Loss:0.0479 | MainLoss:0.0383 | Alpha:0.4907 | SPLoss:0.0016 | CLSLoss:0.0181 | top1:98.7750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1221 | MainLoss:0.1221 | SPLoss:0.0031 | CLSLoss:0.0182 | top1:96.0374 | AUROC:0.9992\n",
      "Test | 77/10 | Loss:0.2642 | MainLoss:0.2642 | SPLoss:0.0031 | CLSLoss:0.0182 | top1:90.9404 | AUROC:0.9659\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.037734\n",
      "Train | 10/10 | Loss:0.0374 | MainLoss:0.0274 | Alpha:0.4922 | SPLoss:0.0014 | CLSLoss:0.0189 | top1:99.1000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0669 | MainLoss:0.0669 | SPLoss:0.0030 | CLSLoss:0.0192 | top1:97.7601 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.3055 | MainLoss:0.3055 | SPLoss:0.0030 | CLSLoss:0.0192 | top1:89.8591 | AUROC:0.9633\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.037705\n",
      "Train | 10/10 | Loss:0.0361 | MainLoss:0.0260 | Alpha:0.4933 | SPLoss:0.0013 | CLSLoss:0.0192 | top1:99.0750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0873 | MainLoss:0.0873 | SPLoss:0.0023 | CLSLoss:0.0197 | top1:97.1402 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2743 | MainLoss:0.2743 | SPLoss:0.0023 | CLSLoss:0.0197 | top1:90.8159 | AUROC:0.9665\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.037675\n",
      "Train | 10/10 | Loss:0.0384 | MainLoss:0.0281 | Alpha:0.4931 | SPLoss:0.0014 | CLSLoss:0.0193 | top1:99.0750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0998 | MainLoss:0.0998 | SPLoss:0.0025 | CLSLoss:0.0194 | top1:96.8100 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2737 | MainLoss:0.2737 | SPLoss:0.0025 | CLSLoss:0.0194 | top1:90.7765 | AUROC:0.9652\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.037646\n",
      "Train | 10/10 | Loss:0.0467 | MainLoss:0.0371 | Alpha:0.4902 | SPLoss:0.0013 | CLSLoss:0.0183 | top1:98.8500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0419 | MainLoss:0.0419 | SPLoss:0.0036 | CLSLoss:0.0179 | top1:98.6106 | AUROC:0.9997\n",
      "Test | 77/10 | Loss:0.3446 | MainLoss:0.3446 | SPLoss:0.0036 | CLSLoss:0.0179 | top1:88.6304 | AUROC:0.9631\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.037616\n",
      "Train | 10/10 | Loss:0.0395 | MainLoss:0.0301 | Alpha:0.4911 | SPLoss:0.0013 | CLSLoss:0.0179 | top1:99.0250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0608 | MainLoss:0.0608 | SPLoss:0.0024 | CLSLoss:0.0182 | top1:98.0499 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2898 | MainLoss:0.2898 | SPLoss:0.0024 | CLSLoss:0.0182 | top1:90.2261 | AUROC:0.9652\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.037586\n",
      "Train | 10/10 | Loss:0.0388 | MainLoss:0.0293 | Alpha:0.4927 | SPLoss:0.0012 | CLSLoss:0.0182 | top1:98.9750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0697 | MainLoss:0.0697 | SPLoss:0.0023 | CLSLoss:0.0183 | top1:97.7539 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2773 | MainLoss:0.2773 | SPLoss:0.0023 | CLSLoss:0.0183 | top1:90.3309 | AUROC:0.9653\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.037556\n",
      "Train | 10/10 | Loss:0.0384 | MainLoss:0.0292 | Alpha:0.4931 | SPLoss:0.0009 | CLSLoss:0.0179 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0687 | MainLoss:0.0687 | SPLoss:0.0016 | CLSLoss:0.0184 | top1:97.7632 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2822 | MainLoss:0.2822 | SPLoss:0.0016 | CLSLoss:0.0184 | top1:90.3080 | AUROC:0.9646\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.037526\n",
      "Train | 10/10 | Loss:0.0346 | MainLoss:0.0249 | Alpha:0.4937 | SPLoss:0.0011 | CLSLoss:0.0187 | top1:99.4000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0961 | MainLoss:0.0961 | SPLoss:0.0021 | CLSLoss:0.0191 | top1:96.8879 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2722 | MainLoss:0.2722 | SPLoss:0.0021 | CLSLoss:0.0191 | top1:90.8519 | AUROC:0.9656\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.037496\n",
      "Train | 10/10 | Loss:0.0395 | MainLoss:0.0299 | Alpha:0.4924 | SPLoss:0.0008 | CLSLoss:0.0187 | top1:99.1000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0813 | MainLoss:0.0813 | SPLoss:0.0020 | CLSLoss:0.0163 | top1:97.2960 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2658 | MainLoss:0.2658 | SPLoss:0.0020 | CLSLoss:0.0163 | top1:90.6684 | AUROC:0.9656\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.037465\n",
      "Train | 10/10 | Loss:0.0299 | MainLoss:0.0207 | Alpha:0.4946 | SPLoss:0.0008 | CLSLoss:0.0177 | top1:99.4500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0897 | MainLoss:0.0897 | SPLoss:0.0016 | CLSLoss:0.0189 | top1:97.2181 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2911 | MainLoss:0.2911 | SPLoss:0.0016 | CLSLoss:0.0189 | top1:90.5177 | AUROC:0.9640\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.037435\n",
      "Train | 10/10 | Loss:0.0437 | MainLoss:0.0344 | Alpha:0.4919 | SPLoss:0.0013 | CLSLoss:0.0175 | top1:98.9250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0803 | MainLoss:0.0803 | SPLoss:0.0027 | CLSLoss:0.0174 | top1:97.3988 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2790 | MainLoss:0.2790 | SPLoss:0.0027 | CLSLoss:0.0174 | top1:90.4849 | AUROC:0.9639\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.037404\n",
      "Train | 10/10 | Loss:0.0389 | MainLoss:0.0295 | Alpha:0.4928 | SPLoss:0.0016 | CLSLoss:0.0175 | top1:99.0750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1004 | MainLoss:0.1004 | SPLoss:0.0026 | CLSLoss:0.0183 | top1:96.6511 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2726 | MainLoss:0.2726 | SPLoss:0.0026 | CLSLoss:0.0183 | top1:90.6946 | AUROC:0.9650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [177 | 1000] LR: 0.037373\n",
      "Train | 10/10 | Loss:0.0376 | MainLoss:0.0283 | Alpha:0.4928 | SPLoss:0.0014 | CLSLoss:0.0176 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0661 | MainLoss:0.0661 | SPLoss:0.0021 | CLSLoss:0.0184 | top1:97.7446 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2992 | MainLoss:0.2992 | SPLoss:0.0021 | CLSLoss:0.0184 | top1:90.1376 | AUROC:0.9642\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.037341\n",
      "Train | 10/10 | Loss:0.0354 | MainLoss:0.0262 | Alpha:0.4937 | SPLoss:0.0009 | CLSLoss:0.0178 | top1:99.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1011 | MainLoss:0.1011 | SPLoss:0.0020 | CLSLoss:0.0180 | top1:96.7446 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2784 | MainLoss:0.2784 | SPLoss:0.0020 | CLSLoss:0.0180 | top1:90.5046 | AUROC:0.9643\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.037310\n",
      "Train | 10/10 | Loss:0.0290 | MainLoss:0.0195 | Alpha:0.4949 | SPLoss:0.0009 | CLSLoss:0.0182 | top1:99.4750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0974 | MainLoss:0.0974 | SPLoss:0.0015 | CLSLoss:0.0188 | top1:96.9284 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2858 | MainLoss:0.2858 | SPLoss:0.0015 | CLSLoss:0.0188 | top1:90.5537 | AUROC:0.9642\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.037278\n",
      "Train | 10/10 | Loss:0.0309 | MainLoss:0.0213 | Alpha:0.4946 | SPLoss:0.0010 | CLSLoss:0.0183 | top1:99.3750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0790 | MainLoss:0.0790 | SPLoss:0.0017 | CLSLoss:0.0180 | top1:97.4579 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2944 | MainLoss:0.2944 | SPLoss:0.0017 | CLSLoss:0.0180 | top1:90.4391 | AUROC:0.9637\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.037247\n",
      "Train | 10/10 | Loss:0.0363 | MainLoss:0.0271 | Alpha:0.4933 | SPLoss:0.0010 | CLSLoss:0.0177 | top1:99.2500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0813 | MainLoss:0.0813 | SPLoss:0.0021 | CLSLoss:0.0170 | top1:97.3894 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2820 | MainLoss:0.2820 | SPLoss:0.0021 | CLSLoss:0.0170 | top1:90.5406 | AUROC:0.9643\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.037215\n",
      "Train | 10/10 | Loss:0.0304 | MainLoss:0.0213 | Alpha:0.4947 | SPLoss:0.0008 | CLSLoss:0.0175 | top1:99.3250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0592 | MainLoss:0.0592 | SPLoss:0.0019 | CLSLoss:0.0174 | top1:98.0717 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.3065 | MainLoss:0.3065 | SPLoss:0.0019 | CLSLoss:0.0174 | top1:90.0131 | AUROC:0.9637\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.037183\n",
      "Train | 10/10 | Loss:0.0318 | MainLoss:0.0226 | Alpha:0.4941 | SPLoss:0.0011 | CLSLoss:0.0175 | top1:99.3500 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.1021 | MainLoss:0.1021 | SPLoss:0.0025 | CLSLoss:0.0175 | top1:96.6293 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2765 | MainLoss:0.2765 | SPLoss:0.0025 | CLSLoss:0.0175 | top1:90.7929 | AUROC:0.9652\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.037151\n",
      "Train | 10/10 | Loss:0.0300 | MainLoss:0.0208 | Alpha:0.4947 | SPLoss:0.0009 | CLSLoss:0.0178 | top1:99.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0845 | MainLoss:0.0845 | SPLoss:0.0018 | CLSLoss:0.0178 | top1:97.2866 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2903 | MainLoss:0.2903 | SPLoss:0.0018 | CLSLoss:0.0178 | top1:90.5079 | AUROC:0.9641\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.037118\n",
      "Train | 10/10 | Loss:0.0325 | MainLoss:0.0232 | Alpha:0.4943 | SPLoss:0.0013 | CLSLoss:0.0175 | top1:99.1000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0829 | MainLoss:0.0829 | SPLoss:0.0023 | CLSLoss:0.0173 | top1:97.3271 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2865 | MainLoss:0.2865 | SPLoss:0.0023 | CLSLoss:0.0173 | top1:90.4423 | AUROC:0.9638\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.037086\n",
      "Train | 10/10 | Loss:0.0249 | MainLoss:0.0157 | Alpha:0.4961 | SPLoss:0.0007 | CLSLoss:0.0178 | top1:99.5250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0869 | MainLoss:0.0869 | SPLoss:0.0014 | CLSLoss:0.0182 | top1:97.2181 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2984 | MainLoss:0.2984 | SPLoss:0.0014 | CLSLoss:0.0182 | top1:90.4784 | AUROC:0.9636\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.037053\n",
      "Train | 10/10 | Loss:0.0339 | MainLoss:0.0245 | Alpha:0.4942 | SPLoss:0.0012 | CLSLoss:0.0178 | top1:99.2750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0909 | MainLoss:0.0909 | SPLoss:0.0026 | CLSLoss:0.0167 | top1:96.8754 | AUROC:0.9994\n",
      "Test | 77/10 | Loss:0.2707 | MainLoss:0.2707 | SPLoss:0.0026 | CLSLoss:0.0167 | top1:90.8224 | AUROC:0.9657\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.037020\n",
      "Train | 10/10 | Loss:0.0343 | MainLoss:0.0257 | Alpha:0.4936 | SPLoss:0.0011 | CLSLoss:0.0165 | top1:99.3000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0693 | MainLoss:0.0693 | SPLoss:0.0018 | CLSLoss:0.0173 | top1:97.6636 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2926 | MainLoss:0.2926 | SPLoss:0.0018 | CLSLoss:0.0173 | top1:90.3670 | AUROC:0.9646\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.036987\n",
      "Train | 10/10 | Loss:0.0313 | MainLoss:0.0224 | Alpha:0.4948 | SPLoss:0.0007 | CLSLoss:0.0173 | top1:99.2250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0630 | MainLoss:0.0630 | SPLoss:0.0017 | CLSLoss:0.0165 | top1:97.9097 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2983 | MainLoss:0.2983 | SPLoss:0.0017 | CLSLoss:0.0165 | top1:90.2294 | AUROC:0.9634\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.036954\n",
      "Train | 10/10 | Loss:0.0362 | MainLoss:0.0274 | Alpha:0.4938 | SPLoss:0.0014 | CLSLoss:0.0163 | top1:99.1250 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0641 | MainLoss:0.0641 | SPLoss:0.0024 | CLSLoss:0.0165 | top1:97.8910 | AUROC:0.9995\n",
      "Test | 77/10 | Loss:0.2967 | MainLoss:0.2967 | SPLoss:0.0024 | CLSLoss:0.0165 | top1:90.0557 | AUROC:0.9624\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.036920\n",
      "Train | 10/10 | Loss:0.0337 | MainLoss:0.0252 | Alpha:0.4939 | SPLoss:0.0006 | CLSLoss:0.0165 | top1:99.4000 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0954 | MainLoss:0.0954 | SPLoss:0.0020 | CLSLoss:0.0157 | top1:96.7913 | AUROC:0.9993\n",
      "Test | 77/10 | Loss:0.2697 | MainLoss:0.2697 | SPLoss:0.0020 | CLSLoss:0.0157 | top1:90.6455 | AUROC:0.9648\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.036887\n",
      "Train | 10/10 | Loss:0.0332 | MainLoss:0.0246 | Alpha:0.4932 | SPLoss:0.0009 | CLSLoss:0.0164 | top1:99.1750 | AUROC:0.0000\n",
      "Test | 81/10 | Loss:0.0762 | MainLoss:0.0762 | SPLoss:0.0024 | CLSLoss:0.0160 | top1:97.4891 | AUROC:0.9996\n",
      "Test | 77/10 | Loss:0.2843 | MainLoss:0.2843 | SPLoss:0.0024 | CLSLoss:0.0160 | top1:90.2752 | AUROC:0.9633\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.036853\n",
      "Train | 10/10 | Loss:0.0332 | MainLoss:0.0249 | Alpha:0.4940 | SPLoss:0.0008 | CLSLoss:0.0161 | top1:99.3500 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
