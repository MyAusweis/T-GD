{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b0/siamese/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 128\n",
    "epochs = 4000\n",
    "start_epoch = 0\n",
    "train_batch = 32\n",
    "test_batch = 64\n",
    "lr = 0.01\n",
    "schedule = [500, 1000, 2000, 3000]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0/to_pggan/siamese' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'\n",
    "\n",
    "# iterative training\n",
    "feedback = 0\n",
    "iter_time = []\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "#         img0 = img0.convert(\"L\")\n",
    "#         img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(target_dir, '100_shot_style1')\n",
    "source_train_dir = os.path.join(target_dir, '100_shot_style1_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_ = SiameseNetworkDataset(datasets.ImageFolder(train_dir), transform=train_aug, should_invert=False)\n",
    "train_loader = DataLoader(train_, shuffle=True, num_workers=num_workers, batch_size=train_batch, pin_memory=True)\n",
    "source_train_loader = DataLoader(datasets.ImageFolder(source_train_dir, transform=val_aug),\n",
    "                                batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_ = SiameseNetworkDataset(datasets.ImageFolder(val_target_dir), transform=val_aug, should_invert=False)\n",
    "val_target_loader = DataLoader(val_target_, shuffle=True, num_workers=num_workers, batch_size=test_batch, pin_memory=True)\n",
    "val_source_ = SiameseNetworkDataset(datasets.ImageFolder(val_source_dir), transform=val_aug, should_invert=False)\n",
    "val_source_loader = DataLoader(val_source_, batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b0/siamese/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                                      override_params={'dropout_rate':0.0})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                                      override_params={'dropout_rate':0.0, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.17M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in student_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=128, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += torch.norm(param, p=1)\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += torch.norm(param - teacher_model_weights[name], p=1)\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=2.0).cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=50, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, source_train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    student_model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs0, inputs1, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs0.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "        \n",
    "        outputs0 = student_model(inputs0)\n",
    "        outputs1 = student_model(inputs1)\n",
    "        \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            target_index = targets[targets==0]\n",
    "            target_index = target_index.long().cuda()\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs0.size(), lam)\n",
    "            inputs0[target_index, :, bbx1:bbx2, bby1:bby2] = inputs1[target_index, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "        outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "        \n",
    "        loss_main = criterion(outputs0, outputs1, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "            \n",
    "            \n",
    "        # compute output\n",
    "        loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        auroc = roc_auc_score(targets.data.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "        losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "        top1.update(prec1[0], inputs0.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "#         if batch_idx % 10 == 0:\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{total:.4f} | SPLoss:{eta:.4f} | | CLLoss:{sp:.4f} | top1:{tp1:.4f} | AUROC:{arc:.4f}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader),loss=loss, total=loss_main, eta=loss_sp, sp=loss_cls, tp1=top1.avg, arc=auroc))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs0, inputs1, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs0 = model(inputs0)\n",
    "            outputs1 = model(inputs1)\n",
    "            outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "\n",
    "            loss_main = criterion(outputs0, outputs1, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + sp_beta*loss_cls + sp_alpha*loss_sp\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.data.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "\n",
    "            losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "            top1.update(prec1[0], inputs0.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(val_loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,)\n",
    "            bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:.4f} | AUROC:{arc:.4f}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg, arc=auroc))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 4000] LR: 0.010000\n",
      "13/13 | Loss:329.6647 | MainLoss:1.9282 | SPLoss:2312.3291 | | CLLoss:965.0356 | top1:44.7917 | AUROC:0.4375\n",
      "502/502 Data:0.009 | Batch:0.562 | Total:0:01:47 | ETA:0:00:00 | Loss:250.62532677005757 | top1:50.0903 | AUROC:0.4644\n",
      "122/122 Data:0.000 | Batch:1.019 | Total:0:00:29 | ETA:0:00:00 | Loss:250.27906689766127 | top1:49.2564 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 4000] LR: 0.010600\n",
      "13/13 | Loss:252.3488 | MainLoss:2.0670 | SPLoss:2036.2588 | | CLLoss:466.5592 | top1:49.4792 | AUROC:0.7333\n",
      "\n",
      "Epoch: [3 | 4000] LR: 0.011200\n",
      "13/13 | Loss:244.5533 | MainLoss:2.8487 | SPLoss:2102.8794 | | CLLoss:314.1666 | top1:45.8333 | AUROC:0.5700\n",
      "\n",
      "Epoch: [4 | 4000] LR: 0.011800\n",
      "13/13 | Loss:255.7706 | MainLoss:2.1118 | SPLoss:2270.3555 | | CLLoss:266.2321 | top1:47.6562 | AUROC:0.4078\n",
      "\n",
      "Epoch: [5 | 4000] LR: 0.012400\n",
      "13/13 | Loss:272.7913 | MainLoss:1.7321 | SPLoss:2463.5464 | | CLLoss:247.0456 | top1:50.7812 | AUROC:0.6310\n",
      "\n",
      "Epoch: [6 | 4000] LR: 0.013000\n",
      "13/13 | Loss:294.2797 | MainLoss:1.6100 | SPLoss:2663.4075 | | CLLoss:263.2891 | top1:54.6875 | AUROC:0.6599\n",
      "\n",
      "Epoch: [7 | 4000] LR: 0.013600\n",
      "13/13 | Loss:315.1636 | MainLoss:1.9814 | SPLoss:2875.7048 | | CLLoss:256.1176 | top1:53.6458 | AUROC:0.8047\n",
      "\n",
      "Epoch: [8 | 4000] LR: 0.014200\n",
      "13/13 | Loss:337.5655 | MainLoss:1.8553 | SPLoss:3077.8665 | | CLLoss:279.2356 | top1:52.6042 | AUROC:0.5922\n",
      "\n",
      "Epoch: [9 | 4000] LR: 0.014800\n",
      "13/13 | Loss:358.8606 | MainLoss:1.7308 | SPLoss:3295.1589 | | CLLoss:276.1386 | top1:48.6979 | AUROC:0.6349\n",
      "\n",
      "Epoch: [10 | 4000] LR: 0.015400\n",
      "13/13 | Loss:381.7644 | MainLoss:2.3526 | SPLoss:3503.5444 | | CLLoss:290.5734 | top1:50.0000 | AUROC:0.7490\n",
      "\n",
      "Epoch: [11 | 4000] LR: 0.016000\n",
      "13/13 | Loss:402.1715 | MainLoss:1.2412 | SPLoss:3708.1372 | | CLLoss:301.1663 | top1:47.9167 | AUROC:0.6136\n",
      "\n",
      "Epoch: [12 | 4000] LR: 0.016600\n",
      "13/13 | Loss:424.1613 | MainLoss:2.5952 | SPLoss:3910.6819 | | CLLoss:304.9795 | top1:46.0938 | AUROC:0.8355\n",
      "\n",
      "Epoch: [13 | 4000] LR: 0.017200\n",
      "13/13 | Loss:445.1939 | MainLoss:1.4867 | SPLoss:4115.6187 | | CLLoss:321.4535 | top1:52.0833 | AUROC:0.7167\n",
      "\n",
      "Epoch: [14 | 4000] LR: 0.017800\n",
      "13/13 | Loss:466.9867 | MainLoss:1.9781 | SPLoss:4327.1094 | | CLLoss:322.9764 | top1:48.9583 | AUROC:0.7617\n",
      "\n",
      "Epoch: [15 | 4000] LR: 0.018400\n",
      "13/13 | Loss:488.4405 | MainLoss:1.6089 | SPLoss:4531.9292 | | CLLoss:336.3867 | top1:50.0000 | AUROC:0.7166\n",
      "\n",
      "Epoch: [16 | 4000] LR: 0.019000\n",
      "13/13 | Loss:510.7004 | MainLoss:1.9807 | SPLoss:4737.6846 | | CLLoss:349.5120 | top1:52.0833 | AUROC:0.5117\n",
      "\n",
      "Epoch: [17 | 4000] LR: 0.019600\n",
      "13/13 | Loss:531.1045 | MainLoss:1.9839 | SPLoss:4936.7471 | | CLLoss:354.4585 | top1:46.6146 | AUROC:0.4453\n",
      "\n",
      "Epoch: [18 | 4000] LR: 0.020200\n",
      "13/13 | Loss:551.6438 | MainLoss:1.3586 | SPLoss:5133.8379 | | CLLoss:369.0144 | top1:53.9062 | AUROC:0.7706\n",
      "\n",
      "Epoch: [19 | 4000] LR: 0.020800\n",
      "13/13 | Loss:573.0357 | MainLoss:1.8518 | SPLoss:5334.3491 | | CLLoss:377.4894 | top1:49.4792 | AUROC:0.6902\n",
      "\n",
      "Epoch: [20 | 4000] LR: 0.021400\n",
      "13/13 | Loss:593.8164 | MainLoss:2.1029 | SPLoss:5529.2490 | | CLLoss:387.8859 | top1:46.0938 | AUROC:0.5020\n",
      "\n",
      "Epoch: [21 | 4000] LR: 0.022000\n",
      "13/13 | Loss:614.1207 | MainLoss:1.7299 | SPLoss:5724.2866 | | CLLoss:399.6212 | top1:52.6042 | AUROC:0.5556\n",
      "\n",
      "Epoch: [22 | 4000] LR: 0.022600\n",
      "13/13 | Loss:635.1165 | MainLoss:1.9808 | SPLoss:5924.8140 | | CLLoss:406.5425 | top1:49.4792 | AUROC:0.5117\n",
      "\n",
      "Epoch: [23 | 4000] LR: 0.023200\n",
      "13/13 | Loss:655.1534 | MainLoss:2.0962 | SPLoss:6109.9097 | | CLLoss:420.6625 | top1:50.5208 | AUROC:0.7412\n",
      "\n",
      "Epoch: [24 | 4000] LR: 0.023800\n",
      "13/13 | Loss:675.9668 | MainLoss:2.3526 | SPLoss:6309.5723 | | CLLoss:426.5707 | top1:48.1771 | AUROC:0.5789\n",
      "\n",
      "Epoch: [25 | 4000] LR: 0.024400\n",
      "13/13 | Loss:695.7102 | MainLoss:1.8512 | SPLoss:6496.9868 | | CLLoss:441.6028 | top1:47.1354 | AUROC:0.7137\n",
      "\n",
      "Epoch: [26 | 4000] LR: 0.025000\n",
      "13/13 | Loss:715.8253 | MainLoss:1.8486 | SPLoss:6682.9272 | | CLLoss:456.8389 | top1:50.5208 | AUROC:0.6863\n",
      "\n",
      "Epoch: [27 | 4000] LR: 0.025600\n",
      "13/13 | Loss:734.7372 | MainLoss:1.9726 | SPLoss:6869.0361 | | CLLoss:458.6094 | top1:48.6979 | AUROC:0.6445\n",
      "\n",
      "Epoch: [28 | 4000] LR: 0.026200\n",
      "13/13 | Loss:755.0186 | MainLoss:1.9768 | SPLoss:7053.7700 | | CLLoss:476.6487 | top1:47.3958 | AUROC:0.5547\n",
      "\n",
      "Epoch: [29 | 4000] LR: 0.026800\n",
      "13/13 | Loss:773.8335 | MainLoss:1.6085 | SPLoss:7240.4805 | | CLLoss:481.7693 | top1:56.2500 | AUROC:0.4980\n",
      "\n",
      "Epoch: [30 | 4000] LR: 0.027400\n",
      "13/13 | Loss:793.5884 | MainLoss:2.2194 | SPLoss:7424.8120 | | CLLoss:488.8781 | top1:51.8229 | AUROC:0.6111\n",
      "\n",
      "Epoch: [31 | 4000] LR: 0.028000\n",
      "13/13 | Loss:812.9791 | MainLoss:1.7175 | SPLoss:7605.6802 | | CLLoss:506.9357 | top1:51.0417 | AUROC:0.6786\n",
      "\n",
      "Epoch: [32 | 4000] LR: 0.028600\n",
      "13/13 | Loss:831.9311 | MainLoss:2.0984 | SPLoss:7787.6587 | | CLLoss:510.6676 | top1:50.7812 | AUROC:0.5882\n",
      "\n",
      "Epoch: [33 | 4000] LR: 0.029200\n",
      "13/13 | Loss:851.2828 | MainLoss:1.2316 | SPLoss:7973.7349 | | CLLoss:526.7775 | top1:51.5625 | AUROC:0.6455\n",
      "\n",
      "Epoch: [34 | 4000] LR: 0.029800\n",
      "13/13 | Loss:870.6977 | MainLoss:2.0987 | SPLoss:8151.0898 | | CLLoss:534.9001 | top1:45.0521 | AUROC:0.4078\n",
      "\n",
      "Epoch: [35 | 4000] LR: 0.030400\n",
      "13/13 | Loss:888.6185 | MainLoss:1.9715 | SPLoss:8325.6855 | | CLLoss:540.7850 | top1:54.9479 | AUROC:0.6992\n",
      "\n",
      "Epoch: [36 | 4000] LR: 0.031000\n",
      "13/13 | Loss:907.1985 | MainLoss:1.4793 | SPLoss:8504.4883 | | CLLoss:552.7031 | top1:55.7292 | AUROC:0.6417\n",
      "\n",
      "Epoch: [37 | 4000] LR: 0.031600\n",
      "13/13 | Loss:925.7764 | MainLoss:1.9633 | SPLoss:8678.1562 | | CLLoss:559.9745 | top1:48.6979 | AUROC:0.6094\n",
      "\n",
      "Epoch: [38 | 4000] LR: 0.032200\n",
      "13/13 | Loss:944.7350 | MainLoss:2.0907 | SPLoss:8846.8662 | | CLLoss:579.5759 | top1:46.8750 | AUROC:0.5922\n",
      "\n",
      "Epoch: [39 | 4000] LR: 0.032800\n",
      "13/13 | Loss:962.6584 | MainLoss:1.8506 | SPLoss:9020.9346 | | CLLoss:587.1437 | top1:48.1771 | AUROC:0.6039\n",
      "\n",
      "Epoch: [40 | 4000] LR: 0.033400\n",
      "13/13 | Loss:979.8768 | MainLoss:1.5922 | SPLoss:9187.5654 | | CLLoss:595.2811 | top1:51.0417 | AUROC:0.7773\n",
      "\n",
      "Epoch: [41 | 4000] LR: 0.034000\n",
      "13/13 | Loss:997.6283 | MainLoss:1.9684 | SPLoss:9349.0488 | | CLLoss:607.5499 | top1:51.0417 | AUROC:0.5547\n",
      "\n",
      "Epoch: [42 | 4000] LR: 0.034600\n",
      "13/13 | Loss:1015.8324 | MainLoss:2.7078 | SPLoss:9514.4316 | | CLLoss:616.8148 | top1:48.6979 | AUROC:0.5000\n",
      "\n",
      "Epoch: [43 | 4000] LR: 0.035200\n",
      "13/13 | Loss:1032.5143 | MainLoss:1.8425 | SPLoss:9674.8379 | | CLLoss:631.8793 | top1:52.3438 | AUROC:0.6039\n",
      "\n",
      "Epoch: [44 | 4000] LR: 0.035800\n",
      "13/13 | Loss:1048.8717 | MainLoss:2.0898 | SPLoss:9836.5947 | | CLLoss:631.2247 | top1:50.5208 | AUROC:0.5451\n",
      "\n",
      "Epoch: [45 | 4000] LR: 0.036400\n",
      "13/13 | Loss:1066.2833 | MainLoss:1.8298 | SPLoss:9987.2568 | | CLLoss:657.2787 | top1:51.8229 | AUROC:0.7608\n",
      "\n",
      "Epoch: [46 | 4000] LR: 0.037000\n",
      "13/13 | Loss:1082.5194 | MainLoss:2.2114 | SPLoss:10144.1934 | | CLLoss:658.8859 | top1:50.0000 | AUROC:0.5714\n",
      "\n",
      "Epoch: [47 | 4000] LR: 0.037600\n",
      "13/13 | Loss:1099.2932 | MainLoss:2.5656 | SPLoss:10291.4590 | | CLLoss:675.8162 | top1:46.3542 | AUROC:0.6364\n",
      "\n",
      "Epoch: [48 | 4000] LR: 0.038200\n",
      "13/13 | Loss:1115.6545 | MainLoss:2.3348 | SPLoss:10444.4863 | | CLLoss:688.7114 | top1:48.1771 | AUROC:0.6154\n",
      "\n",
      "Epoch: [49 | 4000] LR: 0.038800\n",
      "13/13 | Loss:1130.7656 | MainLoss:1.7106 | SPLoss:10596.2891 | | CLLoss:694.2619 | top1:49.4792 | AUROC:0.6548\n",
      "\n",
      "Epoch: [50 | 4000] LR: 0.039400\n",
      "13/13 | Loss:1146.0977 | MainLoss:1.7138 | SPLoss:10737.9443 | | CLLoss:705.8948 | top1:51.0417 | AUROC:0.5675\n",
      "\n",
      "Epoch: [51 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1162.6572 | MainLoss:1.8398 | SPLoss:10887.5801 | | CLLoss:720.5947 | top1:47.6562 | AUROC:0.5608\n",
      "502/502 Data:0.001 | Batch:0.117 | Total:0:01:18 | ETA:0:00:00 | Loss:594.6807308399046 | top1:50.2368 | AUROC:0.5460\n",
      "122/122 Data:0.000 | Batch:0.284 | Total:0:00:25 | ETA:0:00:00 | Loss:594.6163762645232 | top1:49.2051 | AUROC:0.9850\n",
      "\n",
      "Epoch: [52 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1158.3838 | MainLoss:2.3319 | SPLoss:10839.2129 | | CLLoss:721.3057 | top1:47.6562 | AUROC:0.4777\n",
      "\n",
      "Epoch: [53 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1160.7524 | MainLoss:2.0777 | SPLoss:10862.0791 | | CLLoss:724.6679 | top1:50.5208 | AUROC:0.8078\n",
      "\n",
      "Epoch: [54 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1164.1798 | MainLoss:1.5897 | SPLoss:10905.0400 | | CLLoss:720.8605 | top1:48.9583 | AUROC:0.6316\n",
      "\n",
      "Epoch: [55 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1163.6877 | MainLoss:1.2200 | SPLoss:10905.0967 | | CLLoss:719.5809 | top1:57.2917 | AUROC:0.6864\n",
      "\n",
      "Epoch: [56 | 4000] LR: 0.040000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 | Loss:1163.6472 | MainLoss:2.0819 | SPLoss:10898.7998 | | CLLoss:716.8532 | top1:47.9167 | AUROC:0.6471\n",
      "\n",
      "Epoch: [57 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1159.4476 | MainLoss:1.4662 | SPLoss:10864.7490 | | CLLoss:715.0649 | top1:51.5625 | AUROC:0.5833\n",
      "\n",
      "Epoch: [58 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1158.2527 | MainLoss:2.4470 | SPLoss:10843.5654 | | CLLoss:714.4906 | top1:49.2188 | AUROC:0.6792\n",
      "\n",
      "Epoch: [59 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1157.5026 | MainLoss:1.5943 | SPLoss:10837.9609 | | CLLoss:721.1217 | top1:52.6042 | AUROC:0.4939\n",
      "\n",
      "Epoch: [60 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1159.0044 | MainLoss:2.0912 | SPLoss:10853.1709 | | CLLoss:715.9604 | top1:47.6562 | AUROC:0.4745\n",
      "\n",
      "Epoch: [61 | 4000] LR: 0.040000\n",
      "13/13 | Loss:1158.2606 | MainLoss:1.8405 | SPLoss:10847.3330 | | CLLoss:716.8687 | top1:52.0833 | AUROC:0.7020\n",
      "\n",
      "Epoch: [62 | 4000] LR: 0.039999\n",
      "13/13 | Loss:1155.8555 | MainLoss:1.8369 | SPLoss:10829.0186 | | CLLoss:711.1671 | top1:48.4375 | AUROC:0.6863\n",
      "\n",
      "Epoch: [63 | 4000] LR: 0.039999\n",
      "13/13 | Loss:1154.8032 | MainLoss:1.9602 | SPLoss:10811.2441 | | CLLoss:717.1869 | top1:50.0000 | AUROC:0.5586\n",
      "\n",
      "Epoch: [64 | 4000] LR: 0.039999\n",
      "13/13 | Loss:1155.4684 | MainLoss:1.9753 | SPLoss:10820.2285 | | CLLoss:714.7026 | top1:47.9167 | AUROC:0.5469\n",
      "\n",
      "Epoch: [65 | 4000] LR: 0.039999\n",
      "13/13 | Loss:1157.1647 | MainLoss:1.9558 | SPLoss:10834.0850 | | CLLoss:718.0034 | top1:47.6562 | AUROC:0.5078\n",
      "\n",
      "Epoch: [66 | 4000] LR: 0.039999\n",
      "13/13 | Loss:1156.0837 | MainLoss:2.0738 | SPLoss:10819.7607 | | CLLoss:720.3388 | top1:49.7396 | AUROC:0.6275\n",
      "\n",
      "Epoch: [67 | 4000] LR: 0.039999\n",
      "13/13 | Loss:1153.8976 | MainLoss:2.0655 | SPLoss:10801.5605 | | CLLoss:716.7588 | top1:50.0000 | AUROC:0.7176\n",
      "\n",
      "Epoch: [68 | 4000] LR: 0.039998\n",
      "13/13 | Loss:1153.7555 | MainLoss:2.1969 | SPLoss:10796.9551 | | CLLoss:718.6307 | top1:46.6146 | AUROC:0.7778\n",
      "\n",
      "Epoch: [69 | 4000] LR: 0.039998\n",
      "13/13 | Loss:1153.9330 | MainLoss:2.0607 | SPLoss:10802.3643 | | CLLoss:716.3589 | top1:54.9479 | AUROC:0.6902\n",
      "\n",
      "Epoch: [70 | 4000] LR: 0.039998\n",
      "13/13 | Loss:1154.3887 | MainLoss:1.9604 | SPLoss:10803.1650 | | CLLoss:721.1176 | top1:52.8646 | AUROC:0.5039\n",
      "\n",
      "Epoch: [71 | 4000] LR: 0.039998\n",
      "13/13 | Loss:1154.8729 | MainLoss:2.3250 | SPLoss:10807.5605 | | CLLoss:717.9184 | top1:43.7500 | AUROC:0.6437\n",
      "\n",
      "Epoch: [72 | 4000] LR: 0.039998\n",
      "13/13 | Loss:1153.7482 | MainLoss:2.0876 | SPLoss:10797.9355 | | CLLoss:718.6697 | top1:48.9583 | AUROC:0.4667\n",
      "\n",
      "Epoch: [73 | 4000] LR: 0.039997\n",
      "13/13 | Loss:1152.6200 | MainLoss:1.7007 | SPLoss:10795.1699 | | CLLoss:714.0237 | top1:51.5625 | AUROC:0.7024\n",
      "\n",
      "Epoch: [74 | 4000] LR: 0.039997\n",
      "13/13 | Loss:1151.3695 | MainLoss:1.3352 | SPLoss:10784.9912 | | CLLoss:715.3516 | top1:52.8646 | AUROC:0.7792\n",
      "\n",
      "Epoch: [75 | 4000] LR: 0.039997\n",
      "13/13 | Loss:1151.7925 | MainLoss:1.9570 | SPLoss:10777.2100 | | CLLoss:721.1439 | top1:49.2188 | AUROC:0.4883\n",
      "\n",
      "Epoch: [76 | 4000] LR: 0.039996\n",
      "13/13 | Loss:1150.5018 | MainLoss:1.4541 | SPLoss:10770.9453 | | CLLoss:719.5316 | top1:50.0000 | AUROC:0.6708\n",
      "\n",
      "Epoch: [77 | 4000] LR: 0.039996\n",
      "13/13 | Loss:1151.1256 | MainLoss:2.0840 | SPLoss:10777.2480 | | CLLoss:713.1666 | top1:53.9062 | AUROC:0.5882\n",
      "\n",
      "Epoch: [78 | 4000] LR: 0.039996\n",
      "13/13 | Loss:1150.6188 | MainLoss:2.3214 | SPLoss:10774.6445 | | CLLoss:708.3293 | top1:52.8646 | AUROC:0.5789\n",
      "\n",
      "Epoch: [79 | 4000] LR: 0.039996\n",
      "13/13 | Loss:1149.8092 | MainLoss:2.3322 | SPLoss:10755.4785 | | CLLoss:719.2913 | top1:49.4792 | AUROC:0.4737\n",
      "\n",
      "Epoch: [80 | 4000] LR: 0.039995\n",
      "13/13 | Loss:1149.7021 | MainLoss:2.2018 | SPLoss:10763.5127 | | CLLoss:711.4899 | top1:49.4792 | AUROC:0.5794\n",
      "\n",
      "Epoch: [81 | 4000] LR: 0.039995\n",
      "13/13 | Loss:1150.5812 | MainLoss:1.9579 | SPLoss:10771.0420 | | CLLoss:715.1899 | top1:54.1667 | AUROC:0.6094\n",
      "\n",
      "Epoch: [82 | 4000] LR: 0.039994\n",
      "13/13 | Loss:1149.7039 | MainLoss:1.4649 | SPLoss:10775.3428 | | CLLoss:707.0453 | top1:51.8229 | AUROC:0.5667\n",
      "\n",
      "Epoch: [83 | 4000] LR: 0.039994\n",
      "13/13 | Loss:1151.2031 | MainLoss:1.4643 | SPLoss:10782.3906 | | CLLoss:714.9976 | top1:49.4792 | AUROC:0.5292\n",
      "\n",
      "Epoch: [84 | 4000] LR: 0.039994\n",
      "13/13 | Loss:1151.8302 | MainLoss:1.9677 | SPLoss:10784.3154 | | CLLoss:714.3085 | top1:52.0833 | AUROC:0.5156\n",
      "\n",
      "Epoch: [85 | 4000] LR: 0.039993\n",
      "13/13 | Loss:1152.4293 | MainLoss:1.9415 | SPLoss:10793.1807 | | CLLoss:711.6973 | top1:52.3438 | AUROC:0.8594\n",
      "\n",
      "Epoch: [86 | 4000] LR: 0.039993\n",
      "13/13 | Loss:1153.3387 | MainLoss:2.1853 | SPLoss:10798.5107 | | CLLoss:713.0236 | top1:45.8333 | AUROC:0.7579\n",
      "\n",
      "Epoch: [87 | 4000] LR: 0.039992\n",
      "13/13 | Loss:1152.6643 | MainLoss:1.7080 | SPLoss:10791.3848 | | CLLoss:718.1781 | top1:48.1771 | AUROC:0.5675\n",
      "\n",
      "Epoch: [88 | 4000] LR: 0.039992\n",
      "13/13 | Loss:1151.9696 | MainLoss:1.7160 | SPLoss:10790.0645 | | CLLoss:712.4708 | top1:51.3021 | AUROC:0.5000\n",
      "\n",
      "Epoch: [89 | 4000] LR: 0.039992\n",
      "13/13 | Loss:1153.1073 | MainLoss:2.3322 | SPLoss:10794.3350 | | CLLoss:713.4153 | top1:49.2188 | AUROC:0.4858\n",
      "\n",
      "Epoch: [90 | 4000] LR: 0.039991\n",
      "13/13 | Loss:1153.4642 | MainLoss:2.3252 | SPLoss:10796.9824 | | CLLoss:714.4084 | top1:49.4792 | AUROC:0.6275\n",
      "\n",
      "Epoch: [91 | 4000] LR: 0.039991\n",
      "13/13 | Loss:1151.8591 | MainLoss:1.7035 | SPLoss:10787.2812 | | CLLoss:714.2745 | top1:52.3438 | AUROC:0.7143\n",
      "\n",
      "Epoch: [92 | 4000] LR: 0.039990\n",
      "13/13 | Loss:1152.5492 | MainLoss:2.0702 | SPLoss:10794.4141 | | CLLoss:710.3757 | top1:52.6042 | AUROC:0.6980\n",
      "\n",
      "Epoch: [93 | 4000] LR: 0.039990\n",
      "13/13 | Loss:1153.2728 | MainLoss:2.0753 | SPLoss:10800.8594 | | CLLoss:711.1152 | top1:53.1250 | AUROC:0.5922\n",
      "\n",
      "Epoch: [94 | 4000] LR: 0.039989\n",
      "13/13 | Loss:1151.9636 | MainLoss:2.0749 | SPLoss:10792.2598 | | CLLoss:706.6276 | top1:51.8229 | AUROC:0.5725\n",
      "\n",
      "Epoch: [95 | 4000] LR: 0.039989\n",
      "13/13 | Loss:1152.2777 | MainLoss:1.7181 | SPLoss:10793.6494 | | CLLoss:711.9467 | top1:55.2083 | AUROC:0.8175\n",
      "\n",
      "Epoch: [96 | 4000] LR: 0.039988\n",
      "13/13 | Loss:1154.5687 | MainLoss:2.0793 | SPLoss:10810.3574 | | CLLoss:714.5357 | top1:51.0417 | AUROC:0.6980\n",
      "\n",
      "Epoch: [97 | 4000] LR: 0.039988\n",
      "13/13 | Loss:1152.8868 | MainLoss:1.9661 | SPLoss:10799.3271 | | CLLoss:709.8798 | top1:51.3021 | AUROC:0.5391\n",
      "\n",
      "Epoch: [98 | 4000] LR: 0.039987\n",
      "13/13 | Loss:1153.2753 | MainLoss:2.3250 | SPLoss:10798.3008 | | CLLoss:711.2020 | top1:49.2188 | AUROC:0.8300\n",
      "\n",
      "Epoch: [99 | 4000] LR: 0.039986\n",
      "13/13 | Loss:1154.6091 | MainLoss:2.1786 | SPLoss:10813.9922 | | CLLoss:710.3121 | top1:47.9167 | AUROC:0.7976\n",
      "\n",
      "Epoch: [100 | 4000] LR: 0.039986\n",
      "13/13 | Loss:1155.9613 | MainLoss:1.9474 | SPLoss:10823.8389 | | CLLoss:716.3006 | top1:47.3958 | AUROC:0.5820\n",
      "\n",
      "Epoch: [101 | 4000] LR: 0.039985\n",
      "13/13 | Loss:1156.2930 | MainLoss:2.3210 | SPLoss:10825.5703 | | CLLoss:714.1508 | top1:48.4375 | AUROC:0.6316\n",
      "502/502 Data:0.000 | Batch:0.080 | Total:0:01:03 | ETA:0:00:00 | Loss:823.7116806914428 | top1:49.6667 | AUROC:0.5016\n",
      "122/122 Data:0.001 | Batch:0.108 | Total:0:00:17 | ETA:0:00:00 | Loss:823.5934924003406 | top1:50.2692 | AUROC:1.0000\n",
      "\n",
      "Epoch: [102 | 4000] LR: 0.039985\n",
      "13/13 | Loss:1157.1805 | MainLoss:1.9202 | SPLoss:10835.2656 | | CLLoss:717.3370 | top1:47.9167 | AUROC:0.7500\n",
      "\n",
      "Epoch: [103 | 4000] LR: 0.039984\n",
      "13/13 | Loss:1157.3619 | MainLoss:1.8376 | SPLoss:10840.3164 | | CLLoss:714.9266 | top1:51.8229 | AUROC:0.5686\n",
      "\n",
      "Epoch: [104 | 4000] LR: 0.039983\n",
      "13/13 | Loss:1157.9338 | MainLoss:1.8266 | SPLoss:10843.4873 | | CLLoss:717.5848 | top1:50.2604 | AUROC:0.6118\n",
      "\n",
      "Epoch: [105 | 4000] LR: 0.039983\n",
      "13/13 | Loss:1159.3704 | MainLoss:2.3277 | SPLoss:10858.0596 | | CLLoss:712.3666 | top1:50.5208 | AUROC:0.4818\n",
      "\n",
      "Epoch: [106 | 4000] LR: 0.039982\n",
      "13/13 | Loss:1159.7054 | MainLoss:2.7114 | SPLoss:10860.3281 | | CLLoss:709.6113 | top1:48.9583 | AUROC:0.4591\n",
      "\n",
      "Epoch: [107 | 4000] LR: 0.039981\n",
      "13/13 | Loss:1157.8224 | MainLoss:1.9579 | SPLoss:10846.1367 | | CLLoss:712.5091 | top1:46.0938 | AUROC:0.5430\n",
      "\n",
      "Epoch: [108 | 4000] LR: 0.039981\n",
      "13/13 | Loss:1159.0001 | MainLoss:2.5607 | SPLoss:10848.3730 | | CLLoss:716.0216 | top1:45.0521 | AUROC:0.7489\n",
      "\n",
      "Epoch: [109 | 4000] LR: 0.039980\n",
      "13/13 | Loss:1159.6646 | MainLoss:2.6780 | SPLoss:10851.4424 | | CLLoss:718.4229 | top1:46.0938 | AUROC:0.6909\n",
      "\n",
      "Epoch: [110 | 4000] LR: 0.039979\n",
      "13/13 | Loss:1158.5846 | MainLoss:1.5832 | SPLoss:10852.4961 | | CLLoss:717.5182 | top1:47.9167 | AUROC:0.5992\n",
      "\n",
      "Epoch: [111 | 4000] LR: 0.039979\n",
      "13/13 | Loss:1159.8782 | MainLoss:2.1929 | SPLoss:10863.5664 | | CLLoss:713.2861 | top1:49.7396 | AUROC:0.7619\n",
      "\n",
      "Epoch: [112 | 4000] LR: 0.039978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 | Loss:1159.6205 | MainLoss:1.8431 | SPLoss:10863.9766 | | CLLoss:713.7961 | top1:48.9583 | AUROC:0.5176\n",
      "\n",
      "Epoch: [113 | 4000] LR: 0.039977\n",
      "13/13 | Loss:1160.7064 | MainLoss:1.9485 | SPLoss:10871.2471 | | CLLoss:716.3316 | top1:53.9062 | AUROC:0.6914\n",
      "\n",
      "Epoch: [114 | 4000] LR: 0.039976\n",
      "13/13 | Loss:1161.0642 | MainLoss:2.4309 | SPLoss:10874.6592 | | CLLoss:711.6732 | top1:51.3021 | AUROC:0.6333\n",
      "\n",
      "Epoch: [115 | 4000] LR: 0.039976\n",
      "13/13 | Loss:1160.8137 | MainLoss:2.0957 | SPLoss:10877.2402 | | CLLoss:709.9407 | top1:47.3958 | AUROC:0.4118\n",
      "\n",
      "Epoch: [116 | 4000] LR: 0.039975\n",
      "13/13 | Loss:1161.6100 | MainLoss:2.1946 | SPLoss:10881.2080 | | CLLoss:712.9457 | top1:47.9167 | AUROC:0.6984\n",
      "\n",
      "Epoch: [117 | 4000] LR: 0.039974\n",
      "13/13 | Loss:1162.5027 | MainLoss:1.9322 | SPLoss:10890.7803 | | CLLoss:714.9249 | top1:47.3958 | AUROC:0.7305\n",
      "\n",
      "Epoch: [118 | 4000] LR: 0.039973\n",
      "13/13 | Loss:1163.0088 | MainLoss:2.1968 | SPLoss:10893.7246 | | CLLoss:714.3961 | top1:48.6979 | AUROC:0.6270\n",
      "\n",
      "Epoch: [119 | 4000] LR: 0.039972\n",
      "13/13 | Loss:1162.5288 | MainLoss:2.0537 | SPLoss:10892.6943 | | CLLoss:712.0573 | top1:50.7812 | AUROC:0.8392\n",
      "\n",
      "Epoch: [120 | 4000] LR: 0.039971\n",
      "13/13 | Loss:1163.2123 | MainLoss:2.0812 | SPLoss:10894.0566 | | CLLoss:717.2538 | top1:45.5729 | AUROC:0.4980\n",
      "\n",
      "Epoch: [121 | 4000] LR: 0.039971\n",
      "13/13 | Loss:1162.9309 | MainLoss:1.8497 | SPLoss:10899.1895 | | CLLoss:711.6224 | top1:48.9583 | AUROC:0.4431\n",
      "\n",
      "Epoch: [122 | 4000] LR: 0.039970\n",
      "13/13 | Loss:1164.0226 | MainLoss:2.2004 | SPLoss:10904.2715 | | CLLoss:713.9517 | top1:48.4375 | AUROC:0.6071\n",
      "\n",
      "Epoch: [123 | 4000] LR: 0.039969\n",
      "13/13 | Loss:1162.0748 | MainLoss:1.3523 | SPLoss:10899.9023 | | CLLoss:707.3228 | top1:54.1667 | AUROC:0.4762\n",
      "\n",
      "Epoch: [124 | 4000] LR: 0.039968\n",
      "13/13 | Loss:1162.6090 | MainLoss:1.8236 | SPLoss:10897.8486 | | CLLoss:710.0046 | top1:52.3438 | AUROC:0.6706\n",
      "\n",
      "Epoch: [125 | 4000] LR: 0.039967\n",
      "13/13 | Loss:1163.4106 | MainLoss:1.8362 | SPLoss:10905.3926 | | CLLoss:710.3516 | top1:48.6979 | AUROC:0.4745\n",
      "\n",
      "Epoch: [126 | 4000] LR: 0.039966\n",
      "13/13 | Loss:1163.6018 | MainLoss:1.8333 | SPLoss:10907.7432 | | CLLoss:709.9431 | top1:53.1250 | AUROC:0.7647\n",
      "\n",
      "Epoch: [127 | 4000] LR: 0.039965\n",
      "13/13 | Loss:1164.3669 | MainLoss:1.8274 | SPLoss:10913.7939 | | CLLoss:711.6014 | top1:47.3958 | AUROC:0.7294\n",
      "\n",
      "Epoch: [128 | 4000] LR: 0.039964\n",
      "13/13 | Loss:1164.9242 | MainLoss:1.9415 | SPLoss:10920.3418 | | CLLoss:709.4857 | top1:45.8333 | AUROC:0.6523\n",
      "\n",
      "Epoch: [129 | 4000] LR: 0.039963\n",
      "13/13 | Loss:1165.1948 | MainLoss:1.3487 | SPLoss:10924.1475 | | CLLoss:714.3140 | top1:52.0833 | AUROC:0.6797\n",
      "\n",
      "Epoch: [130 | 4000] LR: 0.039962\n",
      "13/13 | Loss:1165.1851 | MainLoss:1.7196 | SPLoss:10928.9150 | | CLLoss:705.7395 | top1:54.6875 | AUROC:0.5238\n",
      "\n",
      "Epoch: [131 | 4000] LR: 0.039962\n",
      "13/13 | Loss:1165.7869 | MainLoss:1.4739 | SPLoss:10932.6865 | | CLLoss:710.4423 | top1:53.3854 | AUROC:0.4750\n",
      "\n",
      "Epoch: [132 | 4000] LR: 0.039961\n",
      "13/13 | Loss:1166.9349 | MainLoss:1.8302 | SPLoss:10936.5518 | | CLLoss:714.4957 | top1:46.0938 | AUROC:0.7686\n",
      "\n",
      "Epoch: [133 | 4000] LR: 0.039960\n",
      "13/13 | Loss:1166.3511 | MainLoss:1.9351 | SPLoss:10936.6348 | | CLLoss:707.5246 | top1:51.0417 | AUROC:0.5820\n",
      "\n",
      "Epoch: [134 | 4000] LR: 0.039959\n",
      "13/13 | Loss:1167.5490 | MainLoss:2.0494 | SPLoss:10937.7559 | | CLLoss:717.2386 | top1:49.2188 | AUROC:0.6667\n",
      "\n",
      "Epoch: [135 | 4000] LR: 0.039958\n",
      "13/13 | Loss:1166.9531 | MainLoss:1.5986 | SPLoss:10943.5723 | | CLLoss:709.9730 | top1:55.4688 | AUROC:0.5223\n",
      "\n",
      "Epoch: [136 | 4000] LR: 0.039956\n",
      "13/13 | Loss:1166.7646 | MainLoss:2.0836 | SPLoss:10938.5254 | | CLLoss:708.2849 | top1:48.1771 | AUROC:0.4667\n",
      "\n",
      "Epoch: [137 | 4000] LR: 0.039955\n",
      "13/13 | Loss:1169.5952 | MainLoss:2.2011 | SPLoss:10957.0693 | | CLLoss:716.8730 | top1:46.3542 | AUROC:0.5040\n",
      "\n",
      "Epoch: [138 | 4000] LR: 0.039954\n",
      "13/13 | Loss:1168.7335 | MainLoss:1.8333 | SPLoss:10959.6914 | | CLLoss:709.3107 | top1:49.4792 | AUROC:0.6157\n",
      "\n",
      "Epoch: [139 | 4000] LR: 0.039953\n",
      "13/13 | Loss:1168.4128 | MainLoss:2.1824 | SPLoss:10949.9609 | | CLLoss:712.3434 | top1:47.6562 | AUROC:0.6627\n",
      "\n",
      "Epoch: [140 | 4000] LR: 0.039952\n",
      "13/13 | Loss:1170.2615 | MainLoss:1.9407 | SPLoss:10968.7900 | | CLLoss:714.4178 | top1:48.4375 | AUROC:0.7969\n",
      "\n",
      "Epoch: [141 | 4000] LR: 0.039951\n",
      "13/13 | Loss:1170.5699 | MainLoss:1.8393 | SPLoss:10966.4355 | | CLLoss:720.8715 | top1:45.5729 | AUROC:0.6471\n",
      "\n",
      "Epoch: [142 | 4000] LR: 0.039950\n",
      "13/13 | Loss:1170.6249 | MainLoss:2.0764 | SPLoss:10974.5889 | | CLLoss:710.8967 | top1:52.8646 | AUROC:0.6941\n",
      "\n",
      "Epoch: [143 | 4000] LR: 0.039949\n",
      "13/13 | Loss:1169.6650 | MainLoss:2.0686 | SPLoss:10965.6270 | | CLLoss:710.3366 | top1:52.6042 | AUROC:0.6471\n",
      "\n",
      "Epoch: [144 | 4000] LR: 0.039948\n",
      "13/13 | Loss:1169.3918 | MainLoss:2.1967 | SPLoss:10962.1094 | | CLLoss:709.8424 | top1:52.0833 | AUROC:0.5516\n",
      "\n",
      "Epoch: [145 | 4000] LR: 0.039947\n",
      "13/13 | Loss:1170.5901 | MainLoss:2.0694 | SPLoss:10970.4600 | | CLLoss:714.7479 | top1:48.4375 | AUROC:0.6471\n",
      "\n",
      "Epoch: [146 | 4000] LR: 0.039946\n",
      "13/13 | Loss:1169.6858 | MainLoss:1.8283 | SPLoss:10968.9824 | | CLLoss:709.5915 | top1:47.3958 | AUROC:0.8000\n",
      "\n",
      "Epoch: [147 | 4000] LR: 0.039944\n",
      "13/13 | Loss:1170.9922 | MainLoss:2.3206 | SPLoss:10975.5098 | | CLLoss:711.2047 | top1:50.7812 | AUROC:0.7045\n",
      "\n",
      "Epoch: [148 | 4000] LR: 0.039943\n",
      "13/13 | Loss:1169.7528 | MainLoss:2.2942 | SPLoss:10965.4727 | | CLLoss:709.1133 | top1:45.0521 | AUROC:0.7449\n",
      "\n",
      "Epoch: [149 | 4000] LR: 0.039942\n",
      "13/13 | Loss:1172.2864 | MainLoss:1.8323 | SPLoss:10987.6904 | | CLLoss:716.8498 | top1:48.9583 | AUROC:0.6039\n",
      "\n",
      "Epoch: [150 | 4000] LR: 0.039941\n",
      "13/13 | Loss:1171.6447 | MainLoss:1.9480 | SPLoss:10989.9238 | | CLLoss:707.0427 | top1:52.3438 | AUROC:0.6680\n",
      "\n",
      "Epoch: [151 | 4000] LR: 0.039940\n",
      "13/13 | Loss:1171.7273 | MainLoss:2.2842 | SPLoss:10982.8691 | | CLLoss:711.5613 | top1:47.9167 | AUROC:0.8340\n",
      "502/502 Data:0.001 | Batch:0.078 | Total:0:01:04 | ETA:0:00:00 | Loss:942.2121145924601 | top1:49.9252 | AUROC:0.5636\n",
      "122/122 Data:0.001 | Batch:0.101 | Total:0:00:18 | ETA:0:00:00 | Loss:942.0998564578325 | top1:50.2308 | AUROC:1.0000\n",
      "\n",
      "Epoch: [152 | 4000] LR: 0.039938\n",
      "13/13 | Loss:1173.3257 | MainLoss:1.9617 | SPLoss:11000.6504 | | CLLoss:712.9890 | top1:51.8229 | AUROC:0.5000\n",
      "\n",
      "Epoch: [153 | 4000] LR: 0.039937\n",
      "13/13 | Loss:1171.7942 | MainLoss:1.4640 | SPLoss:10993.8027 | | CLLoss:709.4995 | top1:51.3021 | AUROC:0.6042\n",
      "\n",
      "Epoch: [154 | 4000] LR: 0.039936\n",
      "13/13 | Loss:1173.0477 | MainLoss:1.4599 | SPLoss:11004.4688 | | CLLoss:711.4082 | top1:52.6042 | AUROC:0.6750\n",
      "\n",
      "Epoch: [155 | 4000] LR: 0.039935\n",
      "13/13 | Loss:1172.9316 | MainLoss:2.3128 | SPLoss:10995.8350 | | CLLoss:710.3529 | top1:48.4375 | AUROC:0.5020\n",
      "\n",
      "Epoch: [156 | 4000] LR: 0.039933\n",
      "13/13 | Loss:1172.1479 | MainLoss:2.0566 | SPLoss:10993.6670 | | CLLoss:707.2458 | top1:53.3854 | AUROC:0.7059\n",
      "\n",
      "Epoch: [157 | 4000] LR: 0.039932\n",
      "13/13 | Loss:1174.2866 | MainLoss:2.6783 | SPLoss:11005.3838 | | CLLoss:710.6976 | top1:48.4375 | AUROC:0.5364\n",
      "\n",
      "Epoch: [158 | 4000] LR: 0.039931\n",
      "13/13 | Loss:1174.5763 | MainLoss:1.6021 | SPLoss:11020.3320 | | CLLoss:709.4100 | top1:52.6042 | AUROC:0.4130\n",
      "\n",
      "Epoch: [159 | 4000] LR: 0.039929\n",
      "13/13 | Loss:1174.0071 | MainLoss:2.0642 | SPLoss:11007.5156 | | CLLoss:711.9127 | top1:51.8229 | AUROC:0.5569\n",
      "\n",
      "Epoch: [160 | 4000] LR: 0.039928\n",
      "13/13 | Loss:1174.7500 | MainLoss:1.9506 | SPLoss:11014.6758 | | CLLoss:713.3183 | top1:53.1250 | AUROC:0.6211\n",
      "\n",
      "Epoch: [161 | 4000] LR: 0.039927\n",
      "13/13 | Loss:1174.2440 | MainLoss:1.9602 | SPLoss:11017.3906 | | CLLoss:705.4463 | top1:51.5625 | AUROC:0.6016\n",
      "\n",
      "Epoch: [162 | 4000] LR: 0.039925\n",
      "13/13 | Loss:1173.2273 | MainLoss:1.5939 | SPLoss:11012.2461 | | CLLoss:704.0884 | top1:53.6458 | AUROC:0.5182\n",
      "\n",
      "Epoch: [163 | 4000] LR: 0.039924\n",
      "13/13 | Loss:1175.5789 | MainLoss:2.3225 | SPLoss:11022.5078 | | CLLoss:710.0553 | top1:47.3958 | AUROC:0.6842\n",
      "\n",
      "Epoch: [164 | 4000] LR: 0.039923\n",
      "13/13 | Loss:1172.9443 | MainLoss:1.5901 | SPLoss:11006.0654 | | CLLoss:707.4763 | top1:50.7812 | AUROC:0.5911\n",
      "\n",
      "Epoch: [165 | 4000] LR: 0.039921\n",
      "13/13 | Loss:1176.3298 | MainLoss:1.8411 | SPLoss:11033.4639 | | CLLoss:711.4220 | top1:47.1354 | AUROC:0.6863\n",
      "\n",
      "Epoch: [166 | 4000] LR: 0.039920\n",
      "13/13 | Loss:1175.8652 | MainLoss:1.8288 | SPLoss:11031.7334 | | CLLoss:708.6320 | top1:50.0000 | AUROC:0.8000\n",
      "\n",
      "Epoch: [167 | 4000] LR: 0.039918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 | Loss:1176.6149 | MainLoss:2.2050 | SPLoss:11029.1768 | | CLLoss:714.9224 | top1:46.6146 | AUROC:0.5754\n",
      "\n",
      "Epoch: [168 | 4000] LR: 0.039917\n",
      "13/13 | Loss:1176.4613 | MainLoss:1.4619 | SPLoss:11043.4043 | | CLLoss:706.5890 | top1:49.2188 | AUROC:0.7083\n",
      "\n",
      "Epoch: [169 | 4000] LR: 0.039916\n",
      "13/13 | Loss:1177.3003 | MainLoss:2.1876 | SPLoss:11041.1709 | | CLLoss:709.9562 | top1:47.3958 | AUROC:0.7103\n",
      "\n",
      "Epoch: [170 | 4000] LR: 0.039914\n",
      "13/13 | Loss:1177.2926 | MainLoss:2.0775 | SPLoss:11040.8027 | | CLLoss:711.3471 | top1:48.1771 | AUROC:0.6353\n",
      "\n",
      "Epoch: [171 | 4000] LR: 0.039913\n",
      "13/13 | Loss:1178.3477 | MainLoss:1.9742 | SPLoss:11053.2773 | | CLLoss:710.4578 | top1:49.7396 | AUROC:0.5234\n",
      "\n",
      "Epoch: [172 | 4000] LR: 0.039911\n",
      "13/13 | Loss:1178.3456 | MainLoss:1.4557 | SPLoss:11058.5039 | | CLLoss:710.3944 | top1:50.2604 | AUROC:0.6208\n",
      "\n",
      "Epoch: [173 | 4000] LR: 0.039910\n",
      "13/13 | Loss:1179.4630 | MainLoss:2.0835 | SPLoss:11064.2920 | | CLLoss:709.5034 | top1:45.5729 | AUROC:0.4863\n",
      "\n",
      "Epoch: [174 | 4000] LR: 0.039908\n",
      "13/13 | Loss:1179.1981 | MainLoss:1.7102 | SPLoss:11062.2939 | | CLLoss:712.5856 | top1:47.6562 | AUROC:0.5000\n",
      "\n",
      "Epoch: [175 | 4000] LR: 0.039907\n",
      "13/13 | Loss:1179.0646 | MainLoss:2.5597 | SPLoss:11061.6982 | | CLLoss:703.3508 | top1:51.0417 | AUROC:0.5931\n",
      "\n",
      "Epoch: [176 | 4000] LR: 0.039905\n",
      "13/13 | Loss:1178.3850 | MainLoss:1.4567 | SPLoss:11058.6006 | | CLLoss:710.6825 | top1:51.5625 | AUROC:0.6333\n",
      "\n",
      "Epoch: [177 | 4000] LR: 0.039904\n",
      "13/13 | Loss:1180.7964 | MainLoss:2.6674 | SPLoss:11063.0166 | | CLLoss:718.2741 | top1:39.8438 | AUROC:0.8045\n",
      "\n",
      "Epoch: [178 | 4000] LR: 0.039902\n",
      "13/13 | Loss:1180.9136 | MainLoss:2.3291 | SPLoss:11074.4238 | | CLLoss:711.4213 | top1:46.8750 | AUROC:0.6194\n",
      "\n",
      "Epoch: [179 | 4000] LR: 0.039901\n",
      "13/13 | Loss:1180.7108 | MainLoss:2.3149 | SPLoss:11077.9180 | | CLLoss:706.0400 | top1:46.3542 | AUROC:0.6113\n",
      "\n",
      "Epoch: [180 | 4000] LR: 0.039899\n",
      "13/13 | Loss:1180.7096 | MainLoss:1.5915 | SPLoss:11076.5361 | | CLLoss:714.6452 | top1:53.6458 | AUROC:0.6194\n",
      "\n",
      "Epoch: [181 | 4000] LR: 0.039897\n",
      "13/13 | Loss:1181.8735 | MainLoss:2.6897 | SPLoss:11081.5205 | | CLLoss:710.3174 | top1:47.1354 | AUROC:0.6182\n",
      "\n",
      "Epoch: [182 | 4000] LR: 0.039896\n",
      "13/13 | Loss:1180.8220 | MainLoss:2.1853 | SPLoss:11076.0996 | | CLLoss:710.2672 | top1:47.3958 | AUROC:0.8611\n",
      "\n",
      "Epoch: [183 | 4000] LR: 0.039894\n",
      "13/13 | Loss:1181.7607 | MainLoss:2.1676 | SPLoss:11085.8525 | | CLLoss:710.0776 | top1:48.4375 | AUROC:0.8175\n",
      "\n",
      "Epoch: [184 | 4000] LR: 0.039893\n",
      "13/13 | Loss:1181.6465 | MainLoss:1.9618 | SPLoss:11082.7529 | | CLLoss:714.0944 | top1:48.9583 | AUROC:0.6836\n",
      "\n",
      "Epoch: [185 | 4000] LR: 0.039891\n",
      "13/13 | Loss:1181.0396 | MainLoss:1.4748 | SPLoss:11084.5410 | | CLLoss:711.1058 | top1:50.0000 | AUROC:0.3750\n",
      "\n",
      "Epoch: [186 | 4000] LR: 0.039889\n",
      "13/13 | Loss:1182.8877 | MainLoss:1.9292 | SPLoss:11100.7070 | | CLLoss:708.8783 | top1:52.8646 | AUROC:0.6758\n",
      "\n",
      "Epoch: [187 | 4000] LR: 0.039888\n",
      "13/13 | Loss:1182.1318 | MainLoss:1.5869 | SPLoss:11098.0830 | | CLLoss:707.3661 | top1:51.0417 | AUROC:0.5992\n",
      "\n",
      "Epoch: [188 | 4000] LR: 0.039886\n",
      "13/13 | Loss:1181.4895 | MainLoss:1.7075 | SPLoss:11088.2832 | | CLLoss:709.5359 | top1:53.3854 | AUROC:0.6349\n",
      "\n",
      "Epoch: [189 | 4000] LR: 0.039884\n",
      "13/13 | Loss:1183.9091 | MainLoss:2.0738 | SPLoss:11107.2236 | | CLLoss:711.1291 | top1:48.9583 | AUROC:0.5373\n",
      "\n",
      "Epoch: [190 | 4000] LR: 0.039883\n",
      "13/13 | Loss:1183.5774 | MainLoss:1.7056 | SPLoss:11108.5439 | | CLLoss:710.1748 | top1:49.2188 | AUROC:0.6349\n",
      "\n",
      "Epoch: [191 | 4000] LR: 0.039881\n",
      "13/13 | Loss:1183.1082 | MainLoss:1.5838 | SPLoss:11106.8721 | | CLLoss:708.3709 | top1:49.4792 | AUROC:0.6680\n",
      "\n",
      "Epoch: [192 | 4000] LR: 0.039879\n",
      "13/13 | Loss:1184.0844 | MainLoss:1.8187 | SPLoss:11115.4043 | | CLLoss:707.2524 | top1:50.7812 | AUROC:0.7922\n",
      "\n",
      "Epoch: [193 | 4000] LR: 0.039877\n",
      "13/13 | Loss:1184.9374 | MainLoss:1.9466 | SPLoss:11121.2002 | | CLLoss:708.7086 | top1:48.1771 | AUROC:0.6445\n",
      "\n",
      "Epoch: [194 | 4000] LR: 0.039876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/cutz/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/cutz/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/cutz/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/cutz/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 | Loss:1185.0535 | MainLoss:1.5874 | SPLoss:11123.3281 | | CLLoss:711.3315 | top1:48.4375 | AUROC:0.6194\n",
      "\n",
      "Epoch: [195 | 4000] LR: 0.039874\n",
      "13/13 | Loss:1184.1786 | MainLoss:2.0760 | SPLoss:11116.9092 | | CLLoss:704.1169 | top1:53.6458 | AUROC:0.6980\n",
      "\n",
      "Epoch: [196 | 4000] LR: 0.039872\n",
      "13/13 | Loss:1185.5491 | MainLoss:1.3477 | SPLoss:11133.2881 | | CLLoss:708.7251 | top1:48.1771 | AUROC:0.5411\n",
      "\n",
      "Epoch: [197 | 4000] LR: 0.039870\n",
      "13/13 | Loss:1185.9016 | MainLoss:1.9505 | SPLoss:11133.9180 | | CLLoss:705.5930 | top1:52.8646 | AUROC:0.6523\n",
      "\n",
      "Epoch: [198 | 4000] LR: 0.039869\n",
      "13/13 | Loss:1185.7712 | MainLoss:1.7131 | SPLoss:11134.9375 | | CLLoss:705.6437 | top1:51.3021 | AUROC:0.4444\n",
      "\n",
      "Epoch: [199 | 4000] LR: 0.039867\n",
      "13/13 | Loss:1187.1375 | MainLoss:2.3242 | SPLoss:11143.1357 | | CLLoss:704.9958 | top1:48.6979 | AUROC:0.7004\n",
      "\n",
      "Epoch: [200 | 4000] LR: 0.039865\n",
      "13/13 | Loss:1185.8939 | MainLoss:2.1945 | SPLoss:11129.1182 | | CLLoss:707.8759 | top1:49.2188 | AUROC:0.5754\n",
      "\n",
      "Epoch: [201 | 4000] LR: 0.039863\n",
      "13/13 | Loss:1186.6924 | MainLoss:1.3373 | SPLoss:11150.4932 | | CLLoss:703.0582 | top1:52.8646 | AUROC:0.6580\n",
      "502/502 Data:0.001 | Batch:0.083 | Total:0:01:12 | ETA:0:00:00 | Loss:1014.0427130183996 | top1:50.2897 | AUROC:0.4489\n",
      "122/122 Data:0.001 | Batch:0.112 | Total:0:00:17 | ETA:0:00:00 | Loss:1013.9855573292267 | top1:49.7436 | AUROC:0.9962\n",
      "\n",
      "Epoch: [202 | 4000] LR: 0.039861\n",
      "13/13 | Loss:1187.5973 | MainLoss:2.0588 | SPLoss:11149.9893 | | CLLoss:705.3956 | top1:46.0938 | AUROC:0.7882\n",
      "\n",
      "Epoch: [203 | 4000] LR: 0.039860\n",
      "13/13 | Loss:1187.8853 | MainLoss:1.8428 | SPLoss:11149.6611 | | CLLoss:710.7632 | top1:47.6562 | AUROC:0.6157\n",
      "\n",
      "Epoch: [204 | 4000] LR: 0.039858\n",
      "13/13 | Loss:1189.3518 | MainLoss:2.4478 | SPLoss:11161.9600 | | CLLoss:707.0784 | top1:47.3958 | AUROC:0.7500\n",
      "\n",
      "Epoch: [205 | 4000] LR: 0.039856\n",
      "13/13 | Loss:1188.8406 | MainLoss:1.7061 | SPLoss:11159.2041 | | CLLoss:712.1396 | top1:50.5208 | AUROC:0.6349\n",
      "\n",
      "Epoch: [206 | 4000] LR: 0.039854\n",
      "13/13 | Loss:1189.5487 | MainLoss:2.4382 | SPLoss:11168.5137 | | CLLoss:702.5908 | top1:45.3125 | AUROC:0.6583\n",
      "\n",
      "Epoch: [207 | 4000] LR: 0.039852\n",
      "13/13 | Loss:1188.9514 | MainLoss:1.7171 | SPLoss:11162.5908 | | CLLoss:709.7523 | top1:51.0417 | AUROC:0.5714\n",
      "\n",
      "Epoch: [208 | 4000] LR: 0.039850\n",
      "13/13 | Loss:1190.1287 | MainLoss:2.1952 | SPLoss:11174.7686 | | CLLoss:704.5657 | top1:49.4792 | AUROC:0.5317\n",
      "\n",
      "Epoch: [209 | 4000] LR: 0.039848\n",
      "13/13 | Loss:1190.0500 | MainLoss:2.0866 | SPLoss:11175.5771 | | CLLoss:704.0558 | top1:49.7396 | AUROC:0.5294\n",
      "\n",
      "Epoch: [210 | 4000] LR: 0.039846\n",
      "13/13 | Loss:1190.1244 | MainLoss:2.0700 | SPLoss:11174.4717 | | CLLoss:706.0728 | top1:46.6146 | AUROC:0.7373\n",
      "\n",
      "Epoch: [211 | 4000] LR: 0.039844\n",
      "13/13 | Loss:1190.8524 | MainLoss:1.5663 | SPLoss:11181.3594 | | CLLoss:711.5016 | top1:51.8229 | AUROC:0.8947\n",
      "\n",
      "Epoch: [212 | 4000] LR: 0.039842\n",
      "13/13 | Loss:1190.9911 | MainLoss:2.0736 | SPLoss:11186.4424 | | CLLoss:702.7324 | top1:52.3438 | AUROC:0.7333\n",
      "\n",
      "Epoch: [213 | 4000] LR: 0.039840\n",
      "13/13 | Loss:1190.5701 | MainLoss:1.8339 | SPLoss:11183.7539 | | CLLoss:703.6084 | top1:50.5208 | AUROC:0.5176\n",
      "\n",
      "Epoch: [214 | 4000] LR: 0.039838\n",
      "13/13 | Loss:1191.4281 | MainLoss:1.4706 | SPLoss:11195.5000 | | CLLoss:704.0746 | top1:49.2188 | AUROC:0.5958\n",
      "\n",
      "Epoch: [215 | 4000] LR: 0.039836\n",
      "13/13 | Loss:1191.6443 | MainLoss:1.9458 | SPLoss:11196.3477 | | CLLoss:700.6371 | top1:53.6458 | AUROC:0.6992\n",
      "\n",
      "Epoch: [216 | 4000] LR: 0.039834\n",
      "13/13 | Loss:1192.4885 | MainLoss:1.9542 | SPLoss:11195.9268 | | CLLoss:709.4168 | top1:49.2188 | AUROC:0.6211\n",
      "\n",
      "Epoch: [217 | 4000] LR: 0.039832\n",
      "13/13 | Loss:1193.2225 | MainLoss:2.6791 | SPLoss:11201.6064 | | CLLoss:703.8277 | top1:51.5625 | AUROC:0.6682\n",
      "\n",
      "Epoch: [218 | 4000] LR: 0.039830\n",
      "13/13 | Loss:1192.8506 | MainLoss:2.1828 | SPLoss:11203.3652 | | CLLoss:703.3122 | top1:52.8646 | AUROC:0.7937\n",
      "\n",
      "Epoch: [219 | 4000] LR: 0.039828\n",
      "13/13 | Loss:1194.3883 | MainLoss:1.9514 | SPLoss:11216.1094 | | CLLoss:708.2588 | top1:48.4375 | AUROC:0.5547\n",
      "\n",
      "Epoch: [220 | 4000] LR: 0.039826\n",
      "13/13 | Loss:1195.2579 | MainLoss:2.1880 | SPLoss:11220.7061 | | CLLoss:709.9922 | top1:50.7812 | AUROC:0.4683\n",
      "\n",
      "Epoch: [221 | 4000] LR: 0.039824\n",
      "13/13 | Loss:1194.9951 | MainLoss:1.8380 | SPLoss:11220.8086 | | CLLoss:710.7620 | top1:50.2604 | AUROC:0.4941\n",
      "\n",
      "Epoch: [222 | 4000] LR: 0.039822\n",
      "13/13 | Loss:1196.0991 | MainLoss:2.0769 | SPLoss:11231.3867 | | CLLoss:708.8361 | top1:48.9583 | AUROC:0.5412\n",
      "\n",
      "Epoch: [223 | 4000] LR: 0.039820\n",
      "13/13 | Loss:1196.1266 | MainLoss:2.1990 | SPLoss:11229.4502 | | CLLoss:709.8257 | top1:51.3021 | AUROC:0.6111\n",
      "\n",
      "Epoch: [224 | 4000] LR: 0.039818\n",
      "13/13 | Loss:1195.1787 | MainLoss:1.9436 | SPLoss:11224.7148 | | CLLoss:707.6358 | top1:50.7812 | AUROC:0.7031\n",
      "\n",
      "Epoch: [225 | 4000] LR: 0.039816\n",
      "13/13 | Loss:1198.0093 | MainLoss:1.9229 | SPLoss:11246.4336 | | CLLoss:714.4296 | top1:44.2708 | AUROC:0.8242\n",
      "\n",
      "Epoch: [226 | 4000] LR: 0.039814\n",
      "13/13 | Loss:1196.4589 | MainLoss:2.3225 | SPLoss:11231.7852 | | CLLoss:709.5782 | top1:49.7396 | AUROC:0.6599\n",
      "\n",
      "Epoch: [227 | 4000] LR: 0.039811\n",
      "13/13 | Loss:1197.0349 | MainLoss:2.0884 | SPLoss:11239.5469 | | CLLoss:709.9182 | top1:46.8750 | AUROC:0.5216\n",
      "\n",
      "Epoch: [228 | 4000] LR: 0.039809\n",
      "13/13 | Loss:1197.1936 | MainLoss:1.9510 | SPLoss:11243.6914 | | CLLoss:708.7359 | top1:51.0417 | AUROC:0.5586\n",
      "\n",
      "Epoch: [229 | 4000] LR: 0.039807\n",
      "13/13 | Loss:1197.7345 | MainLoss:2.4346 | SPLoss:11243.4717 | | CLLoss:709.5280 | top1:50.5208 | AUROC:0.6458\n",
      "\n",
      "Epoch: [230 | 4000] LR: 0.039805\n",
      "13/13 | Loss:1198.4695 | MainLoss:2.5612 | SPLoss:11253.9639 | | CLLoss:705.1202 | top1:48.4375 | AUROC:0.5498\n",
      "\n",
      "Epoch: [231 | 4000] LR: 0.039803\n",
      "13/13 | Loss:1199.3879 | MainLoss:2.5567 | SPLoss:11261.3555 | | CLLoss:706.9562 | top1:46.8750 | AUROC:0.5195\n",
      "\n",
      "Epoch: [232 | 4000] LR: 0.039800\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    # teacher feedback\n",
    "#     if epoch in iter_time:\n",
    "#         print(\"iterative training: feedback {}\".format(epoch))\n",
    "#         teacher_model.load_state_dict(student_model.state_dict())\n",
    "#         teacher_model_weights = {}\n",
    "#         for name, param in teacher_model.named_parameters():\n",
    "#             teacher_model_weights[name] = param.detach()\n",
    "    \n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, source_train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        test_loss, test_acc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "        source_loss, source_acc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "    \n",
    "        logger.append([state['lr'], train_loss, test_loss, source_loss, train_acc, test_acc, source_acc, train_a])\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict' : student_model.state_dict(),\n",
    "            'acc': test_acc,\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
