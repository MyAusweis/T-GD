{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = './log/pggan/128/b0/siamese/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 128\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 160\n",
    "test_batch = 160\n",
    "lr = 0.04\n",
    "schedule = [75,150,225]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/siamese' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'\n",
    "\n",
    "# iterative training\n",
    "feedback = 0\n",
    "# iter_time = [1000, 2000, 2500]\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# constrastive\n",
    "thresh = 0.5\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "#         img0 = img0.convert(\"L\")\n",
    "#         img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "#     transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_ = SiameseNetworkDataset(datasets.ImageFolder(train_dir), transform=train_aug, should_invert=False)\n",
    "train_loader = DataLoader(train_, shuffle=True, num_workers=num_workers, batch_size=train_batch)\n",
    "val_ = SiameseNetworkDataset(datasets.ImageFolder(val_dir), transform=val_aug, should_invert=False)\n",
    "val_loader = DataLoader(val_, shuffle=True, num_workers=num_workers, batch_size=test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.0, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    pt = torch.load(pretrained)['state_dict']\n",
    "    model.load_state_dict(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.17M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=1.0).cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=50, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs0, inputs1, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs0.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            target_index = targets[targets==0]\n",
    "            target_index = target_index.long().cuda()\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs0.size(), lam)\n",
    "            inputs0[target_index, :, bbx1:bbx2, bby1:bby2] = inputs1[target_index, :, bbx1:bbx2, bby1:bby2]\n",
    "        \n",
    "\n",
    "        \n",
    "        outputs0 = model(inputs0)\n",
    "        outputs1 = model(inputs1)\n",
    "        \n",
    "        loss = criterion(outputs0, outputs1, targets)\n",
    "            \n",
    "        # compute output\n",
    "        outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        pred = outputs.data\n",
    "        pred[pred < thresh] = 0.\n",
    "        pred[pred >= thresh] = 1.\n",
    "        prec1 = [accuracy_score(targets.data.cpu().numpy(), pred.cpu().numpy())]\n",
    "        \n",
    "        losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "        top1.update(prec1[0], inputs0.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "#         if batch_idx % 10 == 0:\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs0, inputs1, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs0, inputs1, targets = inputs0.cuda(), inputs1.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs0 = model(inputs0)\n",
    "            outputs1 = model(inputs1)\n",
    "            loss = criterion(outputs0, outputs1, targets)\n",
    "            outputs = F.pairwise_distance(outputs0, outputs1, keepdim=True)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            pred = outputs.data\n",
    "            pred[pred < thresh] = 0.\n",
    "            pred[pred >= thresh] = 1.\n",
    "            prec1 = [accuracy_score(targets.data.cpu().numpy(), pred.cpu().numpy())]\n",
    "            losses.update(loss.data.tolist(), inputs0.size(0))\n",
    "            top1.update(prec1[0], inputs0.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(val_loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        total=bar.elapsed_td,\n",
    "                        eta=bar.eta_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,)\n",
    "            bar.next()\n",
    "        print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "             batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "        bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [170 | 300] LR: 0.000004\n",
      "803/803 Data:0.019 | Batch:0.396 | Total:0:10:13 | ETA:0:00:01 | Loss:0.019608123752113172 | top1:0.9775015586034913\n",
      "201/201 Data:0.012 | Batch:0.641 | Total:0:00:29 | ETA:0:00:00 | Loss:0.07217540102677182 | top1:0.9173520249221184\n",
      "\n",
      "Epoch: [171 | 300] LR: 0.000424\n",
      "803/803 Data:0.023 | Batch:0.802 | Total:0:09:28 | ETA:0:00:01 | Loss:0.019560468421454664 | top1:0.9776028678304239\n",
      "201/201 Data:0.048 | Batch:0.245 | Total:0:00:53 | ETA:0:00:00 | Loss:0.07033096096504514 | top1:0.9192834890965732\n",
      "\n",
      "Epoch: [172 | 300] LR: 0.000448\n",
      "803/803 Data:0.021 | Batch:0.758 | Total:0:10:29 | ETA:0:00:01 | Loss:0.01935934853292084 | top1:0.9778912094763093\n",
      "201/201 Data:0.019 | Batch:0.159 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07188618736939267 | top1:0.9170404984423676\n",
      "\n",
      "Epoch: [173 | 300] LR: 0.000472\n",
      "803/803 Data:0.050 | Batch:0.761 | Total:0:11:27 | ETA:0:00:01 | Loss:0.018587625312167184 | top1:0.978732855361596\n",
      "201/201 Data:0.032 | Batch:0.309 | Total:0:00:56 | ETA:0:00:00 | Loss:0.07103750716395839 | top1:0.9186915887850468\n",
      "\n",
      "Epoch: [174 | 300] LR: 0.000496\n",
      "803/803 Data:0.043 | Batch:0.997 | Total:0:11:31 | ETA:0:00:01 | Loss:0.019506484161010893 | top1:0.9779223815461346\n",
      "201/201 Data:0.028 | Batch:0.163 | Total:0:00:58 | ETA:0:00:00 | Loss:0.06886026474917047 | top1:0.9210903426791277\n",
      "\n",
      "Epoch: [175 | 300] LR: 0.000520\n",
      "803/803 Data:0.022 | Batch:0.827 | Total:0:11:16 | ETA:0:00:01 | Loss:0.019265984622976438 | top1:0.977821072319202\n",
      "201/201 Data:0.011 | Batch:0.161 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07148869709340955 | top1:0.9182866043613707\n",
      "\n",
      "Epoch: [176 | 300] LR: 0.000544\n",
      "803/803 Data:0.012 | Batch:0.811 | Total:0:10:30 | ETA:0:00:01 | Loss:0.019171016905665805 | top1:0.9781951371571073\n",
      "201/201 Data:0.013 | Batch:0.152 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07035797764300557 | top1:0.9193769470404984\n",
      "\n",
      "Epoch: [177 | 300] LR: 0.000568\n",
      "803/803 Data:0.009 | Batch:0.796 | Total:0:10:26 | ETA:0:00:01 | Loss:0.019330260337462782 | top1:0.9780236907730673\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07122264354956856 | top1:0.9197507788161994\n",
      "\n",
      "Epoch: [178 | 300] LR: 0.000592\n",
      "803/803 Data:0.011 | Batch:0.776 | Total:0:10:25 | ETA:0:00:01 | Loss:0.019820726862289893 | top1:0.9775483167082294\n",
      "201/201 Data:0.011 | Batch:0.167 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07147302918043277 | top1:0.9188473520249221\n",
      "\n",
      "Epoch: [179 | 300] LR: 0.000616\n",
      "803/803 Data:0.010 | Batch:0.791 | Total:0:10:23 | ETA:0:00:01 | Loss:0.019304118208015836 | top1:0.9781951371571073\n",
      "201/201 Data:0.009 | Batch:0.163 | Total:0:00:48 | ETA:0:00:00 | Loss:0.06990372098959123 | top1:0.9206542056074767\n",
      "\n",
      "Epoch: [180 | 300] LR: 0.000640\n",
      "803/803 Data:0.011 | Batch:0.791 | Total:0:10:24 | ETA:0:00:01 | Loss:0.019092502516268768 | top1:0.9783042394014962\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07106067148788696 | top1:0.9187538940809968\n",
      "\n",
      "Epoch: [181 | 300] LR: 0.000664\n",
      "803/803 Data:0.012 | Batch:0.701 | Total:0:10:26 | ETA:0:00:01 | Loss:0.01882315849934117 | top1:0.9785847880299252\n",
      "201/201 Data:0.011 | Batch:0.165 | Total:0:00:45 | ETA:0:00:00 | Loss:0.07071823083166022 | top1:0.9190965732087227\n",
      "\n",
      "Epoch: [182 | 300] LR: 0.000688\n",
      "803/803 Data:0.010 | Batch:0.781 | Total:0:10:33 | ETA:0:00:01 | Loss:0.01935798869955416 | top1:0.9779925187032419\n",
      "201/201 Data:0.012 | Batch:0.136 | Total:0:00:44 | ETA:0:00:00 | Loss:0.07346439369928057 | top1:0.9161993769470405\n",
      "\n",
      "Epoch: [183 | 300] LR: 0.000712\n",
      "803/803 Data:0.011 | Batch:0.773 | Total:0:10:30 | ETA:0:00:01 | Loss:0.01914034020281954 | top1:0.9784990648379053\n",
      "201/201 Data:0.011 | Batch:0.147 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07157581005233841 | top1:0.9181308411214953\n",
      "\n",
      "Epoch: [184 | 300] LR: 0.000736\n",
      "803/803 Data:0.010 | Batch:0.774 | Total:0:10:24 | ETA:0:00:01 | Loss:0.018591276137811478 | top1:0.9787640274314214\n",
      "201/201 Data:0.010 | Batch:0.162 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0711966218467442 | top1:0.9184735202492211\n",
      "\n",
      "Epoch: [185 | 300] LR: 0.000760\n",
      "803/803 Data:0.010 | Batch:0.798 | Total:0:10:24 | ETA:0:00:01 | Loss:0.0195009109013693 | top1:0.9780704488778055\n",
      "201/201 Data:0.011 | Batch:0.147 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07354336613063872 | top1:0.9160436137071651\n",
      "\n",
      "Epoch: [186 | 300] LR: 0.000784\n",
      "803/803 Data:0.011 | Batch:0.784 | Total:0:10:24 | ETA:0:00:01 | Loss:0.01920542490102195 | top1:0.9783977556109725\n",
      "201/201 Data:0.011 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07152198922829094 | top1:0.918785046728972\n",
      "\n",
      "Epoch: [187 | 300] LR: 0.000808\n",
      "803/803 Data:0.010 | Batch:0.784 | Total:0:10:25 | ETA:0:00:01 | Loss:0.0191256286312351 | top1:0.9781873441396509\n",
      "201/201 Data:0.010 | Batch:0.164 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07194299513584357 | top1:0.9176323987538941\n",
      "\n",
      "Epoch: [188 | 300] LR: 0.000832\n",
      "803/803 Data:0.011 | Batch:0.778 | Total:0:10:26 | ETA:0:00:01 | Loss:0.019756583934501765 | top1:0.9776730049875312\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07145682810994324 | top1:0.9183177570093458\n",
      "\n",
      "Epoch: [189 | 300] LR: 0.000856\n",
      "803/803 Data:0.011 | Batch:0.786 | Total:0:10:24 | ETA:0:00:01 | Loss:0.018860068106642074 | top1:0.9788575436408977\n",
      "201/201 Data:0.010 | Batch:0.152 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07051563676319018 | top1:0.9197507788161994\n",
      "\n",
      "Epoch: [190 | 300] LR: 0.000880\n",
      "803/803 Data:0.011 | Batch:0.789 | Total:0:10:25 | ETA:0:00:01 | Loss:0.019243285121329994 | top1:0.977930174563591\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07065548226701508 | top1:0.9190342679127725\n",
      "\n",
      "Epoch: [191 | 300] LR: 0.000904\n",
      "803/803 Data:0.010 | Batch:0.771 | Total:0:10:26 | ETA:0:00:01 | Loss:0.019256330863616205 | top1:0.9778756234413966\n",
      "201/201 Data:0.012 | Batch:0.167 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07279021522244931 | top1:0.9168847352024923\n",
      "\n",
      "Epoch: [192 | 300] LR: 0.000928\n",
      "803/803 Data:0.010 | Batch:0.780 | Total:0:10:26 | ETA:0:00:01 | Loss:0.01905519713661018 | top1:0.9783432044887781\n",
      "201/201 Data:0.011 | Batch:0.156 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07109627648955937 | top1:0.9188473520249221\n",
      "\n",
      "Epoch: [193 | 300] LR: 0.000952\n",
      "803/803 Data:0.011 | Batch:0.714 | Total:0:10:30 | ETA:0:00:01 | Loss:0.019036552652137752 | top1:0.9783120324189526\n",
      "201/201 Data:0.010 | Batch:0.150 | Total:0:00:45 | ETA:0:00:00 | Loss:0.07210908942040625 | top1:0.9179127725856698\n",
      "\n",
      "Epoch: [194 | 300] LR: 0.000976\n",
      "803/803 Data:0.011 | Batch:0.774 | Total:0:10:35 | ETA:0:00:01 | Loss:0.018360040500627114 | top1:0.9793874688279302\n",
      "201/201 Data:0.011 | Batch:0.144 | Total:0:00:44 | ETA:0:00:00 | Loss:0.07090762524961311 | top1:0.9195015576323987\n",
      "\n",
      "Epoch: [195 | 300] LR: 0.001000\n",
      "803/803 Data:0.010 | Batch:0.786 | Total:0:10:30 | ETA:0:00:01 | Loss:0.018855727266809414 | top1:0.9788341645885287\n",
      "201/201 Data:0.014 | Batch:0.166 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07048583783186113 | top1:0.9205607476635514\n",
      "\n",
      "Epoch: [196 | 300] LR: 0.001024\n",
      "803/803 Data:0.011 | Batch:0.780 | Total:0:10:26 | ETA:0:00:01 | Loss:0.019391244069712023 | top1:0.9779769326683292\n",
      "201/201 Data:0.010 | Batch:0.150 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07079808080214949 | top1:0.9188785046728972\n",
      "\n",
      "Epoch: [197 | 300] LR: 0.001048\n",
      "803/803 Data:0.011 | Batch:0.791 | Total:0:10:26 | ETA:0:00:01 | Loss:0.018929913071154887 | top1:0.9787172693266832\n",
      "201/201 Data:0.011 | Batch:0.147 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07204257459655358 | top1:0.9177570093457944\n",
      "\n",
      "Epoch: [198 | 300] LR: 0.001072\n",
      "803/803 Data:0.010 | Batch:0.796 | Total:0:10:26 | ETA:0:00:01 | Loss:0.019323797579410548 | top1:0.977930174563591\n",
      "201/201 Data:0.011 | Batch:0.165 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07318049662534692 | top1:0.917165109034268\n",
      "\n",
      "Epoch: [199 | 300] LR: 0.001096\n",
      "803/803 Data:0.010 | Batch:0.781 | Total:0:10:25 | ETA:0:00:01 | Loss:0.018696800112724885 | top1:0.978818578553616\n",
      "201/201 Data:0.012 | Batch:0.167 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07532726438060357 | top1:0.9142679127725857\n",
      "\n",
      "Epoch: [200 | 300] LR: 0.001120\n",
      "803/803 Data:0.010 | Batch:0.785 | Total:0:10:24 | ETA:0:00:01 | Loss:0.019443631034426884 | top1:0.9780003117206982\n",
      "201/201 Data:0.011 | Batch:0.156 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07130931002143016 | top1:0.9184735202492211\n",
      "\n",
      "Epoch: [201 | 300] LR: 0.001144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803/803 Data:0.010 | Batch:0.793 | Total:0:10:26 | ETA:0:00:01 | Loss:0.01915567363522357 | top1:0.978428927680798\n",
      "201/201 Data:0.012 | Batch:0.152 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0736843740944736 | top1:0.9146417445482866\n",
      "\n",
      "Epoch: [202 | 300] LR: 0.001168\n",
      "803/803 Data:0.010 | Batch:0.769 | Total:0:10:26 | ETA:0:00:01 | Loss:0.01897501092259842 | top1:0.9786081670822943\n",
      "201/201 Data:0.011 | Batch:0.149 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07135026071023347 | top1:0.9186292834890966\n",
      "\n",
      "Epoch: [203 | 300] LR: 0.001192\n",
      "803/803 Data:0.011 | Batch:0.780 | Total:0:10:24 | ETA:0:00:01 | Loss:0.018945925622717467 | top1:0.9783977556109725\n",
      "201/201 Data:0.012 | Batch:0.150 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07192592424952723 | top1:0.9172585669781932\n",
      "\n",
      "Epoch: [204 | 300] LR: 0.001216\n",
      "803/803 Data:0.010 | Batch:0.774 | Total:0:10:26 | ETA:0:00:01 | Loss:0.01915510846910118 | top1:0.9782029301745636\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07413307763055849 | top1:0.9141121495327102\n",
      "\n",
      "Epoch: [205 | 300] LR: 0.001240\n",
      "803/803 Data:0.011 | Batch:0.692 | Total:0:10:27 | ETA:0:00:01 | Loss:0.018976532718999326 | top1:0.9785614089775561\n",
      "201/201 Data:0.011 | Batch:0.150 | Total:0:00:46 | ETA:0:00:00 | Loss:0.07663180248759617 | top1:0.9119003115264798\n",
      "\n",
      "Epoch: [206 | 300] LR: 0.001264\n",
      "803/803 Data:0.010 | Batch:0.808 | Total:0:10:32 | ETA:0:00:01 | Loss:0.019212456389000532 | top1:0.9783665835411471\n",
      "201/201 Data:0.011 | Batch:0.148 | Total:0:00:43 | ETA:0:00:00 | Loss:0.0734383119833061 | top1:0.9166043613707165\n",
      "\n",
      "Epoch: [207 | 300] LR: 0.001288\n",
      "803/803 Data:0.011 | Batch:0.802 | Total:0:10:30 | ETA:0:00:01 | Loss:0.019494254023598466 | top1:0.977797693266833\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07506063759234093 | top1:0.9145482866043614\n",
      "\n",
      "Epoch: [208 | 300] LR: 0.001312\n",
      "803/803 Data:0.015 | Batch:0.779 | Total:0:10:25 | ETA:0:00:01 | Loss:0.018863499236215914 | top1:0.9784834788029925\n",
      "201/201 Data:0.011 | Batch:0.152 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07264713021082299 | top1:0.917227414330218\n",
      "\n",
      "Epoch: [209 | 300] LR: 0.001336\n",
      "803/803 Data:0.011 | Batch:0.784 | Total:0:10:24 | ETA:0:00:01 | Loss:0.018927953977000197 | top1:0.978514650872818\n",
      "201/201 Data:0.010 | Batch:0.162 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07140610479862891 | top1:0.9183489096573209\n",
      "\n",
      "Epoch: [210 | 300] LR: 0.001360\n",
      "803/803 Data:0.010 | Batch:0.793 | Total:0:10:24 | ETA:0:00:01 | Loss:0.018724743544792945 | top1:0.978841957605985\n",
      "201/201 Data:0.011 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0739345813698115 | top1:0.9145482866043614\n",
      "\n",
      "Epoch: [211 | 300] LR: 0.001384\n",
      "803/803 Data:0.011 | Batch:0.774 | Total:0:10:25 | ETA:0:00:01 | Loss:0.01944107498042866 | top1:0.9780003117206982\n",
      "201/201 Data:0.011 | Batch:0.165 | Total:0:00:48 | ETA:0:00:00 | Loss:0.06933937657195088 | top1:0.9202180685358256\n",
      "\n",
      "Epoch: [212 | 300] LR: 0.001408\n",
      "803/803 Data:0.009 | Batch:0.787 | Total:0:10:24 | ETA:0:00:01 | Loss:0.018854057389538516 | top1:0.9787016832917705\n",
      "201/201 Data:0.010 | Batch:0.151 | Total:0:00:47 | ETA:0:00:00 | Loss:0.06935243189938343 | top1:0.9211214953271029\n",
      "\n",
      "Epoch: [213 | 300] LR: 0.001432\n",
      "803/803 Data:0.011 | Batch:0.796 | Total:0:10:24 | ETA:0:00:01 | Loss:0.01850632148022018 | top1:0.9789744389027432\n",
      "201/201 Data:0.011 | Batch:0.160 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0688540659960928 | top1:0.9212149532710281\n",
      "\n",
      "Epoch: [214 | 300] LR: 0.001456\n",
      "803/803 Data:0.010 | Batch:0.789 | Total:0:10:25 | ETA:0:00:01 | Loss:0.018752420540728336 | top1:0.9788341645885287\n",
      "201/201 Data:0.010 | Batch:0.146 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07314832015572308 | top1:0.9166355140186916\n",
      "\n",
      "Epoch: [215 | 300] LR: 0.001480\n",
      "803/803 Data:0.011 | Batch:0.808 | Total:0:10:25 | ETA:0:00:01 | Loss:0.018509224545648392 | top1:0.979145885286783\n",
      "201/201 Data:0.011 | Batch:0.166 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07039428004223239 | top1:0.9190342679127725\n",
      "\n",
      "Epoch: [216 | 300] LR: 0.001504\n",
      "803/803 Data:0.011 | Batch:0.783 | Total:0:10:24 | ETA:0:00:01 | Loss:0.019393277329498984 | top1:0.9782964463840399\n",
      "201/201 Data:0.011 | Batch:0.169 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07159792506063467 | top1:0.9180373831775701\n",
      "\n",
      "Epoch: [217 | 300] LR: 0.001528\n",
      "803/803 Data:0.012 | Batch:0.696 | Total:0:10:30 | ETA:0:00:01 | Loss:0.01879904298696776 | top1:0.9789432668329177\n",
      "201/201 Data:0.011 | Batch:0.152 | Total:0:00:46 | ETA:0:00:00 | Loss:0.07065065213220885 | top1:0.9192523364485982\n",
      "\n",
      "Epoch: [218 | 300] LR: 0.001552\n",
      "803/803 Data:0.010 | Batch:0.781 | Total:0:10:34 | ETA:0:00:01 | Loss:0.01862440079911043 | top1:0.9788107855361596\n",
      "201/201 Data:0.011 | Batch:0.136 | Total:0:00:43 | ETA:0:00:00 | Loss:0.07084829108876603 | top1:0.9186915887850468\n",
      "\n",
      "Epoch: [219 | 300] LR: 0.001576\n",
      "803/803 Data:0.011 | Batch:0.787 | Total:0:10:31 | ETA:0:00:01 | Loss:0.018830784757771387 | top1:0.9786081670822943\n",
      "201/201 Data:0.011 | Batch:0.165 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07329791133927407 | top1:0.915638629283489\n",
      "\n",
      "Epoch: [220 | 300] LR: 0.001600\n",
      "803/803 Data:0.011 | Batch:0.778 | Total:0:10:28 | ETA:0:00:01 | Loss:0.01872416448177296 | top1:0.9788653366583541\n",
      "201/201 Data:0.012 | Batch:0.165 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07255071823099321 | top1:0.9162928348909657\n",
      "\n",
      "Epoch: [221 | 300] LR: 0.001600\n",
      "803/803 Data:0.010 | Batch:0.794 | Total:0:10:27 | ETA:0:00:01 | Loss:0.018628771487280014 | top1:0.9791848503740649\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:48 | ETA:0:00:00 | Loss:0.06828137392474112 | top1:0.9219314641744548\n",
      "\n",
      "Epoch: [222 | 300] LR: 0.001600\n",
      "803/803 Data:0.011 | Batch:0.809 | Total:0:10:28 | ETA:0:00:01 | Loss:0.019265620453060232 | top1:0.97821072319202\n",
      "201/201 Data:0.017 | Batch:0.155 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07007504016903703 | top1:0.9189096573208723\n",
      "\n",
      "Epoch: [223 | 300] LR: 0.001600\n",
      "803/803 Data:0.012 | Batch:0.793 | Total:0:10:34 | ETA:0:00:01 | Loss:0.01854730384101211 | top1:0.9789900249376559\n",
      "201/201 Data:0.012 | Batch:0.163 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07113414118501628 | top1:0.9188473520249221\n",
      "\n",
      "Epoch: [224 | 300] LR: 0.001600\n",
      "803/803 Data:0.012 | Batch:0.816 | Total:0:10:31 | ETA:0:00:01 | Loss:0.01832250683465938 | top1:0.9794108478802993\n",
      "201/201 Data:0.010 | Batch:0.153 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07123840879242739 | top1:0.9179750778816199\n",
      "\n",
      "Epoch: [225 | 300] LR: 0.001599\n",
      "803/803 Data:0.012 | Batch:0.796 | Total:0:10:30 | ETA:0:00:01 | Loss:0.01853530536174363 | top1:0.9788965087281796\n",
      "201/201 Data:0.011 | Batch:0.171 | Total:0:00:48 | ETA:0:00:00 | Loss:0.06762351895511336 | top1:0.9226479750778817\n",
      "\n",
      "Epoch: [226 | 300] LR: 0.001599\n",
      "803/803 Data:0.011 | Batch:0.777 | Total:0:10:30 | ETA:0:00:01 | Loss:0.01827484758666884 | top1:0.9794030548628428\n",
      "201/201 Data:0.013 | Batch:0.161 | Total:0:00:49 | ETA:0:00:00 | Loss:0.0700682254455914 | top1:0.919190031152648\n",
      "\n",
      "Epoch: [227 | 300] LR: 0.000160\n",
      "803/803 Data:0.011 | Batch:0.781 | Total:0:10:30 | ETA:0:00:01 | Loss:0.019611199625127338 | top1:0.9776496259351621\n",
      "201/201 Data:0.012 | Batch:0.151 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07227501793999538 | top1:0.9164797507788162\n",
      "\n",
      "Epoch: [228 | 300] LR: 0.000160\n",
      "803/803 Data:0.012 | Batch:0.787 | Total:0:10:31 | ETA:0:00:01 | Loss:0.01880952350660564 | top1:0.9787016832917705\n",
      "201/201 Data:0.012 | Batch:0.159 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07086981024790405 | top1:0.9188785046728972\n",
      "\n",
      "Epoch: [229 | 300] LR: 0.000160\n",
      "803/803 Data:0.013 | Batch:0.802 | Total:0:10:31 | ETA:0:00:01 | Loss:0.018998022841054778 | top1:0.978623753117207\n",
      "201/201 Data:0.011 | Batch:0.171 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07174316839172833 | top1:0.9176635514018692\n",
      "\n",
      "Epoch: [230 | 300] LR: 0.000160\n",
      "803/803 Data:0.013 | Batch:0.807 | Total:0:10:30 | ETA:0:00:01 | Loss:0.018389703591814046 | top1:0.9793251246882793\n",
      "201/201 Data:0.011 | Batch:0.171 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07045776278151901 | top1:0.9192523364485982\n",
      "\n",
      "Epoch: [231 | 300] LR: 0.000160\n",
      "803/803 Data:0.015 | Batch:0.798 | Total:0:10:32 | ETA:0:00:01 | Loss:0.018789400783642673 | top1:0.9786393391521196\n",
      "201/201 Data:0.012 | Batch:0.155 | Total:0:00:49 | ETA:0:00:00 | Loss:0.0695876071293406 | top1:0.9202180685358256\n",
      "\n",
      "Epoch: [232 | 300] LR: 0.000159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803/803 Data:0.026 | Batch:0.735 | Total:0:10:38 | ETA:0:00:01 | Loss:0.018526932411519998 | top1:0.9790211970074812\n",
      "201/201 Data:0.012 | Batch:0.171 | Total:0:00:45 | ETA:0:00:00 | Loss:0.07034490730996444 | top1:0.9189096573208723\n",
      "\n",
      "Epoch: [233 | 300] LR: 0.000159\n",
      "803/803 Data:0.012 | Batch:0.794 | Total:0:10:37 | ETA:0:00:01 | Loss:0.018313874365797293 | top1:0.9790134039900249\n",
      "201/201 Data:0.011 | Batch:0.159 | Total:0:00:46 | ETA:0:00:00 | Loss:0.07374223741322664 | top1:0.9163862928348909\n",
      "\n",
      "Epoch: [234 | 300] LR: 0.000159\n",
      "803/803 Data:0.012 | Batch:0.791 | Total:0:10:34 | ETA:0:00:01 | Loss:0.018539928412601988 | top1:0.9793640897755611\n",
      "201/201 Data:0.012 | Batch:0.156 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07241459677338229 | top1:0.9168535825545171\n",
      "\n",
      "Epoch: [235 | 300] LR: 0.000159\n",
      "803/803 Data:0.012 | Batch:0.809 | Total:0:10:30 | ETA:0:00:01 | Loss:0.018406592737976228 | top1:0.9792705735660848\n",
      "201/201 Data:0.012 | Batch:0.171 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07422490307010965 | top1:0.9150155763239876\n",
      "\n",
      "Epoch: [236 | 300] LR: 0.000159\n",
      "803/803 Data:0.010 | Batch:0.795 | Total:0:10:30 | ETA:0:00:01 | Loss:0.01847571019748658 | top1:0.9789900249376559\n",
      "201/201 Data:0.012 | Batch:0.174 | Total:0:00:49 | ETA:0:00:00 | Loss:0.0718546246396047 | top1:0.9174766355140187\n",
      "\n",
      "Epoch: [237 | 300] LR: 0.000159\n",
      "803/803 Data:0.013 | Batch:0.781 | Total:0:10:30 | ETA:0:00:01 | Loss:0.018241580156783604 | top1:0.9795511221945137\n",
      "201/201 Data:0.012 | Batch:0.171 | Total:0:00:49 | ETA:0:00:00 | Loss:0.0719481356372343 | top1:0.9174454828660437\n",
      "\n",
      "Epoch: [238 | 300] LR: 0.000159\n",
      "803/803 Data:0.011 | Batch:0.789 | Total:0:10:30 | ETA:0:00:01 | Loss:0.01816280839793173 | top1:0.9791848503740649\n",
      "201/201 Data:0.012 | Batch:0.164 | Total:0:00:49 | ETA:0:00:00 | Loss:0.06985315884756522 | top1:0.920404984423676\n",
      "\n",
      "Epoch: [239 | 300] LR: 0.000159\n",
      "803/803 Data:0.011 | Batch:0.801 | Total:0:10:30 | ETA:0:00:01 | Loss:0.018382249313433935 | top1:0.979254987531172\n",
      "201/201 Data:0.012 | Batch:0.171 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07047956984292866 | top1:0.918816199376947\n",
      "\n",
      "Epoch: [240 | 300] LR: 0.000158\n",
      "803/803 Data:0.012 | Batch:0.800 | Total:0:10:31 | ETA:0:00:01 | Loss:0.01799468387123267 | top1:0.9797069825436409\n",
      "201/201 Data:0.011 | Batch:0.155 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07378466604654663 | top1:0.9160747663551402\n",
      "\n",
      "Epoch: [241 | 300] LR: 0.000158\n",
      "803/803 Data:0.012 | Batch:0.791 | Total:0:10:31 | ETA:0:00:01 | Loss:0.019074419804673382 | top1:0.978428927680798\n",
      "201/201 Data:0.010 | Batch:0.148 | Total:0:00:49 | ETA:0:00:00 | Loss:0.0718862466096321 | top1:0.917601246105919\n",
      "\n",
      "Epoch: [242 | 300] LR: 0.000158\n",
      "803/803 Data:0.013 | Batch:0.802 | Total:0:10:31 | ETA:0:00:01 | Loss:0.01836127507618798 | top1:0.9793017456359102\n",
      "201/201 Data:0.012 | Batch:0.153 | Total:0:00:49 | ETA:0:00:00 | Loss:0.06946019155446242 | top1:0.9195327102803739\n",
      "\n",
      "Epoch: [243 | 300] LR: 0.000158\n",
      "803/803 Data:0.010 | Batch:0.770 | Total:0:10:27 | ETA:0:00:01 | Loss:0.018038892833849282 | top1:0.9796602244389028\n",
      "201/201 Data:0.011 | Batch:0.148 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07296076262387159 | top1:0.9161682242990654\n",
      "\n",
      "Epoch: [244 | 300] LR: 0.000158\n",
      "803/803 Data:0.010 | Batch:0.792 | Total:0:10:22 | ETA:0:00:01 | Loss:0.018532622684164324 | top1:0.9790601620947631\n",
      "201/201 Data:0.010 | Batch:0.161 | Total:0:00:47 | ETA:0:00:00 | Loss:0.0725194683861324 | top1:0.9164485981308411\n",
      "\n",
      "Epoch: [245 | 300] LR: 0.000157\n",
      "803/803 Data:0.014 | Batch:0.771 | Total:0:10:23 | ETA:0:00:01 | Loss:0.018619627104938596 | top1:0.9790757481296758\n",
      "201/201 Data:0.010 | Batch:0.146 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07136532354438416 | top1:0.9183177570093458\n",
      "\n",
      "Epoch: [246 | 300] LR: 0.000157\n",
      "803/803 Data:0.011 | Batch:0.720 | Total:0:10:31 | ETA:0:00:01 | Loss:0.01796628427196775 | top1:0.9795199501246883\n",
      "201/201 Data:0.010 | Batch:0.147 | Total:0:00:43 | ETA:0:00:00 | Loss:0.06904397070686394 | top1:0.9208411214953272\n",
      "\n",
      "Epoch: [247 | 300] LR: 0.000157\n",
      "803/803 Data:0.011 | Batch:0.792 | Total:0:10:34 | ETA:0:00:01 | Loss:0.01838982436690419 | top1:0.9791770573566084\n",
      "201/201 Data:0.011 | Batch:0.136 | Total:0:00:46 | ETA:0:00:00 | Loss:0.0734831239204169 | top1:0.9152336448598131\n",
      "\n",
      "Epoch: [248 | 300] LR: 0.000157\n",
      "803/803 Data:0.010 | Batch:0.800 | Total:0:10:26 | ETA:0:00:01 | Loss:0.019088360944883597 | top1:0.9781561720698254\n",
      "201/201 Data:0.010 | Batch:0.159 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07293716865907948 | top1:0.9166666666666666\n",
      "\n",
      "Epoch: [249 | 300] LR: 0.000157\n",
      "803/803 Data:0.010 | Batch:0.789 | Total:0:10:26 | ETA:0:00:01 | Loss:0.018672360000612814 | top1:0.9788263715710723\n",
      "201/201 Data:0.010 | Batch:0.156 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07265718814647086 | top1:0.9170716510903427\n",
      "\n",
      "Epoch: [250 | 300] LR: 0.000156\n",
      "803/803 Data:0.011 | Batch:0.805 | Total:0:10:24 | ETA:0:00:01 | Loss:0.01838204847425054 | top1:0.9790913341645885\n",
      "201/201 Data:0.010 | Batch:0.158 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07004579701156259 | top1:0.9193146417445482\n",
      "\n",
      "Epoch: [251 | 300] LR: 0.000156\n",
      "803/803 Data:0.010 | Batch:0.796 | Total:0:10:23 | ETA:0:00:01 | Loss:0.01972682051820172 | top1:0.9778522443890274\n",
      "201/201 Data:0.010 | Batch:0.145 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07277855403790964 | top1:0.91601246105919\n",
      "\n",
      "Epoch: [252 | 300] LR: 0.000156\n",
      "803/803 Data:0.010 | Batch:0.783 | Total:0:10:23 | ETA:0:00:01 | Loss:0.018079289853830643 | top1:0.9796134663341646\n",
      "201/201 Data:0.010 | Batch:0.161 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07118940918105784 | top1:0.9177881619937694\n",
      "\n",
      "Epoch: [253 | 300] LR: 0.000156\n",
      "803/803 Data:0.010 | Batch:0.772 | Total:0:10:24 | ETA:0:00:01 | Loss:0.01848962481859131 | top1:0.9791848503740649\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07226340755402484 | top1:0.9166666666666666\n",
      "\n",
      "Epoch: [254 | 300] LR: 0.000155\n",
      "803/803 Data:0.011 | Batch:0.786 | Total:0:10:22 | ETA:0:00:01 | Loss:0.018389530327784228 | top1:0.9792861596009975\n",
      "201/201 Data:0.010 | Batch:0.157 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07032432931466637 | top1:0.9188473520249221\n",
      "\n",
      "Epoch: [255 | 300] LR: 0.000155\n",
      "803/803 Data:0.010 | Batch:0.798 | Total:0:10:24 | ETA:0:00:01 | Loss:0.01875090925980991 | top1:0.9787562344139651\n",
      "201/201 Data:0.010 | Batch:0.158 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07115935651684105 | top1:0.9181619937694704\n",
      "\n",
      "Epoch: [256 | 300] LR: 0.000155\n",
      "803/803 Data:0.009 | Batch:0.805 | Total:0:10:24 | ETA:0:00:01 | Loss:0.01853312729791671 | top1:0.9791147132169576\n",
      "201/201 Data:0.012 | Batch:0.149 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07152192568017686 | top1:0.9174143302180685\n",
      "\n",
      "Epoch: [257 | 300] LR: 0.000154\n",
      "803/803 Data:0.011 | Batch:0.712 | Total:0:10:25 | ETA:0:00:01 | Loss:0.018469129141363224 | top1:0.9790913341645885\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:46 | ETA:0:00:00 | Loss:0.07098987905128723 | top1:0.9182242990654206\n",
      "\n",
      "Epoch: [258 | 300] LR: 0.000154\n",
      "803/803 Data:0.010 | Batch:0.786 | Total:0:10:33 | ETA:0:00:01 | Loss:0.01853931674552603 | top1:0.979231608478803\n",
      "201/201 Data:0.010 | Batch:0.134 | Total:0:00:44 | ETA:0:00:00 | Loss:0.07034995760594573 | top1:0.9189408099688473\n",
      "\n",
      "Epoch: [259 | 300] LR: 0.000154\n",
      "803/803 Data:0.014 | Batch:0.791 | Total:0:10:30 | ETA:0:00:01 | Loss:0.018501218259923898 | top1:0.9792861596009975\n",
      "201/201 Data:0.014 | Batch:0.169 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07198424429351295 | top1:0.9172585669781932\n",
      "\n",
      "Epoch: [260 | 300] LR: 0.000153\n",
      "803/803 Data:0.011 | Batch:0.784 | Total:0:10:25 | ETA:0:00:01 | Loss:0.01818235415146347 | top1:0.9794342269326684\n",
      "201/201 Data:0.015 | Batch:0.151 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07503471421861203 | top1:0.9140498442367602\n",
      "\n",
      "Epoch: [261 | 300] LR: 0.000153\n",
      "803/803 Data:0.010 | Batch:0.816 | Total:0:10:27 | ETA:0:00:01 | Loss:0.01809574659306154 | top1:0.9793796758104738\n",
      "201/201 Data:0.012 | Batch:0.164 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07272294952861988 | top1:0.9163862928348909\n",
      "\n",
      "Epoch: [262 | 300] LR: 0.000153\n",
      "803/803 Data:0.011 | Batch:0.788 | Total:0:10:27 | ETA:0:00:01 | Loss:0.018644099285407455 | top1:0.9788731296758105\n",
      "201/201 Data:0.010 | Batch:0.151 | Total:0:00:47 | ETA:0:00:00 | Loss:0.07398190477926783 | top1:0.9146728971962617\n",
      "\n",
      "Epoch: [263 | 300] LR: 0.000152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803/803 Data:0.011 | Batch:0.773 | Total:0:10:20 | ETA:0:00:01 | Loss:0.018112806680317887 | top1:0.9797927057356608\n",
      "201/201 Data:0.016 | Batch:0.170 | Total:0:00:47 | ETA:0:00:00 | Loss:0.06950906964013139 | top1:0.920404984423676\n",
      "\n",
      "Epoch: [264 | 300] LR: 0.000152\n",
      "803/803 Data:0.011 | Batch:0.813 | Total:0:10:27 | ETA:0:00:01 | Loss:0.01900968282147336 | top1:0.9783743765586035\n",
      "201/201 Data:0.010 | Batch:0.150 | Total:0:00:47 | ETA:0:00:00 | Loss:0.06781050686106503 | top1:0.9212149532710281\n",
      "\n",
      "Epoch: [265 | 300] LR: 0.000152\n",
      "803/803 Data:0.021 | Batch:0.721 | Total:0:10:20 | ETA:0:00:01 | Loss:0.018461933472072426 | top1:0.9793017456359102\n",
      "201/201 Data:0.017 | Batch:0.164 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07342046785670278 | top1:0.9164174454828661\n",
      "\n",
      "Epoch: [266 | 300] LR: 0.000151\n",
      "803/803 Data:0.010 | Batch:0.774 | Total:0:10:22 | ETA:0:00:01 | Loss:0.018312809543081437 | top1:0.9795121571072319\n",
      "201/201 Data:0.010 | Batch:0.165 | Total:0:00:47 | ETA:0:00:00 | Loss:0.06972018910159945 | top1:0.9196884735202492\n",
      "\n",
      "Epoch: [267 | 300] LR: 0.000151\n",
      "803/803 Data:0.016 | Batch:0.802 | Total:0:10:23 | ETA:0:00:01 | Loss:0.018282206873992977 | top1:0.9795199501246883\n",
      "201/201 Data:0.018 | Batch:0.132 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07336452761635974 | top1:0.9153582554517133\n",
      "\n",
      "Epoch: [268 | 300] LR: 0.000151\n",
      "803/803 Data:0.023 | Batch:0.801 | Total:0:10:31 | ETA:0:00:01 | Loss:0.0190436225725591 | top1:0.9785692019950125\n",
      "201/201 Data:0.037 | Batch:0.290 | Total:0:00:46 | ETA:0:00:00 | Loss:0.07208769451420627 | top1:0.916791277258567\n",
      "\n",
      "Epoch: [269 | 300] LR: 0.000150\n",
      "803/803 Data:0.027 | Batch:0.706 | Total:0:10:41 | ETA:0:00:01 | Loss:0.018662161992510383 | top1:0.9789043017456359\n",
      "201/201 Data:0.024 | Batch:0.186 | Total:0:00:55 | ETA:0:00:00 | Loss:0.07432784849888067 | top1:0.914797507788162\n",
      "\n",
      "Epoch: [270 | 300] LR: 0.000150\n",
      "803/803 Data:0.031 | Batch:0.774 | Total:0:10:27 | ETA:0:00:01 | Loss:0.01822782683288133 | top1:0.9796446384039901\n",
      "201/201 Data:0.011 | Batch:0.167 | Total:0:00:50 | ETA:0:00:00 | Loss:0.0725144440186358 | top1:0.9169781931464175\n",
      "\n",
      "Epoch: [271 | 300] LR: 0.000149\n",
      "803/803 Data:0.016 | Batch:0.802 | Total:0:10:24 | ETA:0:00:01 | Loss:0.019045087623530698 | top1:0.9784445137157107\n",
      "201/201 Data:0.021 | Batch:0.145 | Total:0:00:50 | ETA:0:00:00 | Loss:0.07217425067337503 | top1:0.9171028037383178\n",
      "\n",
      "Epoch: [272 | 300] LR: 0.000149\n",
      "803/803 Data:0.033 | Batch:0.765 | Total:0:10:21 | ETA:0:00:01 | Loss:0.01866245248560803 | top1:0.9789588528678305\n",
      "201/201 Data:0.040 | Batch:0.224 | Total:0:00:54 | ETA:0:00:00 | Loss:0.07200308114383079 | top1:0.9175700934579439\n",
      "\n",
      "Epoch: [273 | 300] LR: 0.000148\n",
      "803/803 Data:0.011 | Batch:0.789 | Total:0:10:21 | ETA:0:00:01 | Loss:0.01847425660413418 | top1:0.979036783042394\n",
      "201/201 Data:0.030 | Batch:0.193 | Total:0:00:51 | ETA:0:00:00 | Loss:0.07416596949286179 | top1:0.9153582554517133\n",
      "\n",
      "Epoch: [274 | 300] LR: 0.000148\n",
      "803/803 Data:0.020 | Batch:0.753 | Total:0:10:25 | ETA:0:00:01 | Loss:0.018552356694770997 | top1:0.9790445760598504\n",
      "201/201 Data:0.029 | Batch:0.149 | Total:0:00:50 | ETA:0:00:00 | Loss:0.07383479399380283 | top1:0.9155451713395638\n",
      "\n",
      "Epoch: [275 | 300] LR: 0.000148\n",
      "803/803 Data:0.035 | Batch:0.622 | Total:0:10:23 | ETA:0:00:01 | Loss:0.01854645786102933 | top1:0.9792238154613466\n",
      "201/201 Data:0.019 | Batch:0.191 | Total:0:00:52 | ETA:0:00:00 | Loss:0.07342580436638954 | top1:0.9157009345794392\n",
      "\n",
      "Epoch: [276 | 300] LR: 0.000147\n",
      "803/803 Data:0.032 | Batch:0.776 | Total:0:10:28 | ETA:0:00:01 | Loss:0.01823941044575602 | top1:0.9795667082294265\n",
      "201/201 Data:0.021 | Batch:0.173 | Total:0:00:49 | ETA:0:00:00 | Loss:0.07275560738118453 | top1:0.9160436137071651\n",
      "\n",
      "Epoch: [277 | 300] LR: 0.000147\n",
      "803/803 Data:0.018 | Batch:0.740 | Total:0:10:17 | ETA:0:00:01 | Loss:0.0185084103413148 | top1:0.9792939526184539\n",
      "201/201 Data:0.010 | Batch:0.156 | Total:0:00:50 | ETA:0:00:00 | Loss:0.07280107384019552 | top1:0.9162305295950156\n",
      "\n",
      "Epoch: [278 | 300] LR: 0.000146\n",
      "803/803 Data:0.016 | Batch:0.780 | Total:0:10:21 | ETA:0:00:01 | Loss:0.018515719960738634 | top1:0.9791614713216957\n",
      "201/201 Data:0.013 | Batch:0.149 | Total:0:00:48 | ETA:0:00:00 | Loss:0.07431318501352893 | top1:0.9154517133956386\n",
      "\n",
      "Epoch: [279 | 300] LR: 0.000146\n",
      "803/803 Data:0.021 | Batch:0.715 | Total:0:10:22 | ETA:0:00:01 | Loss:0.01878606881063775 | top1:0.9788497506234414\n",
      "201/201 Data:0.016 | Batch:0.134 | Total:0:00:54 | ETA:0:00:00 | Loss:0.07554852149661084 | top1:0.913613707165109\n",
      "\n",
      "Epoch: [280 | 300] LR: 0.000145\n",
      "803/803 Data:0.032 | Batch:0.734 | Total:0:10:15 | ETA:0:00:01 | Loss:0.018485597526045526 | top1:0.9793173316708229\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):    \n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
