{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained =  './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.004\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_style2/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:1.3000 | MainLoss:1.2558 | Alpha:0.0230 | SPLoss:0.0377 | CLSLoss:1.8819 | top1:51.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7688 | MainLoss:0.7688 | SPLoss:0.0623 | CLSLoss:1.8606 | top1:52.3846 | AUROC:0.5340\n",
      "Test | 161/20 | Loss:0.0166 | MainLoss:0.0166 | SPLoss:0.0623 | CLSLoss:1.8606 | top1:99.9502 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.005200\n",
      "Train | 20/20 | Loss:0.7750 | MainLoss:0.7307 | Alpha:0.0230 | SPLoss:0.0752 | CLSLoss:1.8508 | top1:52.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7102 | MainLoss:0.7102 | SPLoss:0.0851 | CLSLoss:1.8416 | top1:53.0513 | AUROC:0.5416\n",
      "Test | 161/20 | Loss:0.0480 | MainLoss:0.0480 | SPLoss:0.0851 | CLSLoss:1.8416 | top1:99.9502 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.006400\n",
      "Train | 20/20 | Loss:0.7492 | MainLoss:0.7049 | Alpha:0.0230 | SPLoss:0.0923 | CLSLoss:1.8337 | top1:52.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6973 | MainLoss:0.6973 | SPLoss:0.0987 | CLSLoss:1.8253 | top1:53.5385 | AUROC:0.5486\n",
      "Test | 161/20 | Loss:0.0792 | MainLoss:0.0792 | SPLoss:0.0987 | CLSLoss:1.8253 | top1:99.9595 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.007600\n",
      "Train | 20/20 | Loss:0.7410 | MainLoss:0.6939 | Alpha:0.0245 | SPLoss:0.1028 | CLSLoss:1.8171 | top1:53.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6921 | MainLoss:0.6921 | SPLoss:0.1075 | CLSLoss:1.8082 | top1:53.8718 | AUROC:0.5552\n",
      "Test | 161/20 | Loss:0.1016 | MainLoss:0.1016 | SPLoss:0.1075 | CLSLoss:1.8082 | top1:99.9626 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.008800\n",
      "Train | 20/20 | Loss:0.7343 | MainLoss:0.6885 | Alpha:0.0240 | SPLoss:0.1109 | CLSLoss:1.7995 | top1:55.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6892 | MainLoss:0.6892 | SPLoss:0.1135 | CLSLoss:1.7906 | top1:54.4103 | AUROC:0.5622\n",
      "Test | 161/20 | Loss:0.1148 | MainLoss:0.1148 | SPLoss:0.1135 | CLSLoss:1.7906 | top1:99.9626 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.010000\n",
      "Train | 20/20 | Loss:0.7292 | MainLoss:0.6853 | Alpha:0.0232 | SPLoss:0.1159 | CLSLoss:1.7821 | top1:55.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6868 | MainLoss:0.6868 | SPLoss:0.1190 | CLSLoss:1.7723 | top1:54.9872 | AUROC:0.5685\n",
      "Test | 161/20 | Loss:0.1212 | MainLoss:0.1212 | SPLoss:0.1190 | CLSLoss:1.7723 | top1:99.9688 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.011200\n",
      "Train | 20/20 | Loss:0.7279 | MainLoss:0.6857 | Alpha:0.0224 | SPLoss:0.1223 | CLSLoss:1.7633 | top1:54.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6846 | MainLoss:0.6846 | SPLoss:0.1247 | CLSLoss:1.7528 | top1:55.4231 | AUROC:0.5776\n",
      "Test | 161/20 | Loss:0.1289 | MainLoss:0.1289 | SPLoss:0.1247 | CLSLoss:1.7528 | top1:99.9720 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.012400\n",
      "Train | 20/20 | Loss:0.7215 | MainLoss:0.6796 | Alpha:0.0225 | SPLoss:0.1254 | CLSLoss:1.7438 | top1:56.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6822 | MainLoss:0.6822 | SPLoss:0.1269 | CLSLoss:1.7329 | top1:56.0641 | AUROC:0.5880\n",
      "Test | 161/20 | Loss:0.1191 | MainLoss:0.1191 | SPLoss:0.1269 | CLSLoss:1.7329 | top1:99.9595 | AUROC:1.0000\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.013600\n",
      "Train | 20/20 | Loss:0.7238 | MainLoss:0.6798 | Alpha:0.0238 | SPLoss:0.1288 | CLSLoss:1.7220 | top1:56.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6789 | MainLoss:0.6789 | SPLoss:0.1318 | CLSLoss:1.7097 | top1:57.0513 | AUROC:0.5980\n",
      "Test | 161/20 | Loss:0.1105 | MainLoss:0.1105 | SPLoss:0.1318 | CLSLoss:1.7097 | top1:99.9595 | AUROC:1.0000\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.014800\n",
      "Train | 20/20 | Loss:0.7208 | MainLoss:0.6770 | Alpha:0.0239 | SPLoss:0.1337 | CLSLoss:1.6983 | top1:58.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6758 | MainLoss:0.6758 | SPLoss:0.1368 | CLSLoss:1.6845 | top1:57.6410 | AUROC:0.6089\n",
      "Test | 161/20 | Loss:0.1013 | MainLoss:0.1013 | SPLoss:0.1368 | CLSLoss:1.6845 | top1:99.9470 | AUROC:1.0000\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:0.7166 | MainLoss:0.6750 | Alpha:0.0229 | SPLoss:0.1421 | CLSLoss:1.6725 | top1:58.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6719 | MainLoss:0.6719 | SPLoss:0.1443 | CLSLoss:1.6597 | top1:58.3077 | AUROC:0.6215\n",
      "Test | 161/20 | Loss:0.0956 | MainLoss:0.0956 | SPLoss:0.1443 | CLSLoss:1.6597 | top1:99.9252 | AUROC:1.0000\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:0.7061 | MainLoss:0.6641 | Alpha:0.0234 | SPLoss:0.1466 | CLSLoss:1.6490 | top1:60.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6658 | MainLoss:0.6658 | SPLoss:0.1521 | CLSLoss:1.6361 | top1:59.3590 | AUROC:0.6368\n",
      "Test | 161/20 | Loss:0.0802 | MainLoss:0.0802 | SPLoss:0.1521 | CLSLoss:1.6361 | top1:99.9190 | AUROC:1.0000\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:0.6979 | MainLoss:0.6532 | Alpha:0.0251 | SPLoss:0.1548 | CLSLoss:1.6249 | top1:61.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6601 | MainLoss:0.6601 | SPLoss:0.1609 | CLSLoss:1.6117 | top1:60.3846 | AUROC:0.6511\n",
      "Test | 161/20 | Loss:0.0593 | MainLoss:0.0593 | SPLoss:0.1609 | CLSLoss:1.6117 | top1:99.8536 | AUROC:1.0000\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:0.6853 | MainLoss:0.6449 | Alpha:0.0228 | SPLoss:0.1705 | CLSLoss:1.6016 | top1:63.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6498 | MainLoss:0.6498 | SPLoss:0.1845 | CLSLoss:1.5880 | top1:61.7308 | AUROC:0.6694\n",
      "Test | 161/20 | Loss:0.0578 | MainLoss:0.0578 | SPLoss:0.1845 | CLSLoss:1.5880 | top1:99.8318 | AUROC:1.0000\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:0.6755 | MainLoss:0.6351 | Alpha:0.0227 | SPLoss:0.1959 | CLSLoss:1.5768 | top1:63.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6373 | MainLoss:0.6373 | SPLoss:0.2068 | CLSLoss:1.5673 | top1:63.8590 | AUROC:0.6904\n",
      "Test | 161/20 | Loss:0.0417 | MainLoss:0.0417 | SPLoss:0.2068 | CLSLoss:1.5673 | top1:99.8442 | AUROC:1.0000\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.015999\n",
      "Train | 20/20 | Loss:0.6650 | MainLoss:0.6212 | Alpha:0.0247 | SPLoss:0.2196 | CLSLoss:1.5556 | top1:65.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6226 | MainLoss:0.6226 | SPLoss:0.2410 | CLSLoss:1.5421 | top1:65.3205 | AUROC:0.7116\n",
      "Test | 161/20 | Loss:0.0420 | MainLoss:0.0420 | SPLoss:0.2410 | CLSLoss:1.5421 | top1:99.7944 | AUROC:1.0000\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.015999\n",
      "Train | 20/20 | Loss:0.6516 | MainLoss:0.6120 | Alpha:0.0221 | SPLoss:0.2587 | CLSLoss:1.5322 | top1:66.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6063 | MainLoss:0.6063 | SPLoss:0.2818 | CLSLoss:1.5204 | top1:66.8333 | AUROC:0.7332\n",
      "Test | 161/20 | Loss:0.0447 | MainLoss:0.0447 | SPLoss:0.2818 | CLSLoss:1.5204 | top1:99.7290 | AUROC:1.0000\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.015999\n",
      "Train | 20/20 | Loss:0.6273 | MainLoss:0.5836 | Alpha:0.0241 | SPLoss:0.3014 | CLSLoss:1.5112 | top1:69.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5888 | MainLoss:0.5888 | SPLoss:0.3256 | CLSLoss:1.5003 | top1:68.7308 | AUROC:0.7557\n",
      "Test | 161/20 | Loss:0.0336 | MainLoss:0.0336 | SPLoss:0.3256 | CLSLoss:1.5003 | top1:99.6760 | AUROC:1.0000\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.015998\n",
      "Train | 20/20 | Loss:0.6064 | MainLoss:0.5615 | Alpha:0.0244 | SPLoss:0.3509 | CLSLoss:1.4895 | top1:70.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5684 | MainLoss:0.5684 | SPLoss:0.3817 | CLSLoss:1.4783 | top1:70.5256 | AUROC:0.7781\n",
      "Test | 161/20 | Loss:0.0326 | MainLoss:0.0326 | SPLoss:0.3817 | CLSLoss:1.4783 | top1:99.5919 | AUROC:1.0000\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.015997\n",
      "Train | 20/20 | Loss:0.5804 | MainLoss:0.5353 | Alpha:0.0239 | SPLoss:0.4150 | CLSLoss:1.4681 | top1:73.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5416 | MainLoss:0.5416 | SPLoss:0.4553 | CLSLoss:1.4553 | top1:73.1282 | AUROC:0.8020\n",
      "Test | 161/20 | Loss:0.0335 | MainLoss:0.0335 | SPLoss:0.4553 | CLSLoss:1.4553 | top1:99.5639 | AUROC:1.0000\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.015997\n",
      "Train | 20/20 | Loss:0.5484 | MainLoss:0.5025 | Alpha:0.0237 | SPLoss:0.4874 | CLSLoss:1.4473 | top1:75.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5173 | MainLoss:0.5173 | SPLoss:0.5281 | CLSLoss:1.4363 | top1:74.8077 | AUROC:0.8239\n",
      "Test | 161/20 | Loss:0.0323 | MainLoss:0.0323 | SPLoss:0.5281 | CLSLoss:1.4363 | top1:99.4393 | AUROC:1.0000\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.015996\n",
      "Train | 20/20 | Loss:0.5169 | MainLoss:0.4729 | Alpha:0.0220 | SPLoss:0.5707 | CLSLoss:1.4283 | top1:77.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4870 | MainLoss:0.4870 | SPLoss:0.6208 | CLSLoss:1.4170 | top1:76.7436 | AUROC:0.8468\n",
      "Test | 161/20 | Loss:0.0389 | MainLoss:0.0389 | SPLoss:0.6208 | CLSLoss:1.4170 | top1:99.1558 | AUROC:1.0000\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.015995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.4847 | MainLoss:0.4402 | Alpha:0.0214 | SPLoss:0.6638 | CLSLoss:1.4103 | top1:79.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4580 | MainLoss:0.4580 | SPLoss:0.7141 | CLSLoss:1.4007 | top1:78.7692 | AUROC:0.8682\n",
      "Test | 161/20 | Loss:0.0445 | MainLoss:0.0445 | SPLoss:0.7141 | CLSLoss:1.4007 | top1:98.7414 | AUROC:1.0000\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.015994\n",
      "Train | 20/20 | Loss:0.7280 | MainLoss:0.5721 | Alpha:0.0236 | SPLoss:5.2359 | CLSLoss:1.3785 | top1:71.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5656 | MainLoss:0.5656 | SPLoss:7.6445 | CLSLoss:1.3646 | top1:70.4359 | AUROC:0.7792\n",
      "Test | 161/20 | Loss:0.1842 | MainLoss:0.1842 | SPLoss:7.6445 | CLSLoss:1.3646 | top1:93.4112 | AUROC:0.9993\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.015993\n",
      "Train | 20/20 | Loss:0.7267 | MainLoss:0.5151 | Alpha:0.0236 | SPLoss:7.6199 | CLSLoss:1.3580 | top1:74.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5296 | MainLoss:0.5296 | SPLoss:7.5991 | CLSLoss:1.3515 | top1:73.9231 | AUROC:0.8165\n",
      "Test | 161/20 | Loss:0.2027 | MainLoss:0.2027 | SPLoss:7.5991 | CLSLoss:1.3515 | top1:90.9128 | AUROC:0.9990\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.015992\n",
      "Train | 20/20 | Loss:0.7104 | MainLoss:0.4936 | Alpha:0.0243 | SPLoss:7.5878 | CLSLoss:1.3418 | top1:75.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4852 | MainLoss:0.4852 | SPLoss:7.5727 | CLSLoss:1.3309 | top1:76.9359 | AUROC:0.8455\n",
      "Test | 161/20 | Loss:0.2037 | MainLoss:0.2037 | SPLoss:7.5727 | CLSLoss:1.3309 | top1:90.7913 | AUROC:0.9987\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.015991\n",
      "Train | 20/20 | Loss:0.6586 | MainLoss:0.4394 | Alpha:0.0247 | SPLoss:7.5597 | CLSLoss:1.3239 | top1:79.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4530 | MainLoss:0.4530 | SPLoss:7.5446 | CLSLoss:1.3141 | top1:78.7564 | AUROC:0.8683\n",
      "Test | 161/20 | Loss:0.2203 | MainLoss:0.2203 | SPLoss:7.5446 | CLSLoss:1.3141 | top1:89.7414 | AUROC:0.9984\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.015990\n",
      "Train | 20/20 | Loss:0.6075 | MainLoss:0.4089 | Alpha:0.0224 | SPLoss:7.5404 | CLSLoss:1.3061 | top1:81.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4248 | MainLoss:0.4248 | SPLoss:7.5398 | CLSLoss:1.2985 | top1:80.6538 | AUROC:0.8870\n",
      "Test | 161/20 | Loss:0.2822 | MainLoss:0.2822 | SPLoss:7.5398 | CLSLoss:1.2985 | top1:86.4424 | AUROC:0.9973\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.015989\n",
      "Train | 20/20 | Loss:0.5875 | MainLoss:0.3809 | Alpha:0.0234 | SPLoss:7.5354 | CLSLoss:1.2887 | top1:83.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4027 | MainLoss:0.4027 | SPLoss:7.5244 | CLSLoss:1.2796 | top1:82.0385 | AUROC:0.9002\n",
      "Test | 161/20 | Loss:0.3452 | MainLoss:0.3452 | SPLoss:7.5244 | CLSLoss:1.2796 | top1:83.2492 | AUROC:0.9968\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.015987\n",
      "Train | 20/20 | Loss:0.5616 | MainLoss:0.3547 | Alpha:0.0229 | SPLoss:7.7563 | CLSLoss:1.2726 | top1:84.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3969 | MainLoss:0.3969 | SPLoss:10.6859 | CLSLoss:1.2613 | top1:81.7692 | AUROC:0.9081\n",
      "Test | 161/20 | Loss:0.3138 | MainLoss:0.3138 | SPLoss:10.6859 | CLSLoss:1.2613 | top1:85.3427 | AUROC:0.9853\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.015986\n",
      "Train | 20/20 | Loss:0.6253 | MainLoss:0.3557 | Alpha:0.0226 | SPLoss:10.6979 | CLSLoss:1.2540 | top1:84.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3678 | MainLoss:0.3678 | SPLoss:10.6691 | CLSLoss:1.2455 | top1:83.6154 | AUROC:0.9202\n",
      "Test | 161/20 | Loss:0.3690 | MainLoss:0.3690 | SPLoss:10.6691 | CLSLoss:1.2455 | top1:82.8598 | AUROC:0.9818\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.015984\n",
      "Train | 20/20 | Loss:0.5979 | MainLoss:0.3274 | Alpha:0.0227 | SPLoss:10.6600 | CLSLoss:1.2391 | top1:85.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3467 | MainLoss:0.3467 | SPLoss:10.6357 | CLSLoss:1.2299 | top1:84.7949 | AUROC:0.9298\n",
      "Test | 161/20 | Loss:0.4364 | MainLoss:0.4364 | SPLoss:10.6357 | CLSLoss:1.2299 | top1:80.4984 | AUROC:0.9783\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.015983\n",
      "Train | 20/20 | Loss:0.5497 | MainLoss:0.2843 | Alpha:0.0224 | SPLoss:10.6095 | CLSLoss:1.2239 | top1:88.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3296 | MainLoss:0.3296 | SPLoss:10.5757 | CLSLoss:1.2151 | top1:85.8462 | AUROC:0.9362\n",
      "Test | 161/20 | Loss:0.5009 | MainLoss:0.5009 | SPLoss:10.5757 | CLSLoss:1.2151 | top1:78.0592 | AUROC:0.9763\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.015981\n",
      "Train | 20/20 | Loss:0.5378 | MainLoss:0.2705 | Alpha:0.0227 | SPLoss:10.5500 | CLSLoss:1.2087 | top1:88.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3141 | MainLoss:0.3141 | SPLoss:10.5129 | CLSLoss:1.2000 | top1:86.7564 | AUROC:0.9412\n",
      "Test | 161/20 | Loss:0.7026 | MainLoss:0.7026 | SPLoss:10.5129 | CLSLoss:1.2000 | top1:71.4330 | AUROC:0.9667\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.015979\n",
      "Train | 20/20 | Loss:0.5194 | MainLoss:0.2566 | Alpha:0.0225 | SPLoss:10.4926 | CLSLoss:1.1947 | top1:89.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2947 | MainLoss:0.2947 | SPLoss:10.4522 | CLSLoss:1.1849 | top1:87.3718 | AUROC:0.9480\n",
      "Test | 161/20 | Loss:0.6161 | MainLoss:0.6161 | SPLoss:10.4522 | CLSLoss:1.1849 | top1:74.3489 | AUROC:0.9675\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.015977\n",
      "Train | 20/20 | Loss:0.5095 | MainLoss:0.2387 | Alpha:0.0233 | SPLoss:10.4192 | CLSLoss:1.1797 | top1:89.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2884 | MainLoss:0.2884 | SPLoss:10.3756 | CLSLoss:1.1699 | top1:88.0897 | AUROC:0.9507\n",
      "Test | 161/20 | Loss:0.7486 | MainLoss:0.7486 | SPLoss:10.3756 | CLSLoss:1.1699 | top1:70.8692 | AUROC:0.9669\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.015975\n",
      "Train | 20/20 | Loss:0.5081 | MainLoss:0.2346 | Alpha:0.0238 | SPLoss:10.3470 | CLSLoss:1.1629 | top1:90.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2803 | MainLoss:0.2803 | SPLoss:10.3137 | CLSLoss:1.1543 | top1:88.0769 | AUROC:0.9562\n",
      "Test | 161/20 | Loss:0.6536 | MainLoss:0.6536 | SPLoss:10.3137 | CLSLoss:1.1543 | top1:74.0997 | AUROC:0.9669\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.015973\n",
      "Train | 20/20 | Loss:0.4726 | MainLoss:0.2027 | Alpha:0.0236 | SPLoss:10.2802 | CLSLoss:1.1480 | top1:91.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2766 | MainLoss:0.2766 | SPLoss:10.2381 | CLSLoss:1.1411 | top1:88.9744 | AUROC:0.9584\n",
      "Test | 161/20 | Loss:0.9601 | MainLoss:0.9601 | SPLoss:10.2381 | CLSLoss:1.1411 | top1:67.2555 | AUROC:0.9616\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.015971\n",
      "Train | 20/20 | Loss:0.4908 | MainLoss:0.2261 | Alpha:0.0233 | SPLoss:10.2073 | CLSLoss:1.1315 | top1:90.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2569 | MainLoss:0.2569 | SPLoss:10.1698 | CLSLoss:1.1225 | top1:89.4872 | AUROC:0.9617\n",
      "Test | 161/20 | Loss:0.7577 | MainLoss:0.7577 | SPLoss:10.1698 | CLSLoss:1.1225 | top1:71.4455 | AUROC:0.9611\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.015969\n",
      "Train | 20/20 | Loss:0.4776 | MainLoss:0.1947 | Alpha:0.0252 | SPLoss:10.1323 | CLSLoss:1.1166 | top1:92.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2521 | MainLoss:0.2521 | SPLoss:10.0783 | CLSLoss:1.1072 | top1:89.6410 | AUROC:0.9634\n",
      "Test | 161/20 | Loss:0.8837 | MainLoss:0.8837 | SPLoss:10.0783 | CLSLoss:1.1072 | top1:69.2025 | AUROC:0.9569\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.015967\n",
      "Train | 20/20 | Loss:0.4497 | MainLoss:0.1884 | Alpha:0.0234 | SPLoss:10.0513 | CLSLoss:1.1002 | top1:92.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2779 | MainLoss:0.2779 | SPLoss:10.0333 | CLSLoss:1.0935 | top1:88.9359 | AUROC:0.9648\n",
      "Test | 161/20 | Loss:0.6790 | MainLoss:0.6790 | SPLoss:10.0333 | CLSLoss:1.0935 | top1:74.4891 | AUROC:0.9588\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.015964\n",
      "Train | 20/20 | Loss:0.4528 | MainLoss:0.1874 | Alpha:0.0239 | SPLoss:9.9926 | CLSLoss:1.0859 | top1:92.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2407 | MainLoss:0.2407 | SPLoss:9.9491 | CLSLoss:1.0784 | top1:90.3205 | AUROC:0.9670\n",
      "Test | 161/20 | Loss:0.9960 | MainLoss:0.9960 | SPLoss:9.9491 | CLSLoss:1.0784 | top1:67.4112 | AUROC:0.9487\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.015962\n",
      "Train | 20/20 | Loss:0.4110 | MainLoss:0.1640 | Alpha:0.0225 | SPLoss:9.9235 | CLSLoss:1.0740 | top1:93.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2439 | MainLoss:0.2439 | SPLoss:9.8972 | CLSLoss:1.0658 | top1:90.0897 | AUROC:0.9685\n",
      "Test | 161/20 | Loss:0.8978 | MainLoss:0.8978 | SPLoss:9.8972 | CLSLoss:1.0658 | top1:69.6355 | AUROC:0.9392\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.015960\n",
      "Train | 20/20 | Loss:0.4113 | MainLoss:0.1641 | Alpha:0.0226 | SPLoss:9.8682 | CLSLoss:1.0595 | top1:93.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2377 | MainLoss:0.2377 | SPLoss:9.8332 | CLSLoss:1.0535 | top1:90.6923 | AUROC:0.9702\n",
      "Test | 161/20 | Loss:0.9744 | MainLoss:0.9744 | SPLoss:9.8332 | CLSLoss:1.0534 | top1:68.4548 | AUROC:0.9401\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.015957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.4274 | MainLoss:0.1703 | Alpha:0.0237 | SPLoss:9.7978 | CLSLoss:1.0451 | top1:93.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2317 | MainLoss:0.2317 | SPLoss:9.7610 | CLSLoss:1.0374 | top1:90.7692 | AUROC:0.9721\n",
      "Test | 161/20 | Loss:0.8979 | MainLoss:0.8979 | SPLoss:9.7610 | CLSLoss:1.0374 | top1:69.7508 | AUROC:0.9443\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.015954\n",
      "Train | 20/20 | Loss:0.3944 | MainLoss:0.1466 | Alpha:0.0230 | SPLoss:9.7218 | CLSLoss:1.0316 | top1:94.3421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2263 | MainLoss:0.2263 | SPLoss:9.6835 | CLSLoss:1.0272 | top1:91.2179 | AUROC:0.9721\n",
      "Test | 161/20 | Loss:1.0917 | MainLoss:1.0917 | SPLoss:9.6835 | CLSLoss:1.0272 | top1:67.2679 | AUROC:0.9412\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.015952\n",
      "Train | 20/20 | Loss:0.4070 | MainLoss:0.1432 | Alpha:0.0247 | SPLoss:9.6504 | CLSLoss:1.0218 | top1:94.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2190 | MainLoss:0.2190 | SPLoss:9.6043 | CLSLoss:1.0136 | top1:91.6026 | AUROC:0.9738\n",
      "Test | 161/20 | Loss:1.1756 | MainLoss:1.1756 | SPLoss:9.6043 | CLSLoss:1.0136 | top1:65.3676 | AUROC:0.9366\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.015949\n",
      "Train | 20/20 | Loss:0.3951 | MainLoss:0.1374 | Alpha:0.0244 | SPLoss:9.5739 | CLSLoss:1.0088 | top1:94.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2183 | MainLoss:0.2183 | SPLoss:9.5256 | CLSLoss:1.0014 | top1:91.7692 | AUROC:0.9741\n",
      "Test | 161/20 | Loss:1.2463 | MainLoss:1.2463 | SPLoss:9.5256 | CLSLoss:1.0014 | top1:64.4081 | AUROC:0.9255\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.015946\n",
      "Train | 20/20 | Loss:0.3823 | MainLoss:0.1368 | Alpha:0.0234 | SPLoss:9.4855 | CLSLoss:0.9948 | top1:95.3421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2168 | MainLoss:0.2168 | SPLoss:9.4528 | CLSLoss:0.9874 | top1:91.7051 | AUROC:0.9747\n",
      "Test | 161/20 | Loss:1.1318 | MainLoss:1.1318 | SPLoss:9.4528 | CLSLoss:0.9874 | top1:66.1122 | AUROC:0.9278\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.015943\n",
      "Train | 20/20 | Loss:0.3662 | MainLoss:0.1255 | Alpha:0.0231 | SPLoss:9.4297 | CLSLoss:0.9835 | top1:95.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2198 | MainLoss:0.2198 | SPLoss:9.3864 | CLSLoss:0.9779 | top1:91.6667 | AUROC:0.9758\n",
      "Test | 161/20 | Loss:1.0882 | MainLoss:1.0882 | SPLoss:9.3864 | CLSLoss:0.9779 | top1:67.8754 | AUROC:0.9422\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.015940\n",
      "Train | 20/20 | Loss:0.3606 | MainLoss:0.1221 | Alpha:0.0231 | SPLoss:9.3654 | CLSLoss:0.9727 | top1:95.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2098 | MainLoss:0.2098 | SPLoss:9.3268 | CLSLoss:0.9672 | top1:92.3846 | AUROC:0.9773\n",
      "Test | 161/20 | Loss:1.2646 | MainLoss:1.2646 | SPLoss:9.3268 | CLSLoss:0.9672 | top1:65.0000 | AUROC:0.9251\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.001594\n",
      "Train | 20/20 | Loss:0.3552 | MainLoss:0.1234 | Alpha:0.0225 | SPLoss:9.3233 | CLSLoss:0.9666 | top1:95.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2121 | MainLoss:0.2121 | SPLoss:9.3194 | CLSLoss:0.9659 | top1:92.2051 | AUROC:0.9770\n",
      "Test | 161/20 | Loss:1.1766 | MainLoss:1.1766 | SPLoss:9.3194 | CLSLoss:0.9659 | top1:66.5234 | AUROC:0.9279\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.001593\n",
      "Train | 20/20 | Loss:0.3316 | MainLoss:0.1040 | Alpha:0.0221 | SPLoss:9.3158 | CLSLoss:0.9654 | top1:95.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2118 | MainLoss:0.2118 | SPLoss:9.3116 | CLSLoss:0.9651 | top1:92.1667 | AUROC:0.9769\n",
      "Test | 161/20 | Loss:1.1878 | MainLoss:1.1878 | SPLoss:9.3116 | CLSLoss:0.9651 | top1:66.3956 | AUROC:0.9287\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.001593\n",
      "Train | 20/20 | Loss:0.3308 | MainLoss:0.0987 | Alpha:0.0226 | SPLoss:9.3074 | CLSLoss:0.9647 | top1:96.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2111 | MainLoss:0.2111 | SPLoss:9.3028 | CLSLoss:0.9643 | top1:92.1538 | AUROC:0.9772\n",
      "Test | 161/20 | Loss:1.2294 | MainLoss:1.2294 | SPLoss:9.3028 | CLSLoss:0.9643 | top1:65.7975 | AUROC:0.9284\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.001593\n",
      "Train | 20/20 | Loss:0.3750 | MainLoss:0.1260 | Alpha:0.0243 | SPLoss:9.2990 | CLSLoss:0.9635 | top1:95.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2145 | MainLoss:0.2145 | SPLoss:9.2964 | CLSLoss:0.9628 | top1:92.1282 | AUROC:0.9771\n",
      "Test | 161/20 | Loss:1.1347 | MainLoss:1.1347 | SPLoss:9.2965 | CLSLoss:0.9628 | top1:67.3209 | AUROC:0.9312\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.001592\n",
      "Train | 20/20 | Loss:0.3603 | MainLoss:0.1156 | Alpha:0.0239 | SPLoss:9.2926 | CLSLoss:0.9622 | top1:95.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2103 | MainLoss:0.2103 | SPLoss:9.2890 | CLSLoss:0.9617 | top1:92.2436 | AUROC:0.9771\n",
      "Test | 161/20 | Loss:1.1890 | MainLoss:1.1890 | SPLoss:9.2890 | CLSLoss:0.9617 | top1:66.3645 | AUROC:0.9298\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.001592\n",
      "Train | 20/20 | Loss:0.3357 | MainLoss:0.1053 | Alpha:0.0225 | SPLoss:9.2847 | CLSLoss:0.9612 | top1:96.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2095 | MainLoss:0.2095 | SPLoss:9.2815 | CLSLoss:0.9608 | top1:92.2436 | AUROC:0.9772\n",
      "Test | 161/20 | Loss:1.2179 | MainLoss:1.2179 | SPLoss:9.2815 | CLSLoss:0.9608 | top1:65.9688 | AUROC:0.9282\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.001592\n",
      "Train | 20/20 | Loss:0.3380 | MainLoss:0.1123 | Alpha:0.0220 | SPLoss:9.2786 | CLSLoss:0.9603 | top1:95.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2084 | MainLoss:0.2084 | SPLoss:9.2743 | CLSLoss:0.9597 | top1:92.2949 | AUROC:0.9775\n",
      "Test | 161/20 | Loss:1.2293 | MainLoss:1.2293 | SPLoss:9.2743 | CLSLoss:0.9597 | top1:65.7321 | AUROC:0.9273\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.001591\n",
      "Train | 20/20 | Loss:0.3281 | MainLoss:0.0909 | Alpha:0.0232 | SPLoss:9.2708 | CLSLoss:0.9595 | top1:96.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2104 | MainLoss:0.2104 | SPLoss:9.2655 | CLSLoss:0.9591 | top1:92.2949 | AUROC:0.9777\n",
      "Test | 161/20 | Loss:1.2204 | MainLoss:1.2204 | SPLoss:9.2655 | CLSLoss:0.9591 | top1:66.0623 | AUROC:0.9286\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.001591\n",
      "Train | 20/20 | Loss:0.3201 | MainLoss:0.0936 | Alpha:0.0222 | SPLoss:9.2622 | CLSLoss:0.9588 | top1:96.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2118 | MainLoss:0.2118 | SPLoss:9.2584 | CLSLoss:0.9585 | top1:92.3077 | AUROC:0.9777\n",
      "Test | 161/20 | Loss:1.2222 | MainLoss:1.2222 | SPLoss:9.2584 | CLSLoss:0.9585 | top1:66.1963 | AUROC:0.9286\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.001591\n",
      "Train | 20/20 | Loss:0.3526 | MainLoss:0.1113 | Alpha:0.0236 | SPLoss:9.2540 | CLSLoss:0.9579 | top1:95.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2112 | MainLoss:0.2112 | SPLoss:9.2496 | CLSLoss:0.9571 | top1:92.2821 | AUROC:0.9777\n",
      "Test | 161/20 | Loss:1.2223 | MainLoss:1.2223 | SPLoss:9.2496 | CLSLoss:0.9571 | top1:66.1713 | AUROC:0.9283\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.001590\n",
      "Train | 20/20 | Loss:0.3667 | MainLoss:0.1122 | Alpha:0.0249 | SPLoss:9.2455 | CLSLoss:0.9563 | top1:95.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2089 | MainLoss:0.2089 | SPLoss:9.2410 | CLSLoss:0.9558 | top1:92.2564 | AUROC:0.9777\n",
      "Test | 161/20 | Loss:1.2490 | MainLoss:1.2490 | SPLoss:9.2409 | CLSLoss:0.9558 | top1:65.7072 | AUROC:0.9270\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.001590\n",
      "Train | 20/20 | Loss:0.3242 | MainLoss:0.0988 | Alpha:0.0221 | SPLoss:9.2377 | CLSLoss:0.9554 | top1:96.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2082 | MainLoss:0.2082 | SPLoss:9.2336 | CLSLoss:0.9550 | top1:92.4359 | AUROC:0.9784\n",
      "Test | 161/20 | Loss:1.2800 | MainLoss:1.2800 | SPLoss:9.2336 | CLSLoss:0.9550 | top1:65.1931 | AUROC:0.9256\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.001589\n",
      "Train | 20/20 | Loss:0.3432 | MainLoss:0.1035 | Alpha:0.0235 | SPLoss:9.2303 | CLSLoss:0.9545 | top1:96.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2109 | MainLoss:0.2109 | SPLoss:9.2269 | CLSLoss:0.9540 | top1:92.2949 | AUROC:0.9778\n",
      "Test | 161/20 | Loss:1.2200 | MainLoss:1.2200 | SPLoss:9.2269 | CLSLoss:0.9540 | top1:66.2305 | AUROC:0.9268\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.001589\n",
      "Train | 20/20 | Loss:0.3096 | MainLoss:0.0890 | Alpha:0.0217 | SPLoss:9.2233 | CLSLoss:0.9538 | top1:97.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2115 | MainLoss:0.2115 | SPLoss:9.2198 | CLSLoss:0.9534 | top1:92.2821 | AUROC:0.9781\n",
      "Test | 161/20 | Loss:1.2340 | MainLoss:1.2340 | SPLoss:9.2198 | CLSLoss:0.9534 | top1:66.1900 | AUROC:0.9278\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.001589\n",
      "Train | 20/20 | Loss:0.3414 | MainLoss:0.1018 | Alpha:0.0236 | SPLoss:9.2160 | CLSLoss:0.9528 | top1:96.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2097 | MainLoss:0.2097 | SPLoss:9.2118 | CLSLoss:0.9525 | top1:92.3718 | AUROC:0.9780\n",
      "Test | 161/20 | Loss:1.2813 | MainLoss:1.2813 | SPLoss:9.2119 | CLSLoss:0.9525 | top1:65.4330 | AUROC:0.9267\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.001588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.3547 | MainLoss:0.1109 | Alpha:0.0240 | SPLoss:9.2075 | CLSLoss:0.9519 | top1:95.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:9.2027 | CLSLoss:0.9511 | top1:92.4231 | AUROC:0.9782\n",
      "Test | 161/20 | Loss:1.2571 | MainLoss:1.2571 | SPLoss:9.2027 | CLSLoss:0.9511 | top1:65.8474 | AUROC:0.9278\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.001588\n",
      "Train | 20/20 | Loss:0.3395 | MainLoss:0.0990 | Alpha:0.0237 | SPLoss:9.1994 | CLSLoss:0.9507 | top1:96.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2092 | MainLoss:0.2092 | SPLoss:9.1948 | CLSLoss:0.9502 | top1:92.5513 | AUROC:0.9779\n",
      "Test | 161/20 | Loss:1.2838 | MainLoss:1.2838 | SPLoss:9.1948 | CLSLoss:0.9502 | top1:65.3707 | AUROC:0.9264\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.001587\n",
      "Train | 20/20 | Loss:0.3413 | MainLoss:0.0934 | Alpha:0.0245 | SPLoss:9.1901 | CLSLoss:0.9497 | top1:96.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2101 | MainLoss:0.2101 | SPLoss:9.1859 | CLSLoss:0.9493 | top1:92.3846 | AUROC:0.9781\n",
      "Test | 161/20 | Loss:1.2698 | MainLoss:1.2698 | SPLoss:9.1859 | CLSLoss:0.9493 | top1:65.7570 | AUROC:0.9281\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.001587\n",
      "Train | 20/20 | Loss:0.3401 | MainLoss:0.0999 | Alpha:0.0237 | SPLoss:9.1825 | CLSLoss:0.9487 | top1:96.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2120 | MainLoss:0.2120 | SPLoss:9.1774 | CLSLoss:0.9481 | top1:92.3205 | AUROC:0.9783\n",
      "Test | 161/20 | Loss:1.2294 | MainLoss:1.2294 | SPLoss:9.1774 | CLSLoss:0.9482 | top1:66.5016 | AUROC:0.9298\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.001586\n",
      "Train | 20/20 | Loss:0.3534 | MainLoss:0.0998 | Alpha:0.0251 | SPLoss:9.1730 | CLSLoss:0.9476 | top1:96.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2120 | MainLoss:0.2120 | SPLoss:9.1683 | CLSLoss:0.9471 | top1:92.3077 | AUROC:0.9783\n",
      "Test | 161/20 | Loss:1.2252 | MainLoss:1.2252 | SPLoss:9.1683 | CLSLoss:0.9471 | top1:66.6449 | AUROC:0.9305\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.001586\n",
      "Train | 20/20 | Loss:0.3433 | MainLoss:0.1087 | Alpha:0.0232 | SPLoss:9.1651 | CLSLoss:0.9467 | top1:96.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:9.1606 | CLSLoss:0.9458 | top1:92.3846 | AUROC:0.9783\n",
      "Test | 161/20 | Loss:1.2413 | MainLoss:1.2413 | SPLoss:9.1606 | CLSLoss:0.9458 | top1:66.2835 | AUROC:0.9297\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.001585\n",
      "Train | 20/20 | Loss:0.3174 | MainLoss:0.0942 | Alpha:0.0221 | SPLoss:9.1570 | CLSLoss:0.9455 | top1:96.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2084 | MainLoss:0.2084 | SPLoss:9.1538 | CLSLoss:0.9451 | top1:92.4744 | AUROC:0.9781\n",
      "Test | 161/20 | Loss:1.3073 | MainLoss:1.3073 | SPLoss:9.1538 | CLSLoss:0.9451 | top1:65.2337 | AUROC:0.9270\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.001585\n",
      "Train | 20/20 | Loss:0.3557 | MainLoss:0.1043 | Alpha:0.0249 | SPLoss:9.1500 | CLSLoss:0.9444 | top1:96.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2080 | MainLoss:0.2080 | SPLoss:9.1449 | CLSLoss:0.9438 | top1:92.4744 | AUROC:0.9785\n",
      "Test | 161/20 | Loss:1.2996 | MainLoss:1.2996 | SPLoss:9.1449 | CLSLoss:0.9438 | top1:65.3925 | AUROC:0.9273\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.001584\n",
      "Train | 20/20 | Loss:0.3240 | MainLoss:0.0897 | Alpha:0.0232 | SPLoss:9.1413 | CLSLoss:0.9434 | top1:96.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2091 | MainLoss:0.2091 | SPLoss:9.1366 | CLSLoss:0.9430 | top1:92.4359 | AUROC:0.9779\n",
      "Test | 161/20 | Loss:1.2972 | MainLoss:1.2972 | SPLoss:9.1366 | CLSLoss:0.9430 | top1:65.5047 | AUROC:0.9276\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.001584\n",
      "Train | 20/20 | Loss:0.3595 | MainLoss:0.1091 | Alpha:0.0248 | SPLoss:9.1335 | CLSLoss:0.9424 | top1:96.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2091 | MainLoss:0.2091 | SPLoss:9.1287 | CLSLoss:0.9415 | top1:92.4872 | AUROC:0.9783\n",
      "Test | 161/20 | Loss:1.2855 | MainLoss:1.2855 | SPLoss:9.1287 | CLSLoss:0.9415 | top1:65.6791 | AUROC:0.9280\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.001583\n",
      "Train | 20/20 | Loss:0.3225 | MainLoss:0.0961 | Alpha:0.0225 | SPLoss:9.1248 | CLSLoss:0.9411 | top1:96.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:9.1217 | CLSLoss:0.9405 | top1:92.4872 | AUROC:0.9781\n",
      "Test | 161/20 | Loss:1.2619 | MainLoss:1.2619 | SPLoss:9.1217 | CLSLoss:0.9405 | top1:66.0872 | AUROC:0.9281\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.001583\n",
      "Train | 20/20 | Loss:0.3435 | MainLoss:0.1051 | Alpha:0.0237 | SPLoss:9.1190 | CLSLoss:0.9401 | top1:96.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2078 | MainLoss:0.2078 | SPLoss:9.1141 | CLSLoss:0.9392 | top1:92.5128 | AUROC:0.9784\n",
      "Test | 161/20 | Loss:1.3147 | MainLoss:1.3147 | SPLoss:9.1141 | CLSLoss:0.9392 | top1:65.1620 | AUROC:0.9250\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.001582\n",
      "Train | 20/20 | Loss:0.3392 | MainLoss:0.1066 | Alpha:0.0231 | SPLoss:9.1109 | CLSLoss:0.9384 | top1:96.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2089 | MainLoss:0.2089 | SPLoss:9.1071 | CLSLoss:0.9379 | top1:92.4615 | AUROC:0.9785\n",
      "Test | 161/20 | Loss:1.2778 | MainLoss:1.2778 | SPLoss:9.1071 | CLSLoss:0.9379 | top1:65.7726 | AUROC:0.9253\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.001582\n",
      "Train | 20/20 | Loss:0.3483 | MainLoss:0.1183 | Alpha:0.0229 | SPLoss:9.1046 | CLSLoss:0.9373 | top1:95.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2057 | MainLoss:0.2057 | SPLoss:9.1003 | CLSLoss:0.9363 | top1:92.5385 | AUROC:0.9785\n",
      "Test | 161/20 | Loss:1.2997 | MainLoss:1.2997 | SPLoss:9.1003 | CLSLoss:0.9363 | top1:65.1838 | AUROC:0.9233\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.001581\n",
      "Train | 20/20 | Loss:0.3422 | MainLoss:0.1036 | Alpha:0.0238 | SPLoss:9.0960 | CLSLoss:0.9357 | top1:96.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2046 | MainLoss:0.2046 | SPLoss:9.0920 | CLSLoss:0.9352 | top1:92.5769 | AUROC:0.9782\n",
      "Test | 161/20 | Loss:1.3275 | MainLoss:1.3275 | SPLoss:9.0920 | CLSLoss:0.9352 | top1:64.6854 | AUROC:0.9225\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.001581\n",
      "Train | 20/20 | Loss:0.3402 | MainLoss:0.1020 | Alpha:0.0238 | SPLoss:9.0879 | CLSLoss:0.9346 | top1:96.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2056 | MainLoss:0.2056 | SPLoss:9.0838 | CLSLoss:0.9340 | top1:92.4615 | AUROC:0.9787\n",
      "Test | 161/20 | Loss:1.2828 | MainLoss:1.2828 | SPLoss:9.0838 | CLSLoss:0.9340 | top1:65.4019 | AUROC:0.9243\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.001580\n",
      "Train | 20/20 | Loss:0.3316 | MainLoss:0.0999 | Alpha:0.0231 | SPLoss:9.0804 | CLSLoss:0.9335 | top1:96.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2049 | MainLoss:0.2049 | SPLoss:9.0763 | CLSLoss:0.9330 | top1:92.5128 | AUROC:0.9787\n",
      "Test | 161/20 | Loss:1.2917 | MainLoss:1.2917 | SPLoss:9.0763 | CLSLoss:0.9330 | top1:65.2866 | AUROC:0.9242\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.001580\n",
      "Train | 20/20 | Loss:0.3178 | MainLoss:0.0939 | Alpha:0.0224 | SPLoss:9.0728 | CLSLoss:0.9325 | top1:96.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2055 | MainLoss:0.2055 | SPLoss:9.0685 | CLSLoss:0.9322 | top1:92.5769 | AUROC:0.9785\n",
      "Test | 161/20 | Loss:1.3062 | MainLoss:1.3062 | SPLoss:9.0685 | CLSLoss:0.9322 | top1:65.1402 | AUROC:0.9241\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.001579\n",
      "Train | 20/20 | Loss:0.3363 | MainLoss:0.0910 | Alpha:0.0245 | SPLoss:9.0645 | CLSLoss:0.9318 | top1:96.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2060 | MainLoss:0.2060 | SPLoss:9.0595 | CLSLoss:0.9313 | top1:92.5256 | AUROC:0.9787\n",
      "Test | 161/20 | Loss:1.3251 | MainLoss:1.3251 | SPLoss:9.0595 | CLSLoss:0.9313 | top1:64.9470 | AUROC:0.9223\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.001578\n",
      "Train | 20/20 | Loss:0.3270 | MainLoss:0.0965 | Alpha:0.0231 | SPLoss:9.0561 | CLSLoss:0.9308 | top1:96.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2055 | MainLoss:0.2055 | SPLoss:9.0519 | CLSLoss:0.9303 | top1:92.5897 | AUROC:0.9785\n",
      "Test | 161/20 | Loss:1.3375 | MainLoss:1.3375 | SPLoss:9.0519 | CLSLoss:0.9303 | top1:64.7788 | AUROC:0.9230\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.001578\n",
      "Train | 20/20 | Loss:0.3395 | MainLoss:0.0974 | Alpha:0.0243 | SPLoss:9.0487 | CLSLoss:0.9297 | top1:96.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2066 | MainLoss:0.2066 | SPLoss:9.0450 | CLSLoss:0.9291 | top1:92.5385 | AUROC:0.9786\n",
      "Test | 161/20 | Loss:1.3037 | MainLoss:1.3037 | SPLoss:9.0450 | CLSLoss:0.9292 | top1:65.3583 | AUROC:0.9235\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.001577\n",
      "Train | 20/20 | Loss:0.3427 | MainLoss:0.1004 | Alpha:0.0243 | SPLoss:9.0415 | CLSLoss:0.9285 | top1:96.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2068 | MainLoss:0.2068 | SPLoss:9.0367 | CLSLoss:0.9279 | top1:92.6410 | AUROC:0.9787\n",
      "Test | 161/20 | Loss:1.3253 | MainLoss:1.3253 | SPLoss:9.0367 | CLSLoss:0.9279 | top1:65.1028 | AUROC:0.9235\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.001577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.3140 | MainLoss:0.0832 | Alpha:0.0232 | SPLoss:9.0333 | CLSLoss:0.9276 | top1:97.1053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2086 | MainLoss:0.2086 | SPLoss:9.0287 | CLSLoss:0.9271 | top1:92.5769 | AUROC:0.9786\n",
      "Test | 161/20 | Loss:1.2807 | MainLoss:1.2807 | SPLoss:9.0287 | CLSLoss:0.9271 | top1:65.8910 | AUROC:0.9263\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.001576\n",
      "Train | 20/20 | Loss:0.3340 | MainLoss:0.0869 | Alpha:0.0248 | SPLoss:9.0245 | CLSLoss:0.9266 | top1:96.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2081 | MainLoss:0.2081 | SPLoss:9.0207 | CLSLoss:0.9262 | top1:92.6410 | AUROC:0.9784\n",
      "Test | 161/20 | Loss:1.2948 | MainLoss:1.2948 | SPLoss:9.0208 | CLSLoss:0.9262 | top1:65.6480 | AUROC:0.9254\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.001575\n",
      "Train | 20/20 | Loss:0.3160 | MainLoss:0.0850 | Alpha:0.0232 | SPLoss:9.0179 | CLSLoss:0.9259 | top1:96.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2088 | MainLoss:0.2088 | SPLoss:9.0136 | CLSLoss:0.9254 | top1:92.6795 | AUROC:0.9786\n",
      "Test | 161/20 | Loss:1.3099 | MainLoss:1.3099 | SPLoss:9.0136 | CLSLoss:0.9254 | top1:65.5483 | AUROC:0.9245\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.001575\n",
      "Train | 20/20 | Loss:0.3078 | MainLoss:0.0852 | Alpha:0.0224 | SPLoss:9.0099 | CLSLoss:0.9250 | top1:97.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2102 | MainLoss:0.2102 | SPLoss:9.0068 | CLSLoss:0.9246 | top1:92.5256 | AUROC:0.9783\n",
      "Test | 161/20 | Loss:1.2991 | MainLoss:1.2991 | SPLoss:9.0068 | CLSLoss:0.9246 | top1:65.7539 | AUROC:0.9241\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.001574\n",
      "Train | 20/20 | Loss:0.3319 | MainLoss:0.1032 | Alpha:0.0230 | SPLoss:9.0032 | CLSLoss:0.9239 | top1:96.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2086 | MainLoss:0.2086 | SPLoss:8.9994 | CLSLoss:0.9233 | top1:92.6026 | AUROC:0.9786\n",
      "Test | 161/20 | Loss:1.2900 | MainLoss:1.2900 | SPLoss:8.9994 | CLSLoss:0.9233 | top1:65.8193 | AUROC:0.9251\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.001574\n",
      "Train | 20/20 | Loss:0.3241 | MainLoss:0.0924 | Alpha:0.0234 | SPLoss:8.9955 | CLSLoss:0.9228 | top1:96.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2126 | MainLoss:0.2126 | SPLoss:8.9927 | CLSLoss:0.9223 | top1:92.2821 | AUROC:0.9788\n",
      "Test | 161/20 | Loss:1.2500 | MainLoss:1.2500 | SPLoss:8.9927 | CLSLoss:0.9223 | top1:66.5732 | AUROC:0.9270\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.001573\n",
      "Train | 20/20 | Loss:0.3302 | MainLoss:0.1036 | Alpha:0.0229 | SPLoss:8.9891 | CLSLoss:0.9217 | top1:96.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2064 | MainLoss:0.2064 | SPLoss:8.9852 | CLSLoss:0.9210 | top1:92.6282 | AUROC:0.9791\n",
      "Test | 161/20 | Loss:1.3305 | MainLoss:1.3305 | SPLoss:8.9852 | CLSLoss:0.9210 | top1:65.2181 | AUROC:0.9244\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.001572\n",
      "Train | 20/20 | Loss:0.3206 | MainLoss:0.0910 | Alpha:0.0232 | SPLoss:8.9819 | CLSLoss:0.9205 | top1:96.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2070 | MainLoss:0.2070 | SPLoss:8.9780 | CLSLoss:0.9200 | top1:92.5641 | AUROC:0.9790\n",
      "Test | 161/20 | Loss:1.2990 | MainLoss:1.2990 | SPLoss:8.9780 | CLSLoss:0.9200 | top1:65.6449 | AUROC:0.9249\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.001572\n",
      "Train | 20/20 | Loss:0.2991 | MainLoss:0.0773 | Alpha:0.0224 | SPLoss:8.9736 | CLSLoss:0.9197 | top1:97.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2065 | MainLoss:0.2065 | SPLoss:8.9691 | CLSLoss:0.9194 | top1:92.6026 | AUROC:0.9788\n",
      "Test | 161/20 | Loss:1.3481 | MainLoss:1.3481 | SPLoss:8.9691 | CLSLoss:0.9194 | top1:65.0499 | AUROC:0.9236\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.001571\n",
      "Train | 20/20 | Loss:0.3101 | MainLoss:0.0844 | Alpha:0.0228 | SPLoss:8.9659 | CLSLoss:0.9190 | top1:97.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2081 | MainLoss:0.2081 | SPLoss:8.9627 | CLSLoss:0.9186 | top1:92.6026 | AUROC:0.9790\n",
      "Test | 161/20 | Loss:1.3183 | MainLoss:1.3183 | SPLoss:8.9627 | CLSLoss:0.9186 | top1:65.5576 | AUROC:0.9243\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.001570\n",
      "Train | 20/20 | Loss:0.3550 | MainLoss:0.1185 | Alpha:0.0239 | SPLoss:8.9598 | CLSLoss:0.9177 | top1:95.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2056 | MainLoss:0.2056 | SPLoss:8.9562 | CLSLoss:0.9169 | top1:92.6026 | AUROC:0.9789\n",
      "Test | 161/20 | Loss:1.3203 | MainLoss:1.3203 | SPLoss:8.9562 | CLSLoss:0.9169 | top1:65.3676 | AUROC:0.9234\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.001570\n",
      "Train | 20/20 | Loss:0.3263 | MainLoss:0.0894 | Alpha:0.0240 | SPLoss:8.9522 | CLSLoss:0.9164 | top1:96.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2054 | MainLoss:0.2054 | SPLoss:8.9478 | CLSLoss:0.9159 | top1:92.7179 | AUROC:0.9790\n",
      "Test | 161/20 | Loss:1.3563 | MainLoss:1.3563 | SPLoss:8.9478 | CLSLoss:0.9159 | top1:64.8660 | AUROC:0.9223\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.001569\n",
      "Train | 20/20 | Loss:0.5403 | MainLoss:0.1089 | Alpha:0.4779 | SPLoss:0.0000 | CLSLoss:0.9025 | top1:96.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2033 | MainLoss:0.2033 | SPLoss:0.0001 | CLSLoss:0.8875 | top1:92.6923 | AUROC:0.9792\n",
      "Test | 161/20 | Loss:1.3259 | MainLoss:1.3259 | SPLoss:0.0001 | CLSLoss:0.8875 | top1:64.9782 | AUROC:0.9217\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.001568\n",
      "Train | 20/20 | Loss:0.5094 | MainLoss:0.0874 | Alpha:0.4825 | SPLoss:0.0001 | CLSLoss:0.8745 | top1:96.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2031 | MainLoss:0.2031 | SPLoss:0.0002 | CLSLoss:0.8603 | top1:92.6410 | AUROC:0.9792\n",
      "Test | 161/20 | Loss:1.2810 | MainLoss:1.2810 | SPLoss:0.0002 | CLSLoss:0.8603 | top1:65.4299 | AUROC:0.9231\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.001568\n",
      "Train | 20/20 | Loss:0.5031 | MainLoss:0.0960 | Alpha:0.4802 | SPLoss:0.0003 | CLSLoss:0.8477 | top1:96.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2016 | MainLoss:0.2016 | SPLoss:0.0003 | CLSLoss:0.8339 | top1:92.7179 | AUROC:0.9794\n",
      "Test | 161/20 | Loss:1.2703 | MainLoss:1.2703 | SPLoss:0.0003 | CLSLoss:0.8339 | top1:65.3707 | AUROC:0.9220\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.001567\n",
      "Train | 20/20 | Loss:0.4883 | MainLoss:0.0926 | Alpha:0.4812 | SPLoss:0.0004 | CLSLoss:0.8218 | top1:96.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1989 | MainLoss:0.1989 | SPLoss:0.0004 | CLSLoss:0.8085 | top1:92.8205 | AUROC:0.9794\n",
      "Test | 161/20 | Loss:1.2893 | MainLoss:1.2893 | SPLoss:0.0004 | CLSLoss:0.8085 | top1:64.8442 | AUROC:0.9205\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.001566\n",
      "Train | 20/20 | Loss:0.4763 | MainLoss:0.0928 | Alpha:0.4810 | SPLoss:0.0005 | CLSLoss:0.7968 | top1:97.0263 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
