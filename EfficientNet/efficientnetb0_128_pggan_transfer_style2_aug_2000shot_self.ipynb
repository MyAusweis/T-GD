{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained =  './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.01\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_style2/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.010000\n",
      "Train | 20/20 | Loss:1.0451 | MainLoss:1.0001 | Alpha:0.0235 | SPLoss:0.0635 | CLSLoss:1.8507 | top1:53.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7060 | MainLoss:0.7060 | SPLoss:0.0941 | CLSLoss:1.8230 | top1:52.7821 | AUROC:0.5435\n",
      "Test | 161/20 | Loss:0.0646 | MainLoss:0.0646 | SPLoss:0.0941 | CLSLoss:1.8230 | top1:99.9439 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.013000\n",
      "Train | 20/20 | Loss:1.2491 | MainLoss:0.6936 | Alpha:0.3319 | SPLoss:0.0006 | CLSLoss:1.6731 | top1:54.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6924 | MainLoss:0.6924 | SPLoss:0.0015 | CLSLoss:1.5156 | top1:53.5256 | AUROC:0.5569\n",
      "Test | 161/20 | Loss:0.1180 | MainLoss:0.1180 | SPLoss:0.0015 | CLSLoss:1.5156 | top1:99.9533 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:1.1448 | MainLoss:0.6897 | Alpha:0.3337 | SPLoss:0.0003 | CLSLoss:1.3633 | top1:54.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6870 | MainLoss:0.6870 | SPLoss:0.0008 | CLSLoss:1.2065 | top1:54.6282 | AUROC:0.5659\n",
      "Test | 161/20 | Loss:0.1625 | MainLoss:0.1625 | SPLoss:0.0008 | CLSLoss:1.2065 | top1:99.9533 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.019000\n",
      "Train | 20/20 | Loss:1.0412 | MainLoss:0.6842 | Alpha:0.3351 | SPLoss:0.0003 | CLSLoss:1.0651 | top1:55.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6848 | MainLoss:0.6848 | SPLoss:0.0007 | CLSLoss:0.9216 | top1:55.1538 | AUROC:0.5779\n",
      "Test | 161/20 | Loss:0.1908 | MainLoss:0.1908 | SPLoss:0.0007 | CLSLoss:0.9216 | top1:99.9626 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.022000\n",
      "Train | 20/20 | Loss:0.9517 | MainLoss:0.6838 | Alpha:0.3355 | SPLoss:0.0003 | CLSLoss:0.7983 | top1:54.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6827 | MainLoss:0.6827 | SPLoss:0.0008 | CLSLoss:0.6749 | top1:56.1410 | AUROC:0.5892\n",
      "Test | 161/20 | Loss:0.2106 | MainLoss:0.2106 | SPLoss:0.0008 | CLSLoss:0.6749 | top1:99.9626 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.025000\n",
      "Train | 20/20 | Loss:0.8731 | MainLoss:0.6799 | Alpha:0.3359 | SPLoss:0.0005 | CLSLoss:0.5747 | top1:56.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6794 | MainLoss:0.6794 | SPLoss:0.0014 | CLSLoss:0.4755 | top1:57.0769 | AUROC:0.6022\n",
      "Test | 161/20 | Loss:0.2026 | MainLoss:0.2026 | SPLoss:0.0014 | CLSLoss:0.4755 | top1:99.9626 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.028000\n",
      "Train | 20/20 | Loss:0.8134 | MainLoss:0.6795 | Alpha:0.3362 | SPLoss:0.0006 | CLSLoss:0.3976 | top1:57.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6761 | MainLoss:0.6761 | SPLoss:0.0016 | CLSLoss:0.3222 | top1:58.0256 | AUROC:0.6169\n",
      "Test | 161/20 | Loss:0.2054 | MainLoss:0.2054 | SPLoss:0.0016 | CLSLoss:0.3222 | top1:99.9439 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.031000\n",
      "Train | 20/20 | Loss:0.7634 | MainLoss:0.6733 | Alpha:0.3373 | SPLoss:0.0010 | CLSLoss:0.2662 | top1:59.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6700 | MainLoss:0.6700 | SPLoss:0.0029 | CLSLoss:0.2129 | top1:59.5000 | AUROC:0.6335\n",
      "Test | 161/20 | Loss:0.1830 | MainLoss:0.1830 | SPLoss:0.0029 | CLSLoss:0.2129 | top1:99.9377 | AUROC:1.0000\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.034000\n",
      "Train | 20/20 | Loss:0.7227 | MainLoss:0.6627 | Alpha:0.3392 | SPLoss:0.0019 | CLSLoss:0.1751 | top1:61.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6589 | MainLoss:0.6589 | SPLoss:0.0056 | CLSLoss:0.1401 | top1:61.3462 | AUROC:0.6545\n",
      "Test | 161/20 | Loss:0.1513 | MainLoss:0.1513 | SPLoss:0.0056 | CLSLoss:0.1401 | top1:99.9190 | AUROC:1.0000\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.037000\n",
      "Train | 20/20 | Loss:0.6878 | MainLoss:0.6468 | Alpha:0.3420 | SPLoss:0.0035 | CLSLoss:0.1164 | top1:63.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6455 | MainLoss:0.6455 | SPLoss:0.0097 | CLSLoss:0.0944 | top1:62.2308 | AUROC:0.6839\n",
      "Test | 161/20 | Loss:0.1488 | MainLoss:0.1488 | SPLoss:0.0097 | CLSLoss:0.0944 | top1:99.7664 | AUROC:1.0000\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.6594 | MainLoss:0.6301 | Alpha:0.3456 | SPLoss:0.0054 | CLSLoss:0.0794 | top1:64.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6208 | MainLoss:0.6208 | SPLoss:0.0132 | CLSLoss:0.0651 | top1:65.9103 | AUROC:0.7193\n",
      "Test | 161/20 | Loss:0.1654 | MainLoss:0.1654 | SPLoss:0.0132 | CLSLoss:0.0651 | top1:99.7103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.6203 | MainLoss:0.5962 | Alpha:0.3529 | SPLoss:0.0072 | CLSLoss:0.0609 | top1:68.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5882 | MainLoss:0.5882 | SPLoss:0.0189 | CLSLoss:0.0569 | top1:68.7308 | AUROC:0.7567\n",
      "Test | 161/20 | Loss:0.1683 | MainLoss:0.1683 | SPLoss:0.0189 | CLSLoss:0.0569 | top1:99.4922 | AUROC:1.0000\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5938 | MainLoss:0.5715 | Alpha:0.3597 | SPLoss:0.0085 | CLSLoss:0.0535 | top1:70.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5574 | MainLoss:0.5574 | SPLoss:0.0199 | CLSLoss:0.0523 | top1:71.4359 | AUROC:0.7947\n",
      "Test | 161/20 | Loss:0.1744 | MainLoss:0.1744 | SPLoss:0.0199 | CLSLoss:0.0523 | top1:99.1838 | AUROC:1.0000\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5536 | MainLoss:0.5293 | Alpha:0.3688 | SPLoss:0.0115 | CLSLoss:0.0541 | top1:74.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5117 | MainLoss:0.5117 | SPLoss:0.0277 | CLSLoss:0.0567 | top1:75.9359 | AUROC:0.8343\n",
      "Test | 161/20 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:0.0277 | CLSLoss:0.0567 | top1:99.0717 | AUROC:1.0000\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.039999\n",
      "Train | 20/20 | Loss:0.5046 | MainLoss:0.4764 | Alpha:0.3812 | SPLoss:0.0135 | CLSLoss:0.0604 | top1:78.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4628 | MainLoss:0.4628 | SPLoss:0.0286 | CLSLoss:0.0650 | top1:78.9615 | AUROC:0.8688\n",
      "Test | 161/20 | Loss:0.1578 | MainLoss:0.1578 | SPLoss:0.0286 | CLSLoss:0.0650 | top1:97.4984 | AUROC:0.9999\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.039998\n",
      "Train | 20/20 | Loss:0.4651 | MainLoss:0.4328 | Alpha:0.3934 | SPLoss:0.0146 | CLSLoss:0.0676 | top1:80.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4349 | MainLoss:0.4349 | SPLoss:0.0314 | CLSLoss:0.0668 | top1:80.4744 | AUROC:0.8932\n",
      "Test | 161/20 | Loss:0.1823 | MainLoss:0.1823 | SPLoss:0.0314 | CLSLoss:0.0668 | top1:95.3894 | AUROC:0.9998\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.039998\n",
      "Train | 20/20 | Loss:0.4247 | MainLoss:0.3876 | Alpha:0.4015 | SPLoss:0.0185 | CLSLoss:0.0739 | top1:84.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4049 | MainLoss:0.4049 | SPLoss:0.0355 | CLSLoss:0.0721 | top1:82.2051 | AUROC:0.9123\n",
      "Test | 161/20 | Loss:0.2218 | MainLoss:0.2218 | SPLoss:0.0355 | CLSLoss:0.0721 | top1:92.1900 | AUROC:0.9995\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.039996\n",
      "Train | 20/20 | Loss:0.3805 | MainLoss:0.3389 | Alpha:0.4109 | SPLoss:0.0218 | CLSLoss:0.0795 | top1:86.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3546 | MainLoss:0.3546 | SPLoss:0.0475 | CLSLoss:0.0865 | top1:85.4487 | AUROC:0.9303\n",
      "Test | 161/20 | Loss:0.1826 | MainLoss:0.1826 | SPLoss:0.0475 | CLSLoss:0.0865 | top1:93.5732 | AUROC:0.9993\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.039995\n",
      "Train | 20/20 | Loss:0.3679 | MainLoss:0.3245 | Alpha:0.4233 | SPLoss:0.0190 | CLSLoss:0.0836 | top1:86.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3372 | MainLoss:0.3372 | SPLoss:0.0350 | CLSLoss:0.0839 | top1:85.6410 | AUROC:0.9381\n",
      "Test | 161/20 | Loss:0.2711 | MainLoss:0.2711 | SPLoss:0.0350 | CLSLoss:0.0839 | top1:88.5140 | AUROC:0.9986\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.039994\n",
      "Train | 20/20 | Loss:0.3506 | MainLoss:0.3074 | Alpha:0.4252 | SPLoss:0.0174 | CLSLoss:0.0843 | top1:87.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3119 | MainLoss:0.3119 | SPLoss:0.0382 | CLSLoss:0.0811 | top1:87.3205 | AUROC:0.9469\n",
      "Test | 161/20 | Loss:0.3320 | MainLoss:0.3320 | SPLoss:0.0382 | CLSLoss:0.0811 | top1:85.1838 | AUROC:0.9973\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.039992\n",
      "Train | 20/20 | Loss:0.3165 | MainLoss:0.2706 | Alpha:0.4337 | SPLoss:0.0207 | CLSLoss:0.0852 | top1:90.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3113 | MainLoss:0.3113 | SPLoss:0.0335 | CLSLoss:0.0886 | top1:87.6282 | AUROC:0.9550\n",
      "Test | 161/20 | Loss:0.2344 | MainLoss:0.2344 | SPLoss:0.0335 | CLSLoss:0.0886 | top1:90.5358 | AUROC:0.9983\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.039990\n",
      "Train | 20/20 | Loss:0.2846 | MainLoss:0.2384 | Alpha:0.4429 | SPLoss:0.0155 | CLSLoss:0.0886 | top1:91.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2681 | MainLoss:0.2681 | SPLoss:0.0272 | CLSLoss:0.0930 | top1:89.4744 | AUROC:0.9621\n",
      "Test | 161/20 | Loss:0.3097 | MainLoss:0.3097 | SPLoss:0.0272 | CLSLoss:0.0930 | top1:87.0280 | AUROC:0.9970\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.039988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.2755 | MainLoss:0.2268 | Alpha:0.4488 | SPLoss:0.0182 | CLSLoss:0.0903 | top1:91.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2588 | MainLoss:0.2588 | SPLoss:0.0307 | CLSLoss:0.0886 | top1:89.9359 | AUROC:0.9636\n",
      "Test | 161/20 | Loss:0.3476 | MainLoss:0.3476 | SPLoss:0.0307 | CLSLoss:0.0886 | top1:85.5857 | AUROC:0.9958\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.039986\n",
      "Train | 20/20 | Loss:0.2486 | MainLoss:0.1999 | Alpha:0.4541 | SPLoss:0.0171 | CLSLoss:0.0899 | top1:93.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2401 | MainLoss:0.2401 | SPLoss:0.0301 | CLSLoss:0.0907 | top1:91.0641 | AUROC:0.9687\n",
      "Test | 161/20 | Loss:0.4060 | MainLoss:0.4060 | SPLoss:0.0301 | CLSLoss:0.0907 | top1:83.4143 | AUROC:0.9930\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.039983\n",
      "Train | 20/20 | Loss:0.2329 | MainLoss:0.1847 | Alpha:0.4596 | SPLoss:0.0152 | CLSLoss:0.0898 | top1:93.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2392 | MainLoss:0.2392 | SPLoss:0.0287 | CLSLoss:0.0890 | top1:90.7308 | AUROC:0.9716\n",
      "Test | 161/20 | Loss:0.3862 | MainLoss:0.3862 | SPLoss:0.0287 | CLSLoss:0.0890 | top1:84.6293 | AUROC:0.9934\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.039981\n",
      "Train | 20/20 | Loss:0.2252 | MainLoss:0.1784 | Alpha:0.4597 | SPLoss:0.0154 | CLSLoss:0.0863 | top1:94.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2208 | MainLoss:0.2208 | SPLoss:0.0272 | CLSLoss:0.0861 | top1:92.0128 | AUROC:0.9740\n",
      "Test | 161/20 | Loss:0.5423 | MainLoss:0.5423 | SPLoss:0.0272 | CLSLoss:0.0861 | top1:78.3707 | AUROC:0.9882\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.039978\n",
      "Train | 20/20 | Loss:0.2099 | MainLoss:0.1620 | Alpha:0.4630 | SPLoss:0.0169 | CLSLoss:0.0866 | top1:94.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2125 | MainLoss:0.2125 | SPLoss:0.0304 | CLSLoss:0.0867 | top1:92.3590 | AUROC:0.9761\n",
      "Test | 161/20 | Loss:0.5428 | MainLoss:0.5428 | SPLoss:0.0304 | CLSLoss:0.0867 | top1:78.6480 | AUROC:0.9862\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.039975\n",
      "Train | 20/20 | Loss:0.2121 | MainLoss:0.1662 | Alpha:0.4628 | SPLoss:0.0156 | CLSLoss:0.0835 | top1:94.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2071 | MainLoss:0.2071 | SPLoss:0.0300 | CLSLoss:0.0861 | top1:92.3974 | AUROC:0.9777\n",
      "Test | 161/20 | Loss:0.5545 | MainLoss:0.5545 | SPLoss:0.0300 | CLSLoss:0.0861 | top1:78.7446 | AUROC:0.9863\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.039971\n",
      "Train | 20/20 | Loss:0.2005 | MainLoss:0.1533 | Alpha:0.4687 | SPLoss:0.0169 | CLSLoss:0.0839 | top1:95.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2057 | MainLoss:0.2057 | SPLoss:0.0305 | CLSLoss:0.0802 | top1:92.4359 | AUROC:0.9786\n",
      "Test | 161/20 | Loss:0.5207 | MainLoss:0.5207 | SPLoss:0.0305 | CLSLoss:0.0802 | top1:79.4361 | AUROC:0.9862\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.039968\n",
      "Train | 20/20 | Loss:0.1880 | MainLoss:0.1428 | Alpha:0.4686 | SPLoss:0.0161 | CLSLoss:0.0803 | top1:95.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2097 | MainLoss:0.2097 | SPLoss:0.0267 | CLSLoss:0.0822 | top1:92.1410 | AUROC:0.9803\n",
      "Test | 161/20 | Loss:0.4964 | MainLoss:0.4964 | SPLoss:0.0267 | CLSLoss:0.0822 | top1:80.8255 | AUROC:0.9878\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.039964\n",
      "Train | 20/20 | Loss:0.1690 | MainLoss:0.1235 | Alpha:0.4754 | SPLoss:0.0137 | CLSLoss:0.0821 | top1:96.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2304 | MainLoss:0.2304 | SPLoss:0.0247 | CLSLoss:0.0830 | top1:91.3077 | AUROC:0.9818\n",
      "Test | 161/20 | Loss:0.4574 | MainLoss:0.4574 | SPLoss:0.0247 | CLSLoss:0.0830 | top1:82.9346 | AUROC:0.9890\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.039961\n",
      "Train | 20/20 | Loss:0.1627 | MainLoss:0.1165 | Alpha:0.4712 | SPLoss:0.0161 | CLSLoss:0.0822 | top1:96.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2000 | MainLoss:0.2000 | SPLoss:0.0342 | CLSLoss:0.0818 | top1:92.7308 | AUROC:0.9815\n",
      "Test | 161/20 | Loss:0.5862 | MainLoss:0.5862 | SPLoss:0.0342 | CLSLoss:0.0818 | top1:79.0280 | AUROC:0.9855\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.039956\n",
      "Train | 20/20 | Loss:0.1690 | MainLoss:0.1230 | Alpha:0.4736 | SPLoss:0.0188 | CLSLoss:0.0784 | top1:96.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1778 | MainLoss:0.1778 | SPLoss:0.0327 | CLSLoss:0.0774 | top1:93.6282 | AUROC:0.9838\n",
      "Test | 161/20 | Loss:0.7130 | MainLoss:0.7130 | SPLoss:0.0327 | CLSLoss:0.0774 | top1:74.9346 | AUROC:0.9766\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.039952\n",
      "Train | 20/20 | Loss:0.1621 | MainLoss:0.1195 | Alpha:0.4747 | SPLoss:0.0129 | CLSLoss:0.0770 | top1:96.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1712 | MainLoss:0.1712 | SPLoss:0.0259 | CLSLoss:0.0736 | top1:93.9872 | AUROC:0.9851\n",
      "Test | 161/20 | Loss:0.7230 | MainLoss:0.7230 | SPLoss:0.0259 | CLSLoss:0.0736 | top1:73.9190 | AUROC:0.9721\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.039948\n",
      "Train | 20/20 | Loss:0.1627 | MainLoss:0.1179 | Alpha:0.4766 | SPLoss:0.0194 | CLSLoss:0.0747 | top1:96.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1692 | MainLoss:0.1692 | SPLoss:0.0278 | CLSLoss:0.0775 | top1:93.9872 | AUROC:0.9861\n",
      "Test | 161/20 | Loss:0.7150 | MainLoss:0.7150 | SPLoss:0.0278 | CLSLoss:0.0775 | top1:75.4579 | AUROC:0.9741\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.039943\n",
      "Train | 20/20 | Loss:0.1608 | MainLoss:0.1171 | Alpha:0.4775 | SPLoss:0.0174 | CLSLoss:0.0740 | top1:96.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1744 | MainLoss:0.1744 | SPLoss:0.0326 | CLSLoss:0.0765 | top1:93.5256 | AUROC:0.9860\n",
      "Test | 161/20 | Loss:0.7262 | MainLoss:0.7262 | SPLoss:0.0326 | CLSLoss:0.0765 | top1:74.9844 | AUROC:0.9694\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.039938\n",
      "Train | 20/20 | Loss:0.1387 | MainLoss:0.0951 | Alpha:0.4807 | SPLoss:0.0133 | CLSLoss:0.0772 | top1:97.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1694 | MainLoss:0.1694 | SPLoss:0.0287 | CLSLoss:0.0748 | top1:93.9103 | AUROC:0.9862\n",
      "Test | 161/20 | Loss:0.7802 | MainLoss:0.7802 | SPLoss:0.0287 | CLSLoss:0.0748 | top1:72.8287 | AUROC:0.9631\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.039933\n",
      "Train | 20/20 | Loss:0.1388 | MainLoss:0.0970 | Alpha:0.4805 | SPLoss:0.0129 | CLSLoss:0.0741 | top1:97.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1669 | MainLoss:0.1669 | SPLoss:0.0252 | CLSLoss:0.0750 | top1:94.2179 | AUROC:0.9868\n",
      "Test | 161/20 | Loss:0.8715 | MainLoss:0.8715 | SPLoss:0.0252 | CLSLoss:0.0750 | top1:71.2087 | AUROC:0.9643\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.039928\n",
      "Train | 20/20 | Loss:0.1294 | MainLoss:0.0870 | Alpha:0.4841 | SPLoss:0.0136 | CLSLoss:0.0739 | top1:97.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1605 | MainLoss:0.1605 | SPLoss:0.0252 | CLSLoss:0.0714 | top1:94.3590 | AUROC:0.9871\n",
      "Test | 161/20 | Loss:0.9236 | MainLoss:0.9236 | SPLoss:0.0252 | CLSLoss:0.0714 | top1:68.7664 | AUROC:0.9560\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.039923\n",
      "Train | 20/20 | Loss:0.1378 | MainLoss:0.0987 | Alpha:0.4802 | SPLoss:0.0107 | CLSLoss:0.0709 | top1:97.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1557 | MainLoss:0.1557 | SPLoss:0.0246 | CLSLoss:0.0674 | top1:94.5769 | AUROC:0.9883\n",
      "Test | 161/20 | Loss:0.8153 | MainLoss:0.8153 | SPLoss:0.0246 | CLSLoss:0.0674 | top1:72.0343 | AUROC:0.9689\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.039917\n",
      "Train | 20/20 | Loss:0.1405 | MainLoss:0.1007 | Alpha:0.4796 | SPLoss:0.0154 | CLSLoss:0.0675 | top1:96.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1563 | MainLoss:0.1563 | SPLoss:0.0261 | CLSLoss:0.0690 | top1:94.5769 | AUROC:0.9890\n",
      "Test | 161/20 | Loss:0.9337 | MainLoss:0.9337 | SPLoss:0.0261 | CLSLoss:0.0690 | top1:69.1495 | AUROC:0.9534\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.039911\n",
      "Train | 20/20 | Loss:0.1187 | MainLoss:0.0807 | Alpha:0.4832 | SPLoss:0.0096 | CLSLoss:0.0692 | top1:97.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1745 | MainLoss:0.1745 | SPLoss:0.0204 | CLSLoss:0.0694 | top1:93.9487 | AUROC:0.9891\n",
      "Test | 161/20 | Loss:0.8778 | MainLoss:0.8778 | SPLoss:0.0204 | CLSLoss:0.0694 | top1:71.6106 | AUROC:0.9578\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.039905\n",
      "Train | 20/20 | Loss:0.1278 | MainLoss:0.0845 | Alpha:0.4826 | SPLoss:0.0207 | CLSLoss:0.0690 | top1:97.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1519 | MainLoss:0.1519 | SPLoss:0.0374 | CLSLoss:0.0674 | top1:94.8974 | AUROC:0.9881\n",
      "Test | 161/20 | Loss:1.0837 | MainLoss:1.0837 | SPLoss:0.0374 | CLSLoss:0.0674 | top1:66.4299 | AUROC:0.9371\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.039899\n",
      "Train | 20/20 | Loss:0.1171 | MainLoss:0.0759 | Alpha:0.4834 | SPLoss:0.0172 | CLSLoss:0.0680 | top1:97.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.0278 | CLSLoss:0.0702 | top1:94.9487 | AUROC:0.9886\n",
      "Test | 161/20 | Loss:1.0674 | MainLoss:1.0674 | SPLoss:0.0278 | CLSLoss:0.0702 | top1:67.2648 | AUROC:0.9336\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.039893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.1068 | MainLoss:0.0667 | Alpha:0.4863 | SPLoss:0.0127 | CLSLoss:0.0700 | top1:98.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1789 | MainLoss:0.1789 | SPLoss:0.0259 | CLSLoss:0.0655 | top1:93.8846 | AUROC:0.9893\n",
      "Test | 161/20 | Loss:0.8885 | MainLoss:0.8885 | SPLoss:0.0259 | CLSLoss:0.0655 | top1:71.7570 | AUROC:0.9453\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.039886\n",
      "Train | 20/20 | Loss:0.1284 | MainLoss:0.0922 | Alpha:0.4795 | SPLoss:0.0129 | CLSLoss:0.0625 | top1:97.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1659 | MainLoss:0.1659 | SPLoss:0.0262 | CLSLoss:0.0634 | top1:94.4103 | AUROC:0.9898\n",
      "Test | 161/20 | Loss:0.9389 | MainLoss:0.9389 | SPLoss:0.0262 | CLSLoss:0.0634 | top1:70.4922 | AUROC:0.9471\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.039879\n",
      "Train | 20/20 | Loss:0.1179 | MainLoss:0.0813 | Alpha:0.4833 | SPLoss:0.0128 | CLSLoss:0.0627 | top1:97.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.0190 | CLSLoss:0.0642 | top1:94.9615 | AUROC:0.9910\n",
      "Test | 161/20 | Loss:0.9711 | MainLoss:0.9711 | SPLoss:0.0190 | CLSLoss:0.0642 | top1:69.9595 | AUROC:0.9470\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.039872\n",
      "Train | 20/20 | Loss:0.1041 | MainLoss:0.0655 | Alpha:0.4872 | SPLoss:0.0142 | CLSLoss:0.0649 | top1:98.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1458 | MainLoss:0.1458 | SPLoss:0.0232 | CLSLoss:0.0640 | top1:95.2564 | AUROC:0.9913\n",
      "Test | 161/20 | Loss:1.1196 | MainLoss:1.1196 | SPLoss:0.0232 | CLSLoss:0.0640 | top1:67.4019 | AUROC:0.9328\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.039865\n",
      "Train | 20/20 | Loss:0.0935 | MainLoss:0.0565 | Alpha:0.4895 | SPLoss:0.0105 | CLSLoss:0.0651 | top1:98.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1591 | MainLoss:0.1591 | SPLoss:0.0243 | CLSLoss:0.0658 | top1:94.7692 | AUROC:0.9910\n",
      "Test | 161/20 | Loss:1.0168 | MainLoss:1.0168 | SPLoss:0.0243 | CLSLoss:0.0658 | top1:70.0280 | AUROC:0.9426\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.039858\n",
      "Train | 20/20 | Loss:0.1104 | MainLoss:0.0738 | Alpha:0.4860 | SPLoss:0.0131 | CLSLoss:0.0623 | top1:98.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1330 | MainLoss:0.1330 | SPLoss:0.0214 | CLSLoss:0.0614 | top1:95.5769 | AUROC:0.9912\n",
      "Test | 161/20 | Loss:1.2032 | MainLoss:1.2032 | SPLoss:0.0214 | CLSLoss:0.0614 | top1:64.2773 | AUROC:0.9300\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.039850\n",
      "Train | 20/20 | Loss:0.1138 | MainLoss:0.0765 | Alpha:0.4826 | SPLoss:0.0160 | CLSLoss:0.0613 | top1:97.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1307 | MainLoss:0.1307 | SPLoss:0.0260 | CLSLoss:0.0589 | top1:95.5641 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.0527 | MainLoss:1.0527 | SPLoss:0.0260 | CLSLoss:0.0589 | top1:66.0966 | AUROC:0.9457\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.003984\n",
      "Train | 20/20 | Loss:0.0946 | MainLoss:0.0655 | Alpha:0.4864 | SPLoss:0.0001 | CLSLoss:0.0596 | top1:98.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1345 | MainLoss:0.1345 | SPLoss:0.0003 | CLSLoss:0.0601 | top1:95.3718 | AUROC:0.9919\n",
      "Test | 161/20 | Loss:1.0382 | MainLoss:1.0382 | SPLoss:0.0003 | CLSLoss:0.0601 | top1:66.8505 | AUROC:0.9478\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.003983\n",
      "Train | 20/20 | Loss:0.0938 | MainLoss:0.0643 | Alpha:0.4865 | SPLoss:0.0001 | CLSLoss:0.0606 | top1:98.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1321 | MainLoss:0.1321 | SPLoss:0.0003 | CLSLoss:0.0609 | top1:95.5256 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.0691 | MainLoss:1.0691 | SPLoss:0.0003 | CLSLoss:0.0609 | top1:66.5296 | AUROC:0.9456\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.003983\n",
      "Train | 20/20 | Loss:0.0879 | MainLoss:0.0579 | Alpha:0.4884 | SPLoss:0.0001 | CLSLoss:0.0614 | top1:98.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1357 | MainLoss:0.1357 | SPLoss:0.0002 | CLSLoss:0.0619 | top1:95.3718 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.0618 | MainLoss:1.0618 | SPLoss:0.0002 | CLSLoss:0.0619 | top1:67.2337 | AUROC:0.9469\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.003982\n",
      "Train | 20/20 | Loss:0.0920 | MainLoss:0.0617 | Alpha:0.4880 | SPLoss:0.0001 | CLSLoss:0.0620 | top1:98.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1372 | MainLoss:0.1372 | SPLoss:0.0003 | CLSLoss:0.0621 | top1:95.3077 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:1.0585 | MainLoss:1.0585 | SPLoss:0.0003 | CLSLoss:0.0621 | top1:67.4735 | AUROC:0.9478\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.003981\n",
      "Train | 20/20 | Loss:0.0815 | MainLoss:0.0508 | Alpha:0.4901 | SPLoss:0.0001 | CLSLoss:0.0624 | top1:98.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1326 | MainLoss:0.1326 | SPLoss:0.0003 | CLSLoss:0.0626 | top1:95.6282 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:1.1136 | MainLoss:1.1136 | SPLoss:0.0003 | CLSLoss:0.0626 | top1:66.4548 | AUROC:0.9428\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.003980\n",
      "Train | 20/20 | Loss:0.0773 | MainLoss:0.0463 | Alpha:0.4909 | SPLoss:0.0001 | CLSLoss:0.0631 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1357 | MainLoss:0.1357 | SPLoss:0.0002 | CLSLoss:0.0635 | top1:95.5000 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.1117 | MainLoss:1.1117 | SPLoss:0.0002 | CLSLoss:0.0635 | top1:66.9284 | AUROC:0.9423\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.003979\n",
      "Train | 20/20 | Loss:0.0911 | MainLoss:0.0601 | Alpha:0.4876 | SPLoss:0.0002 | CLSLoss:0.0634 | top1:98.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1355 | MainLoss:0.1355 | SPLoss:0.0003 | CLSLoss:0.0632 | top1:95.5641 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1099 | MainLoss:1.1099 | SPLoss:0.0003 | CLSLoss:0.0632 | top1:66.9595 | AUROC:0.9434\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.003978\n",
      "Train | 20/20 | Loss:0.0927 | MainLoss:0.0619 | Alpha:0.4878 | SPLoss:0.0001 | CLSLoss:0.0631 | top1:98.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1365 | MainLoss:0.1365 | SPLoss:0.0003 | CLSLoss:0.0628 | top1:95.4487 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:1.1010 | MainLoss:1.1010 | SPLoss:0.0003 | CLSLoss:0.0628 | top1:67.1495 | AUROC:0.9428\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.003977\n",
      "Train | 20/20 | Loss:0.0820 | MainLoss:0.0511 | Alpha:0.4898 | SPLoss:0.0001 | CLSLoss:0.0629 | top1:98.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1344 | MainLoss:0.1344 | SPLoss:0.0002 | CLSLoss:0.0630 | top1:95.6282 | AUROC:0.9924\n",
      "Test | 161/20 | Loss:1.1247 | MainLoss:1.1247 | SPLoss:0.0002 | CLSLoss:0.0630 | top1:66.7165 | AUROC:0.9410\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.003976\n",
      "Train | 20/20 | Loss:0.0783 | MainLoss:0.0473 | Alpha:0.4902 | SPLoss:0.0001 | CLSLoss:0.0631 | top1:98.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1348 | MainLoss:0.1348 | SPLoss:0.0002 | CLSLoss:0.0632 | top1:95.6667 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:1.1351 | MainLoss:1.1351 | SPLoss:0.0002 | CLSLoss:0.0632 | top1:66.6168 | AUROC:0.9397\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.003975\n",
      "Train | 20/20 | Loss:0.0823 | MainLoss:0.0513 | Alpha:0.4896 | SPLoss:0.0002 | CLSLoss:0.0632 | top1:98.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1390 | MainLoss:0.1390 | SPLoss:0.0003 | CLSLoss:0.0633 | top1:95.5128 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:1.1126 | MainLoss:1.1126 | SPLoss:0.0003 | CLSLoss:0.0633 | top1:67.3115 | AUROC:0.9428\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.003974\n",
      "Train | 20/20 | Loss:0.0803 | MainLoss:0.0493 | Alpha:0.4896 | SPLoss:0.0001 | CLSLoss:0.0633 | top1:98.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1365 | MainLoss:0.1365 | SPLoss:0.0002 | CLSLoss:0.0634 | top1:95.6282 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.1344 | MainLoss:1.1344 | SPLoss:0.0002 | CLSLoss:0.0634 | top1:66.9190 | AUROC:0.9407\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.003973\n",
      "Train | 20/20 | Loss:0.0898 | MainLoss:0.0588 | Alpha:0.4886 | SPLoss:0.0002 | CLSLoss:0.0633 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1362 | MainLoss:0.1362 | SPLoss:0.0004 | CLSLoss:0.0627 | top1:95.6154 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:1.1281 | MainLoss:1.1281 | SPLoss:0.0004 | CLSLoss:0.0627 | top1:66.8255 | AUROC:0.9397\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.003972\n",
      "Train | 20/20 | Loss:0.0838 | MainLoss:0.0531 | Alpha:0.4896 | SPLoss:0.0001 | CLSLoss:0.0627 | top1:98.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1358 | MainLoss:0.1358 | SPLoss:0.0002 | CLSLoss:0.0628 | top1:95.6026 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:1.1383 | MainLoss:1.1383 | SPLoss:0.0002 | CLSLoss:0.0628 | top1:66.7009 | AUROC:0.9396\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.003971\n",
      "Train | 20/20 | Loss:0.0803 | MainLoss:0.0495 | Alpha:0.4901 | SPLoss:0.0001 | CLSLoss:0.0628 | top1:98.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1339 | MainLoss:0.1339 | SPLoss:0.0003 | CLSLoss:0.0631 | top1:95.6795 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:1.1760 | MainLoss:1.1760 | SPLoss:0.0003 | CLSLoss:0.0631 | top1:66.0125 | AUROC:0.9377\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.003970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0840 | MainLoss:0.0532 | Alpha:0.4899 | SPLoss:0.0001 | CLSLoss:0.0629 | top1:98.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1341 | MainLoss:0.1341 | SPLoss:0.0002 | CLSLoss:0.0627 | top1:95.6282 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:1.1661 | MainLoss:1.1661 | SPLoss:0.0002 | CLSLoss:0.0627 | top1:66.2118 | AUROC:0.9375\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.003969\n",
      "Train | 20/20 | Loss:0.0825 | MainLoss:0.0517 | Alpha:0.4895 | SPLoss:0.0001 | CLSLoss:0.0627 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1344 | MainLoss:0.1344 | SPLoss:0.0002 | CLSLoss:0.0627 | top1:95.6026 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1690 | MainLoss:1.1690 | SPLoss:0.0002 | CLSLoss:0.0627 | top1:66.2461 | AUROC:0.9370\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.003968\n",
      "Train | 20/20 | Loss:0.0874 | MainLoss:0.0568 | Alpha:0.4886 | SPLoss:0.0002 | CLSLoss:0.0624 | top1:98.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1332 | MainLoss:0.1332 | SPLoss:0.0003 | CLSLoss:0.0623 | top1:95.6923 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1683 | MainLoss:1.1683 | SPLoss:0.0003 | CLSLoss:0.0623 | top1:66.1900 | AUROC:0.9376\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.003967\n",
      "Train | 20/20 | Loss:0.0802 | MainLoss:0.0496 | Alpha:0.4905 | SPLoss:0.0001 | CLSLoss:0.0624 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1384 | MainLoss:0.1384 | SPLoss:0.0003 | CLSLoss:0.0623 | top1:95.6026 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1312 | MainLoss:1.1312 | SPLoss:0.0003 | CLSLoss:0.0623 | top1:67.1745 | AUROC:0.9403\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.003966\n",
      "Train | 20/20 | Loss:0.0871 | MainLoss:0.0568 | Alpha:0.4893 | SPLoss:0.0001 | CLSLoss:0.0620 | top1:98.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1352 | MainLoss:0.1352 | SPLoss:0.0003 | CLSLoss:0.0618 | top1:95.6538 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1461 | MainLoss:1.1461 | SPLoss:0.0003 | CLSLoss:0.0618 | top1:66.6885 | AUROC:0.9377\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.003965\n",
      "Train | 20/20 | Loss:0.0795 | MainLoss:0.0491 | Alpha:0.4904 | SPLoss:0.0002 | CLSLoss:0.0618 | top1:98.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1343 | MainLoss:0.1343 | SPLoss:0.0004 | CLSLoss:0.0619 | top1:95.6795 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1452 | MainLoss:1.1452 | SPLoss:0.0004 | CLSLoss:0.0619 | top1:66.8224 | AUROC:0.9373\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.003963\n",
      "Train | 20/20 | Loss:0.0825 | MainLoss:0.0521 | Alpha:0.4893 | SPLoss:0.0002 | CLSLoss:0.0620 | top1:98.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1419 | MainLoss:0.1419 | SPLoss:0.0004 | CLSLoss:0.0621 | top1:95.3333 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1031 | MainLoss:1.1031 | SPLoss:0.0004 | CLSLoss:0.0621 | top1:67.8598 | AUROC:0.9426\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.003962\n",
      "Train | 20/20 | Loss:0.0838 | MainLoss:0.0534 | Alpha:0.4893 | SPLoss:0.0001 | CLSLoss:0.0620 | top1:98.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1343 | MainLoss:0.1343 | SPLoss:0.0003 | CLSLoss:0.0618 | top1:95.7051 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1451 | MainLoss:1.1451 | SPLoss:0.0003 | CLSLoss:0.0618 | top1:66.9034 | AUROC:0.9385\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.003961\n",
      "Train | 20/20 | Loss:0.0790 | MainLoss:0.0486 | Alpha:0.4905 | SPLoss:0.0001 | CLSLoss:0.0620 | top1:98.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1363 | MainLoss:0.1363 | SPLoss:0.0002 | CLSLoss:0.0618 | top1:95.5897 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1332 | MainLoss:1.1332 | SPLoss:0.0002 | CLSLoss:0.0618 | top1:67.2150 | AUROC:0.9405\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.003960\n",
      "Train | 20/20 | Loss:0.0864 | MainLoss:0.0562 | Alpha:0.4887 | SPLoss:0.0002 | CLSLoss:0.0616 | top1:98.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1311 | MainLoss:0.1311 | SPLoss:0.0003 | CLSLoss:0.0614 | top1:95.6795 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:1.1766 | MainLoss:1.1766 | SPLoss:0.0003 | CLSLoss:0.0614 | top1:66.1122 | AUROC:0.9350\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.003958\n",
      "Train | 20/20 | Loss:0.0904 | MainLoss:0.0604 | Alpha:0.4883 | SPLoss:0.0001 | CLSLoss:0.0613 | top1:98.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1298 | MainLoss:0.1298 | SPLoss:0.0003 | CLSLoss:0.0611 | top1:95.7308 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1860 | MainLoss:1.1860 | SPLoss:0.0003 | CLSLoss:0.0611 | top1:65.8442 | AUROC:0.9333\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.003957\n",
      "Train | 20/20 | Loss:0.0900 | MainLoss:0.0602 | Alpha:0.4887 | SPLoss:0.0001 | CLSLoss:0.0609 | top1:98.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1304 | MainLoss:0.1304 | SPLoss:0.0003 | CLSLoss:0.0605 | top1:95.7051 | AUROC:0.9928\n",
      "Test | 161/20 | Loss:1.1629 | MainLoss:1.1629 | SPLoss:0.0003 | CLSLoss:0.0605 | top1:66.1745 | AUROC:0.9357\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.003956\n",
      "Train | 20/20 | Loss:0.0805 | MainLoss:0.0507 | Alpha:0.4898 | SPLoss:0.0001 | CLSLoss:0.0606 | top1:98.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1312 | MainLoss:0.1312 | SPLoss:0.0002 | CLSLoss:0.0608 | top1:95.6538 | AUROC:0.9928\n",
      "Test | 161/20 | Loss:1.1596 | MainLoss:1.1596 | SPLoss:0.0002 | CLSLoss:0.0608 | top1:66.3988 | AUROC:0.9357\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.003955\n",
      "Train | 20/20 | Loss:0.0768 | MainLoss:0.0469 | Alpha:0.4905 | SPLoss:0.0001 | CLSLoss:0.0609 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1368 | MainLoss:0.1368 | SPLoss:0.0003 | CLSLoss:0.0612 | top1:95.6154 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:1.1280 | MainLoss:1.1280 | SPLoss:0.0003 | CLSLoss:0.0612 | top1:67.2991 | AUROC:0.9386\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.003953\n",
      "Train | 20/20 | Loss:0.0904 | MainLoss:0.0605 | Alpha:0.4874 | SPLoss:0.0001 | CLSLoss:0.0612 | top1:98.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1311 | MainLoss:0.1311 | SPLoss:0.0003 | CLSLoss:0.0607 | top1:95.7308 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:1.1697 | MainLoss:1.1697 | SPLoss:0.0003 | CLSLoss:0.0607 | top1:66.2960 | AUROC:0.9364\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.003952\n",
      "Train | 20/20 | Loss:0.0787 | MainLoss:0.0488 | Alpha:0.4899 | SPLoss:0.0001 | CLSLoss:0.0608 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1357 | MainLoss:0.1357 | SPLoss:0.0003 | CLSLoss:0.0610 | top1:95.6923 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1371 | MainLoss:1.1371 | SPLoss:0.0003 | CLSLoss:0.0610 | top1:67.2056 | AUROC:0.9418\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.003950\n",
      "Train | 20/20 | Loss:0.0806 | MainLoss:0.0506 | Alpha:0.4898 | SPLoss:0.0001 | CLSLoss:0.0611 | top1:98.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1370 | MainLoss:0.1370 | SPLoss:0.0002 | CLSLoss:0.0609 | top1:95.6154 | AUROC:0.9928\n",
      "Test | 161/20 | Loss:1.1267 | MainLoss:1.1267 | SPLoss:0.0002 | CLSLoss:0.0609 | top1:67.4330 | AUROC:0.9440\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.003949\n",
      "Train | 20/20 | Loss:0.0775 | MainLoss:0.0475 | Alpha:0.4913 | SPLoss:0.0001 | CLSLoss:0.0609 | top1:98.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1355 | MainLoss:0.1355 | SPLoss:0.0002 | CLSLoss:0.0608 | top1:95.6795 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:1.1444 | MainLoss:1.1444 | SPLoss:0.0002 | CLSLoss:0.0608 | top1:67.1526 | AUROC:0.9426\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.003948\n",
      "Train | 20/20 | Loss:0.0813 | MainLoss:0.0515 | Alpha:0.4896 | SPLoss:0.0002 | CLSLoss:0.0609 | top1:98.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1282 | MainLoss:0.1282 | SPLoss:0.0004 | CLSLoss:0.0608 | top1:96.0128 | AUROC:0.9927\n",
      "Test | 161/20 | Loss:1.2149 | MainLoss:1.2149 | SPLoss:0.0004 | CLSLoss:0.0608 | top1:65.5576 | AUROC:0.9355\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.003946\n",
      "Train | 20/20 | Loss:0.0814 | MainLoss:0.0516 | Alpha:0.4902 | SPLoss:0.0001 | CLSLoss:0.0606 | top1:98.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1293 | MainLoss:0.1293 | SPLoss:0.0002 | CLSLoss:0.0605 | top1:95.8846 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1958 | MainLoss:1.1958 | SPLoss:0.0002 | CLSLoss:0.0605 | top1:65.7726 | AUROC:0.9371\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.003945\n",
      "Train | 20/20 | Loss:0.0783 | MainLoss:0.0485 | Alpha:0.4902 | SPLoss:0.0001 | CLSLoss:0.0608 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1331 | MainLoss:0.1331 | SPLoss:0.0002 | CLSLoss:0.0607 | top1:95.7692 | AUROC:0.9927\n",
      "Test | 161/20 | Loss:1.1750 | MainLoss:1.1750 | SPLoss:0.0002 | CLSLoss:0.0607 | top1:66.4486 | AUROC:0.9395\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.003943\n",
      "Train | 20/20 | Loss:0.0832 | MainLoss:0.0534 | Alpha:0.4895 | SPLoss:0.0002 | CLSLoss:0.0607 | top1:98.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1342 | MainLoss:0.1342 | SPLoss:0.0003 | CLSLoss:0.0605 | top1:95.7564 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1628 | MainLoss:1.1628 | SPLoss:0.0003 | CLSLoss:0.0605 | top1:66.7508 | AUROC:0.9395\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.003942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0697 | MainLoss:0.0398 | Alpha:0.4925 | SPLoss:0.0001 | CLSLoss:0.0607 | top1:99.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1317 | MainLoss:0.1317 | SPLoss:0.0001 | CLSLoss:0.0609 | top1:95.8462 | AUROC:0.9928\n",
      "Test | 161/20 | Loss:1.1983 | MainLoss:1.1983 | SPLoss:0.0001 | CLSLoss:0.0609 | top1:66.1745 | AUROC:0.9360\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.003940\n",
      "Train | 20/20 | Loss:0.0793 | MainLoss:0.0495 | Alpha:0.4915 | SPLoss:0.0001 | CLSLoss:0.0605 | top1:98.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1358 | MainLoss:0.1358 | SPLoss:0.0002 | CLSLoss:0.0604 | top1:95.7564 | AUROC:0.9928\n",
      "Test | 161/20 | Loss:1.1457 | MainLoss:1.1457 | SPLoss:0.0002 | CLSLoss:0.0604 | top1:67.2087 | AUROC:0.9401\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.003939\n",
      "Train | 20/20 | Loss:0.0750 | MainLoss:0.0453 | Alpha:0.4915 | SPLoss:0.0001 | CLSLoss:0.0604 | top1:98.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1328 | MainLoss:0.1328 | SPLoss:0.0002 | CLSLoss:0.0603 | top1:95.8077 | AUROC:0.9926\n",
      "Test | 161/20 | Loss:1.1696 | MainLoss:1.1696 | SPLoss:0.0002 | CLSLoss:0.0603 | top1:66.6449 | AUROC:0.9362\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.003937\n",
      "Train | 20/20 | Loss:0.0903 | MainLoss:0.0611 | Alpha:0.4879 | SPLoss:0.0002 | CLSLoss:0.0598 | top1:98.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1303 | MainLoss:0.1303 | SPLoss:0.0003 | CLSLoss:0.0597 | top1:95.9103 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:1.1670 | MainLoss:1.1670 | SPLoss:0.0003 | CLSLoss:0.0597 | top1:66.4579 | AUROC:0.9353\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.003936\n",
      "Train | 20/20 | Loss:0.0735 | MainLoss:0.0439 | Alpha:0.4918 | SPLoss:0.0001 | CLSLoss:0.0600 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1323 | MainLoss:0.1323 | SPLoss:0.0002 | CLSLoss:0.0603 | top1:95.8333 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.1734 | MainLoss:1.1734 | SPLoss:0.0002 | CLSLoss:0.0603 | top1:66.5358 | AUROC:0.9351\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.003934\n",
      "Train | 20/20 | Loss:0.0800 | MainLoss:0.0504 | Alpha:0.4909 | SPLoss:0.0001 | CLSLoss:0.0602 | top1:98.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1306 | MainLoss:0.1306 | SPLoss:0.0002 | CLSLoss:0.0601 | top1:95.8718 | AUROC:0.9930\n",
      "Test | 161/20 | Loss:1.1899 | MainLoss:1.1899 | SPLoss:0.0002 | CLSLoss:0.0601 | top1:66.2586 | AUROC:0.9343\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.003932\n",
      "Train | 20/20 | Loss:0.0901 | MainLoss:0.0609 | Alpha:0.4883 | SPLoss:0.0002 | CLSLoss:0.0597 | top1:98.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1324 | MainLoss:0.1324 | SPLoss:0.0003 | CLSLoss:0.0594 | top1:95.7179 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:1.1549 | MainLoss:1.1549 | SPLoss:0.0003 | CLSLoss:0.0594 | top1:66.6978 | AUROC:0.9390\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.003931\n",
      "Train | 20/20 | Loss:0.0722 | MainLoss:0.0427 | Alpha:0.4918 | SPLoss:0.0001 | CLSLoss:0.0598 | top1:99.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1317 | MainLoss:0.1317 | SPLoss:0.0002 | CLSLoss:0.0599 | top1:95.7821 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.1755 | MainLoss:1.1755 | SPLoss:0.0002 | CLSLoss:0.0599 | top1:66.4891 | AUROC:0.9374\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.003929\n",
      "Train | 20/20 | Loss:0.0822 | MainLoss:0.0530 | Alpha:0.4892 | SPLoss:0.0001 | CLSLoss:0.0596 | top1:98.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1385 | MainLoss:0.1385 | SPLoss:0.0003 | CLSLoss:0.0596 | top1:95.4487 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.1236 | MainLoss:1.1236 | SPLoss:0.0003 | CLSLoss:0.0596 | top1:67.6698 | AUROC:0.9426\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.003927\n",
      "Train | 20/20 | Loss:0.0750 | MainLoss:0.0457 | Alpha:0.4905 | SPLoss:0.0001 | CLSLoss:0.0598 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1353 | MainLoss:0.1353 | SPLoss:0.0002 | CLSLoss:0.0597 | top1:95.6410 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:1.1596 | MainLoss:1.1596 | SPLoss:0.0002 | CLSLoss:0.0597 | top1:67.0561 | AUROC:0.9402\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.003926\n",
      "Train | 20/20 | Loss:0.0714 | MainLoss:0.0419 | Alpha:0.4928 | SPLoss:0.0001 | CLSLoss:0.0598 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1289 | MainLoss:0.1289 | SPLoss:0.0003 | CLSLoss:0.0600 | top1:95.8974 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:1.2262 | MainLoss:1.2262 | SPLoss:0.0003 | CLSLoss:0.0600 | top1:65.6044 | AUROC:0.9339\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.003924\n",
      "Train | 20/20 | Loss:0.0819 | MainLoss:0.0525 | Alpha:0.4893 | SPLoss:0.0001 | CLSLoss:0.0599 | top1:98.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1292 | MainLoss:0.1292 | SPLoss:0.0003 | CLSLoss:0.0597 | top1:95.9359 | AUROC:0.9928\n",
      "Test | 161/20 | Loss:1.2202 | MainLoss:1.2202 | SPLoss:0.0003 | CLSLoss:0.0597 | top1:65.7165 | AUROC:0.9342\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.003922\n",
      "Train | 20/20 | Loss:0.0740 | MainLoss:0.0445 | Alpha:0.4911 | SPLoss:0.0001 | CLSLoss:0.0599 | top1:98.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1287 | MainLoss:0.1287 | SPLoss:0.0002 | CLSLoss:0.0598 | top1:95.9487 | AUROC:0.9930\n",
      "Test | 161/20 | Loss:1.2286 | MainLoss:1.2286 | SPLoss:0.0002 | CLSLoss:0.0598 | top1:65.5047 | AUROC:0.9335\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.003921\n",
      "Train | 20/20 | Loss:0.0810 | MainLoss:0.0517 | Alpha:0.4897 | SPLoss:0.0003 | CLSLoss:0.0595 | top1:98.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1276 | MainLoss:0.1276 | SPLoss:0.0004 | CLSLoss:0.0595 | top1:95.9359 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.2365 | MainLoss:1.2365 | SPLoss:0.0004 | CLSLoss:0.0595 | top1:65.2461 | AUROC:0.9317\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.003919\n",
      "Train | 20/20 | Loss:0.0747 | MainLoss:0.0454 | Alpha:0.4907 | SPLoss:0.0001 | CLSLoss:0.0597 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1298 | MainLoss:0.1298 | SPLoss:0.0003 | CLSLoss:0.0598 | top1:95.8718 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:1.2261 | MainLoss:1.2261 | SPLoss:0.0003 | CLSLoss:0.0598 | top1:65.5389 | AUROC:0.9304\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.003917\n",
      "Train | 20/20 | Loss:0.0689 | MainLoss:0.0393 | Alpha:0.4927 | SPLoss:0.0001 | CLSLoss:0.0600 | top1:99.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1282 | MainLoss:0.1282 | SPLoss:0.0002 | CLSLoss:0.0603 | top1:95.9615 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:1.2614 | MainLoss:1.2614 | SPLoss:0.0002 | CLSLoss:0.0603 | top1:64.9938 | AUROC:0.9262\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.003915\n",
      "Train | 20/20 | Loss:0.0790 | MainLoss:0.0494 | Alpha:0.4904 | SPLoss:0.0001 | CLSLoss:0.0602 | top1:98.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1278 | MainLoss:0.1278 | SPLoss:0.0003 | CLSLoss:0.0599 | top1:96.0000 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:1.2625 | MainLoss:1.2625 | SPLoss:0.0003 | CLSLoss:0.0599 | top1:64.9128 | AUROC:0.9275\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.003913\n",
      "Train | 20/20 | Loss:0.0824 | MainLoss:0.0532 | Alpha:0.4892 | SPLoss:0.0002 | CLSLoss:0.0596 | top1:98.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1299 | MainLoss:0.1299 | SPLoss:0.0003 | CLSLoss:0.0592 | top1:95.9231 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.2370 | MainLoss:1.2370 | SPLoss:0.0003 | CLSLoss:0.0592 | top1:65.3583 | AUROC:0.9295\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.003912\n",
      "Train | 20/20 | Loss:0.0815 | MainLoss:0.0525 | Alpha:0.4901 | SPLoss:0.0001 | CLSLoss:0.0591 | top1:98.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1332 | MainLoss:0.1332 | SPLoss:0.0003 | CLSLoss:0.0590 | top1:95.7692 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:1.1918 | MainLoss:1.1918 | SPLoss:0.0003 | CLSLoss:0.0590 | top1:66.2492 | AUROC:0.9345\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.003910\n",
      "Train | 20/20 | Loss:0.0703 | MainLoss:0.0412 | Alpha:0.4924 | SPLoss:0.0001 | CLSLoss:0.0591 | top1:99.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1292 | MainLoss:0.1292 | SPLoss:0.0003 | CLSLoss:0.0593 | top1:95.9744 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2314 | MainLoss:1.2314 | SPLoss:0.0003 | CLSLoss:0.0593 | top1:65.5047 | AUROC:0.9318\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.003908\n",
      "Train | 20/20 | Loss:0.0722 | MainLoss:0.0430 | Alpha:0.4917 | SPLoss:0.0001 | CLSLoss:0.0594 | top1:99.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1320 | MainLoss:0.1320 | SPLoss:0.0002 | CLSLoss:0.0595 | top1:95.8590 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.2241 | MainLoss:1.2241 | SPLoss:0.0002 | CLSLoss:0.0595 | top1:65.9502 | AUROC:0.9309\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.003906\n",
      "Train | 20/20 | Loss:0.0726 | MainLoss:0.0433 | Alpha:0.4922 | SPLoss:0.0001 | CLSLoss:0.0594 | top1:99.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1289 | MainLoss:0.1289 | SPLoss:0.0001 | CLSLoss:0.0594 | top1:96.0000 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2580 | MainLoss:1.2580 | SPLoss:0.0001 | CLSLoss:0.0594 | top1:65.1526 | AUROC:0.9274\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.003904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0679 | MainLoss:0.0385 | Alpha:0.4922 | SPLoss:0.0002 | CLSLoss:0.0596 | top1:99.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1306 | MainLoss:0.1306 | SPLoss:0.0003 | CLSLoss:0.0598 | top1:95.9872 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:1.2522 | MainLoss:1.2522 | SPLoss:0.0003 | CLSLoss:0.0598 | top1:65.4143 | AUROC:0.9283\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.003902\n",
      "Train | 20/20 | Loss:0.0812 | MainLoss:0.0519 | Alpha:0.4901 | SPLoss:0.0001 | CLSLoss:0.0597 | top1:98.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1284 | MainLoss:0.1284 | SPLoss:0.0002 | CLSLoss:0.0591 | top1:96.0385 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:1.2516 | MainLoss:1.2516 | SPLoss:0.0002 | CLSLoss:0.0591 | top1:65.2118 | AUROC:0.9269\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.003900\n",
      "Train | 20/20 | Loss:0.0766 | MainLoss:0.0476 | Alpha:0.4904 | SPLoss:0.0002 | CLSLoss:0.0589 | top1:98.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1285 | MainLoss:0.1285 | SPLoss:0.0004 | CLSLoss:0.0589 | top1:96.0256 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:1.2289 | MainLoss:1.2289 | SPLoss:0.0004 | CLSLoss:0.0589 | top1:65.5483 | AUROC:0.9282\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.003898\n",
      "Train | 20/20 | Loss:0.0759 | MainLoss:0.0471 | Alpha:0.4899 | SPLoss:0.0001 | CLSLoss:0.0588 | top1:98.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1278 | MainLoss:0.1278 | SPLoss:0.0002 | CLSLoss:0.0589 | top1:96.0897 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:1.2487 | MainLoss:1.2487 | SPLoss:0.0002 | CLSLoss:0.0589 | top1:65.2710 | AUROC:0.9269\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.003896\n",
      "Train | 20/20 | Loss:0.0747 | MainLoss:0.0458 | Alpha:0.4913 | SPLoss:0.0001 | CLSLoss:0.0587 | top1:98.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1317 | MainLoss:0.1317 | SPLoss:0.0002 | CLSLoss:0.0589 | top1:95.8718 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:1.2109 | MainLoss:1.2109 | SPLoss:0.0002 | CLSLoss:0.0589 | top1:66.0280 | AUROC:0.9306\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.003894\n",
      "Train | 20/20 | Loss:0.0698 | MainLoss:0.0408 | Alpha:0.4915 | SPLoss:0.0001 | CLSLoss:0.0590 | top1:99.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1313 | MainLoss:0.1313 | SPLoss:0.0002 | CLSLoss:0.0591 | top1:95.8974 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2254 | MainLoss:1.2254 | SPLoss:0.0002 | CLSLoss:0.0591 | top1:65.8411 | AUROC:0.9286\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.003892\n",
      "Train | 20/20 | Loss:0.0883 | MainLoss:0.0595 | Alpha:0.4878 | SPLoss:0.0003 | CLSLoss:0.0588 | top1:98.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1245 | MainLoss:0.1245 | SPLoss:0.0004 | CLSLoss:0.0584 | top1:96.1282 | AUROC:0.9935\n",
      "Test | 161/20 | Loss:1.2605 | MainLoss:1.2605 | SPLoss:0.0004 | CLSLoss:0.0584 | top1:64.7913 | AUROC:0.9251\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.003890\n",
      "Train | 20/20 | Loss:0.0751 | MainLoss:0.0463 | Alpha:0.4913 | SPLoss:0.0001 | CLSLoss:0.0585 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1236 | MainLoss:0.1236 | SPLoss:0.0002 | CLSLoss:0.0583 | top1:96.1154 | AUROC:0.9930\n",
      "Test | 161/20 | Loss:1.2673 | MainLoss:1.2673 | SPLoss:0.0002 | CLSLoss:0.0583 | top1:64.6542 | AUROC:0.9248\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.003888\n",
      "Train | 20/20 | Loss:0.0700 | MainLoss:0.0413 | Alpha:0.4912 | SPLoss:0.0001 | CLSLoss:0.0583 | top1:99.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1285 | MainLoss:0.1285 | SPLoss:0.0002 | CLSLoss:0.0586 | top1:95.9487 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2258 | MainLoss:1.2258 | SPLoss:0.0002 | CLSLoss:0.0586 | top1:65.7695 | AUROC:0.9300\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.003886\n",
      "Train | 20/20 | Loss:0.0794 | MainLoss:0.0506 | Alpha:0.4898 | SPLoss:0.0001 | CLSLoss:0.0587 | top1:98.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1275 | MainLoss:0.1275 | SPLoss:0.0003 | CLSLoss:0.0584 | top1:95.9359 | AUROC:0.9935\n",
      "Test | 161/20 | Loss:1.2317 | MainLoss:1.2317 | SPLoss:0.0003 | CLSLoss:0.0584 | top1:65.5981 | AUROC:0.9288\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.003884\n",
      "Train | 20/20 | Loss:0.0841 | MainLoss:0.0556 | Alpha:0.4888 | SPLoss:0.0001 | CLSLoss:0.0582 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1267 | MainLoss:0.1267 | SPLoss:0.0003 | CLSLoss:0.0576 | top1:95.9487 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2127 | MainLoss:1.2127 | SPLoss:0.0003 | CLSLoss:0.0576 | top1:65.7664 | AUROC:0.9300\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.003882\n",
      "Train | 20/20 | Loss:0.0813 | MainLoss:0.0530 | Alpha:0.4905 | SPLoss:0.0001 | CLSLoss:0.0576 | top1:98.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1246 | MainLoss:0.1246 | SPLoss:0.0002 | CLSLoss:0.0575 | top1:96.0000 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2303 | MainLoss:1.2303 | SPLoss:0.0002 | CLSLoss:0.0575 | top1:65.3146 | AUROC:0.9294\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.003880\n",
      "Train | 20/20 | Loss:0.0760 | MainLoss:0.0476 | Alpha:0.4905 | SPLoss:0.0001 | CLSLoss:0.0577 | top1:98.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1252 | MainLoss:0.1252 | SPLoss:0.0002 | CLSLoss:0.0576 | top1:95.9872 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:1.2295 | MainLoss:1.2295 | SPLoss:0.0002 | CLSLoss:0.0576 | top1:65.4268 | AUROC:0.9306\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.003877\n",
      "Train | 20/20 | Loss:0.0777 | MainLoss:0.0494 | Alpha:0.4902 | SPLoss:0.0001 | CLSLoss:0.0576 | top1:98.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1224 | MainLoss:0.1224 | SPLoss:0.0002 | CLSLoss:0.0576 | top1:96.1026 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:1.2675 | MainLoss:1.2675 | SPLoss:0.0002 | CLSLoss:0.0576 | top1:64.7165 | AUROC:0.9270\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.003875\n",
      "Train | 20/20 | Loss:0.0683 | MainLoss:0.0398 | Alpha:0.4926 | SPLoss:0.0001 | CLSLoss:0.0578 | top1:99.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1276 | MainLoss:0.1276 | SPLoss:0.0002 | CLSLoss:0.0581 | top1:95.9744 | AUROC:0.9935\n",
      "Test | 161/20 | Loss:1.2266 | MainLoss:1.2266 | SPLoss:0.0002 | CLSLoss:0.0581 | top1:65.7477 | AUROC:0.9308\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.003873\n",
      "Train | 20/20 | Loss:0.0709 | MainLoss:0.0423 | Alpha:0.4911 | SPLoss:0.0001 | CLSLoss:0.0582 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1277 | MainLoss:0.1277 | SPLoss:0.0002 | CLSLoss:0.0581 | top1:95.9359 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2263 | MainLoss:1.2263 | SPLoss:0.0002 | CLSLoss:0.0581 | top1:65.8816 | AUROC:0.9313\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.003871\n",
      "Train | 20/20 | Loss:0.0728 | MainLoss:0.0441 | Alpha:0.4916 | SPLoss:0.0001 | CLSLoss:0.0582 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1256 | MainLoss:0.1256 | SPLoss:0.0002 | CLSLoss:0.0582 | top1:96.0128 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2480 | MainLoss:1.2480 | SPLoss:0.0002 | CLSLoss:0.0582 | top1:65.3988 | AUROC:0.9287\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.003869\n",
      "Train | 20/20 | Loss:0.0765 | MainLoss:0.0479 | Alpha:0.4903 | SPLoss:0.0001 | CLSLoss:0.0580 | top1:98.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1266 | MainLoss:0.1266 | SPLoss:0.0003 | CLSLoss:0.0581 | top1:95.9359 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2422 | MainLoss:1.2422 | SPLoss:0.0003 | CLSLoss:0.0581 | top1:65.5639 | AUROC:0.9269\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.003866\n",
      "Train | 20/20 | Loss:0.0729 | MainLoss:0.0444 | Alpha:0.4911 | SPLoss:0.0001 | CLSLoss:0.0580 | top1:98.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1258 | MainLoss:0.1258 | SPLoss:0.0002 | CLSLoss:0.0580 | top1:95.9872 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2415 | MainLoss:1.2415 | SPLoss:0.0002 | CLSLoss:0.0580 | top1:65.6293 | AUROC:0.9266\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.003864\n",
      "Train | 20/20 | Loss:0.0768 | MainLoss:0.0483 | Alpha:0.4911 | SPLoss:0.0001 | CLSLoss:0.0580 | top1:98.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1291 | MainLoss:0.1291 | SPLoss:0.0002 | CLSLoss:0.0577 | top1:95.7692 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2169 | MainLoss:1.2169 | SPLoss:0.0002 | CLSLoss:0.0577 | top1:66.0841 | AUROC:0.9276\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.003862\n",
      "Train | 20/20 | Loss:0.0688 | MainLoss:0.0403 | Alpha:0.4927 | SPLoss:0.0001 | CLSLoss:0.0579 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1295 | MainLoss:0.1295 | SPLoss:0.0002 | CLSLoss:0.0579 | top1:95.7821 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:1.2216 | MainLoss:1.2216 | SPLoss:0.0002 | CLSLoss:0.0579 | top1:66.0810 | AUROC:0.9281\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.003860\n",
      "Train | 20/20 | Loss:0.0726 | MainLoss:0.0441 | Alpha:0.4912 | SPLoss:0.0002 | CLSLoss:0.0579 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1208 | MainLoss:0.1208 | SPLoss:0.0004 | CLSLoss:0.0579 | top1:96.2179 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.3273 | MainLoss:1.3273 | SPLoss:0.0004 | CLSLoss:0.0579 | top1:63.8255 | AUROC:0.9161\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.003857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0645 | MainLoss:0.0358 | Alpha:0.4932 | SPLoss:0.0001 | CLSLoss:0.0581 | top1:99.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1251 | MainLoss:0.1251 | SPLoss:0.0002 | CLSLoss:0.0583 | top1:96.1667 | AUROC:0.9937\n",
      "Test | 161/20 | Loss:1.2843 | MainLoss:1.2843 | SPLoss:0.0002 | CLSLoss:0.0583 | top1:64.8380 | AUROC:0.9211\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.003855\n",
      "Train | 20/20 | Loss:0.0686 | MainLoss:0.0399 | Alpha:0.4916 | SPLoss:0.0001 | CLSLoss:0.0582 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1276 | MainLoss:0.1276 | SPLoss:0.0003 | CLSLoss:0.0583 | top1:95.9615 | AUROC:0.9935\n",
      "Test | 161/20 | Loss:1.2523 | MainLoss:1.2523 | SPLoss:0.0003 | CLSLoss:0.0583 | top1:65.4393 | AUROC:0.9232\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.003853\n",
      "Train | 20/20 | Loss:0.0679 | MainLoss:0.0392 | Alpha:0.4927 | SPLoss:0.0001 | CLSLoss:0.0582 | top1:99.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1272 | MainLoss:0.1272 | SPLoss:0.0002 | CLSLoss:0.0584 | top1:96.0128 | AUROC:0.9935\n",
      "Test | 161/20 | Loss:1.2731 | MainLoss:1.2731 | SPLoss:0.0002 | CLSLoss:0.0584 | top1:65.1495 | AUROC:0.9206\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.003850\n",
      "Train | 20/20 | Loss:0.0703 | MainLoss:0.0415 | Alpha:0.4920 | SPLoss:0.0001 | CLSLoss:0.0584 | top1:99.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1300 | MainLoss:0.1300 | SPLoss:0.0002 | CLSLoss:0.0581 | top1:95.8590 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2412 | MainLoss:1.2412 | SPLoss:0.0002 | CLSLoss:0.0581 | top1:65.6947 | AUROC:0.9238\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.003848\n",
      "Train | 20/20 | Loss:0.0724 | MainLoss:0.0438 | Alpha:0.4918 | SPLoss:0.0001 | CLSLoss:0.0581 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1227 | MainLoss:0.1227 | SPLoss:0.0002 | CLSLoss:0.0579 | top1:96.1410 | AUROC:0.9937\n",
      "Test | 161/20 | Loss:1.3171 | MainLoss:1.3171 | SPLoss:0.0002 | CLSLoss:0.0579 | top1:64.1371 | AUROC:0.9146\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.003845\n",
      "Train | 20/20 | Loss:0.0723 | MainLoss:0.0439 | Alpha:0.4915 | SPLoss:0.0001 | CLSLoss:0.0576 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1242 | MainLoss:0.1242 | SPLoss:0.0002 | CLSLoss:0.0577 | top1:96.1538 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:1.2974 | MainLoss:1.2974 | SPLoss:0.0002 | CLSLoss:0.0577 | top1:64.6075 | AUROC:0.9177\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.003843\n",
      "Train | 20/20 | Loss:0.0716 | MainLoss:0.0433 | Alpha:0.4921 | SPLoss:0.0001 | CLSLoss:0.0574 | top1:99.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1286 | MainLoss:0.1286 | SPLoss:0.0002 | CLSLoss:0.0574 | top1:96.0128 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2603 | MainLoss:1.2603 | SPLoss:0.0002 | CLSLoss:0.0574 | top1:65.3583 | AUROC:0.9216\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.003840\n",
      "Train | 20/20 | Loss:0.0674 | MainLoss:0.0391 | Alpha:0.4925 | SPLoss:0.0001 | CLSLoss:0.0575 | top1:99.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1280 | MainLoss:0.1280 | SPLoss:0.0001 | CLSLoss:0.0574 | top1:96.0769 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2695 | MainLoss:1.2695 | SPLoss:0.0001 | CLSLoss:0.0574 | top1:65.2305 | AUROC:0.9207\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.003838\n",
      "Train | 20/20 | Loss:0.0648 | MainLoss:0.0364 | Alpha:0.4923 | SPLoss:0.0001 | CLSLoss:0.0576 | top1:99.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1229 | MainLoss:0.1229 | SPLoss:0.0002 | CLSLoss:0.0576 | top1:96.1795 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.3407 | MainLoss:1.3407 | SPLoss:0.0002 | CLSLoss:0.0576 | top1:63.9470 | AUROC:0.9122\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.003836\n",
      "Train | 20/20 | Loss:0.0676 | MainLoss:0.0392 | Alpha:0.4927 | SPLoss:0.0000 | CLSLoss:0.0577 | top1:99.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1260 | MainLoss:0.1260 | SPLoss:0.0001 | CLSLoss:0.0574 | top1:96.0513 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:1.3015 | MainLoss:1.3015 | SPLoss:0.0001 | CLSLoss:0.0574 | top1:64.7321 | AUROC:0.9188\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.003833\n",
      "Train | 20/20 | Loss:0.0728 | MainLoss:0.0446 | Alpha:0.4919 | SPLoss:0.0001 | CLSLoss:0.0572 | top1:99.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1260 | MainLoss:0.1260 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:96.0641 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:1.2948 | MainLoss:1.2948 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:64.8131 | AUROC:0.9186\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.003830\n",
      "Train | 20/20 | Loss:0.0725 | MainLoss:0.0445 | Alpha:0.4905 | SPLoss:0.0002 | CLSLoss:0.0570 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1288 | MainLoss:0.1288 | SPLoss:0.0003 | CLSLoss:0.0569 | top1:95.9487 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2465 | MainLoss:1.2465 | SPLoss:0.0003 | CLSLoss:0.0569 | top1:65.5701 | AUROC:0.9234\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.003828\n",
      "Train | 20/20 | Loss:0.0696 | MainLoss:0.0415 | Alpha:0.4920 | SPLoss:0.0001 | CLSLoss:0.0570 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1247 | MainLoss:0.1247 | SPLoss:0.0002 | CLSLoss:0.0569 | top1:96.1154 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:1.2751 | MainLoss:1.2751 | SPLoss:0.0002 | CLSLoss:0.0569 | top1:64.9408 | AUROC:0.9196\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.003825\n",
      "Train | 20/20 | Loss:0.0692 | MainLoss:0.0412 | Alpha:0.4920 | SPLoss:0.0001 | CLSLoss:0.0569 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1232 | MainLoss:0.1232 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:96.1538 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2948 | MainLoss:1.2948 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:64.5919 | AUROC:0.9175\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.003823\n",
      "Train | 20/20 | Loss:0.0724 | MainLoss:0.0443 | Alpha:0.4925 | SPLoss:0.0002 | CLSLoss:0.0569 | top1:98.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1250 | MainLoss:0.1250 | SPLoss:0.0003 | CLSLoss:0.0569 | top1:95.9487 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2621 | MainLoss:1.2621 | SPLoss:0.0003 | CLSLoss:0.0569 | top1:65.2461 | AUROC:0.9224\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.003820\n",
      "Train | 20/20 | Loss:0.0735 | MainLoss:0.0456 | Alpha:0.4908 | SPLoss:0.0001 | CLSLoss:0.0567 | top1:98.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1227 | MainLoss:0.1227 | SPLoss:0.0003 | CLSLoss:0.0566 | top1:96.0385 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2818 | MainLoss:1.2818 | SPLoss:0.0003 | CLSLoss:0.0566 | top1:64.7259 | AUROC:0.9204\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.003818\n",
      "Train | 20/20 | Loss:0.0740 | MainLoss:0.0461 | Alpha:0.4906 | SPLoss:0.0001 | CLSLoss:0.0567 | top1:98.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1236 | MainLoss:0.1236 | SPLoss:0.0002 | CLSLoss:0.0564 | top1:95.9615 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2785 | MainLoss:1.2785 | SPLoss:0.0002 | CLSLoss:0.0564 | top1:64.8474 | AUROC:0.9198\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.003815\n",
      "Train | 20/20 | Loss:0.0822 | MainLoss:0.0547 | Alpha:0.4890 | SPLoss:0.0001 | CLSLoss:0.0561 | top1:98.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1228 | MainLoss:0.1228 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:96.0128 | AUROC:0.9941\n",
      "Test | 161/20 | Loss:1.2748 | MainLoss:1.2748 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:64.6480 | AUROC:0.9200\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.003812\n",
      "Train | 20/20 | Loss:0.0681 | MainLoss:0.0406 | Alpha:0.4923 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:98.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1241 | MainLoss:0.1241 | SPLoss:0.0002 | CLSLoss:0.0559 | top1:96.0000 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:1.2656 | MainLoss:1.2656 | SPLoss:0.0002 | CLSLoss:0.0559 | top1:64.9284 | AUROC:0.9214\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.003810\n",
      "Train | 20/20 | Loss:0.0662 | MainLoss:0.0386 | Alpha:0.4927 | SPLoss:0.0000 | CLSLoss:0.0560 | top1:99.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1242 | MainLoss:0.1242 | SPLoss:0.0001 | CLSLoss:0.0561 | top1:96.0000 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2786 | MainLoss:1.2786 | SPLoss:0.0001 | CLSLoss:0.0561 | top1:64.8131 | AUROC:0.9212\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.003807\n",
      "Train | 20/20 | Loss:0.0657 | MainLoss:0.0380 | Alpha:0.4923 | SPLoss:0.0001 | CLSLoss:0.0563 | top1:99.3421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1237 | MainLoss:0.1237 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:96.0385 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:1.2978 | MainLoss:1.2978 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:64.5483 | AUROC:0.9200\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.003804\n",
      "Train | 20/20 | Loss:0.0584 | MainLoss:0.0303 | Alpha:0.4940 | SPLoss:0.0000 | CLSLoss:0.0569 | top1:99.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1278 | MainLoss:0.1278 | SPLoss:0.0001 | CLSLoss:0.0571 | top1:95.9744 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2681 | MainLoss:1.2681 | SPLoss:0.0001 | CLSLoss:0.0571 | top1:65.4081 | AUROC:0.9232\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.003802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0667 | MainLoss:0.0386 | Alpha:0.4926 | SPLoss:0.0001 | CLSLoss:0.0570 | top1:99.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1231 | MainLoss:0.1231 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:96.1795 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.3333 | MainLoss:1.3333 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:64.1215 | AUROC:0.9164\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.003799\n",
      "Train | 20/20 | Loss:0.0631 | MainLoss:0.0348 | Alpha:0.4933 | SPLoss:0.0001 | CLSLoss:0.0572 | top1:99.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1266 | MainLoss:0.1266 | SPLoss:0.0002 | CLSLoss:0.0572 | top1:96.0769 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2994 | MainLoss:1.2994 | SPLoss:0.0002 | CLSLoss:0.0572 | top1:64.8847 | AUROC:0.9198\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.003796\n",
      "Train | 20/20 | Loss:0.0693 | MainLoss:0.0412 | Alpha:0.4922 | SPLoss:0.0001 | CLSLoss:0.0571 | top1:99.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1322 | MainLoss:0.1322 | SPLoss:0.0002 | CLSLoss:0.0569 | top1:95.8718 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2451 | MainLoss:1.2451 | SPLoss:0.0002 | CLSLoss:0.0569 | top1:65.9315 | AUROC:0.9238\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.003793\n",
      "Train | 20/20 | Loss:0.0634 | MainLoss:0.0352 | Alpha:0.4940 | SPLoss:0.0002 | CLSLoss:0.0568 | top1:99.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1292 | MainLoss:0.1292 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:96.0385 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2707 | MainLoss:1.2707 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:65.5514 | AUROC:0.9209\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.003790\n",
      "Train | 20/20 | Loss:0.0716 | MainLoss:0.0436 | Alpha:0.4914 | SPLoss:0.0001 | CLSLoss:0.0569 | top1:99.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1278 | MainLoss:0.1278 | SPLoss:0.0002 | CLSLoss:0.0567 | top1:96.0256 | AUROC:0.9941\n",
      "Test | 161/20 | Loss:1.2712 | MainLoss:1.2712 | SPLoss:0.0002 | CLSLoss:0.0567 | top1:65.4455 | AUROC:0.9194\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.003788\n",
      "Train | 20/20 | Loss:0.0606 | MainLoss:0.0324 | Alpha:0.4933 | SPLoss:0.0001 | CLSLoss:0.0569 | top1:99.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1286 | MainLoss:0.1286 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:96.0256 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2702 | MainLoss:1.2702 | SPLoss:0.0002 | CLSLoss:0.0571 | top1:65.5794 | AUROC:0.9197\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.003785\n",
      "Train | 20/20 | Loss:0.0711 | MainLoss:0.0431 | Alpha:0.4915 | SPLoss:0.0001 | CLSLoss:0.0569 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1277 | MainLoss:0.1277 | SPLoss:0.0003 | CLSLoss:0.0568 | top1:96.0513 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:1.2843 | MainLoss:1.2843 | SPLoss:0.0003 | CLSLoss:0.0568 | top1:65.3645 | AUROC:0.9174\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.003782\n",
      "Train | 20/20 | Loss:0.0694 | MainLoss:0.0415 | Alpha:0.4913 | SPLoss:0.0001 | CLSLoss:0.0566 | top1:99.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1289 | MainLoss:0.1289 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:96.0385 | AUROC:0.9941\n",
      "Test | 161/20 | Loss:1.2693 | MainLoss:1.2693 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:65.5888 | AUROC:0.9222\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.003779\n",
      "Train | 20/20 | Loss:0.0679 | MainLoss:0.0399 | Alpha:0.4929 | SPLoss:0.0001 | CLSLoss:0.0566 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1287 | MainLoss:0.1287 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:96.0385 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2804 | MainLoss:1.2804 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:65.4174 | AUROC:0.9205\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.003776\n",
      "Train | 20/20 | Loss:0.0643 | MainLoss:0.0365 | Alpha:0.4928 | SPLoss:0.0001 | CLSLoss:0.0564 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1248 | MainLoss:0.1248 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:96.1154 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.3205 | MainLoss:1.3205 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:64.5483 | AUROC:0.9157\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.003773\n",
      "Train | 20/20 | Loss:0.0674 | MainLoss:0.0396 | Alpha:0.4923 | SPLoss:0.0001 | CLSLoss:0.0563 | top1:99.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1230 | MainLoss:0.1230 | SPLoss:0.0002 | CLSLoss:0.0563 | top1:96.0897 | AUROC:0.9942\n",
      "Test | 161/20 | Loss:1.3301 | MainLoss:1.3301 | SPLoss:0.0002 | CLSLoss:0.0563 | top1:64.2773 | AUROC:0.9157\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.003770\n",
      "Train | 20/20 | Loss:0.0707 | MainLoss:0.0430 | Alpha:0.4916 | SPLoss:0.0001 | CLSLoss:0.0561 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1243 | MainLoss:0.1243 | SPLoss:0.0002 | CLSLoss:0.0560 | top1:96.1282 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.3099 | MainLoss:1.3099 | SPLoss:0.0002 | CLSLoss:0.0560 | top1:64.6636 | AUROC:0.9175\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.003768\n",
      "Train | 20/20 | Loss:0.0720 | MainLoss:0.0446 | Alpha:0.4917 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:99.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1232 | MainLoss:0.1232 | SPLoss:0.0002 | CLSLoss:0.0557 | top1:96.1282 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.3045 | MainLoss:1.3045 | SPLoss:0.0002 | CLSLoss:0.0557 | top1:64.5483 | AUROC:0.9167\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.003765\n",
      "Train | 20/20 | Loss:0.0729 | MainLoss:0.0457 | Alpha:0.4907 | SPLoss:0.0001 | CLSLoss:0.0555 | top1:98.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1203 | MainLoss:0.1203 | SPLoss:0.0003 | CLSLoss:0.0553 | top1:96.1410 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.3074 | MainLoss:1.3074 | SPLoss:0.0003 | CLSLoss:0.0553 | top1:64.1526 | AUROC:0.9164\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.003762\n",
      "Train | 20/20 | Loss:0.0628 | MainLoss:0.0354 | Alpha:0.4929 | SPLoss:0.0001 | CLSLoss:0.0555 | top1:99.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1234 | MainLoss:0.1234 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:96.0769 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2910 | MainLoss:1.2910 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:64.8193 | AUROC:0.9197\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.003759\n",
      "Train | 20/20 | Loss:0.0677 | MainLoss:0.0402 | Alpha:0.4918 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:99.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1280 | MainLoss:0.1280 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:96.0000 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:1.2414 | MainLoss:1.2414 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:65.7788 | AUROC:0.9232\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.003756\n",
      "Train | 20/20 | Loss:0.0643 | MainLoss:0.0368 | Alpha:0.4930 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:99.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1282 | MainLoss:0.1282 | SPLoss:0.0002 | CLSLoss:0.0558 | top1:95.9872 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2574 | MainLoss:1.2574 | SPLoss:0.0002 | CLSLoss:0.0558 | top1:65.6075 | AUROC:0.9239\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.003753\n",
      "Train | 20/20 | Loss:0.0737 | MainLoss:0.0463 | Alpha:0.4907 | SPLoss:0.0001 | CLSLoss:0.0557 | top1:98.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1283 | MainLoss:0.1283 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:95.9103 | AUROC:0.9940\n",
      "Test | 161/20 | Loss:1.2455 | MainLoss:1.2455 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:65.8349 | AUROC:0.9251\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.003750\n",
      "Train | 20/20 | Loss:0.0677 | MainLoss:0.0404 | Alpha:0.4920 | SPLoss:0.0001 | CLSLoss:0.0553 | top1:98.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1297 | MainLoss:0.1297 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:95.9231 | AUROC:0.9943\n",
      "Test | 161/20 | Loss:1.2408 | MainLoss:1.2408 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:65.9875 | AUROC:0.9248\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.003747\n",
      "Train | 20/20 | Loss:0.0677 | MainLoss:0.0405 | Alpha:0.4922 | SPLoss:0.0001 | CLSLoss:0.0552 | top1:99.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1250 | MainLoss:0.1250 | SPLoss:0.0002 | CLSLoss:0.0552 | top1:96.0641 | AUROC:0.9941\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    \n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "\n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
