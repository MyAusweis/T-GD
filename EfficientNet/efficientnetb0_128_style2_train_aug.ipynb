{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 512\n",
    "test_batch = 512\n",
    "lr = 0.1\n",
    "schedule = [75, 175, 250]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/b0/aug' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.2\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.2\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.2\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.', 'Train AUROC.', 'Valid AUROC.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        arc.update(auroc, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 300] LR: 0.100000\n",
      "1/152 | Loss:0.7040 | top1:47.2656 | AUROC:0.4984\n",
      "101/152 | Loss:2.7218 | top1:50.9031 | AUROC:0.5029\n",
      "152/152 | Loss:2.0512 | top1:51.4099 | AUROC:0.5030\n",
      "16/16 | Loss:0.6932 | top1:49.9359 | AUROC:0.5000\n",
      "\n",
      "Epoch: [2 | 300] LR: 0.170000\n",
      "1/152 | Loss:0.6966 | top1:50.5859 | AUROC:0.4998\n",
      "101/152 | Loss:0.6968 | top1:52.9394 | AUROC:0.4973\n",
      "152/152 | Loss:0.6955 | top1:53.0086 | AUROC:0.4978\n",
      "16/16 | Loss:0.7010 | top1:50.0000 | AUROC:0.5000\n",
      "\n",
      "Epoch: [3 | 300] LR: 0.240000\n",
      "1/152 | Loss:0.6970 | top1:50.7812 | AUROC:0.5157\n",
      "101/152 | Loss:0.6927 | top1:53.4576 | AUROC:0.4999\n",
      "152/152 | Loss:0.6925 | top1:53.3436 | AUROC:0.4994\n",
      "16/16 | Loss:0.6953 | top1:50.0000 | AUROC:0.5000\n",
      "\n",
      "Epoch: [4 | 300] LR: 0.310000\n",
      "1/152 | Loss:0.6914 | top1:54.4922 | AUROC:0.4876\n",
      "101/152 | Loss:0.6925 | top1:53.1250 | AUROC:0.5019\n",
      "152/152 | Loss:0.6920 | top1:53.4988 | AUROC:0.5028\n",
      "16/16 | Loss:0.6973 | top1:50.0000 | AUROC:0.5022\n",
      "\n",
      "Epoch: [5 | 300] LR: 0.380000\n",
      "1/152 | Loss:0.6885 | top1:55.8594 | AUROC:0.4731\n",
      "101/152 | Loss:0.6925 | top1:53.6433 | AUROC:0.4997\n",
      "152/152 | Loss:0.6915 | top1:54.0265 | AUROC:0.4985\n",
      "16/16 | Loss:0.6955 | top1:50.0000 | AUROC:0.5054\n",
      "\n",
      "Epoch: [6 | 300] LR: 0.450000\n",
      "1/152 | Loss:0.6970 | top1:48.2422 | AUROC:0.5004\n",
      "101/152 | Loss:0.6899 | top1:54.3472 | AUROC:0.5034\n",
      "152/152 | Loss:0.6900 | top1:54.3913 | AUROC:0.5012\n",
      "16/16 | Loss:0.7023 | top1:50.0000 | AUROC:0.5114\n",
      "\n",
      "Epoch: [7 | 300] LR: 0.520000\n",
      "1/152 | Loss:0.6945 | top1:52.9297 | AUROC:0.4856\n",
      "101/152 | Loss:0.6905 | top1:53.7225 | AUROC:0.5029\n",
      "152/152 | Loss:0.6912 | top1:53.6437 | AUROC:0.5015\n",
      "16/16 | Loss:0.7040 | top1:50.0000 | AUROC:0.5183\n",
      "\n",
      "Epoch: [8 | 300] LR: 0.590000\n",
      "1/152 | Loss:0.6869 | top1:55.4688 | AUROC:0.5464\n",
      "101/152 | Loss:0.6917 | top1:53.5427 | AUROC:0.5072\n",
      "152/152 | Loss:0.6917 | top1:53.7109 | AUROC:0.5067\n",
      "16/16 | Loss:0.6930 | top1:50.0128 | AUROC:0.5303\n",
      "\n",
      "Epoch: [9 | 300] LR: 0.660000\n",
      "1/152 | Loss:0.6916 | top1:56.6406 | AUROC:0.5287\n",
      "101/152 | Loss:0.6894 | top1:54.3839 | AUROC:0.5329\n",
      "152/152 | Loss:0.6842 | top1:55.3342 | AUROC:0.5658\n",
      "16/16 | Loss:0.6004 | top1:68.5256 | AUROC:0.7477\n",
      "\n",
      "Epoch: [10 | 300] LR: 0.730000\n",
      "1/152 | Loss:0.6411 | top1:63.8672 | AUROC:0.6802\n",
      "101/152 | Loss:0.5133 | top1:75.1044 | AUROC:0.8376\n",
      "152/152 | Loss:0.4378 | top1:79.4599 | AUROC:0.8757\n",
      "16/16 | Loss:0.2506 | top1:90.6795 | AUROC:0.9748\n",
      "\n",
      "Epoch: [11 | 300] LR: 0.800000\n",
      "1/152 | Loss:0.2821 | top1:87.8906 | AUROC:0.9610\n",
      "101/152 | Loss:0.1924 | top1:92.6110 | AUROC:0.9788\n",
      "152/152 | Loss:0.1758 | top1:93.2507 | AUROC:0.9820\n",
      "16/16 | Loss:0.1017 | top1:96.1026 | AUROC:0.9949\n",
      "\n",
      "Epoch: [12 | 300] LR: 0.800000\n",
      "1/152 | Loss:0.1166 | top1:95.3125 | AUROC:0.9907\n",
      "101/152 | Loss:0.1157 | top1:95.7785 | AUROC:0.9923\n",
      "152/152 | Loss:0.1105 | top1:95.9735 | AUROC:0.9929\n",
      "16/16 | Loss:0.0536 | top1:98.1154 | AUROC:0.9977\n",
      "\n",
      "Epoch: [13 | 300] LR: 0.799978\n",
      "1/152 | Loss:0.0602 | top1:98.4375 | AUROC:0.9975\n",
      "101/152 | Loss:0.0845 | top1:96.9543 | AUROC:0.9957\n",
      "152/152 | Loss:0.0825 | top1:97.0121 | AUROC:0.9960\n",
      "16/16 | Loss:0.0489 | top1:98.3590 | AUROC:0.9986\n",
      "\n",
      "Epoch: [14 | 300] LR: 0.799912\n",
      "1/152 | Loss:0.0684 | top1:97.8516 | AUROC:0.9980\n",
      "101/152 | Loss:0.0734 | top1:97.3971 | AUROC:0.9968\n",
      "152/152 | Loss:0.0694 | top1:97.5204 | AUROC:0.9971\n",
      "16/16 | Loss:0.0354 | top1:98.6795 | AUROC:0.9993\n",
      "\n",
      "Epoch: [15 | 300] LR: 0.799803\n",
      "1/152 | Loss:0.0762 | top1:97.2656 | AUROC:0.9990\n",
      "101/152 | Loss:0.0661 | top1:97.6795 | AUROC:0.9974\n",
      "152/152 | Loss:0.0630 | top1:97.8050 | AUROC:0.9976\n",
      "16/16 | Loss:0.0270 | top1:99.1538 | AUROC:0.9993\n",
      "\n",
      "Epoch: [16 | 300] LR: 0.799649\n",
      "1/152 | Loss:0.0643 | top1:98.4375 | AUROC:0.9957\n",
      "101/152 | Loss:0.0598 | top1:97.8786 | AUROC:0.9980\n",
      "152/152 | Loss:0.0576 | top1:97.9628 | AUROC:0.9981\n",
      "16/16 | Loss:0.0292 | top1:98.9744 | AUROC:0.9994\n",
      "\n",
      "Epoch: [17 | 300] LR: 0.799452\n",
      "1/152 | Loss:0.0456 | top1:98.2422 | AUROC:0.9986\n",
      "101/152 | Loss:0.0549 | top1:98.1474 | AUROC:0.9979\n",
      "152/152 | Loss:0.0505 | top1:98.2512 | AUROC:0.9982\n",
      "16/16 | Loss:0.0292 | top1:99.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [18 | 300] LR: 0.799211\n",
      "1/152 | Loss:0.0531 | top1:98.8281 | AUROC:0.9980\n",
      "101/152 | Loss:0.0424 | top1:98.5129 | AUROC:0.9986\n",
      "152/152 | Loss:0.0414 | top1:98.5371 | AUROC:0.9988\n",
      "16/16 | Loss:0.0319 | top1:98.8718 | AUROC:0.9995\n",
      "\n",
      "Epoch: [19 | 300] LR: 0.798926\n",
      "1/152 | Loss:0.0203 | top1:99.4141 | AUROC:0.9999\n",
      "101/152 | Loss:0.0437 | top1:98.4414 | AUROC:0.9988\n",
      "152/152 | Loss:0.0442 | top1:98.4543 | AUROC:0.9988\n",
      "16/16 | Loss:0.0214 | top1:99.3590 | AUROC:0.9997\n",
      "\n",
      "Epoch: [20 | 300] LR: 0.798597\n",
      "1/152 | Loss:0.0370 | top1:98.4375 | AUROC:0.9996\n",
      "101/152 | Loss:0.0442 | top1:98.4742 | AUROC:0.9988\n",
      "152/152 | Loss:0.0475 | top1:98.3651 | AUROC:0.9987\n",
      "16/16 | Loss:0.0329 | top1:98.9103 | AUROC:0.9994\n",
      "\n",
      "Epoch: [21 | 300] LR: 0.798225\n",
      "1/152 | Loss:0.0649 | top1:97.2656 | AUROC:0.9987\n",
      "101/152 | Loss:0.0455 | top1:98.3892 | AUROC:0.9987\n",
      "152/152 | Loss:0.0436 | top1:98.4414 | AUROC:0.9988\n",
      "16/16 | Loss:0.0142 | top1:99.5128 | AUROC:0.9999\n",
      "\n",
      "Epoch: [22 | 300] LR: 0.797809\n",
      "1/152 | Loss:0.0258 | top1:99.2188 | AUROC:0.9996\n",
      "101/152 | Loss:0.0412 | top1:98.5845 | AUROC:0.9989\n",
      "152/152 | Loss:0.0406 | top1:98.5966 | AUROC:0.9989\n",
      "16/16 | Loss:0.0181 | top1:99.3333 | AUROC:0.9998\n",
      "\n",
      "Epoch: [23 | 300] LR: 0.797349\n",
      "1/152 | Loss:0.0222 | top1:99.2188 | AUROC:0.9998\n",
      "101/152 | Loss:0.0479 | top1:98.3157 | AUROC:0.9986\n",
      "152/152 | Loss:0.0458 | top1:98.4013 | AUROC:0.9988\n",
      "16/16 | Loss:0.0229 | top1:99.2308 | AUROC:0.9998\n",
      "\n",
      "Epoch: [24 | 300] LR: 0.796846\n",
      "1/152 | Loss:0.0671 | top1:97.8516 | AUROC:0.9975\n",
      "101/152 | Loss:0.0367 | top1:98.7314 | AUROC:0.9991\n",
      "152/152 | Loss:0.0368 | top1:98.7285 | AUROC:0.9992\n",
      "16/16 | Loss:0.0152 | top1:99.5128 | AUROC:0.9999\n",
      "\n",
      "Epoch: [25 | 300] LR: 0.796299\n",
      "1/152 | Loss:0.0276 | top1:98.8281 | AUROC:0.9996\n",
      "101/152 | Loss:0.0393 | top1:98.7082 | AUROC:0.9990\n",
      "152/152 | Loss:0.0416 | top1:98.6238 | AUROC:0.9989\n",
      "16/16 | Loss:0.0194 | top1:99.3846 | AUROC:0.9997\n",
      "\n",
      "Epoch: [26 | 300] LR: 0.795709\n",
      "1/152 | Loss:0.0462 | top1:98.4375 | AUROC:0.9985\n",
      "101/152 | Loss:0.0454 | top1:98.3621 | AUROC:0.9990\n",
      "152/152 | Loss:0.0428 | top1:98.4440 | AUROC:0.9990\n",
      "16/16 | Loss:0.0215 | top1:99.3077 | AUROC:0.9999\n",
      "\n",
      "Epoch: [27 | 300] LR: 0.795075\n",
      "1/152 | Loss:0.0295 | top1:98.6328 | AUROC:0.9996\n",
      "101/152 | Loss:0.0398 | top1:98.6270 | AUROC:0.9990\n",
      "152/152 | Loss:0.0393 | top1:98.6367 | AUROC:0.9991\n",
      "16/16 | Loss:0.0228 | top1:99.1923 | AUROC:0.9996\n",
      "\n",
      "Epoch: [28 | 300] LR: 0.794398\n",
      "1/152 | Loss:0.0267 | top1:98.8281 | AUROC:0.9996\n",
      "101/152 | Loss:0.0452 | top1:98.4201 | AUROC:0.9988\n",
      "152/152 | Loss:0.0418 | top1:98.5449 | AUROC:0.9990\n",
      "16/16 | Loss:0.0138 | top1:99.5128 | AUROC:0.9999\n",
      "\n",
      "Epoch: [29 | 300] LR: 0.793678\n",
      "1/152 | Loss:0.0356 | top1:98.8281 | AUROC:0.9992\n",
      "101/152 | Loss:0.0398 | top1:98.6328 | AUROC:0.9990\n",
      "152/152 | Loss:0.0388 | top1:98.7039 | AUROC:0.9990\n",
      "16/16 | Loss:0.0288 | top1:99.0769 | AUROC:0.9999\n",
      "\n",
      "Epoch: [30 | 300] LR: 0.792915\n",
      "1/152 | Loss:0.0250 | top1:99.0234 | AUROC:0.9998\n",
      "101/152 | Loss:0.0358 | top1:98.6657 | AUROC:0.9993\n",
      "152/152 | Loss:0.0362 | top1:98.6729 | AUROC:0.9992\n",
      "16/16 | Loss:0.0111 | top1:99.6410 | AUROC:0.9998\n",
      "\n",
      "Epoch: [31 | 300] LR: 0.792108\n",
      "1/152 | Loss:0.0250 | top1:99.2188 | AUROC:0.9996\n",
      "101/152 | Loss:0.0433 | top1:98.4704 | AUROC:0.9990\n",
      "152/152 | Loss:0.0415 | top1:98.5371 | AUROC:0.9991\n",
      "16/16 | Loss:0.0376 | top1:98.5256 | AUROC:0.9999\n",
      "\n",
      "Epoch: [32 | 300] LR: 0.791259\n",
      "1/152 | Loss:0.0327 | top1:98.8281 | AUROC:0.9998\n",
      "101/152 | Loss:0.0333 | top1:98.8455 | AUROC:0.9992\n",
      "152/152 | Loss:0.0332 | top1:98.8579 | AUROC:0.9993\n",
      "16/16 | Loss:0.0105 | top1:99.6282 | AUROC:0.9999\n",
      "\n",
      "Epoch: [33 | 300] LR: 0.790367\n",
      "1/152 | Loss:0.0150 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0298 | top1:98.9345 | AUROC:0.9993\n",
      "152/152 | Loss:0.0318 | top1:98.8799 | AUROC:0.9993\n",
      "16/16 | Loss:0.0234 | top1:99.1026 | AUROC:0.9999\n",
      "\n",
      "Epoch: [34 | 300] LR: 0.789432\n",
      "1/152 | Loss:0.0364 | top1:97.8516 | AUROC:0.9997\n",
      "101/152 | Loss:0.0376 | top1:98.7469 | AUROC:0.9992\n",
      "152/152 | Loss:0.0397 | top1:98.6639 | AUROC:0.9991\n",
      "16/16 | Loss:0.0204 | top1:99.2564 | AUROC:0.9997\n",
      "\n",
      "Epoch: [35 | 300] LR: 0.788454\n",
      "1/152 | Loss:0.0179 | top1:99.4141 | AUROC:0.9999\n",
      "101/152 | Loss:0.0360 | top1:98.7682 | AUROC:0.9992\n",
      "152/152 | Loss:0.0369 | top1:98.7234 | AUROC:0.9992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 | Loss:0.0166 | top1:99.5897 | AUROC:0.9998\n",
      "\n",
      "Epoch: [36 | 300] LR: 0.787433\n",
      "1/152 | Loss:0.0400 | top1:98.8281 | AUROC:0.9989\n",
      "101/152 | Loss:0.0362 | top1:98.6792 | AUROC:0.9993\n",
      "152/152 | Loss:0.0358 | top1:98.7337 | AUROC:0.9993\n",
      "16/16 | Loss:0.0600 | top1:98.2179 | AUROC:0.9995\n",
      "\n",
      "Epoch: [37 | 300] LR: 0.786370\n",
      "1/152 | Loss:0.1025 | top1:96.0938 | AUROC:0.9979\n",
      "101/152 | Loss:0.0465 | top1:98.3756 | AUROC:0.9989\n",
      "152/152 | Loss:0.0424 | top1:98.5384 | AUROC:0.9990\n",
      "16/16 | Loss:0.0117 | top1:99.5641 | AUROC:0.9999\n",
      "\n",
      "Epoch: [38 | 300] LR: 0.785265\n",
      "1/152 | Loss:0.0281 | top1:99.0234 | AUROC:0.9995\n",
      "101/152 | Loss:0.0308 | top1:98.9268 | AUROC:0.9994\n",
      "152/152 | Loss:0.0317 | top1:98.9238 | AUROC:0.9994\n",
      "16/16 | Loss:0.0207 | top1:99.1667 | AUROC:0.9997\n",
      "\n",
      "Epoch: [39 | 300] LR: 0.784117\n",
      "1/152 | Loss:0.0164 | top1:99.4141 | AUROC:0.9999\n",
      "101/152 | Loss:0.0392 | top1:98.5903 | AUROC:0.9991\n",
      "152/152 | Loss:0.0399 | top1:98.5617 | AUROC:0.9991\n",
      "16/16 | Loss:0.0249 | top1:99.0769 | AUROC:0.9998\n",
      "\n",
      "Epoch: [40 | 300] LR: 0.782928\n",
      "1/152 | Loss:0.0524 | top1:98.0469 | AUROC:0.9977\n",
      "101/152 | Loss:0.0339 | top1:98.8649 | AUROC:0.9994\n",
      "152/152 | Loss:0.0340 | top1:98.8579 | AUROC:0.9993\n",
      "16/16 | Loss:0.0132 | top1:99.5897 | AUROC:0.9999\n",
      "\n",
      "Epoch: [41 | 300] LR: 0.781696\n",
      "1/152 | Loss:0.0060 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0328 | top1:98.8301 | AUROC:0.9994\n",
      "152/152 | Loss:0.0329 | top1:98.8087 | AUROC:0.9994\n",
      "16/16 | Loss:0.0187 | top1:99.2308 | AUROC:0.9998\n",
      "\n",
      "Epoch: [42 | 300] LR: 0.780423\n",
      "1/152 | Loss:0.0495 | top1:98.2422 | AUROC:0.9988\n",
      "101/152 | Loss:0.0352 | top1:98.8204 | AUROC:0.9993\n",
      "152/152 | Loss:0.0339 | top1:98.8501 | AUROC:0.9994\n",
      "16/16 | Loss:0.0165 | top1:99.4487 | AUROC:0.9998\n",
      "\n",
      "Epoch: [43 | 300] LR: 0.779107\n",
      "1/152 | Loss:0.0424 | top1:98.6328 | AUROC:0.9981\n",
      "101/152 | Loss:0.0329 | top1:98.8533 | AUROC:0.9994\n",
      "152/152 | Loss:0.0312 | top1:98.9161 | AUROC:0.9994\n",
      "16/16 | Loss:0.0112 | top1:99.6154 | AUROC:0.9999\n",
      "\n",
      "Epoch: [44 | 300] LR: 0.777751\n",
      "1/152 | Loss:0.0186 | top1:99.2188 | AUROC:0.9999\n",
      "101/152 | Loss:0.0335 | top1:98.8127 | AUROC:0.9994\n",
      "152/152 | Loss:0.0356 | top1:98.7570 | AUROC:0.9993\n",
      "16/16 | Loss:0.0176 | top1:99.4359 | AUROC:0.9997\n",
      "\n",
      "Epoch: [45 | 300] LR: 0.776352\n",
      "1/152 | Loss:0.0371 | top1:98.6328 | AUROC:0.9996\n",
      "101/152 | Loss:0.0327 | top1:98.8552 | AUROC:0.9992\n",
      "152/152 | Loss:0.0324 | top1:98.8812 | AUROC:0.9993\n",
      "16/16 | Loss:0.0140 | top1:99.4744 | AUROC:0.9999\n",
      "\n",
      "Epoch: [46 | 300] LR: 0.774913\n",
      "1/152 | Loss:0.0564 | top1:98.2422 | AUROC:0.9985\n",
      "101/152 | Loss:0.0310 | top1:98.9074 | AUROC:0.9994\n",
      "152/152 | Loss:0.0307 | top1:98.9238 | AUROC:0.9994\n",
      "16/16 | Loss:0.0180 | top1:99.3333 | AUROC:0.9999\n",
      "\n",
      "Epoch: [47 | 300] LR: 0.773432\n",
      "1/152 | Loss:0.0421 | top1:97.8516 | AUROC:0.9990\n",
      "101/152 | Loss:0.0381 | top1:98.6947 | AUROC:0.9992\n",
      "152/152 | Loss:0.0364 | top1:98.7570 | AUROC:0.9992\n",
      "16/16 | Loss:0.0183 | top1:99.3846 | AUROC:0.9999\n",
      "\n",
      "Epoch: [48 | 300] LR: 0.771911\n",
      "1/152 | Loss:0.0154 | top1:99.4141 | AUROC:0.9999\n",
      "101/152 | Loss:0.0307 | top1:98.9132 | AUROC:0.9995\n",
      "152/152 | Loss:0.0309 | top1:98.9174 | AUROC:0.9995\n",
      "16/16 | Loss:0.0171 | top1:99.3846 | AUROC:0.9999\n",
      "\n",
      "Epoch: [49 | 300] LR: 0.770348\n",
      "1/152 | Loss:0.0524 | top1:97.8516 | AUROC:0.9990\n",
      "101/152 | Loss:0.0338 | top1:98.8204 | AUROC:0.9993\n",
      "152/152 | Loss:0.0311 | top1:98.9238 | AUROC:0.9994\n",
      "16/16 | Loss:0.0087 | top1:99.7308 | AUROC:1.0000\n",
      "\n",
      "Epoch: [50 | 300] LR: 0.768745\n",
      "1/152 | Loss:0.0293 | top1:99.2188 | AUROC:0.9995\n",
      "101/152 | Loss:0.0288 | top1:99.0254 | AUROC:0.9995\n",
      "152/152 | Loss:0.0301 | top1:98.9613 | AUROC:0.9994\n",
      "16/16 | Loss:0.0133 | top1:99.5641 | AUROC:0.9999\n",
      "\n",
      "Epoch: [51 | 300] LR: 0.767102\n",
      "1/152 | Loss:0.0390 | top1:98.8281 | AUROC:0.9992\n",
      "101/152 | Loss:0.0309 | top1:98.9461 | AUROC:0.9996\n",
      "152/152 | Loss:0.0301 | top1:98.9898 | AUROC:0.9995\n",
      "16/16 | Loss:0.0170 | top1:99.4231 | AUROC:0.9999\n",
      "\n",
      "Epoch: [52 | 300] LR: 0.765418\n",
      "1/152 | Loss:0.0267 | top1:99.2188 | AUROC:0.9996\n",
      "101/152 | Loss:0.0268 | top1:99.0312 | AUROC:0.9996\n",
      "152/152 | Loss:0.0267 | top1:99.0377 | AUROC:0.9996\n",
      "16/16 | Loss:0.0202 | top1:99.3077 | AUROC:0.9999\n",
      "\n",
      "Epoch: [53 | 300] LR: 0.763694\n",
      "1/152 | Loss:0.0431 | top1:98.6328 | AUROC:0.9966\n",
      "101/152 | Loss:0.0299 | top1:98.9635 | AUROC:0.9994\n",
      "152/152 | Loss:0.0303 | top1:98.9549 | AUROC:0.9994\n",
      "16/16 | Loss:0.0222 | top1:99.3462 | AUROC:0.9997\n",
      "\n",
      "Epoch: [54 | 300] LR: 0.761931\n",
      "1/152 | Loss:0.0409 | top1:98.6328 | AUROC:0.9981\n",
      "101/152 | Loss:0.0374 | top1:98.7392 | AUROC:0.9992\n",
      "152/152 | Loss:0.0375 | top1:98.7453 | AUROC:0.9991\n",
      "16/16 | Loss:0.0147 | top1:99.5641 | AUROC:0.9999\n",
      "\n",
      "Epoch: [55 | 300] LR: 0.760128\n",
      "1/152 | Loss:0.0314 | top1:98.4375 | AUROC:0.9994\n",
      "101/152 | Loss:0.0290 | top1:99.0176 | AUROC:0.9994\n",
      "152/152 | Loss:0.0285 | top1:99.0428 | AUROC:0.9994\n",
      "16/16 | Loss:0.0296 | top1:99.0000 | AUROC:0.9998\n",
      "\n",
      "Epoch: [56 | 300] LR: 0.758285\n",
      "1/152 | Loss:0.0534 | top1:98.0469 | AUROC:0.9998\n",
      "101/152 | Loss:0.0329 | top1:98.8165 | AUROC:0.9993\n",
      "152/152 | Loss:0.0335 | top1:98.7829 | AUROC:0.9993\n",
      "16/16 | Loss:0.0112 | top1:99.5513 | AUROC:0.9999\n",
      "\n",
      "Epoch: [57 | 300] LR: 0.756403\n",
      "1/152 | Loss:0.0230 | top1:99.4141 | AUROC:0.9997\n",
      "101/152 | Loss:0.0389 | top1:98.6889 | AUROC:0.9991\n",
      "152/152 | Loss:0.0358 | top1:98.8036 | AUROC:0.9992\n",
      "16/16 | Loss:0.0130 | top1:99.6026 | AUROC:0.9999\n",
      "\n",
      "Epoch: [58 | 300] LR: 0.754481\n",
      "1/152 | Loss:0.0128 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0264 | top1:99.0776 | AUROC:0.9995\n",
      "152/152 | Loss:0.0269 | top1:99.0454 | AUROC:0.9995\n",
      "16/16 | Loss:0.0164 | top1:99.4744 | AUROC:0.9999\n",
      "\n",
      "Epoch: [59 | 300] LR: 0.752521\n",
      "1/152 | Loss:0.0151 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0284 | top1:98.9577 | AUROC:0.9995\n",
      "152/152 | Loss:0.0321 | top1:98.8424 | AUROC:0.9994\n",
      "16/16 | Loss:0.0167 | top1:99.2949 | AUROC:0.9998\n",
      "\n",
      "Epoch: [60 | 300] LR: 0.750523\n",
      "1/152 | Loss:0.0620 | top1:98.0469 | AUROC:0.9973\n",
      "101/152 | Loss:0.0452 | top1:98.3988 | AUROC:0.9988\n",
      "152/152 | Loss:0.0420 | top1:98.5125 | AUROC:0.9989\n",
      "16/16 | Loss:0.0218 | top1:99.2949 | AUROC:0.9999\n",
      "\n",
      "Epoch: [61 | 300] LR: 0.748486\n",
      "1/152 | Loss:0.0292 | top1:99.0234 | AUROC:0.9989\n",
      "101/152 | Loss:0.0317 | top1:98.9093 | AUROC:0.9993\n",
      "152/152 | Loss:0.0309 | top1:98.9200 | AUROC:0.9993\n",
      "16/16 | Loss:0.0099 | top1:99.6667 | AUROC:0.9999\n",
      "\n",
      "Epoch: [62 | 300] LR: 0.746410\n",
      "1/152 | Loss:0.0567 | top1:98.4375 | AUROC:0.9983\n",
      "101/152 | Loss:0.0310 | top1:98.9519 | AUROC:0.9994\n",
      "152/152 | Loss:0.0314 | top1:98.9497 | AUROC:0.9994\n",
      "16/16 | Loss:0.0305 | top1:98.8333 | AUROC:0.9999\n",
      "\n",
      "Epoch: [63 | 300] LR: 0.744297\n",
      "1/152 | Loss:0.0273 | top1:98.4375 | AUROC:0.9998\n",
      "101/152 | Loss:0.0315 | top1:98.8707 | AUROC:0.9994\n",
      "152/152 | Loss:0.0318 | top1:98.8837 | AUROC:0.9994\n",
      "16/16 | Loss:0.0213 | top1:99.4359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [64 | 300] LR: 0.742146\n",
      "1/152 | Loss:0.0326 | top1:99.0234 | AUROC:0.9994\n",
      "101/152 | Loss:0.0306 | top1:98.9654 | AUROC:0.9993\n",
      "152/152 | Loss:0.0316 | top1:98.9238 | AUROC:0.9993\n",
      "16/16 | Loss:0.0118 | top1:99.5513 | AUROC:0.9999\n",
      "\n",
      "Epoch: [65 | 300] LR: 0.739957\n",
      "1/152 | Loss:0.0169 | top1:99.4141 | AUROC:0.9998\n",
      "101/152 | Loss:0.0342 | top1:98.7836 | AUROC:0.9993\n",
      "152/152 | Loss:0.0350 | top1:98.7751 | AUROC:0.9993\n",
      "16/16 | Loss:0.0172 | top1:99.4231 | AUROC:0.9998\n",
      "\n",
      "Epoch: [66 | 300] LR: 0.737731\n",
      "1/152 | Loss:0.0265 | top1:99.2188 | AUROC:0.9994\n",
      "101/152 | Loss:0.0341 | top1:98.8049 | AUROC:0.9993\n",
      "152/152 | Loss:0.0338 | top1:98.8255 | AUROC:0.9994\n",
      "16/16 | Loss:0.0541 | top1:98.2436 | AUROC:0.9999\n",
      "\n",
      "Epoch: [67 | 300] LR: 0.735468\n",
      "1/152 | Loss:0.0585 | top1:97.8516 | AUROC:0.9994\n",
      "101/152 | Loss:0.0295 | top1:98.9577 | AUROC:0.9994\n",
      "152/152 | Loss:0.0300 | top1:98.9381 | AUROC:0.9994\n",
      "16/16 | Loss:0.0140 | top1:99.4487 | AUROC:0.9999\n",
      "\n",
      "Epoch: [68 | 300] LR: 0.733168\n",
      "1/152 | Loss:0.0542 | top1:98.4375 | AUROC:0.9987\n",
      "101/152 | Loss:0.0277 | top1:99.0331 | AUROC:0.9995\n",
      "152/152 | Loss:0.0323 | top1:98.8708 | AUROC:0.9993\n",
      "16/16 | Loss:0.0330 | top1:98.9359 | AUROC:0.9997\n",
      "\n",
      "Epoch: [69 | 300] LR: 0.730832\n",
      "1/152 | Loss:0.0376 | top1:98.4375 | AUROC:0.9993\n",
      "101/152 | Loss:0.0435 | top1:98.5226 | AUROC:0.9988\n",
      "152/152 | Loss:0.0408 | top1:98.6147 | AUROC:0.9989\n",
      "16/16 | Loss:0.0236 | top1:99.2179 | AUROC:0.9997\n",
      "\n",
      "Epoch: [70 | 300] LR: 0.728460\n",
      "1/152 | Loss:0.0327 | top1:99.0234 | AUROC:0.9995\n",
      "101/152 | Loss:0.0313 | top1:98.8784 | AUROC:0.9994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 | Loss:0.0313 | top1:98.8863 | AUROC:0.9994\n",
      "16/16 | Loss:0.0149 | top1:99.3718 | AUROC:0.9999\n",
      "\n",
      "Epoch: [71 | 300] LR: 0.726051\n",
      "1/152 | Loss:0.0355 | top1:98.6328 | AUROC:0.9994\n",
      "101/152 | Loss:0.0297 | top1:98.9074 | AUROC:0.9995\n",
      "152/152 | Loss:0.0301 | top1:98.8915 | AUROC:0.9995\n",
      "16/16 | Loss:0.0307 | top1:99.0128 | AUROC:0.9999\n",
      "\n",
      "Epoch: [72 | 300] LR: 0.723607\n",
      "1/152 | Loss:0.0290 | top1:98.8281 | AUROC:0.9996\n",
      "101/152 | Loss:0.0288 | top1:99.0254 | AUROC:0.9995\n",
      "152/152 | Loss:0.0308 | top1:98.9368 | AUROC:0.9994\n",
      "16/16 | Loss:0.0139 | top1:99.6410 | AUROC:0.9998\n",
      "\n",
      "Epoch: [73 | 300] LR: 0.721127\n",
      "1/152 | Loss:0.0249 | top1:98.8281 | AUROC:0.9997\n",
      "101/152 | Loss:0.0298 | top1:98.9577 | AUROC:0.9994\n",
      "152/152 | Loss:0.0287 | top1:99.0066 | AUROC:0.9995\n",
      "16/16 | Loss:0.0119 | top1:99.5256 | AUROC:0.9999\n",
      "\n",
      "Epoch: [74 | 300] LR: 0.718612\n",
      "1/152 | Loss:0.0173 | top1:99.2188 | AUROC:0.9999\n",
      "101/152 | Loss:0.0313 | top1:98.9480 | AUROC:0.9994\n",
      "152/152 | Loss:0.0307 | top1:98.9678 | AUROC:0.9994\n",
      "16/16 | Loss:0.0371 | top1:98.5897 | AUROC:0.9998\n",
      "\n",
      "Epoch: [75 | 300] LR: 0.716062\n",
      "1/152 | Loss:0.0428 | top1:98.2422 | AUROC:0.9996\n",
      "101/152 | Loss:0.0325 | top1:98.8629 | AUROC:0.9994\n",
      "152/152 | Loss:0.0322 | top1:98.8721 | AUROC:0.9994\n",
      "16/16 | Loss:0.0158 | top1:99.4231 | AUROC:0.9999\n",
      "\n",
      "Epoch: [76 | 300] LR: 0.713477\n",
      "1/152 | Loss:0.0470 | top1:98.2422 | AUROC:0.9987\n",
      "101/152 | Loss:0.0308 | top1:98.9325 | AUROC:0.9994\n",
      "152/152 | Loss:0.0302 | top1:98.9536 | AUROC:0.9994\n",
      "16/16 | Loss:0.0192 | top1:99.3846 | AUROC:0.9998\n",
      "\n",
      "Epoch: [77 | 300] LR: 0.071086\n",
      "1/152 | Loss:0.0483 | top1:98.4375 | AUROC:0.9979\n",
      "101/152 | Loss:0.0190 | top1:99.3212 | AUROC:0.9997\n",
      "152/152 | Loss:0.0169 | top1:99.3960 | AUROC:0.9998\n",
      "16/16 | Loss:0.0055 | top1:99.7949 | AUROC:1.0000\n",
      "\n",
      "Epoch: [78 | 300] LR: 0.070821\n",
      "1/152 | Loss:0.0179 | top1:99.4141 | AUROC:0.9998\n",
      "101/152 | Loss:0.0104 | top1:99.6442 | AUROC:0.9999\n",
      "152/152 | Loss:0.0107 | top1:99.6301 | AUROC:0.9999\n",
      "16/16 | Loss:0.0043 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [79 | 300] LR: 0.070552\n",
      "1/152 | Loss:0.0145 | top1:99.4141 | AUROC:0.9999\n",
      "101/152 | Loss:0.0108 | top1:99.6287 | AUROC:0.9999\n",
      "152/152 | Loss:0.0107 | top1:99.6378 | AUROC:0.9999\n",
      "16/16 | Loss:0.0036 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [80 | 300] LR: 0.070280\n",
      "1/152 | Loss:0.0081 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0098 | top1:99.6693 | AUROC:0.9999\n",
      "152/152 | Loss:0.0094 | top1:99.6598 | AUROC:0.9999\n",
      "16/16 | Loss:0.0037 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [81 | 300] LR: 0.070004\n",
      "1/152 | Loss:0.0010 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0088 | top1:99.7003 | AUROC:0.9999\n",
      "152/152 | Loss:0.0086 | top1:99.7064 | AUROC:1.0000\n",
      "16/16 | Loss:0.0056 | top1:99.8205 | AUROC:1.0000\n",
      "\n",
      "Epoch: [82 | 300] LR: 0.069726\n",
      "1/152 | Loss:0.0036 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0073 | top1:99.7389 | AUROC:1.0000\n",
      "152/152 | Loss:0.0079 | top1:99.7154 | AUROC:1.0000\n",
      "16/16 | Loss:0.0038 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [83 | 300] LR: 0.069444\n",
      "1/152 | Loss:0.0033 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0079 | top1:99.7351 | AUROC:1.0000\n",
      "152/152 | Loss:0.0080 | top1:99.7297 | AUROC:1.0000\n",
      "16/16 | Loss:0.0033 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [84 | 300] LR: 0.069159\n",
      "1/152 | Loss:0.0085 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0071 | top1:99.7486 | AUROC:1.0000\n",
      "152/152 | Loss:0.0076 | top1:99.7452 | AUROC:0.9999\n",
      "16/16 | Loss:0.0042 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [85 | 300] LR: 0.068870\n",
      "1/152 | Loss:0.0008 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0072 | top1:99.7602 | AUROC:1.0000\n",
      "152/152 | Loss:0.0071 | top1:99.7581 | AUROC:1.0000\n",
      "16/16 | Loss:0.0035 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [86 | 300] LR: 0.068579\n",
      "1/152 | Loss:0.0070 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0076 | top1:99.7563 | AUROC:1.0000\n",
      "152/152 | Loss:0.0072 | top1:99.7762 | AUROC:1.0000\n",
      "16/16 | Loss:0.0033 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [87 | 300] LR: 0.068284\n",
      "1/152 | Loss:0.0133 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0064 | top1:99.7834 | AUROC:1.0000\n",
      "152/152 | Loss:0.0070 | top1:99.7581 | AUROC:1.0000\n",
      "16/16 | Loss:0.0032 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [88 | 300] LR: 0.067987\n",
      "1/152 | Loss:0.0047 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0068 | top1:99.7621 | AUROC:1.0000\n",
      "152/152 | Loss:0.0069 | top1:99.7736 | AUROC:0.9999\n",
      "16/16 | Loss:0.0041 | top1:99.8462 | AUROC:1.0000\n",
      "\n",
      "Epoch: [89 | 300] LR: 0.067686\n",
      "1/152 | Loss:0.0139 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0151 | top1:99.4972 | AUROC:0.9999\n",
      "152/152 | Loss:0.0126 | top1:99.5835 | AUROC:0.9999\n",
      "16/16 | Loss:0.0031 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [90 | 300] LR: 0.067382\n",
      "1/152 | Loss:0.0081 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0065 | top1:99.7776 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7724 | AUROC:1.0000\n",
      "16/16 | Loss:0.0030 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [91 | 300] LR: 0.067075\n",
      "1/152 | Loss:0.0180 | top1:99.6094 | AUROC:0.9998\n",
      "101/152 | Loss:0.0073 | top1:99.7544 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7724 | AUROC:1.0000\n",
      "16/16 | Loss:0.0031 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [92 | 300] LR: 0.066765\n",
      "1/152 | Loss:0.0337 | top1:99.6094 | AUROC:0.9994\n",
      "101/152 | Loss:0.0071 | top1:99.7583 | AUROC:1.0000\n",
      "152/152 | Loss:0.0072 | top1:99.7555 | AUROC:1.0000\n",
      "16/16 | Loss:0.0023 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [93 | 300] LR: 0.066452\n",
      "1/152 | Loss:0.0103 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0067 | top1:99.7621 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7594 | AUROC:1.0000\n",
      "16/16 | Loss:0.0017 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [94 | 300] LR: 0.066137\n",
      "1/152 | Loss:0.0120 | top1:99.8047 | AUROC:0.9998\n",
      "101/152 | Loss:0.0062 | top1:99.8027 | AUROC:1.0000\n",
      "152/152 | Loss:0.0066 | top1:99.7866 | AUROC:1.0000\n",
      "16/16 | Loss:0.0026 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [95 | 300] LR: 0.065818\n",
      "1/152 | Loss:0.0048 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0064 | top1:99.8008 | AUROC:0.9999\n",
      "152/152 | Loss:0.0065 | top1:99.7918 | AUROC:0.9999\n",
      "16/16 | Loss:0.0026 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [96 | 300] LR: 0.065497\n",
      "1/152 | Loss:0.0035 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0060 | top1:99.7757 | AUROC:1.0000\n",
      "152/152 | Loss:0.0063 | top1:99.7788 | AUROC:1.0000\n",
      "16/16 | Loss:0.0025 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [97 | 300] LR: 0.065173\n",
      "1/152 | Loss:0.0038 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0064 | top1:99.7795 | AUROC:1.0000\n",
      "152/152 | Loss:0.0063 | top1:99.7788 | AUROC:1.0000\n",
      "16/16 | Loss:0.0027 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [98 | 300] LR: 0.064846\n",
      "1/152 | Loss:0.0009 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0063 | top1:99.7950 | AUROC:1.0000\n",
      "152/152 | Loss:0.0064 | top1:99.7930 | AUROC:1.0000\n",
      "16/16 | Loss:0.0029 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [99 | 300] LR: 0.064516\n",
      "1/152 | Loss:0.0012 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0056 | top1:99.8202 | AUROC:1.0000\n",
      "152/152 | Loss:0.0059 | top1:99.8137 | AUROC:1.0000\n",
      "16/16 | Loss:0.0028 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [100 | 300] LR: 0.064184\n",
      "1/152 | Loss:0.0070 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0066 | top1:99.8047 | AUROC:0.9999\n",
      "152/152 | Loss:0.0069 | top1:99.7827 | AUROC:1.0000\n",
      "16/16 | Loss:0.0027 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [101 | 300] LR: 0.063849\n",
      "1/152 | Loss:0.0031 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0062 | top1:99.7834 | AUROC:0.9999\n",
      "152/152 | Loss:0.0064 | top1:99.7801 | AUROC:1.0000\n",
      "16/16 | Loss:0.0025 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [102 | 300] LR: 0.063511\n",
      "1/152 | Loss:0.0035 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0072 | top1:99.7912 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7943 | AUROC:1.0000\n",
      "16/16 | Loss:0.0029 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [103 | 300] LR: 0.063171\n",
      "1/152 | Loss:0.0099 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0056 | top1:99.8124 | AUROC:1.0000\n",
      "152/152 | Loss:0.0060 | top1:99.8034 | AUROC:1.0000\n",
      "16/16 | Loss:0.0025 | top1:99.9487 | AUROC:1.0000\n",
      "\n",
      "Epoch: [104 | 300] LR: 0.062829\n",
      "1/152 | Loss:0.0008 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0052 | top1:99.8105 | AUROC:1.0000\n",
      "152/152 | Loss:0.0055 | top1:99.8021 | AUROC:1.0000\n",
      "16/16 | Loss:0.0035 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [105 | 300] LR: 0.062483\n",
      "1/152 | Loss:0.0054 | top1:99.6094 | AUROC:1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101/152 | Loss:0.0058 | top1:99.7931 | AUROC:1.0000\n",
      "152/152 | Loss:0.0056 | top1:99.7969 | AUROC:1.0000\n",
      "16/16 | Loss:0.0035 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [106 | 300] LR: 0.062136\n",
      "1/152 | Loss:0.0002 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0063 | top1:99.8260 | AUROC:0.9999\n",
      "152/152 | Loss:0.0055 | top1:99.8383 | AUROC:1.0000\n",
      "16/16 | Loss:0.0031 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [107 | 300] LR: 0.061786\n",
      "1/152 | Loss:0.0009 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0057 | top1:99.8240 | AUROC:1.0000\n",
      "152/152 | Loss:0.0059 | top1:99.8137 | AUROC:1.0000\n",
      "16/16 | Loss:0.0030 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [108 | 300] LR: 0.061433\n",
      "1/152 | Loss:0.0147 | top1:99.8047 | AUROC:0.9999\n",
      "101/152 | Loss:0.0070 | top1:99.7853 | AUROC:1.0000\n",
      "152/152 | Loss:0.0066 | top1:99.7943 | AUROC:1.0000\n",
      "16/16 | Loss:0.0032 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [109 | 300] LR: 0.061078\n",
      "1/152 | Loss:0.0173 | top1:99.6094 | AUROC:0.9998\n",
      "101/152 | Loss:0.0064 | top1:99.7989 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7827 | AUROC:1.0000\n",
      "16/16 | Loss:0.0028 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [110 | 300] LR: 0.060721\n",
      "1/152 | Loss:0.0074 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0654 | top1:97.8052 | AUROC:0.9942\n",
      "152/152 | Loss:0.0648 | top1:97.8257 | AUROC:0.9952\n",
      "16/16 | Loss:0.0383 | top1:98.8077 | AUROC:0.9997\n",
      "\n",
      "Epoch: [111 | 300] LR: 0.060362\n",
      "1/152 | Loss:0.0510 | top1:98.2422 | AUROC:0.9991\n",
      "101/152 | Loss:0.0284 | top1:99.0563 | AUROC:0.9994\n",
      "152/152 | Loss:0.0253 | top1:99.1256 | AUROC:0.9995\n",
      "16/16 | Loss:0.0073 | top1:99.7949 | AUROC:1.0000\n",
      "\n",
      "Epoch: [112 | 300] LR: 0.060000\n",
      "1/152 | Loss:0.0144 | top1:99.4141 | AUROC:0.9999\n",
      "101/152 | Loss:0.0155 | top1:99.4817 | AUROC:0.9997\n",
      "152/152 | Loss:0.0160 | top1:99.4710 | AUROC:0.9997\n",
      "16/16 | Loss:0.0058 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [113 | 300] LR: 0.059636\n",
      "1/152 | Loss:0.0049 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0124 | top1:99.5804 | AUROC:0.9999\n",
      "152/152 | Loss:0.0125 | top1:99.5809 | AUROC:0.9998\n",
      "16/16 | Loss:0.0043 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [114 | 300] LR: 0.059270\n",
      "1/152 | Loss:0.0106 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0117 | top1:99.5630 | AUROC:0.9998\n",
      "152/152 | Loss:0.0109 | top1:99.5913 | AUROC:0.9999\n",
      "16/16 | Loss:0.0059 | top1:99.7692 | AUROC:1.0000\n",
      "\n",
      "Epoch: [115 | 300] LR: 0.058902\n",
      "1/152 | Loss:0.0350 | top1:99.2188 | AUROC:0.9996\n",
      "101/152 | Loss:0.0112 | top1:99.6229 | AUROC:0.9999\n",
      "152/152 | Loss:0.0108 | top1:99.6339 | AUROC:0.9999\n",
      "16/16 | Loss:0.0053 | top1:99.8462 | AUROC:1.0000\n",
      "\n",
      "Epoch: [116 | 300] LR: 0.058532\n",
      "1/152 | Loss:0.0062 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0109 | top1:99.6248 | AUROC:0.9999\n",
      "152/152 | Loss:0.0106 | top1:99.6534 | AUROC:0.9999\n",
      "16/16 | Loss:0.0038 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [117 | 300] LR: 0.058160\n",
      "1/152 | Loss:0.0035 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0113 | top1:99.6306 | AUROC:0.9999\n",
      "152/152 | Loss:0.0110 | top1:99.6378 | AUROC:0.9999\n",
      "16/16 | Loss:0.0052 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [118 | 300] LR: 0.057785\n",
      "1/152 | Loss:0.0050 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0103 | top1:99.6364 | AUROC:0.9999\n",
      "152/152 | Loss:0.0118 | top1:99.5770 | AUROC:0.9999\n",
      "16/16 | Loss:0.0051 | top1:99.7949 | AUROC:1.0000\n",
      "\n",
      "Epoch: [119 | 300] LR: 0.057409\n",
      "1/152 | Loss:0.0099 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0109 | top1:99.6480 | AUROC:0.9999\n",
      "152/152 | Loss:0.0112 | top1:99.6443 | AUROC:0.9999\n",
      "16/16 | Loss:0.0052 | top1:99.7821 | AUROC:1.0000\n",
      "\n",
      "Epoch: [120 | 300] LR: 0.057031\n",
      "1/152 | Loss:0.0137 | top1:99.8047 | AUROC:0.9998\n",
      "101/152 | Loss:0.0112 | top1:99.6364 | AUROC:0.9999\n",
      "152/152 | Loss:0.0108 | top1:99.6611 | AUROC:0.9999\n",
      "16/16 | Loss:0.0033 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [121 | 300] LR: 0.056651\n",
      "1/152 | Loss:0.0090 | top1:99.8047 | AUROC:0.9999\n",
      "101/152 | Loss:0.0097 | top1:99.6751 | AUROC:0.9999\n",
      "152/152 | Loss:0.0098 | top1:99.6650 | AUROC:0.9999\n",
      "16/16 | Loss:0.0089 | top1:99.8077 | AUROC:1.0000\n",
      "\n",
      "Epoch: [122 | 300] LR: 0.056269\n",
      "1/152 | Loss:0.0177 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0090 | top1:99.6751 | AUROC:0.9999\n",
      "152/152 | Loss:0.0090 | top1:99.6792 | AUROC:0.9999\n",
      "16/16 | Loss:0.0052 | top1:99.8205 | AUROC:1.0000\n",
      "\n",
      "Epoch: [123 | 300] LR: 0.055886\n",
      "1/152 | Loss:0.0066 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0085 | top1:99.7003 | AUROC:1.0000\n",
      "152/152 | Loss:0.0090 | top1:99.6857 | AUROC:0.9999\n",
      "16/16 | Loss:0.0057 | top1:99.8333 | AUROC:1.0000\n",
      "\n",
      "Epoch: [124 | 300] LR: 0.055501\n",
      "1/152 | Loss:0.0048 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0091 | top1:99.6906 | AUROC:0.9999\n",
      "152/152 | Loss:0.0092 | top1:99.6922 | AUROC:0.9999\n",
      "16/16 | Loss:0.0046 | top1:99.8333 | AUROC:1.0000\n",
      "\n",
      "Epoch: [125 | 300] LR: 0.055114\n",
      "1/152 | Loss:0.0009 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0082 | top1:99.7177 | AUROC:0.9999\n",
      "152/152 | Loss:0.0086 | top1:99.7038 | AUROC:0.9999\n",
      "16/16 | Loss:0.0027 | top1:99.9487 | AUROC:1.0000\n",
      "\n",
      "Epoch: [126 | 300] LR: 0.054725\n",
      "1/152 | Loss:0.0114 | top1:99.6094 | AUROC:0.9999\n",
      "101/152 | Loss:0.0083 | top1:99.6964 | AUROC:1.0000\n",
      "152/152 | Loss:0.0085 | top1:99.7038 | AUROC:0.9999\n",
      "16/16 | Loss:0.0029 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [127 | 300] LR: 0.054335\n",
      "1/152 | Loss:0.0157 | top1:99.6094 | AUROC:0.9997\n",
      "101/152 | Loss:0.0083 | top1:99.7215 | AUROC:1.0000\n",
      "152/152 | Loss:0.0090 | top1:99.6960 | AUROC:0.9999\n",
      "16/16 | Loss:0.0026 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [128 | 300] LR: 0.053943\n",
      "1/152 | Loss:0.0015 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0075 | top1:99.7525 | AUROC:1.0000\n",
      "152/152 | Loss:0.0078 | top1:99.7245 | AUROC:1.0000\n",
      "16/16 | Loss:0.0053 | top1:99.8077 | AUROC:1.0000\n",
      "\n",
      "Epoch: [129 | 300] LR: 0.053550\n",
      "1/152 | Loss:0.0080 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0082 | top1:99.7273 | AUROC:1.0000\n",
      "152/152 | Loss:0.0088 | top1:99.7232 | AUROC:0.9999\n",
      "16/16 | Loss:0.0035 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [130 | 300] LR: 0.053155\n",
      "1/152 | Loss:0.0025 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0076 | top1:99.7621 | AUROC:1.0000\n",
      "152/152 | Loss:0.0073 | top1:99.7685 | AUROC:1.0000\n",
      "16/16 | Loss:0.0043 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [131 | 300] LR: 0.052758\n",
      "1/152 | Loss:0.0090 | top1:99.8047 | AUROC:0.9999\n",
      "101/152 | Loss:0.0091 | top1:99.6790 | AUROC:0.9999\n",
      "152/152 | Loss:0.0090 | top1:99.6805 | AUROC:1.0000\n",
      "16/16 | Loss:0.0040 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [132 | 300] LR: 0.052361\n",
      "1/152 | Loss:0.0105 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0075 | top1:99.7428 | AUROC:1.0000\n",
      "152/152 | Loss:0.0078 | top1:99.7206 | AUROC:1.0000\n",
      "16/16 | Loss:0.0036 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [133 | 300] LR: 0.051962\n",
      "1/152 | Loss:0.0031 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0074 | top1:99.7563 | AUROC:0.9999\n",
      "152/152 | Loss:0.0073 | top1:99.7594 | AUROC:1.0000\n",
      "16/16 | Loss:0.0034 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [134 | 300] LR: 0.051561\n",
      "1/152 | Loss:0.0043 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0062 | top1:99.7795 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7542 | AUROC:1.0000\n",
      "16/16 | Loss:0.0067 | top1:99.8077 | AUROC:1.0000\n",
      "\n",
      "Epoch: [135 | 300] LR: 0.051160\n",
      "1/152 | Loss:0.0105 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0075 | top1:99.7389 | AUROC:0.9999\n",
      "152/152 | Loss:0.0072 | top1:99.7491 | AUROC:1.0000\n",
      "16/16 | Loss:0.0053 | top1:99.8205 | AUROC:1.0000\n",
      "\n",
      "Epoch: [136 | 300] LR: 0.050757\n",
      "1/152 | Loss:0.0063 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0080 | top1:99.7273 | AUROC:1.0000\n",
      "152/152 | Loss:0.0082 | top1:99.7154 | AUROC:1.0000\n",
      "16/16 | Loss:0.0030 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [137 | 300] LR: 0.050353\n",
      "1/152 | Loss:0.0029 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0057 | top1:99.8086 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7736 | AUROC:1.0000\n",
      "16/16 | Loss:0.0052 | top1:99.7949 | AUROC:1.0000\n",
      "\n",
      "Epoch: [138 | 300] LR: 0.049948\n",
      "1/152 | Loss:0.0018 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0068 | top1:99.7544 | AUROC:1.0000\n",
      "152/152 | Loss:0.0071 | top1:99.7517 | AUROC:1.0000\n",
      "16/16 | Loss:0.0057 | top1:99.7949 | AUROC:1.0000\n",
      "\n",
      "Epoch: [139 | 300] LR: 0.049541\n",
      "1/152 | Loss:0.0052 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0074 | top1:99.7273 | AUROC:1.0000\n",
      "152/152 | Loss:0.0075 | top1:99.7245 | AUROC:1.0000\n",
      "16/16 | Loss:0.0033 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [140 | 300] LR: 0.049134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/152 | Loss:0.0110 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0069 | top1:99.7679 | AUROC:1.0000\n",
      "152/152 | Loss:0.0066 | top1:99.7685 | AUROC:1.0000\n",
      "16/16 | Loss:0.0047 | top1:99.8462 | AUROC:1.0000\n",
      "\n",
      "Epoch: [141 | 300] LR: 0.048726\n",
      "1/152 | Loss:0.0047 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0077 | top1:99.7467 | AUROC:0.9999\n",
      "152/152 | Loss:0.0075 | top1:99.7698 | AUROC:0.9999\n",
      "16/16 | Loss:0.0026 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [142 | 300] LR: 0.048316\n",
      "1/152 | Loss:0.0026 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0076 | top1:99.7409 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7698 | AUROC:1.0000\n",
      "16/16 | Loss:0.0060 | top1:99.8333 | AUROC:1.0000\n",
      "\n",
      "Epoch: [143 | 300] LR: 0.047906\n",
      "1/152 | Loss:0.0121 | top1:99.8047 | AUROC:0.9999\n",
      "101/152 | Loss:0.0073 | top1:99.7679 | AUROC:1.0000\n",
      "152/152 | Loss:0.0076 | top1:99.7439 | AUROC:1.0000\n",
      "16/16 | Loss:0.0033 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [144 | 300] LR: 0.047495\n",
      "1/152 | Loss:0.0019 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0074 | top1:99.7583 | AUROC:1.0000\n",
      "152/152 | Loss:0.0073 | top1:99.7491 | AUROC:1.0000\n",
      "16/16 | Loss:0.0039 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [145 | 300] LR: 0.047083\n",
      "1/152 | Loss:0.0060 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0069 | top1:99.7660 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7749 | AUROC:1.0000\n",
      "16/16 | Loss:0.0025 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [146 | 300] LR: 0.046671\n",
      "1/152 | Loss:0.0068 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0074 | top1:99.7428 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7724 | AUROC:1.0000\n",
      "16/16 | Loss:0.0025 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [147 | 300] LR: 0.046257\n",
      "1/152 | Loss:0.0023 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0077 | top1:99.7235 | AUROC:1.0000\n",
      "152/152 | Loss:0.0072 | top1:99.7542 | AUROC:1.0000\n",
      "16/16 | Loss:0.0023 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [148 | 300] LR: 0.045843\n",
      "1/152 | Loss:0.0016 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0067 | top1:99.7486 | AUROC:1.0000\n",
      "152/152 | Loss:0.0067 | top1:99.7620 | AUROC:1.0000\n",
      "16/16 | Loss:0.0053 | top1:99.7949 | AUROC:1.0000\n",
      "\n",
      "Epoch: [149 | 300] LR: 0.045429\n",
      "1/152 | Loss:0.0037 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0072 | top1:99.7351 | AUROC:1.0000\n",
      "152/152 | Loss:0.0070 | top1:99.7439 | AUROC:1.0000\n",
      "16/16 | Loss:0.0068 | top1:99.7436 | AUROC:1.0000\n",
      "\n",
      "Epoch: [150 | 300] LR: 0.045013\n",
      "1/152 | Loss:0.0026 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0064 | top1:99.7737 | AUROC:1.0000\n",
      "152/152 | Loss:0.0069 | top1:99.7698 | AUROC:1.0000\n",
      "16/16 | Loss:0.0038 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [151 | 300] LR: 0.044597\n",
      "1/152 | Loss:0.0062 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0061 | top1:99.7718 | AUROC:1.0000\n",
      "152/152 | Loss:0.0065 | top1:99.7659 | AUROC:0.9999\n",
      "16/16 | Loss:0.0022 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [152 | 300] LR: 0.044181\n",
      "1/152 | Loss:0.0017 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0080 | top1:99.7273 | AUROC:0.9999\n",
      "152/152 | Loss:0.0072 | top1:99.7581 | AUROC:1.0000\n",
      "16/16 | Loss:0.0029 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [153 | 300] LR: 0.043764\n",
      "1/152 | Loss:0.0033 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0068 | top1:99.7699 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7478 | AUROC:1.0000\n",
      "16/16 | Loss:0.0067 | top1:99.7051 | AUROC:1.0000\n",
      "\n",
      "Epoch: [154 | 300] LR: 0.043347\n",
      "1/152 | Loss:0.0161 | top1:99.8047 | AUROC:0.9999\n",
      "101/152 | Loss:0.0070 | top1:99.7641 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7762 | AUROC:1.0000\n",
      "16/16 | Loss:0.0029 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [155 | 300] LR: 0.042930\n",
      "1/152 | Loss:0.0081 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0064 | top1:99.7563 | AUROC:1.0000\n",
      "152/152 | Loss:0.0064 | top1:99.7698 | AUROC:1.0000\n",
      "16/16 | Loss:0.0036 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [156 | 300] LR: 0.042512\n",
      "1/152 | Loss:0.0056 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0062 | top1:99.7931 | AUROC:1.0000\n",
      "152/152 | Loss:0.0063 | top1:99.7866 | AUROC:1.0000\n",
      "16/16 | Loss:0.0026 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [157 | 300] LR: 0.042093\n",
      "1/152 | Loss:0.0087 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0059 | top1:99.7853 | AUROC:1.0000\n",
      "152/152 | Loss:0.0062 | top1:99.7736 | AUROC:1.0000\n",
      "16/16 | Loss:0.0017 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [158 | 300] LR: 0.041675\n",
      "1/152 | Loss:0.0007 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0060 | top1:99.7912 | AUROC:1.0000\n",
      "152/152 | Loss:0.0059 | top1:99.7853 | AUROC:1.0000\n",
      "16/16 | Loss:0.0023 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [159 | 300] LR: 0.041256\n",
      "1/152 | Loss:0.0034 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0073 | top1:99.7525 | AUROC:1.0000\n",
      "152/152 | Loss:0.0073 | top1:99.7530 | AUROC:1.0000\n",
      "16/16 | Loss:0.0029 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [160 | 300] LR: 0.040838\n",
      "1/152 | Loss:0.0013 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0064 | top1:99.7950 | AUROC:1.0000\n",
      "152/152 | Loss:0.0059 | top1:99.8086 | AUROC:1.0000\n",
      "16/16 | Loss:0.0042 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [161 | 300] LR: 0.040419\n",
      "1/152 | Loss:0.0075 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0072 | top1:99.7351 | AUROC:1.0000\n",
      "152/152 | Loss:0.0068 | top1:99.7594 | AUROC:1.0000\n",
      "16/16 | Loss:0.0012 | top1:99.9615 | AUROC:1.0000\n",
      "\n",
      "Epoch: [162 | 300] LR: 0.040000\n",
      "1/152 | Loss:0.0010 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0065 | top1:99.7737 | AUROC:1.0000\n",
      "152/152 | Loss:0.0063 | top1:99.7762 | AUROC:1.0000\n",
      "16/16 | Loss:0.0014 | top1:99.9487 | AUROC:1.0000\n",
      "\n",
      "Epoch: [163 | 300] LR: 0.039581\n",
      "1/152 | Loss:0.0010 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0058 | top1:99.7950 | AUROC:1.0000\n",
      "152/152 | Loss:0.0057 | top1:99.8060 | AUROC:1.0000\n",
      "16/16 | Loss:0.0026 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [164 | 300] LR: 0.039162\n",
      "1/152 | Loss:0.0091 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0057 | top1:99.7970 | AUROC:1.0000\n",
      "152/152 | Loss:0.0060 | top1:99.7840 | AUROC:1.0000\n",
      "16/16 | Loss:0.0043 | top1:99.8462 | AUROC:1.0000\n",
      "\n",
      "Epoch: [165 | 300] LR: 0.038744\n",
      "1/152 | Loss:0.0062 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0075 | top1:99.7525 | AUROC:0.9999\n",
      "152/152 | Loss:0.0070 | top1:99.7685 | AUROC:1.0000\n",
      "16/16 | Loss:0.0033 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [166 | 300] LR: 0.038325\n",
      "1/152 | Loss:0.0034 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0066 | top1:99.7621 | AUROC:1.0000\n",
      "152/152 | Loss:0.0064 | top1:99.7659 | AUROC:1.0000\n",
      "16/16 | Loss:0.0038 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [167 | 300] LR: 0.037907\n",
      "1/152 | Loss:0.0090 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0057 | top1:99.7970 | AUROC:1.0000\n",
      "152/152 | Loss:0.0060 | top1:99.7840 | AUROC:1.0000\n",
      "16/16 | Loss:0.0047 | top1:99.8462 | AUROC:1.0000\n",
      "\n",
      "Epoch: [168 | 300] LR: 0.037488\n",
      "1/152 | Loss:0.0062 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0056 | top1:99.8356 | AUROC:1.0000\n",
      "152/152 | Loss:0.0054 | top1:99.8267 | AUROC:1.0000\n",
      "16/16 | Loss:0.0026 | top1:99.8974 | AUROC:1.0000\n",
      "\n",
      "Epoch: [169 | 300] LR: 0.037070\n",
      "1/152 | Loss:0.0005 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0068 | top1:99.7873 | AUROC:0.9999\n",
      "152/152 | Loss:0.0067 | top1:99.7905 | AUROC:0.9999\n",
      "16/16 | Loss:0.0021 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [170 | 300] LR: 0.036653\n",
      "1/152 | Loss:0.0032 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0067 | top1:99.7892 | AUROC:1.0000\n",
      "152/152 | Loss:0.0070 | top1:99.7749 | AUROC:1.0000\n",
      "16/16 | Loss:0.0022 | top1:99.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [171 | 300] LR: 0.036236\n",
      "1/152 | Loss:0.0009 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0058 | top1:99.8105 | AUROC:1.0000\n",
      "152/152 | Loss:0.0066 | top1:99.7943 | AUROC:1.0000\n",
      "16/16 | Loss:0.0018 | top1:99.9487 | AUROC:1.0000\n",
      "\n",
      "Epoch: [172 | 300] LR: 0.035819\n",
      "1/152 | Loss:0.0015 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0053 | top1:99.8298 | AUROC:1.0000\n",
      "152/152 | Loss:0.0060 | top1:99.7982 | AUROC:1.0000\n",
      "16/16 | Loss:0.0025 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [173 | 300] LR: 0.035403\n",
      "1/152 | Loss:0.0032 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0059 | top1:99.7912 | AUROC:1.0000\n",
      "152/152 | Loss:0.0063 | top1:99.7814 | AUROC:1.0000\n",
      "16/16 | Loss:0.0041 | top1:99.8590 | AUROC:1.0000\n",
      "\n",
      "Epoch: [174 | 300] LR: 0.034987\n",
      "1/152 | Loss:0.0066 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0049 | top1:99.8202 | AUROC:1.0000\n",
      "152/152 | Loss:0.0055 | top1:99.8047 | AUROC:1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 | Loss:0.0039 | top1:99.8846 | AUROC:1.0000\n",
      "\n",
      "Epoch: [175 | 300] LR: 0.034571\n",
      "1/152 | Loss:0.0062 | top1:99.8047 | AUROC:1.0000\n",
      "101/152 | Loss:0.0063 | top1:99.7873 | AUROC:1.0000\n",
      "152/152 | Loss:0.0063 | top1:99.7853 | AUROC:1.0000\n",
      "16/16 | Loss:0.0036 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [176 | 300] LR: 0.034157\n",
      "1/152 | Loss:0.0106 | top1:99.4141 | AUROC:1.0000\n",
      "101/152 | Loss:0.0060 | top1:99.8008 | AUROC:1.0000\n",
      "152/152 | Loss:0.0064 | top1:99.7840 | AUROC:1.0000\n",
      "16/16 | Loss:0.0023 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [177 | 300] LR: 0.003374\n",
      "1/152 | Loss:0.0124 | top1:99.8047 | AUROC:0.9998\n",
      "101/152 | Loss:0.0048 | top1:99.8414 | AUROC:1.0000\n",
      "152/152 | Loss:0.0047 | top1:99.8461 | AUROC:1.0000\n",
      "16/16 | Loss:0.0029 | top1:99.8718 | AUROC:1.0000\n",
      "\n",
      "Epoch: [178 | 300] LR: 0.003333\n",
      "1/152 | Loss:0.0012 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0045 | top1:99.8298 | AUROC:1.0000\n",
      "152/152 | Loss:0.0044 | top1:99.8357 | AUROC:1.0000\n",
      "16/16 | Loss:0.0019 | top1:99.9487 | AUROC:1.0000\n",
      "\n",
      "Epoch: [179 | 300] LR: 0.003292\n",
      "1/152 | Loss:0.0013 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0039 | top1:99.8569 | AUROC:1.0000\n",
      "152/152 | Loss:0.0041 | top1:99.8474 | AUROC:1.0000\n",
      "16/16 | Loss:0.0024 | top1:99.9231 | AUROC:1.0000\n",
      "\n",
      "Epoch: [180 | 300] LR: 0.003250\n",
      "1/152 | Loss:0.0075 | top1:99.6094 | AUROC:1.0000\n",
      "101/152 | Loss:0.0043 | top1:99.8434 | AUROC:1.0000\n",
      "152/152 | Loss:0.0048 | top1:99.8357 | AUROC:1.0000\n",
      "16/16 | Loss:0.0018 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [181 | 300] LR: 0.003209\n",
      "1/152 | Loss:0.0018 | top1:100.0000 | AUROC:1.0000\n",
      "101/152 | Loss:0.0046 | top1:99.8472 | AUROC:1.0000\n",
      "152/152 | Loss:0.0041 | top1:99.8668 | AUROC:1.0000\n",
      "16/16 | Loss:0.0021 | top1:99.9359 | AUROC:1.0000\n",
      "\n",
      "Epoch: [182 | 300] LR: 0.003168\n",
      "1/152 | Loss:0.0011 | top1:100.0000 | AUROC:1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1594583eaac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: [%d | %d] LR: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-061456a0ae46>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, use_cuda)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# measure elapsed time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc, train_auroc, test_auroc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
