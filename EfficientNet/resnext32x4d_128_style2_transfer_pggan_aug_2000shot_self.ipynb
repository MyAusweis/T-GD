{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import resnext50_32x4d\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style2/128/32x4d/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'resnext32x4d' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 125\n",
    "test_batch = 125\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/32x4d/to_pggan/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = 'fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style2/128/32x4d/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "student_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 22.98M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.001000\n",
      "Train | 64/64 | Loss:0.9469 | MainLoss:0.5407 | Alpha:0.0090 | SPLoss:40.3306 | CLSLoss:4.7565 | top1:77.1175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1942 | MainLoss:0.1942 | SPLoss:14.4805 | CLSLoss:4.4144 | top1:99.4455 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6477 | MainLoss:0.6477 | SPLoss:14.4805 | CLSLoss:4.4145 | top1:50.9231 | AUROC:0.9828\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.001300\n",
      "Train | 64/64 | Loss:1.6608 | MainLoss:0.4027 | Alpha:0.4013 | SPLoss:0.2287 | CLSLoss:2.9099 | top1:81.8286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1986 | MainLoss:0.1986 | SPLoss:0.1897 | CLSLoss:1.8098 | top1:98.9221 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6639 | MainLoss:0.6639 | SPLoss:0.1897 | CLSLoss:1.8098 | top1:52.2821 | AUROC:0.9638\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.001600\n",
      "Train | 64/64 | Loss:0.9678 | MainLoss:0.4016 | Alpha:0.4020 | SPLoss:0.1960 | CLSLoss:1.2110 | top1:81.8540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1975 | MainLoss:0.1975 | SPLoss:0.1720 | CLSLoss:0.7748 | top1:99.0717 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6922 | MainLoss:0.6922 | SPLoss:0.1720 | CLSLoss:0.7748 | top1:51.7821 | AUROC:0.9483\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.001900\n",
      "Train | 64/64 | Loss:1.0452 | MainLoss:0.4189 | Alpha:0.4011 | SPLoss:1.0420 | CLSLoss:0.5227 | top1:80.9905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2286 | MainLoss:0.2286 | SPLoss:0.2873 | CLSLoss:0.3352 | top1:97.9346 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6636 | MainLoss:0.6636 | SPLoss:0.2873 | CLSLoss:0.3352 | top1:54.5769 | AUROC:0.9236\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.002200\n",
      "Train | 64/64 | Loss:0.6085 | MainLoss:0.4117 | Alpha:0.3980 | SPLoss:0.2691 | CLSLoss:0.2255 | top1:81.4857 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1985 | MainLoss:0.1985 | SPLoss:0.3137 | CLSLoss:0.1476 | top1:99.3333 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7189 | MainLoss:0.7189 | SPLoss:0.3137 | CLSLoss:0.1476 | top1:51.5513 | AUROC:0.9071\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.002500\n",
      "Train | 64/64 | Loss:0.5518 | MainLoss:0.4034 | Alpha:0.4000 | SPLoss:0.2670 | CLSLoss:0.1040 | top1:81.6381 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1737 | MainLoss:0.1737 | SPLoss:0.3304 | CLSLoss:0.0719 | top1:99.4642 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7606 | MainLoss:0.7606 | SPLoss:0.3304 | CLSLoss:0.0719 | top1:51.4231 | AUROC:0.8013\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.002800\n",
      "Train | 64/64 | Loss:0.6258 | MainLoss:0.4169 | Alpha:0.4011 | SPLoss:0.4637 | CLSLoss:0.0571 | top1:81.3460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1490 | MainLoss:0.1490 | SPLoss:0.3378 | CLSLoss:0.0462 | top1:99.6604 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.8291 | MainLoss:0.8291 | SPLoss:0.3378 | CLSLoss:0.0462 | top1:50.2179 | AUROC:0.9059\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.003100\n",
      "Train | 64/64 | Loss:0.7368 | MainLoss:0.4126 | Alpha:0.3999 | SPLoss:0.7733 | CLSLoss:0.0396 | top1:81.2825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1960 | MainLoss:0.1960 | SPLoss:1.1070 | CLSLoss:0.0339 | top1:99.6386 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7680 | MainLoss:0.7680 | SPLoss:1.1070 | CLSLoss:0.0339 | top1:50.0513 | AUROC:0.8774\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.003400\n",
      "Train | 64/64 | Loss:0.6463 | MainLoss:0.4068 | Alpha:0.3979 | SPLoss:0.5718 | CLSLoss:0.0301 | top1:81.5619 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.2870 | CLSLoss:0.0265 | top1:99.6947 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.8120 | MainLoss:0.8120 | SPLoss:0.2870 | CLSLoss:0.0265 | top1:50.3718 | AUROC:0.8735\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.003700\n",
      "Train | 64/64 | Loss:0.6423 | MainLoss:0.4067 | Alpha:0.4018 | SPLoss:0.5620 | CLSLoss:0.0242 | top1:81.7016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2069 | MainLoss:0.2069 | SPLoss:1.2522 | CLSLoss:0.0235 | top1:99.2492 | AUROC:0.9990\n",
      "Test | 63/64 | Loss:0.7809 | MainLoss:0.7809 | SPLoss:1.2522 | CLSLoss:0.0235 | top1:50.0000 | AUROC:0.8722\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.9336 | MainLoss:0.4141 | Alpha:0.3947 | SPLoss:1.2891 | CLSLoss:0.0312 | top1:81.0794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1855 | MainLoss:0.1855 | SPLoss:0.4021 | CLSLoss:0.0285 | top1:99.6542 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7678 | MainLoss:0.7678 | SPLoss:0.4021 | CLSLoss:0.0285 | top1:50.1667 | AUROC:0.9246\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:1.2993 | MainLoss:0.4167 | Alpha:0.3992 | SPLoss:2.1698 | CLSLoss:0.0307 | top1:80.7238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1694 | MainLoss:0.1694 | SPLoss:0.4737 | CLSLoss:0.0244 | top1:99.5203 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7740 | MainLoss:0.7740 | SPLoss:0.4737 | CLSLoss:0.0244 | top1:51.3205 | AUROC:0.9009\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.5640 | MainLoss:0.4025 | Alpha:0.4023 | SPLoss:0.3795 | CLSLoss:0.0216 | top1:81.5873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1385 | MainLoss:0.1385 | SPLoss:0.3562 | CLSLoss:0.0221 | top1:99.6168 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8359 | MainLoss:0.8359 | SPLoss:0.3562 | CLSLoss:0.0221 | top1:50.8590 | AUROC:0.9171\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.5865 | MainLoss:0.3937 | Alpha:0.4038 | SPLoss:0.4557 | CLSLoss:0.0217 | top1:82.3365 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1539 | MainLoss:0.1539 | SPLoss:0.4113 | CLSLoss:0.0209 | top1:99.6822 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8076 | MainLoss:0.8076 | SPLoss:0.4113 | CLSLoss:0.0209 | top1:50.5513 | AUROC:0.9016\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.6179 | MainLoss:0.4037 | Alpha:0.4014 | SPLoss:0.5152 | CLSLoss:0.0187 | top1:81.4730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1685 | MainLoss:0.1685 | SPLoss:0.2689 | CLSLoss:0.0187 | top1:99.5919 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7788 | MainLoss:0.7788 | SPLoss:0.2689 | CLSLoss:0.0187 | top1:50.8974 | AUROC:0.9084\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.5674 | MainLoss:0.3989 | Alpha:0.4021 | SPLoss:0.4010 | CLSLoss:0.0182 | top1:81.9937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1877 | MainLoss:0.1877 | SPLoss:0.2925 | CLSLoss:0.0166 | top1:99.4268 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7425 | MainLoss:0.7425 | SPLoss:0.2925 | CLSLoss:0.0166 | top1:51.3846 | AUROC:0.9005\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.6073 | MainLoss:0.4028 | Alpha:0.4014 | SPLoss:0.4919 | CLSLoss:0.0185 | top1:81.6889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1681 | MainLoss:0.1681 | SPLoss:0.5477 | CLSLoss:0.0167 | top1:99.6885 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7803 | MainLoss:0.7803 | SPLoss:0.5477 | CLSLoss:0.0167 | top1:50.4872 | AUROC:0.9103\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.5889 | MainLoss:0.4005 | Alpha:0.4016 | SPLoss:0.4515 | CLSLoss:0.0177 | top1:81.7778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1676 | MainLoss:0.1676 | SPLoss:0.2892 | CLSLoss:0.0182 | top1:99.5140 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7753 | MainLoss:0.7753 | SPLoss:0.2892 | CLSLoss:0.0182 | top1:51.1923 | AUROC:0.8678\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.004000\n",
      "Train | 64/64 | Loss:0.5343 | MainLoss:0.4010 | Alpha:0.4018 | SPLoss:0.3148 | CLSLoss:0.0158 | top1:81.6635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1745 | MainLoss:0.1745 | SPLoss:0.1757 | CLSLoss:0.0149 | top1:99.1776 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7470 | MainLoss:0.7470 | SPLoss:0.1757 | CLSLoss:0.0149 | top1:52.2821 | AUROC:0.8395\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.003999\n",
      "Train | 64/64 | Loss:0.5413 | MainLoss:0.4043 | Alpha:0.4017 | SPLoss:0.3255 | CLSLoss:0.0153 | top1:81.5619 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1780 | MainLoss:0.1780 | SPLoss:1.6991 | CLSLoss:0.0155 | top1:98.7383 | AUROC:0.9990\n",
      "Test | 63/64 | Loss:0.8069 | MainLoss:0.8069 | SPLoss:1.6991 | CLSLoss:0.0155 | top1:50.1026 | AUROC:0.9329\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.003999\n",
      "Train | 64/64 | Loss:0.6110 | MainLoss:0.4113 | Alpha:0.3940 | SPLoss:0.4902 | CLSLoss:0.0175 | top1:80.9905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1777 | MainLoss:0.1777 | SPLoss:0.4104 | CLSLoss:0.0162 | top1:99.4206 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7711 | MainLoss:0.7711 | SPLoss:0.4104 | CLSLoss:0.0162 | top1:50.3590 | AUROC:0.8959\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.003999\n",
      "Train | 64/64 | Loss:0.5982 | MainLoss:0.4054 | Alpha:0.3989 | SPLoss:0.4669 | CLSLoss:0.0167 | top1:81.4603 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1747 | MainLoss:0.1747 | SPLoss:0.3219 | CLSLoss:0.0167 | top1:99.5296 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7490 | MainLoss:0.7490 | SPLoss:0.3219 | CLSLoss:0.0167 | top1:50.7308 | AUROC:0.9150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [23 | 1000] LR: 0.003999\n",
      "Train | 64/64 | Loss:0.6003 | MainLoss:0.4030 | Alpha:0.4004 | SPLoss:0.4779 | CLSLoss:0.0158 | top1:81.3714 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1898 | MainLoss:0.1898 | SPLoss:0.2916 | CLSLoss:0.0149 | top1:99.2430 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7271 | MainLoss:0.7271 | SPLoss:0.2916 | CLSLoss:0.0149 | top1:52.3718 | AUROC:0.8903\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.003999\n",
      "Train | 64/64 | Loss:0.5769 | MainLoss:0.4001 | Alpha:0.4015 | SPLoss:0.4248 | CLSLoss:0.0153 | top1:81.6254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1812 | MainLoss:0.1812 | SPLoss:0.4279 | CLSLoss:0.0143 | top1:99.6012 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7506 | MainLoss:0.7506 | SPLoss:0.4279 | CLSLoss:0.0143 | top1:50.7564 | AUROC:0.8812\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.003998\n",
      "Train | 64/64 | Loss:0.5497 | MainLoss:0.3979 | Alpha:0.4016 | SPLoss:0.3626 | CLSLoss:0.0150 | top1:81.9556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:0.3616 | CLSLoss:0.0161 | top1:99.6729 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7970 | MainLoss:0.7970 | SPLoss:0.3616 | CLSLoss:0.0161 | top1:50.5513 | AUROC:0.8829\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.003998\n",
      "Train | 64/64 | Loss:0.6313 | MainLoss:0.4016 | Alpha:0.4019 | SPLoss:0.5578 | CLSLoss:0.0153 | top1:81.6254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1686 | MainLoss:0.1686 | SPLoss:0.5989 | CLSLoss:0.0154 | top1:99.5888 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7554 | MainLoss:0.7554 | SPLoss:0.5989 | CLSLoss:0.0154 | top1:51.1667 | AUROC:0.8981\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.003998\n",
      "Train | 64/64 | Loss:0.5954 | MainLoss:0.3951 | Alpha:0.4025 | SPLoss:0.4824 | CLSLoss:0.0155 | top1:81.9556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1758 | MainLoss:0.1758 | SPLoss:0.4120 | CLSLoss:0.0158 | top1:98.4424 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6883 | MainLoss:0.6883 | SPLoss:0.4120 | CLSLoss:0.0158 | top1:55.8718 | AUROC:0.8949\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.003997\n",
      "Train | 64/64 | Loss:0.6329 | MainLoss:0.4032 | Alpha:0.4017 | SPLoss:0.5570 | CLSLoss:0.0145 | top1:81.3587 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1810 | MainLoss:0.1810 | SPLoss:0.4005 | CLSLoss:0.0117 | top1:99.5514 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7386 | MainLoss:0.7386 | SPLoss:0.4005 | CLSLoss:0.0117 | top1:51.4744 | AUROC:0.8683\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.003997\n",
      "Train | 64/64 | Loss:0.6192 | MainLoss:0.3961 | Alpha:0.4022 | SPLoss:0.5398 | CLSLoss:0.0145 | top1:82.0063 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1592 | MainLoss:0.1592 | SPLoss:0.3295 | CLSLoss:0.0137 | top1:99.5265 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7587 | MainLoss:0.7587 | SPLoss:0.3295 | CLSLoss:0.0137 | top1:51.4744 | AUROC:0.9332\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.003997\n",
      "Train | 64/64 | Loss:0.6727 | MainLoss:0.3902 | Alpha:0.4044 | SPLoss:0.6852 | CLSLoss:0.0151 | top1:82.4254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1375 | MainLoss:0.1375 | SPLoss:0.6049 | CLSLoss:0.0140 | top1:99.6916 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8158 | MainLoss:0.8158 | SPLoss:0.6049 | CLSLoss:0.0140 | top1:50.7051 | AUROC:0.8918\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.003996\n",
      "Train | 64/64 | Loss:0.6322 | MainLoss:0.3991 | Alpha:0.4019 | SPLoss:0.5661 | CLSLoss:0.0141 | top1:81.9810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1657 | MainLoss:0.1657 | SPLoss:0.5430 | CLSLoss:0.0159 | top1:99.2492 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7441 | MainLoss:0.7441 | SPLoss:0.5430 | CLSLoss:0.0159 | top1:52.6154 | AUROC:0.9014\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.003996\n",
      "Train | 64/64 | Loss:0.6126 | MainLoss:0.3983 | Alpha:0.4030 | SPLoss:0.5172 | CLSLoss:0.0153 | top1:81.8921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1774 | MainLoss:0.1774 | SPLoss:0.5075 | CLSLoss:0.0165 | top1:99.2461 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7020 | MainLoss:0.7020 | SPLoss:0.5075 | CLSLoss:0.0165 | top1:52.6667 | AUROC:0.9311\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.003996\n",
      "Train | 64/64 | Loss:0.7112 | MainLoss:0.3978 | Alpha:0.4031 | SPLoss:0.7622 | CLSLoss:0.0161 | top1:82.0571 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1896 | MainLoss:0.1896 | SPLoss:0.6923 | CLSLoss:0.0148 | top1:99.3458 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7055 | MainLoss:0.7055 | SPLoss:0.6923 | CLSLoss:0.0148 | top1:52.6538 | AUROC:0.9042\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.003995\n",
      "Train | 64/64 | Loss:0.6219 | MainLoss:0.4015 | Alpha:0.4019 | SPLoss:0.5336 | CLSLoss:0.0155 | top1:81.7397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1624 | MainLoss:0.1624 | SPLoss:0.4175 | CLSLoss:0.0163 | top1:99.2212 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7458 | MainLoss:0.7458 | SPLoss:0.4175 | CLSLoss:0.0163 | top1:52.9615 | AUROC:0.8671\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.003995\n",
      "Train | 64/64 | Loss:0.6379 | MainLoss:0.3933 | Alpha:0.4043 | SPLoss:0.5908 | CLSLoss:0.0168 | top1:82.0444 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1883 | MainLoss:0.1883 | SPLoss:0.5658 | CLSLoss:0.0154 | top1:99.6231 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7423 | MainLoss:0.7423 | SPLoss:0.5658 | CLSLoss:0.0154 | top1:50.8974 | AUROC:0.8894\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.003994\n",
      "Train | 64/64 | Loss:0.6332 | MainLoss:0.3976 | Alpha:0.4022 | SPLoss:0.5697 | CLSLoss:0.0171 | top1:82.0063 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:0.2845 | CLSLoss:0.0156 | top1:99.2274 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7350 | MainLoss:0.7350 | SPLoss:0.2845 | CLSLoss:0.0156 | top1:53.1410 | AUROC:0.9140\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.003994\n",
      "Train | 64/64 | Loss:0.5833 | MainLoss:0.3871 | Alpha:0.4054 | SPLoss:0.4691 | CLSLoss:0.0149 | top1:82.7175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:0.7967 | CLSLoss:0.0157 | top1:98.1963 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7169 | MainLoss:0.7169 | SPLoss:0.7967 | CLSLoss:0.0157 | top1:56.2949 | AUROC:0.9156\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.003993\n",
      "Train | 64/64 | Loss:0.9319 | MainLoss:0.4051 | Alpha:0.4037 | SPLoss:1.2809 | CLSLoss:0.0233 | top1:81.8667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1531 | MainLoss:0.1531 | SPLoss:0.7179 | CLSLoss:0.0160 | top1:99.1589 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7592 | MainLoss:0.7592 | SPLoss:0.7179 | CLSLoss:0.0160 | top1:53.2308 | AUROC:0.9020\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.003993\n",
      "Train | 64/64 | Loss:0.6460 | MainLoss:0.3962 | Alpha:0.4037 | SPLoss:0.6038 | CLSLoss:0.0147 | top1:81.9937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1471 | MainLoss:0.1471 | SPLoss:0.4814 | CLSLoss:0.0159 | top1:99.5576 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7890 | MainLoss:0.7890 | SPLoss:0.4814 | CLSLoss:0.0159 | top1:51.5128 | AUROC:0.9225\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.003992\n",
      "Train | 64/64 | Loss:0.6120 | MainLoss:0.3982 | Alpha:0.4021 | SPLoss:0.5197 | CLSLoss:0.0140 | top1:81.8032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1640 | MainLoss:0.1640 | SPLoss:0.7721 | CLSLoss:0.0163 | top1:99.5670 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7642 | MainLoss:0.7642 | SPLoss:0.7721 | CLSLoss:0.0163 | top1:51.4103 | AUROC:0.8532\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.003992\n",
      "Train | 64/64 | Loss:0.5928 | MainLoss:0.3915 | Alpha:0.4035 | SPLoss:0.4841 | CLSLoss:0.0145 | top1:82.4000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2277 | MainLoss:0.2277 | SPLoss:0.5115 | CLSLoss:0.0147 | top1:97.9315 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6507 | MainLoss:0.6507 | SPLoss:0.5115 | CLSLoss:0.0147 | top1:57.8462 | AUROC:0.9218\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.003991\n",
      "Train | 64/64 | Loss:0.6822 | MainLoss:0.3942 | Alpha:0.4005 | SPLoss:0.7038 | CLSLoss:0.0153 | top1:82.1460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1224 | MainLoss:0.1224 | SPLoss:1.3526 | CLSLoss:0.0175 | top1:99.7913 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8734 | MainLoss:0.8734 | SPLoss:1.3526 | CLSLoss:0.0175 | top1:50.1410 | AUROC:0.8528\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.003991\n",
      "Train | 64/64 | Loss:0.9193 | MainLoss:0.4019 | Alpha:0.3992 | SPLoss:1.2629 | CLSLoss:0.0146 | top1:81.6889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1331 | MainLoss:0.1331 | SPLoss:11.3417 | CLSLoss:0.0213 | top1:99.6386 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8255 | MainLoss:0.8255 | SPLoss:11.3416 | CLSLoss:0.0213 | top1:51.0256 | AUROC:0.8376\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.003990\n",
      "Train | 64/64 | Loss:0.6596 | MainLoss:0.3968 | Alpha:0.4027 | SPLoss:0.6378 | CLSLoss:0.0144 | top1:82.0063 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1156 | MainLoss:0.1156 | SPLoss:0.2685 | CLSLoss:0.0143 | top1:99.7882 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8856 | MainLoss:0.8856 | SPLoss:0.2685 | CLSLoss:0.0143 | top1:50.3205 | AUROC:0.7756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [45 | 1000] LR: 0.003989\n",
      "Train | 64/64 | Loss:0.5945 | MainLoss:0.3957 | Alpha:0.4009 | SPLoss:0.4837 | CLSLoss:0.0124 | top1:81.9556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1585 | MainLoss:0.1585 | SPLoss:0.2032 | CLSLoss:0.0116 | top1:99.5421 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7661 | MainLoss:0.7661 | SPLoss:0.2032 | CLSLoss:0.0116 | top1:51.7564 | AUROC:0.8094\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.003989\n",
      "Train | 64/64 | Loss:0.5571 | MainLoss:0.3972 | Alpha:0.4035 | SPLoss:0.3843 | CLSLoss:0.0117 | top1:81.8286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1692 | MainLoss:0.1692 | SPLoss:0.7645 | CLSLoss:0.0105 | top1:99.7944 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7648 | MainLoss:0.7648 | SPLoss:0.7645 | CLSLoss:0.0105 | top1:50.3077 | AUROC:0.8320\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.003988\n",
      "Train | 64/64 | Loss:0.5327 | MainLoss:0.3968 | Alpha:0.4016 | SPLoss:0.3272 | CLSLoss:0.0123 | top1:81.8667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1680 | MainLoss:0.1680 | SPLoss:0.3058 | CLSLoss:0.0152 | top1:99.1215 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7420 | MainLoss:0.7420 | SPLoss:0.3058 | CLSLoss:0.0152 | top1:52.8974 | AUROC:0.8570\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.003987\n",
      "Train | 64/64 | Loss:0.5880 | MainLoss:0.3941 | Alpha:0.4045 | SPLoss:0.4669 | CLSLoss:0.0122 | top1:81.9937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1716 | MainLoss:0.1716 | SPLoss:0.2870 | CLSLoss:0.0100 | top1:99.8037 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7789 | MainLoss:0.7789 | SPLoss:0.2870 | CLSLoss:0.0100 | top1:50.1538 | AUROC:0.8768\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.003987\n",
      "Train | 64/64 | Loss:0.6232 | MainLoss:0.3953 | Alpha:0.4021 | SPLoss:0.5551 | CLSLoss:0.0116 | top1:82.1587 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1597 | MainLoss:0.1597 | SPLoss:0.5406 | CLSLoss:0.0131 | top1:99.7944 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7962 | MainLoss:0.7962 | SPLoss:0.5406 | CLSLoss:0.0131 | top1:50.1795 | AUROC:0.8793\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.003986\n",
      "Train | 64/64 | Loss:0.6478 | MainLoss:0.3946 | Alpha:0.4029 | SPLoss:0.6167 | CLSLoss:0.0126 | top1:82.1079 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1597 | MainLoss:0.1597 | SPLoss:1.8100 | CLSLoss:0.0148 | top1:98.6075 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.8416 | MainLoss:0.8416 | SPLoss:1.8100 | CLSLoss:0.0148 | top1:50.0000 | AUROC:0.9211\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.003985\n",
      "Train | 64/64 | Loss:0.7204 | MainLoss:0.3981 | Alpha:0.3949 | SPLoss:0.7994 | CLSLoss:0.0155 | top1:81.7270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1599 | MainLoss:0.1599 | SPLoss:0.5727 | CLSLoss:0.0166 | top1:99.3053 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.8047 | MainLoss:0.8047 | SPLoss:0.5727 | CLSLoss:0.0166 | top1:50.0256 | AUROC:0.9261\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4153 | MainLoss:0.3919 | Alpha:0.4000 | SPLoss:0.0432 | CLSLoss:0.0155 | top1:82.0191 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1737 | MainLoss:0.1737 | SPLoss:0.0383 | CLSLoss:0.0146 | top1:99.4330 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7373 | MainLoss:0.7373 | SPLoss:0.0383 | CLSLoss:0.0146 | top1:51.3846 | AUROC:0.8999\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4100 | MainLoss:0.3901 | Alpha:0.4035 | SPLoss:0.0355 | CLSLoss:0.0139 | top1:82.1206 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1457 | MainLoss:0.1457 | SPLoss:0.0443 | CLSLoss:0.0132 | top1:99.5483 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7997 | MainLoss:0.7997 | SPLoss:0.0443 | CLSLoss:0.0132 | top1:50.9744 | AUROC:0.8812\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4092 | MainLoss:0.3871 | Alpha:0.4045 | SPLoss:0.0423 | CLSLoss:0.0124 | top1:82.5524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1693 | MainLoss:0.1693 | SPLoss:0.0324 | CLSLoss:0.0122 | top1:99.0000 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7307 | MainLoss:0.7307 | SPLoss:0.0324 | CLSLoss:0.0122 | top1:53.1026 | AUROC:0.8876\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4152 | MainLoss:0.3932 | Alpha:0.4034 | SPLoss:0.0430 | CLSLoss:0.0114 | top1:81.9810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1638 | MainLoss:0.1638 | SPLoss:0.0757 | CLSLoss:0.0111 | top1:99.5576 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7542 | MainLoss:0.7542 | SPLoss:0.0757 | CLSLoss:0.0111 | top1:51.2051 | AUROC:0.8910\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4020 | MainLoss:0.3847 | Alpha:0.4049 | SPLoss:0.0311 | CLSLoss:0.0116 | top1:82.6032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1659 | MainLoss:0.1659 | SPLoss:0.0316 | CLSLoss:0.0114 | top1:99.3209 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7430 | MainLoss:0.7430 | SPLoss:0.0316 | CLSLoss:0.0114 | top1:51.9615 | AUROC:0.9020\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4080 | MainLoss:0.3878 | Alpha:0.4044 | SPLoss:0.0383 | CLSLoss:0.0115 | top1:82.1714 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1717 | MainLoss:0.1717 | SPLoss:0.0312 | CLSLoss:0.0112 | top1:99.3333 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7287 | MainLoss:0.7287 | SPLoss:0.0312 | CLSLoss:0.0112 | top1:52.1410 | AUROC:0.9066\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4091 | MainLoss:0.3849 | Alpha:0.4053 | SPLoss:0.0487 | CLSLoss:0.0112 | top1:82.6413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1555 | MainLoss:0.1555 | SPLoss:0.0562 | CLSLoss:0.0106 | top1:99.6947 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7759 | MainLoss:0.7759 | SPLoss:0.0562 | CLSLoss:0.0106 | top1:50.8590 | AUROC:0.9080\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4170 | MainLoss:0.3866 | Alpha:0.4044 | SPLoss:0.0638 | CLSLoss:0.0111 | top1:82.4127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1872 | MainLoss:0.1872 | SPLoss:0.0663 | CLSLoss:0.0111 | top1:99.2087 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6965 | MainLoss:0.6965 | SPLoss:0.0663 | CLSLoss:0.0111 | top1:52.5256 | AUROC:0.9319\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4003 | MainLoss:0.3768 | Alpha:0.4062 | SPLoss:0.0461 | CLSLoss:0.0118 | top1:82.9206 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1629 | MainLoss:0.1629 | SPLoss:0.0622 | CLSLoss:0.0118 | top1:99.2835 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7330 | MainLoss:0.7330 | SPLoss:0.0622 | CLSLoss:0.0118 | top1:52.4744 | AUROC:0.9237\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4052 | MainLoss:0.3821 | Alpha:0.4060 | SPLoss:0.0453 | CLSLoss:0.0116 | top1:82.6286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1375 | MainLoss:0.1375 | SPLoss:0.0523 | CLSLoss:0.0115 | top1:99.6885 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8103 | MainLoss:0.8103 | SPLoss:0.0523 | CLSLoss:0.0115 | top1:50.9231 | AUROC:0.9025\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.000398\n",
      "Train | 64/64 | Loss:0.4110 | MainLoss:0.3846 | Alpha:0.4048 | SPLoss:0.0540 | CLSLoss:0.0112 | top1:82.2349 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1918 | MainLoss:0.1918 | SPLoss:0.0480 | CLSLoss:0.0109 | top1:98.8692 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6785 | MainLoss:0.6785 | SPLoss:0.0480 | CLSLoss:0.0109 | top1:53.3590 | AUROC:0.9288\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4044 | MainLoss:0.3828 | Alpha:0.4049 | SPLoss:0.0424 | CLSLoss:0.0109 | top1:82.5905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1299 | MainLoss:0.1299 | SPLoss:0.0634 | CLSLoss:0.0110 | top1:99.6199 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8334 | MainLoss:0.8334 | SPLoss:0.0634 | CLSLoss:0.0110 | top1:51.3846 | AUROC:0.8853\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4084 | MainLoss:0.3776 | Alpha:0.4062 | SPLoss:0.0653 | CLSLoss:0.0106 | top1:83.0857 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1764 | MainLoss:0.1764 | SPLoss:0.0652 | CLSLoss:0.0099 | top1:98.5483 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7016 | MainLoss:0.7016 | SPLoss:0.0652 | CLSLoss:0.0099 | top1:54.7692 | AUROC:0.9242\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4085 | MainLoss:0.3824 | Alpha:0.4055 | SPLoss:0.0542 | CLSLoss:0.0102 | top1:82.4762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1519 | MainLoss:0.1519 | SPLoss:0.0406 | CLSLoss:0.0109 | top1:99.0312 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7539 | MainLoss:0.7539 | SPLoss:0.0406 | CLSLoss:0.0109 | top1:53.3718 | AUROC:0.9257\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4099 | MainLoss:0.3792 | Alpha:0.4072 | SPLoss:0.0649 | CLSLoss:0.0106 | top1:82.7937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1326 | MainLoss:0.1326 | SPLoss:0.0622 | CLSLoss:0.0109 | top1:99.5327 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8180 | MainLoss:0.8180 | SPLoss:0.0622 | CLSLoss:0.0109 | top1:51.8590 | AUROC:0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [67 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4132 | MainLoss:0.3787 | Alpha:0.4059 | SPLoss:0.0747 | CLSLoss:0.0102 | top1:82.8952 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1373 | MainLoss:0.1373 | SPLoss:0.0922 | CLSLoss:0.0105 | top1:99.4174 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7992 | MainLoss:0.7992 | SPLoss:0.0922 | CLSLoss:0.0105 | top1:52.0641 | AUROC:0.9081\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4099 | MainLoss:0.3824 | Alpha:0.4054 | SPLoss:0.0578 | CLSLoss:0.0100 | top1:82.6794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:0.0430 | CLSLoss:0.0103 | top1:98.8847 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7449 | MainLoss:0.7449 | SPLoss:0.0430 | CLSLoss:0.0103 | top1:53.8205 | AUROC:0.9178\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4124 | MainLoss:0.3741 | Alpha:0.4080 | SPLoss:0.0834 | CLSLoss:0.0104 | top1:83.0730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1385 | MainLoss:0.1385 | SPLoss:0.0884 | CLSLoss:0.0103 | top1:99.6947 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7899 | MainLoss:0.7899 | SPLoss:0.0884 | CLSLoss:0.0103 | top1:51.2179 | AUROC:0.9265\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4099 | MainLoss:0.3803 | Alpha:0.4057 | SPLoss:0.0626 | CLSLoss:0.0105 | top1:82.7302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1504 | MainLoss:0.1504 | SPLoss:0.0588 | CLSLoss:0.0105 | top1:98.9813 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7489 | MainLoss:0.7489 | SPLoss:0.0588 | CLSLoss:0.0105 | top1:53.3205 | AUROC:0.9207\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.000397\n",
      "Train | 64/64 | Loss:0.4044 | MainLoss:0.3724 | Alpha:0.4085 | SPLoss:0.0681 | CLSLoss:0.0104 | top1:82.9968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1490 | MainLoss:0.1490 | SPLoss:0.0618 | CLSLoss:0.0107 | top1:98.7664 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7398 | MainLoss:0.7398 | SPLoss:0.0618 | CLSLoss:0.0107 | top1:54.3718 | AUROC:0.9317\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4215 | MainLoss:0.3811 | Alpha:0.4064 | SPLoss:0.0892 | CLSLoss:0.0103 | top1:82.3238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1780 | MainLoss:0.1780 | SPLoss:0.0813 | CLSLoss:0.0098 | top1:97.8754 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6865 | MainLoss:0.6865 | SPLoss:0.0813 | CLSLoss:0.0098 | top1:56.7564 | AUROC:0.9176\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4148 | MainLoss:0.3754 | Alpha:0.4072 | SPLoss:0.0867 | CLSLoss:0.0101 | top1:82.8064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1238 | MainLoss:0.1238 | SPLoss:0.1106 | CLSLoss:0.0104 | top1:99.4517 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8268 | MainLoss:0.8268 | SPLoss:0.1106 | CLSLoss:0.0104 | top1:52.1410 | AUROC:0.8963\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4071 | MainLoss:0.3757 | Alpha:0.4059 | SPLoss:0.0674 | CLSLoss:0.0098 | top1:82.9333 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1626 | MainLoss:0.1626 | SPLoss:0.0647 | CLSLoss:0.0102 | top1:98.3271 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7273 | MainLoss:0.7273 | SPLoss:0.0647 | CLSLoss:0.0102 | top1:55.3718 | AUROC:0.9104\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4217 | MainLoss:0.3803 | Alpha:0.4068 | SPLoss:0.0918 | CLSLoss:0.0097 | top1:82.5397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1633 | MainLoss:0.1633 | SPLoss:0.0617 | CLSLoss:0.0094 | top1:99.1277 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7261 | MainLoss:0.7261 | SPLoss:0.0617 | CLSLoss:0.0094 | top1:53.4487 | AUROC:0.9359\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4134 | MainLoss:0.3732 | Alpha:0.4084 | SPLoss:0.0887 | CLSLoss:0.0100 | top1:82.8952 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1623 | MainLoss:0.1623 | SPLoss:0.0866 | CLSLoss:0.0101 | top1:98.4611 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7254 | MainLoss:0.7254 | SPLoss:0.0866 | CLSLoss:0.0101 | top1:54.8974 | AUROC:0.9236\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4170 | MainLoss:0.3789 | Alpha:0.4078 | SPLoss:0.0837 | CLSLoss:0.0096 | top1:82.7810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1704 | MainLoss:0.1704 | SPLoss:0.0659 | CLSLoss:0.0095 | top1:98.8754 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7056 | MainLoss:0.7056 | SPLoss:0.0659 | CLSLoss:0.0095 | top1:53.7949 | AUROC:0.9379\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4181 | MainLoss:0.3827 | Alpha:0.4059 | SPLoss:0.0775 | CLSLoss:0.0095 | top1:82.2476 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1491 | MainLoss:0.1491 | SPLoss:0.1366 | CLSLoss:0.0095 | top1:99.0966 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7428 | MainLoss:0.7428 | SPLoss:0.1366 | CLSLoss:0.0095 | top1:53.3333 | AUROC:0.9171\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.000396\n",
      "Train | 64/64 | Loss:0.4079 | MainLoss:0.3741 | Alpha:0.4079 | SPLoss:0.0731 | CLSLoss:0.0099 | top1:82.8191 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1833 | MainLoss:0.1833 | SPLoss:0.0811 | CLSLoss:0.0098 | top1:97.3583 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6738 | MainLoss:0.6738 | SPLoss:0.0811 | CLSLoss:0.0098 | top1:57.7949 | AUROC:0.9126\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4111 | MainLoss:0.3747 | Alpha:0.4075 | SPLoss:0.0799 | CLSLoss:0.0094 | top1:82.9587 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1356 | MainLoss:0.1356 | SPLoss:0.0841 | CLSLoss:0.0088 | top1:99.2897 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7821 | MainLoss:0.7821 | SPLoss:0.0841 | CLSLoss:0.0088 | top1:52.7179 | AUROC:0.9246\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4117 | MainLoss:0.3725 | Alpha:0.4075 | SPLoss:0.0865 | CLSLoss:0.0096 | top1:82.8444 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1530 | MainLoss:0.1530 | SPLoss:0.0930 | CLSLoss:0.0099 | top1:99.0343 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7293 | MainLoss:0.7293 | SPLoss:0.0930 | CLSLoss:0.0099 | top1:53.5641 | AUROC:0.9445\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4008 | MainLoss:0.3676 | Alpha:0.4093 | SPLoss:0.0707 | CLSLoss:0.0102 | top1:83.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1180 | MainLoss:0.1180 | SPLoss:0.0799 | CLSLoss:0.0107 | top1:99.5389 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8229 | MainLoss:0.8229 | SPLoss:0.0799 | CLSLoss:0.0107 | top1:52.1538 | AUROC:0.9316\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4068 | MainLoss:0.3730 | Alpha:0.4060 | SPLoss:0.0730 | CLSLoss:0.0104 | top1:82.8698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1618 | MainLoss:0.1618 | SPLoss:0.0904 | CLSLoss:0.0103 | top1:98.5576 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6950 | MainLoss:0.6950 | SPLoss:0.0904 | CLSLoss:0.0103 | top1:55.3846 | AUROC:0.9476\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4168 | MainLoss:0.3725 | Alpha:0.4092 | SPLoss:0.0985 | CLSLoss:0.0098 | top1:82.9460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1346 | MainLoss:0.1346 | SPLoss:0.0837 | CLSLoss:0.0096 | top1:98.9844 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7676 | MainLoss:0.7676 | SPLoss:0.0837 | CLSLoss:0.0096 | top1:53.9231 | AUROC:0.9299\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4235 | MainLoss:0.3739 | Alpha:0.4072 | SPLoss:0.1122 | CLSLoss:0.0096 | top1:82.8317 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1757 | MainLoss:0.1757 | SPLoss:0.1125 | CLSLoss:0.0086 | top1:98.5389 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.6607 | MainLoss:0.6607 | SPLoss:0.1125 | CLSLoss:0.0086 | top1:55.8205 | AUROC:0.9551\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.000395\n",
      "Train | 64/64 | Loss:0.4109 | MainLoss:0.3704 | Alpha:0.4084 | SPLoss:0.0895 | CLSLoss:0.0096 | top1:83.1873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1871 | MainLoss:0.1871 | SPLoss:0.0825 | CLSLoss:0.0102 | top1:97.0841 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6327 | MainLoss:0.6327 | SPLoss:0.0825 | CLSLoss:0.0102 | top1:59.9103 | AUROC:0.9563\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.4154 | MainLoss:0.3687 | Alpha:0.4091 | SPLoss:0.1040 | CLSLoss:0.0102 | top1:83.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1160 | MainLoss:0.1160 | SPLoss:0.2138 | CLSLoss:0.0100 | top1:99.2523 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8180 | MainLoss:0.8180 | SPLoss:0.2138 | CLSLoss:0.0100 | top1:53.1795 | AUROC:0.8978\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.4136 | MainLoss:0.3692 | Alpha:0.4073 | SPLoss:0.0990 | CLSLoss:0.0098 | top1:82.9841 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1241 | MainLoss:0.1241 | SPLoss:0.0932 | CLSLoss:0.0102 | top1:98.8131 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8000 | MainLoss:0.8000 | SPLoss:0.0932 | CLSLoss:0.0102 | top1:54.3333 | AUROC:0.8988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [89 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.4104 | MainLoss:0.3686 | Alpha:0.4083 | SPLoss:0.0925 | CLSLoss:0.0098 | top1:82.9841 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2172 | MainLoss:0.2172 | SPLoss:0.1321 | CLSLoss:0.0101 | top1:94.3770 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.5958 | MainLoss:0.5958 | SPLoss:0.1321 | CLSLoss:0.0101 | top1:63.1538 | AUROC:0.9487\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.4136 | MainLoss:0.3631 | Alpha:0.4089 | SPLoss:0.1133 | CLSLoss:0.0100 | top1:83.5556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1388 | MainLoss:0.1388 | SPLoss:0.1364 | CLSLoss:0.0107 | top1:98.0467 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7710 | MainLoss:0.7710 | SPLoss:0.1364 | CLSLoss:0.0107 | top1:57.0897 | AUROC:0.9133\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.4344 | MainLoss:0.3684 | Alpha:0.4088 | SPLoss:0.1512 | CLSLoss:0.0098 | top1:82.9841 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1436 | MainLoss:0.1436 | SPLoss:0.1608 | CLSLoss:0.0088 | top1:99.0343 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7434 | MainLoss:0.7434 | SPLoss:0.1608 | CLSLoss:0.0088 | top1:53.4231 | AUROC:0.9377\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.3963 | MainLoss:0.3529 | Alpha:0.4125 | SPLoss:0.0954 | CLSLoss:0.0096 | top1:84.0889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1198 | MainLoss:0.1198 | SPLoss:0.1332 | CLSLoss:0.0106 | top1:99.0685 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.7976 | MainLoss:0.7976 | SPLoss:0.1332 | CLSLoss:0.0106 | top1:53.9103 | AUROC:0.9374\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.000394\n",
      "Train | 64/64 | Loss:0.4109 | MainLoss:0.3619 | Alpha:0.4088 | SPLoss:0.1096 | CLSLoss:0.0101 | top1:83.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1748 | MainLoss:0.1748 | SPLoss:0.1144 | CLSLoss:0.0098 | top1:96.4019 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6698 | MainLoss:0.6698 | SPLoss:0.1144 | CLSLoss:0.0098 | top1:60.1154 | AUROC:0.9371\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.000393\n",
      "Train | 64/64 | Loss:0.4190 | MainLoss:0.3633 | Alpha:0.4119 | SPLoss:0.1260 | CLSLoss:0.0093 | top1:83.7460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1106 | MainLoss:0.1106 | SPLoss:0.1274 | CLSLoss:0.0094 | top1:99.1932 | AUROC:1.0000\n",
      "Test | 63/64 | Loss:0.8355 | MainLoss:0.8355 | SPLoss:0.1274 | CLSLoss:0.0094 | top1:53.5769 | AUROC:0.9156\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.000393\n",
      "Train | 64/64 | Loss:0.4211 | MainLoss:0.3690 | Alpha:0.4068 | SPLoss:0.1189 | CLSLoss:0.0090 | top1:83.0984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1978 | MainLoss:0.1978 | SPLoss:0.1177 | CLSLoss:0.0088 | top1:93.9875 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6083 | MainLoss:0.6083 | SPLoss:0.1177 | CLSLoss:0.0088 | top1:65.5256 | AUROC:0.9433\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.000393\n",
      "Train | 64/64 | Loss:0.4193 | MainLoss:0.3600 | Alpha:0.4112 | SPLoss:0.1352 | CLSLoss:0.0090 | top1:83.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1283 | MainLoss:0.1283 | SPLoss:0.1483 | CLSLoss:0.0092 | top1:98.0312 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7542 | MainLoss:0.7542 | SPLoss:0.1483 | CLSLoss:0.0092 | top1:57.9487 | AUROC:0.9244\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.000393\n",
      "Train | 64/64 | Loss:0.4192 | MainLoss:0.3573 | Alpha:0.4125 | SPLoss:0.1405 | CLSLoss:0.0090 | top1:83.6190 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1558 | MainLoss:0.1558 | SPLoss:0.1349 | CLSLoss:0.0095 | top1:96.8193 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6778 | MainLoss:0.6778 | SPLoss:0.1349 | CLSLoss:0.0095 | top1:60.9103 | AUROC:0.9338\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.000393\n",
      "Train | 64/64 | Loss:0.4150 | MainLoss:0.3494 | Alpha:0.4146 | SPLoss:0.1484 | CLSLoss:0.0097 | top1:84.1524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1431 | MainLoss:0.1431 | SPLoss:0.1606 | CLSLoss:0.0096 | top1:97.7695 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7173 | MainLoss:0.7173 | SPLoss:0.1606 | CLSLoss:0.0096 | top1:58.3333 | AUROC:0.9292\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.000393\n",
      "Train | 64/64 | Loss:0.4114 | MainLoss:0.3470 | Alpha:0.4150 | SPLoss:0.1455 | CLSLoss:0.0096 | top1:84.5714 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1631 | MainLoss:0.1631 | SPLoss:0.1255 | CLSLoss:0.0095 | top1:96.7726 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6620 | MainLoss:0.6620 | SPLoss:0.1255 | CLSLoss:0.0095 | top1:61.0000 | AUROC:0.9360\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.000392\n",
      "Train | 64/64 | Loss:0.4115 | MainLoss:0.3500 | Alpha:0.4146 | SPLoss:0.1385 | CLSLoss:0.0098 | top1:84.1651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1267 | MainLoss:0.1267 | SPLoss:0.1664 | CLSLoss:0.0099 | top1:98.2368 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7429 | MainLoss:0.7429 | SPLoss:0.1664 | CLSLoss:0.0099 | top1:57.8846 | AUROC:0.9321\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.000392\n",
      "Train | 64/64 | Loss:0.4158 | MainLoss:0.3522 | Alpha:0.4134 | SPLoss:0.1441 | CLSLoss:0.0098 | top1:84.1778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2298 | MainLoss:0.2298 | SPLoss:0.1950 | CLSLoss:0.0096 | top1:91.9813 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.5456 | MainLoss:0.5456 | SPLoss:0.1950 | CLSLoss:0.0096 | top1:69.6667 | AUROC:0.9492\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.000392\n",
      "Train | 64/64 | Loss:0.4218 | MainLoss:0.3519 | Alpha:0.4115 | SPLoss:0.1600 | CLSLoss:0.0097 | top1:83.7968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:0.1753 | CLSLoss:0.0100 | top1:96.6698 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6784 | MainLoss:0.6784 | SPLoss:0.1753 | CLSLoss:0.0100 | top1:60.3718 | AUROC:0.9453\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.000392\n",
      "Train | 64/64 | Loss:0.4196 | MainLoss:0.3536 | Alpha:0.4146 | SPLoss:0.1495 | CLSLoss:0.0099 | top1:83.9365 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1717 | MainLoss:0.1717 | SPLoss:0.1580 | CLSLoss:0.0099 | top1:95.8629 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6483 | MainLoss:0.6483 | SPLoss:0.1580 | CLSLoss:0.0099 | top1:61.8846 | AUROC:0.9548\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.000392\n",
      "Train | 64/64 | Loss:0.4103 | MainLoss:0.3434 | Alpha:0.4154 | SPLoss:0.1507 | CLSLoss:0.0101 | top1:84.2286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1133 | MainLoss:0.1133 | SPLoss:0.1676 | CLSLoss:0.0105 | top1:98.4237 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7894 | MainLoss:0.7894 | SPLoss:0.1676 | CLSLoss:0.0105 | top1:56.4231 | AUROC:0.9380\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.000392\n",
      "Train | 64/64 | Loss:0.4116 | MainLoss:0.3433 | Alpha:0.4145 | SPLoss:0.1544 | CLSLoss:0.0104 | top1:84.5968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1886 | MainLoss:0.1886 | SPLoss:0.1330 | CLSLoss:0.0105 | top1:93.5639 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6127 | MainLoss:0.6127 | SPLoss:0.1330 | CLSLoss:0.0105 | top1:66.5641 | AUROC:0.9371\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.000391\n",
      "Train | 64/64 | Loss:0.4200 | MainLoss:0.3479 | Alpha:0.4152 | SPLoss:0.1640 | CLSLoss:0.0097 | top1:84.4191 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1389 | MainLoss:0.1389 | SPLoss:0.1857 | CLSLoss:0.0098 | top1:97.2804 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7042 | MainLoss:0.7042 | SPLoss:0.1857 | CLSLoss:0.0098 | top1:59.9359 | AUROC:0.9360\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.000391\n",
      "Train | 64/64 | Loss:0.4178 | MainLoss:0.3428 | Alpha:0.4160 | SPLoss:0.1703 | CLSLoss:0.0100 | top1:84.5841 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1338 | MainLoss:0.1338 | SPLoss:0.2599 | CLSLoss:0.0099 | top1:97.9284 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6977 | MainLoss:0.6977 | SPLoss:0.2599 | CLSLoss:0.0099 | top1:58.9103 | AUROC:0.9345\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.000391\n",
      "Train | 64/64 | Loss:0.4110 | MainLoss:0.3391 | Alpha:0.4170 | SPLoss:0.1622 | CLSLoss:0.0100 | top1:84.7746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1685 | MainLoss:0.1685 | SPLoss:0.1758 | CLSLoss:0.0103 | top1:95.2991 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6368 | MainLoss:0.6368 | SPLoss:0.1758 | CLSLoss:0.0103 | top1:63.7179 | AUROC:0.9337\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.000391\n",
      "Train | 64/64 | Loss:0.4154 | MainLoss:0.3452 | Alpha:0.4169 | SPLoss:0.1583 | CLSLoss:0.0102 | top1:84.4571 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1471 | MainLoss:0.1471 | SPLoss:0.1511 | CLSLoss:0.0098 | top1:96.4704 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7103 | MainLoss:0.7103 | SPLoss:0.1511 | CLSLoss:0.0098 | top1:60.5769 | AUROC:0.9114\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.000391\n",
      "Train | 64/64 | Loss:0.4029 | MainLoss:0.3330 | Alpha:0.4180 | SPLoss:0.1572 | CLSLoss:0.0099 | top1:85.1048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1406 | MainLoss:0.1406 | SPLoss:0.2372 | CLSLoss:0.0104 | top1:97.0810 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7389 | MainLoss:0.7389 | SPLoss:0.2372 | CLSLoss:0.0104 | top1:58.1667 | AUROC:0.9151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [111 | 1000] LR: 0.000390\n",
      "Train | 64/64 | Loss:0.4224 | MainLoss:0.3352 | Alpha:0.4171 | SPLoss:0.1987 | CLSLoss:0.0106 | top1:84.7492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1370 | MainLoss:0.1370 | SPLoss:0.2321 | CLSLoss:0.0105 | top1:98.2181 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6805 | MainLoss:0.6805 | SPLoss:0.2321 | CLSLoss:0.0105 | top1:58.5256 | AUROC:0.9435\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.000390\n",
      "Train | 64/64 | Loss:0.4071 | MainLoss:0.3336 | Alpha:0.4188 | SPLoss:0.1653 | CLSLoss:0.0104 | top1:85.1556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1656 | MainLoss:0.1656 | SPLoss:0.1825 | CLSLoss:0.0103 | top1:95.5545 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6183 | MainLoss:0.6183 | SPLoss:0.1825 | CLSLoss:0.0103 | top1:64.6667 | AUROC:0.9445\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.000390\n",
      "Train | 64/64 | Loss:0.4117 | MainLoss:0.3286 | Alpha:0.4196 | SPLoss:0.1874 | CLSLoss:0.0108 | top1:85.0286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2013 | MainLoss:0.2013 | SPLoss:0.2091 | CLSLoss:0.0109 | top1:91.2181 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.5729 | MainLoss:0.5729 | SPLoss:0.2091 | CLSLoss:0.0109 | top1:70.3333 | AUROC:0.9409\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.000390\n",
      "Train | 64/64 | Loss:0.4202 | MainLoss:0.3327 | Alpha:0.4174 | SPLoss:0.1989 | CLSLoss:0.0107 | top1:84.8127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1721 | MainLoss:0.1721 | SPLoss:0.1954 | CLSLoss:0.0105 | top1:94.5763 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.5995 | MainLoss:0.5995 | SPLoss:0.1954 | CLSLoss:0.0105 | top1:67.3462 | AUROC:0.9370\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.000390\n",
      "Train | 64/64 | Loss:0.4142 | MainLoss:0.3257 | Alpha:0.4220 | SPLoss:0.1993 | CLSLoss:0.0106 | top1:85.5873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1330 | MainLoss:0.1330 | SPLoss:0.1927 | CLSLoss:0.0103 | top1:97.0280 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7153 | MainLoss:0.7153 | SPLoss:0.1927 | CLSLoss:0.0103 | top1:61.3974 | AUROC:0.9222\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.000389\n",
      "Train | 64/64 | Loss:0.4099 | MainLoss:0.3280 | Alpha:0.4198 | SPLoss:0.1854 | CLSLoss:0.0099 | top1:85.4222 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1671 | MainLoss:0.1671 | SPLoss:0.1718 | CLSLoss:0.0098 | top1:95.4953 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.6250 | MainLoss:0.6250 | SPLoss:0.1718 | CLSLoss:0.0098 | top1:65.1282 | AUROC:0.9366\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.000389\n",
      "Train | 64/64 | Loss:0.4278 | MainLoss:0.3324 | Alpha:0.4210 | SPLoss:0.2170 | CLSLoss:0.0098 | top1:85.1810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1408 | MainLoss:0.1408 | SPLoss:0.2049 | CLSLoss:0.0098 | top1:96.4268 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6922 | MainLoss:0.6922 | SPLoss:0.2049 | CLSLoss:0.0098 | top1:62.8846 | AUROC:0.9245\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.000389\n",
      "Train | 64/64 | Loss:0.4093 | MainLoss:0.3237 | Alpha:0.4196 | SPLoss:0.1940 | CLSLoss:0.0100 | top1:85.5492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1757 | MainLoss:0.1757 | SPLoss:0.2247 | CLSLoss:0.0103 | top1:94.3894 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6248 | MainLoss:0.6248 | SPLoss:0.2247 | CLSLoss:0.0103 | top1:65.8974 | AUROC:0.9381\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.000389\n",
      "Train | 64/64 | Loss:0.4104 | MainLoss:0.3240 | Alpha:0.4220 | SPLoss:0.1946 | CLSLoss:0.0104 | top1:85.6127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1046 | MainLoss:0.1046 | SPLoss:0.2137 | CLSLoss:0.0103 | top1:97.9782 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7937 | MainLoss:0.7937 | SPLoss:0.2137 | CLSLoss:0.0103 | top1:58.8333 | AUROC:0.9294\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.000389\n",
      "Train | 64/64 | Loss:0.4106 | MainLoss:0.3205 | Alpha:0.4190 | SPLoss:0.2049 | CLSLoss:0.0101 | top1:86.1968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1312 | MainLoss:0.1312 | SPLoss:0.1921 | CLSLoss:0.0102 | top1:96.7477 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7069 | MainLoss:0.7069 | SPLoss:0.1921 | CLSLoss:0.0102 | top1:63.0256 | AUROC:0.9216\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.000388\n",
      "Train | 64/64 | Loss:0.4184 | MainLoss:0.3203 | Alpha:0.4227 | SPLoss:0.2222 | CLSLoss:0.0098 | top1:85.8667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2988 | MainLoss:0.2988 | SPLoss:0.2433 | CLSLoss:0.0100 | top1:86.6106 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.5015 | MainLoss:0.5015 | SPLoss:0.2433 | CLSLoss:0.0100 | top1:74.3205 | AUROC:0.9345\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.000388\n",
      "Train | 64/64 | Loss:0.4165 | MainLoss:0.3131 | Alpha:0.4135 | SPLoss:0.2397 | CLSLoss:0.0102 | top1:86.2984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1079 | MainLoss:0.1079 | SPLoss:0.2687 | CLSLoss:0.0104 | top1:97.6978 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.7735 | MainLoss:0.7735 | SPLoss:0.2687 | CLSLoss:0.0104 | top1:59.7308 | AUROC:0.9110\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.000388\n",
      "Train | 64/64 | Loss:0.4158 | MainLoss:0.3184 | Alpha:0.4205 | SPLoss:0.2213 | CLSLoss:0.0102 | top1:85.7778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1417 | MainLoss:0.1417 | SPLoss:0.2524 | CLSLoss:0.0099 | top1:96.4237 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6713 | MainLoss:0.6713 | SPLoss:0.2524 | CLSLoss:0.0099 | top1:63.2564 | AUROC:0.9356\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.000388\n",
      "Train | 64/64 | Loss:0.4119 | MainLoss:0.3188 | Alpha:0.4247 | SPLoss:0.2094 | CLSLoss:0.0097 | top1:86.0698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1033 | MainLoss:0.1033 | SPLoss:0.2029 | CLSLoss:0.0095 | top1:98.2773 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.8003 | MainLoss:0.8003 | SPLoss:0.2029 | CLSLoss:0.0095 | top1:57.7949 | AUROC:0.9041\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.000388\n",
      "Train | 64/64 | Loss:0.4055 | MainLoss:0.3144 | Alpha:0.4204 | SPLoss:0.2069 | CLSLoss:0.0095 | top1:86.2730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2454 | MainLoss:0.2454 | SPLoss:0.2267 | CLSLoss:0.0099 | top1:90.9813 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.5674 | MainLoss:0.5674 | SPLoss:0.2267 | CLSLoss:0.0099 | top1:70.1410 | AUROC:0.9284\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.000387\n",
      "Train | 64/64 | Loss:0.4181 | MainLoss:0.3069 | Alpha:0.4225 | SPLoss:0.2534 | CLSLoss:0.0097 | top1:86.7683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1571 | MainLoss:0.1571 | SPLoss:0.2772 | CLSLoss:0.0099 | top1:95.5296 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6250 | MainLoss:0.6250 | SPLoss:0.2772 | CLSLoss:0.0099 | top1:66.0769 | AUROC:0.9383\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.000387\n",
      "Train | 64/64 | Loss:0.4141 | MainLoss:0.3109 | Alpha:0.4260 | SPLoss:0.2320 | CLSLoss:0.0101 | top1:86.1460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1842 | MainLoss:0.1842 | SPLoss:0.2243 | CLSLoss:0.0097 | top1:94.2741 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.5765 | MainLoss:0.5765 | SPLoss:0.2243 | CLSLoss:0.0097 | top1:67.1154 | AUROC:0.9434\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.000387\n",
      "Train | 64/64 | Loss:0.3950 | MainLoss:0.2986 | Alpha:0.4259 | SPLoss:0.2160 | CLSLoss:0.0106 | top1:86.8825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1251 | MainLoss:0.1251 | SPLoss:0.2528 | CLSLoss:0.0115 | top1:97.1184 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6948 | MainLoss:0.6948 | SPLoss:0.2528 | CLSLoss:0.0115 | top1:62.8205 | AUROC:0.9298\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.000387\n",
      "Train | 64/64 | Loss:0.4152 | MainLoss:0.3036 | Alpha:0.4270 | SPLoss:0.2500 | CLSLoss:0.0114 | top1:86.5524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1633 | MainLoss:0.1633 | SPLoss:0.2269 | CLSLoss:0.0110 | top1:94.5981 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6164 | MainLoss:0.6164 | SPLoss:0.2269 | CLSLoss:0.0110 | top1:67.5769 | AUROC:0.9176\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.000386\n",
      "Train | 64/64 | Loss:0.3943 | MainLoss:0.2887 | Alpha:0.4313 | SPLoss:0.2339 | CLSLoss:0.0110 | top1:87.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1224 | MainLoss:0.1224 | SPLoss:0.2767 | CLSLoss:0.0112 | top1:97.0187 | AUROC:0.9999\n",
      "Test | 63/64 | Loss:0.6768 | MainLoss:0.6768 | SPLoss:0.2767 | CLSLoss:0.0112 | top1:63.6026 | AUROC:0.9278\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.000386\n",
      "Train | 64/64 | Loss:0.4030 | MainLoss:0.2983 | Alpha:0.4277 | SPLoss:0.2333 | CLSLoss:0.0116 | top1:86.9206 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1026 | MainLoss:0.1026 | SPLoss:0.2596 | CLSLoss:0.0112 | top1:97.9377 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7546 | MainLoss:0.7546 | SPLoss:0.2596 | CLSLoss:0.0112 | top1:60.2564 | AUROC:0.9310\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.000386\n",
      "Train | 64/64 | Loss:0.3970 | MainLoss:0.2897 | Alpha:0.4275 | SPLoss:0.2393 | CLSLoss:0.0116 | top1:87.5302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1262 | MainLoss:0.1262 | SPLoss:0.2590 | CLSLoss:0.0116 | top1:96.7134 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.6971 | MainLoss:0.6971 | SPLoss:0.2590 | CLSLoss:0.0116 | top1:63.4744 | AUROC:0.9318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [133 | 1000] LR: 0.000386\n",
      "Train | 64/64 | Loss:0.4089 | MainLoss:0.2894 | Alpha:0.4294 | SPLoss:0.2666 | CLSLoss:0.0116 | top1:87.5937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1024 | MainLoss:0.1024 | SPLoss:0.3206 | CLSLoss:0.0116 | top1:98.0187 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.7652 | MainLoss:0.7652 | SPLoss:0.3206 | CLSLoss:0.0116 | top1:60.2436 | AUROC:0.9285\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.000385\n",
      "Train | 64/64 | Loss:0.3907 | MainLoss:0.2787 | Alpha:0.4287 | SPLoss:0.2489 | CLSLoss:0.0120 | top1:87.9238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1103 | MainLoss:0.1103 | SPLoss:0.2992 | CLSLoss:0.0121 | top1:97.1807 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7446 | MainLoss:0.7446 | SPLoss:0.2992 | CLSLoss:0.0121 | top1:62.7820 | AUROC:0.9284\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.000385\n",
      "Train | 64/64 | Loss:0.4093 | MainLoss:0.2841 | Alpha:0.4299 | SPLoss:0.2792 | CLSLoss:0.0121 | top1:87.5556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1311 | MainLoss:0.1311 | SPLoss:0.2960 | CLSLoss:0.0120 | top1:95.7788 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6965 | MainLoss:0.6965 | SPLoss:0.2960 | CLSLoss:0.0120 | top1:66.7692 | AUROC:0.9186\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.000385\n",
      "Train | 64/64 | Loss:0.4045 | MainLoss:0.2858 | Alpha:0.4330 | SPLoss:0.2634 | CLSLoss:0.0112 | top1:87.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0943 | MainLoss:0.0943 | SPLoss:0.2978 | CLSLoss:0.0109 | top1:97.6386 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.8622 | MainLoss:0.8622 | SPLoss:0.2978 | CLSLoss:0.0109 | top1:59.7820 | AUROC:0.8851\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.000385\n",
      "Train | 64/64 | Loss:0.3971 | MainLoss:0.2782 | Alpha:0.4300 | SPLoss:0.2651 | CLSLoss:0.0112 | top1:88.0000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1397 | MainLoss:0.1397 | SPLoss:0.2848 | CLSLoss:0.0116 | top1:95.7352 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7223 | MainLoss:0.7223 | SPLoss:0.2848 | CLSLoss:0.0116 | top1:63.6026 | AUROC:0.9137\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.000385\n",
      "Train | 64/64 | Loss:0.4048 | MainLoss:0.2861 | Alpha:0.4338 | SPLoss:0.2624 | CLSLoss:0.0113 | top1:87.7460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1299 | MainLoss:0.1299 | SPLoss:0.3850 | CLSLoss:0.0109 | top1:98.4798 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.6736 | MainLoss:0.6736 | SPLoss:0.3850 | CLSLoss:0.0109 | top1:57.4744 | AUROC:0.9249\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.000384\n",
      "Train | 64/64 | Loss:0.3741 | MainLoss:0.2728 | Alpha:0.4270 | SPLoss:0.2247 | CLSLoss:0.0122 | top1:88.1143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1108 | MainLoss:0.1108 | SPLoss:0.2674 | CLSLoss:0.0129 | top1:97.3458 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7215 | MainLoss:0.7215 | SPLoss:0.2674 | CLSLoss:0.0129 | top1:62.5000 | AUROC:0.9174\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.000384\n",
      "Train | 64/64 | Loss:0.3870 | MainLoss:0.2691 | Alpha:0.4329 | SPLoss:0.2593 | CLSLoss:0.0126 | top1:88.5714 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1209 | MainLoss:0.1209 | SPLoss:0.3016 | CLSLoss:0.0123 | top1:96.2150 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7045 | MainLoss:0.7045 | SPLoss:0.3016 | CLSLoss:0.0123 | top1:66.1282 | AUROC:0.8977\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.000384\n",
      "Train | 64/64 | Loss:0.4043 | MainLoss:0.2671 | Alpha:0.4354 | SPLoss:0.3030 | CLSLoss:0.0120 | top1:88.6857 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1656 | MainLoss:0.1656 | SPLoss:0.3659 | CLSLoss:0.0119 | top1:94.0685 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.6735 | MainLoss:0.6735 | SPLoss:0.3659 | CLSLoss:0.0119 | top1:67.1026 | AUROC:0.9037\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.000384\n",
      "Train | 64/64 | Loss:0.4078 | MainLoss:0.2641 | Alpha:0.4365 | SPLoss:0.3171 | CLSLoss:0.0120 | top1:88.7746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2857 | MainLoss:0.2857 | SPLoss:0.3574 | CLSLoss:0.0121 | top1:86.5296 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.4535 | MainLoss:0.4535 | SPLoss:0.3574 | CLSLoss:0.0121 | top1:76.5769 | AUROC:0.9362\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.000383\n",
      "Train | 64/64 | Loss:0.3808 | MainLoss:0.2599 | Alpha:0.4214 | SPLoss:0.2741 | CLSLoss:0.0129 | top1:88.6222 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0939 | MainLoss:0.0939 | SPLoss:0.4174 | CLSLoss:0.0138 | top1:97.6012 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7861 | MainLoss:0.7861 | SPLoss:0.4174 | CLSLoss:0.0138 | top1:61.4744 | AUROC:0.8997\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.000383\n",
      "Train | 64/64 | Loss:0.3957 | MainLoss:0.2732 | Alpha:0.4328 | SPLoss:0.2699 | CLSLoss:0.0137 | top1:88.0889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1627 | MainLoss:0.1627 | SPLoss:0.3351 | CLSLoss:0.0129 | top1:93.9221 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.6159 | MainLoss:0.6159 | SPLoss:0.3351 | CLSLoss:0.0129 | top1:70.0769 | AUROC:0.9062\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.000383\n",
      "Train | 64/64 | Loss:0.3837 | MainLoss:0.2576 | Alpha:0.4395 | SPLoss:0.2742 | CLSLoss:0.0129 | top1:88.8254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1301 | MainLoss:0.1301 | SPLoss:0.3000 | CLSLoss:0.0131 | top1:95.4579 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.6761 | MainLoss:0.6761 | SPLoss:0.3000 | CLSLoss:0.0131 | top1:67.7179 | AUROC:0.9069\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.000383\n",
      "Train | 64/64 | Loss:0.3914 | MainLoss:0.2612 | Alpha:0.4387 | SPLoss:0.2844 | CLSLoss:0.0128 | top1:88.9143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.3769 | CLSLoss:0.0121 | top1:94.4953 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.6119 | MainLoss:0.6119 | SPLoss:0.3769 | CLSLoss:0.0121 | top1:69.5897 | AUROC:0.9111\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.000382\n",
      "Train | 64/64 | Loss:0.3814 | MainLoss:0.2526 | Alpha:0.4395 | SPLoss:0.2806 | CLSLoss:0.0124 | top1:89.1810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1684 | MainLoss:0.1684 | SPLoss:0.3881 | CLSLoss:0.0129 | top1:93.5140 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.6468 | MainLoss:0.6468 | SPLoss:0.3881 | CLSLoss:0.0129 | top1:68.5641 | AUROC:0.8954\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.000382\n",
      "Train | 64/64 | Loss:0.3730 | MainLoss:0.2464 | Alpha:0.4420 | SPLoss:0.2735 | CLSLoss:0.0129 | top1:89.7651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1073 | MainLoss:0.1073 | SPLoss:0.3066 | CLSLoss:0.0130 | top1:96.4673 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.8354 | MainLoss:0.8354 | SPLoss:0.3066 | CLSLoss:0.0130 | top1:62.6026 | AUROC:0.8722\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.000382\n",
      "Train | 64/64 | Loss:0.3809 | MainLoss:0.2384 | Alpha:0.4424 | SPLoss:0.3089 | CLSLoss:0.0133 | top1:90.1333 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2037 | MainLoss:0.2037 | SPLoss:0.3657 | CLSLoss:0.0131 | top1:91.1059 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.5937 | MainLoss:0.5937 | SPLoss:0.3657 | CLSLoss:0.0131 | top1:70.8718 | AUROC:0.9095\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.000381\n",
      "Train | 64/64 | Loss:0.3910 | MainLoss:0.2401 | Alpha:0.4401 | SPLoss:0.3299 | CLSLoss:0.0133 | top1:89.9683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1895 | MainLoss:0.1895 | SPLoss:0.2976 | CLSLoss:0.0134 | top1:92.3084 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.6609 | MainLoss:0.6609 | SPLoss:0.2976 | CLSLoss:0.0134 | top1:70.0641 | AUROC:0.8914\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.000381\n",
      "Train | 64/64 | Loss:0.3796 | MainLoss:0.2359 | Alpha:0.4448 | SPLoss:0.3099 | CLSLoss:0.0134 | top1:89.9810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1230 | MainLoss:0.1230 | SPLoss:0.3041 | CLSLoss:0.0134 | top1:95.8318 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7641 | MainLoss:0.7641 | SPLoss:0.3041 | CLSLoss:0.0134 | top1:65.2949 | AUROC:0.8811\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.000381\n",
      "Train | 64/64 | Loss:0.3730 | MainLoss:0.2278 | Alpha:0.4458 | SPLoss:0.3121 | CLSLoss:0.0135 | top1:90.8444 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1036 | MainLoss:0.1036 | SPLoss:0.3677 | CLSLoss:0.0133 | top1:96.6791 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.7653 | MainLoss:0.7653 | SPLoss:0.3677 | CLSLoss:0.0133 | top1:65.7821 | AUROC:0.8644\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.000381\n",
      "Train | 64/64 | Loss:0.3884 | MainLoss:0.2315 | Alpha:0.4454 | SPLoss:0.3397 | CLSLoss:0.0131 | top1:90.4635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0890 | MainLoss:0.0890 | SPLoss:0.3855 | CLSLoss:0.0127 | top1:97.3209 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:0.7963 | MainLoss:0.7963 | SPLoss:0.3855 | CLSLoss:0.0127 | top1:64.7179 | AUROC:0.8762\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.000380\n",
      "Train | 64/64 | Loss:0.3637 | MainLoss:0.2307 | Alpha:0.4419 | SPLoss:0.2886 | CLSLoss:0.0128 | top1:90.5778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1223 | MainLoss:0.1223 | SPLoss:0.3407 | CLSLoss:0.0127 | top1:96.2710 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.7226 | MainLoss:0.7226 | SPLoss:0.3407 | CLSLoss:0.0127 | top1:65.4872 | AUROC:0.8821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [155 | 1000] LR: 0.000380\n",
      "Train | 64/64 | Loss:0.3687 | MainLoss:0.2250 | Alpha:0.4459 | SPLoss:0.3093 | CLSLoss:0.0130 | top1:90.5270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2382 | MainLoss:0.2382 | SPLoss:0.3149 | CLSLoss:0.0131 | top1:89.9938 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.5519 | MainLoss:0.5519 | SPLoss:0.3149 | CLSLoss:0.0131 | top1:74.2179 | AUROC:0.8906\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.000380\n",
      "Train | 64/64 | Loss:0.3607 | MainLoss:0.2136 | Alpha:0.4445 | SPLoss:0.3175 | CLSLoss:0.0134 | top1:91.4032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1596 | MainLoss:0.1596 | SPLoss:0.3446 | CLSLoss:0.0137 | top1:94.1526 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.7060 | MainLoss:0.7060 | SPLoss:0.3446 | CLSLoss:0.0137 | top1:68.0000 | AUROC:0.8875\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.000380\n",
      "Train | 64/64 | Loss:0.3685 | MainLoss:0.2188 | Alpha:0.4486 | SPLoss:0.3205 | CLSLoss:0.0133 | top1:91.0857 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1330 | MainLoss:0.1330 | SPLoss:0.3595 | CLSLoss:0.0133 | top1:95.4517 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.8407 | MainLoss:0.8407 | SPLoss:0.3595 | CLSLoss:0.0133 | top1:65.5128 | AUROC:0.8549\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.000379\n",
      "Train | 64/64 | Loss:0.3730 | MainLoss:0.2167 | Alpha:0.4482 | SPLoss:0.3355 | CLSLoss:0.0130 | top1:90.8064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2246 | MainLoss:0.2246 | SPLoss:0.3245 | CLSLoss:0.0133 | top1:91.8255 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.7623 | MainLoss:0.7623 | SPLoss:0.3245 | CLSLoss:0.0133 | top1:69.2051 | AUROC:0.8409\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.000379\n",
      "Train | 64/64 | Loss:0.3726 | MainLoss:0.2098 | Alpha:0.4502 | SPLoss:0.3491 | CLSLoss:0.0126 | top1:91.6825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1306 | MainLoss:0.1306 | SPLoss:0.3751 | CLSLoss:0.0124 | top1:95.3084 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7195 | MainLoss:0.7195 | SPLoss:0.3751 | CLSLoss:0.0124 | top1:68.2051 | AUROC:0.8795\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.000379\n",
      "Train | 64/64 | Loss:0.3625 | MainLoss:0.2131 | Alpha:0.4505 | SPLoss:0.3189 | CLSLoss:0.0125 | top1:91.5429 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2029 | MainLoss:0.2029 | SPLoss:0.3843 | CLSLoss:0.0129 | top1:92.2430 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.6272 | MainLoss:0.6272 | SPLoss:0.3843 | CLSLoss:0.0129 | top1:72.0128 | AUROC:0.8971\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.000378\n",
      "Train | 64/64 | Loss:0.3602 | MainLoss:0.2045 | Alpha:0.4521 | SPLoss:0.3311 | CLSLoss:0.0130 | top1:91.9492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1181 | MainLoss:0.1181 | SPLoss:0.3943 | CLSLoss:0.0132 | top1:96.1090 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.7753 | MainLoss:0.7753 | SPLoss:0.3943 | CLSLoss:0.0132 | top1:66.0385 | AUROC:0.8783\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.000378\n",
      "Train | 64/64 | Loss:0.3511 | MainLoss:0.2071 | Alpha:0.4510 | SPLoss:0.3058 | CLSLoss:0.0133 | top1:91.3397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1957 | MainLoss:0.1957 | SPLoss:0.3535 | CLSLoss:0.0133 | top1:92.0249 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.6717 | MainLoss:0.6717 | SPLoss:0.3535 | CLSLoss:0.0133 | top1:71.0128 | AUROC:0.8775\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.000378\n",
      "Train | 64/64 | Loss:0.3493 | MainLoss:0.1990 | Alpha:0.4536 | SPLoss:0.3180 | CLSLoss:0.0133 | top1:91.9365 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2391 | MainLoss:0.2391 | SPLoss:0.3745 | CLSLoss:0.0135 | top1:90.0000 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.5402 | MainLoss:0.5402 | SPLoss:0.3745 | CLSLoss:0.0135 | top1:76.1282 | AUROC:0.8944\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.000378\n",
      "Train | 64/64 | Loss:0.3515 | MainLoss:0.1989 | Alpha:0.4489 | SPLoss:0.3264 | CLSLoss:0.0140 | top1:91.8730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.3287 | MainLoss:0.3287 | SPLoss:0.3846 | CLSLoss:0.0139 | top1:86.7882 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.5136 | MainLoss:0.5136 | SPLoss:0.3846 | CLSLoss:0.0139 | top1:77.9231 | AUROC:0.8911\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.000377\n",
      "Train | 64/64 | Loss:0.3629 | MainLoss:0.1975 | Alpha:0.4387 | SPLoss:0.3639 | CLSLoss:0.0135 | top1:92.0635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1513 | MainLoss:0.1513 | SPLoss:0.4029 | CLSLoss:0.0136 | top1:94.1215 | AUROC:0.9990\n",
      "Test | 63/64 | Loss:0.7586 | MainLoss:0.7586 | SPLoss:0.4029 | CLSLoss:0.0136 | top1:68.8333 | AUROC:0.8595\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.000377\n",
      "Train | 64/64 | Loss:0.3517 | MainLoss:0.1868 | Alpha:0.4560 | SPLoss:0.3483 | CLSLoss:0.0137 | top1:92.5460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1332 | MainLoss:0.1332 | SPLoss:0.3004 | CLSLoss:0.0140 | top1:94.9097 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.7919 | MainLoss:0.7919 | SPLoss:0.3004 | CLSLoss:0.0140 | top1:69.5641 | AUROC:0.8570\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.000377\n",
      "Train | 64/64 | Loss:0.3437 | MainLoss:0.1906 | Alpha:0.4561 | SPLoss:0.3218 | CLSLoss:0.0137 | top1:92.4191 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1409 | MainLoss:0.1409 | SPLoss:0.4306 | CLSLoss:0.0131 | top1:94.5265 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.7919 | MainLoss:0.7919 | SPLoss:0.4306 | CLSLoss:0.0131 | top1:68.7564 | AUROC:0.8448\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.000376\n",
      "Train | 64/64 | Loss:0.3634 | MainLoss:0.1984 | Alpha:0.4555 | SPLoss:0.3496 | CLSLoss:0.0126 | top1:92.0889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1125 | MainLoss:0.1125 | SPLoss:0.4127 | CLSLoss:0.0125 | top1:96.2461 | AUROC:0.9991\n",
      "Test | 63/64 | Loss:0.7790 | MainLoss:0.7790 | SPLoss:0.4127 | CLSLoss:0.0125 | top1:66.2949 | AUROC:0.8752\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.000376\n",
      "Train | 64/64 | Loss:0.3469 | MainLoss:0.1924 | Alpha:0.4521 | SPLoss:0.3291 | CLSLoss:0.0128 | top1:92.4825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1505 | MainLoss:0.1505 | SPLoss:0.3556 | CLSLoss:0.0130 | top1:94.1932 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7092 | MainLoss:0.7092 | SPLoss:0.3556 | CLSLoss:0.0130 | top1:70.0769 | AUROC:0.8740\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.000376\n",
      "Train | 64/64 | Loss:0.3326 | MainLoss:0.1839 | Alpha:0.4590 | SPLoss:0.3107 | CLSLoss:0.0129 | top1:92.7873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2021 | MainLoss:0.2021 | SPLoss:0.3366 | CLSLoss:0.0130 | top1:91.9813 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.6223 | MainLoss:0.6223 | SPLoss:0.3366 | CLSLoss:0.0130 | top1:74.5897 | AUROC:0.8754\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.000376\n",
      "Train | 64/64 | Loss:0.3448 | MainLoss:0.1782 | Alpha:0.4569 | SPLoss:0.3514 | CLSLoss:0.0133 | top1:92.9651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1931 | MainLoss:0.1931 | SPLoss:0.3880 | CLSLoss:0.0134 | top1:91.7477 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.6265 | MainLoss:0.6265 | SPLoss:0.3880 | CLSLoss:0.0134 | top1:74.4231 | AUROC:0.8849\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.000375\n",
      "Train | 64/64 | Loss:0.3330 | MainLoss:0.1810 | Alpha:0.4590 | SPLoss:0.3182 | CLSLoss:0.0132 | top1:92.7238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1864 | MainLoss:0.1864 | SPLoss:0.3840 | CLSLoss:0.0135 | top1:92.8287 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.6593 | MainLoss:0.6593 | SPLoss:0.3840 | CLSLoss:0.0135 | top1:73.5385 | AUROC:0.8888\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.000375\n",
      "Train | 64/64 | Loss:0.3453 | MainLoss:0.1704 | Alpha:0.4629 | SPLoss:0.3646 | CLSLoss:0.0134 | top1:93.5111 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1909 | MainLoss:0.1909 | SPLoss:0.3476 | CLSLoss:0.0131 | top1:92.5670 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.6821 | MainLoss:0.6821 | SPLoss:0.3476 | CLSLoss:0.0131 | top1:73.8077 | AUROC:0.8730\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.000375\n",
      "Train | 64/64 | Loss:0.3220 | MainLoss:0.1638 | Alpha:0.4629 | SPLoss:0.3281 | CLSLoss:0.0131 | top1:93.4603 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1146 | MainLoss:0.1146 | SPLoss:0.3742 | CLSLoss:0.0132 | top1:95.6729 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.8914 | MainLoss:0.8914 | SPLoss:0.3742 | CLSLoss:0.0132 | top1:68.3333 | AUROC:0.8447\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.000374\n",
      "Train | 64/64 | Loss:0.3493 | MainLoss:0.1733 | Alpha:0.4629 | SPLoss:0.3672 | CLSLoss:0.0129 | top1:93.0921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1959 | MainLoss:0.1959 | SPLoss:0.4239 | CLSLoss:0.0129 | top1:92.2804 | AUROC:0.9989\n",
      "Test | 63/64 | Loss:0.7009 | MainLoss:0.7009 | SPLoss:0.4239 | CLSLoss:0.0129 | top1:72.5256 | AUROC:0.8674\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.000374\n",
      "Train | 64/64 | Loss:0.3280 | MainLoss:0.1630 | Alpha:0.4599 | SPLoss:0.3457 | CLSLoss:0.0130 | top1:93.6889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1444 | MainLoss:0.1444 | SPLoss:0.4303 | CLSLoss:0.0133 | top1:94.3022 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7387 | MainLoss:0.7387 | SPLoss:0.4303 | CLSLoss:0.0133 | top1:71.1154 | AUROC:0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [177 | 1000] LR: 0.000374\n",
      "Train | 64/64 | Loss:0.3336 | MainLoss:0.1605 | Alpha:0.4628 | SPLoss:0.3609 | CLSLoss:0.0134 | top1:93.9048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1469 | MainLoss:0.1469 | SPLoss:0.3886 | CLSLoss:0.0135 | top1:94.3770 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.7940 | MainLoss:0.7940 | SPLoss:0.3886 | CLSLoss:0.0135 | top1:71.2564 | AUROC:0.8561\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.000373\n",
      "Train | 64/64 | Loss:0.3227 | MainLoss:0.1564 | Alpha:0.4647 | SPLoss:0.3445 | CLSLoss:0.0133 | top1:93.8286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1501 | MainLoss:0.1501 | SPLoss:0.4337 | CLSLoss:0.0130 | top1:93.9844 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.7983 | MainLoss:0.7983 | SPLoss:0.4337 | CLSLoss:0.0130 | top1:69.4615 | AUROC:0.8452\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.000373\n",
      "Train | 64/64 | Loss:0.3001 | MainLoss:0.1469 | Alpha:0.4651 | SPLoss:0.3158 | CLSLoss:0.0137 | top1:94.2095 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1140 | MainLoss:0.1140 | SPLoss:0.3096 | CLSLoss:0.0139 | top1:95.5732 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9619 | MainLoss:0.9619 | SPLoss:0.3096 | CLSLoss:0.0139 | top1:67.6026 | AUROC:0.8414\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.000373\n",
      "Train | 64/64 | Loss:0.3248 | MainLoss:0.1600 | Alpha:0.4632 | SPLoss:0.3421 | CLSLoss:0.0132 | top1:93.5238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1585 | MainLoss:0.1585 | SPLoss:0.3822 | CLSLoss:0.0127 | top1:93.4019 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.6738 | MainLoss:0.6738 | SPLoss:0.3822 | CLSLoss:0.0127 | top1:73.0128 | AUROC:0.8645\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.000372\n",
      "Train | 64/64 | Loss:0.3137 | MainLoss:0.1640 | Alpha:0.4625 | SPLoss:0.3105 | CLSLoss:0.0130 | top1:93.7143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1492 | MainLoss:0.1492 | SPLoss:0.3630 | CLSLoss:0.0129 | top1:94.0218 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7399 | MainLoss:0.7399 | SPLoss:0.3630 | CLSLoss:0.0129 | top1:72.2949 | AUROC:0.8603\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.000372\n",
      "Train | 64/64 | Loss:0.3335 | MainLoss:0.1496 | Alpha:0.4674 | SPLoss:0.3802 | CLSLoss:0.0134 | top1:93.9683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0918 | MainLoss:0.0918 | SPLoss:0.4690 | CLSLoss:0.0131 | top1:96.5234 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.8760 | MainLoss:0.8760 | SPLoss:0.4690 | CLSLoss:0.0131 | top1:67.8590 | AUROC:0.8460\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.000372\n",
      "Train | 64/64 | Loss:0.2971 | MainLoss:0.1492 | Alpha:0.4623 | SPLoss:0.3067 | CLSLoss:0.0131 | top1:94.2349 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1383 | MainLoss:0.1383 | SPLoss:0.3359 | CLSLoss:0.0131 | top1:94.3925 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.7605 | MainLoss:0.7605 | SPLoss:0.3359 | CLSLoss:0.0131 | top1:70.8974 | AUROC:0.8479\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.000372\n",
      "Train | 64/64 | Loss:0.2965 | MainLoss:0.1378 | Alpha:0.4677 | SPLoss:0.3255 | CLSLoss:0.0138 | top1:94.6159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.3331 | MainLoss:0.3331 | SPLoss:0.4589 | CLSLoss:0.0140 | top1:85.6822 | AUROC:0.9989\n",
      "Test | 63/64 | Loss:0.5732 | MainLoss:0.5732 | SPLoss:0.4589 | CLSLoss:0.0140 | top1:78.0128 | AUROC:0.8712\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.000371\n",
      "Train | 64/64 | Loss:0.3039 | MainLoss:0.1458 | Alpha:0.4512 | SPLoss:0.3361 | CLSLoss:0.0140 | top1:94.1968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1112 | MainLoss:0.1112 | SPLoss:0.3420 | CLSLoss:0.0140 | top1:95.6386 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9179 | MainLoss:0.9179 | SPLoss:0.3420 | CLSLoss:0.0140 | top1:68.3974 | AUROC:0.8373\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.000371\n",
      "Train | 64/64 | Loss:0.3098 | MainLoss:0.1522 | Alpha:0.4667 | SPLoss:0.3245 | CLSLoss:0.0136 | top1:94.1968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1382 | MainLoss:0.1382 | SPLoss:0.3615 | CLSLoss:0.0127 | top1:94.6231 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.8038 | MainLoss:0.8038 | SPLoss:0.3615 | CLSLoss:0.0127 | top1:70.5256 | AUROC:0.8434\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.000371\n",
      "Train | 64/64 | Loss:0.3395 | MainLoss:0.1439 | Alpha:0.4673 | SPLoss:0.4058 | CLSLoss:0.0128 | top1:94.4508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2199 | MainLoss:0.2199 | SPLoss:0.3524 | CLSLoss:0.0133 | top1:92.1122 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7176 | MainLoss:0.7176 | SPLoss:0.3524 | CLSLoss:0.0133 | top1:74.3846 | AUROC:0.8555\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.000370\n",
      "Train | 64/64 | Loss:0.3239 | MainLoss:0.1428 | Alpha:0.4674 | SPLoss:0.3743 | CLSLoss:0.0127 | top1:94.3746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1049 | MainLoss:0.1049 | SPLoss:0.3978 | CLSLoss:0.0127 | top1:96.0125 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:0.8445 | MainLoss:0.8445 | SPLoss:0.3978 | CLSLoss:0.0127 | top1:69.6923 | AUROC:0.8536\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.000370\n",
      "Train | 64/64 | Loss:0.2934 | MainLoss:0.1352 | Alpha:0.4681 | SPLoss:0.3252 | CLSLoss:0.0129 | top1:94.9079 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:0.3190 | CLSLoss:0.0127 | top1:91.9159 | AUROC:0.9990\n",
      "Test | 63/64 | Loss:0.7266 | MainLoss:0.7266 | SPLoss:0.3190 | CLSLoss:0.0127 | top1:73.9487 | AUROC:0.8645\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.000370\n",
      "Train | 64/64 | Loss:0.3250 | MainLoss:0.1459 | Alpha:0.4646 | SPLoss:0.3731 | CLSLoss:0.0124 | top1:94.4889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2419 | MainLoss:0.2419 | SPLoss:0.3902 | CLSLoss:0.0124 | top1:90.6791 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.6915 | MainLoss:0.6915 | SPLoss:0.3902 | CLSLoss:0.0124 | top1:74.2821 | AUROC:0.8676\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.000369\n",
      "Train | 64/64 | Loss:0.2768 | MainLoss:0.1244 | Alpha:0.4671 | SPLoss:0.3132 | CLSLoss:0.0129 | top1:95.2889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1610 | MainLoss:0.1610 | SPLoss:0.3747 | CLSLoss:0.0129 | top1:94.2804 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.8081 | MainLoss:0.8081 | SPLoss:0.3747 | CLSLoss:0.0129 | top1:70.4615 | AUROC:0.8645\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.000369\n",
      "Train | 64/64 | Loss:0.2688 | MainLoss:0.1218 | Alpha:0.4697 | SPLoss:0.3001 | CLSLoss:0.0129 | top1:95.5175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1101 | MainLoss:0.1101 | SPLoss:0.3631 | CLSLoss:0.0132 | top1:96.0156 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.9356 | MainLoss:0.9356 | SPLoss:0.3631 | CLSLoss:0.0132 | top1:68.7179 | AUROC:0.8592\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.000369\n",
      "Train | 64/64 | Loss:0.2895 | MainLoss:0.1270 | Alpha:0.4699 | SPLoss:0.3329 | CLSLoss:0.0132 | top1:95.0984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0660 | MainLoss:0.0660 | SPLoss:0.3680 | CLSLoss:0.0130 | top1:97.8131 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.0612 | MainLoss:1.0612 | SPLoss:0.3680 | CLSLoss:0.0130 | top1:65.5128 | AUROC:0.8454\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.000368\n",
      "Train | 64/64 | Loss:0.2976 | MainLoss:0.1297 | Alpha:0.4594 | SPLoss:0.3535 | CLSLoss:0.0127 | top1:95.2000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0777 | MainLoss:0.0777 | SPLoss:0.5715 | CLSLoss:0.0124 | top1:97.4081 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.9278 | MainLoss:0.9278 | SPLoss:0.5715 | CLSLoss:0.0124 | top1:64.4103 | AUROC:0.8677\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.000368\n",
      "Train | 64/64 | Loss:0.2619 | MainLoss:0.1243 | Alpha:0.4639 | SPLoss:0.2837 | CLSLoss:0.0129 | top1:95.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1707 | MainLoss:0.1707 | SPLoss:0.4341 | CLSLoss:0.0136 | top1:93.6667 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.7113 | MainLoss:0.7113 | SPLoss:0.4341 | CLSLoss:0.0136 | top1:73.5000 | AUROC:0.8559\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.000368\n",
      "Train | 64/64 | Loss:0.2657 | MainLoss:0.1159 | Alpha:0.4696 | SPLoss:0.3053 | CLSLoss:0.0134 | top1:95.7206 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1607 | MainLoss:0.1607 | SPLoss:0.3980 | CLSLoss:0.0136 | top1:93.6604 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.9821 | MainLoss:0.9821 | SPLoss:0.3980 | CLSLoss:0.0136 | top1:67.6923 | AUROC:0.8099\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.000367\n",
      "Train | 64/64 | Loss:0.2954 | MainLoss:0.1340 | Alpha:0.4693 | SPLoss:0.3310 | CLSLoss:0.0130 | top1:94.8571 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0427 | MainLoss:0.0427 | SPLoss:0.4761 | CLSLoss:0.0123 | top1:98.6480 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2788 | MainLoss:1.2788 | SPLoss:0.4761 | CLSLoss:0.0123 | top1:58.8077 | AUROC:0.8273\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.000367\n",
      "Train | 64/64 | Loss:0.2977 | MainLoss:0.1366 | Alpha:0.4519 | SPLoss:0.3440 | CLSLoss:0.0123 | top1:94.8064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1346 | MainLoss:0.1346 | SPLoss:0.3761 | CLSLoss:0.0131 | top1:94.6324 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.8556 | MainLoss:0.8556 | SPLoss:0.3761 | CLSLoss:0.0131 | top1:71.1282 | AUROC:0.8375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [199 | 1000] LR: 0.000366\n",
      "Train | 64/64 | Loss:0.2735 | MainLoss:0.1109 | Alpha:0.4734 | SPLoss:0.3301 | CLSLoss:0.0134 | top1:96.0889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1380 | MainLoss:0.1380 | SPLoss:0.4271 | CLSLoss:0.0132 | top1:94.8910 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.8861 | MainLoss:0.8861 | SPLoss:0.4271 | CLSLoss:0.0132 | top1:69.8077 | AUROC:0.8392\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.000366\n",
      "Train | 64/64 | Loss:0.3133 | MainLoss:0.1186 | Alpha:0.4759 | SPLoss:0.3958 | CLSLoss:0.0129 | top1:95.4794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1123 | MainLoss:0.1123 | SPLoss:0.4181 | CLSLoss:0.0125 | top1:96.0592 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9358 | MainLoss:0.9358 | SPLoss:0.4181 | CLSLoss:0.0125 | top1:67.8846 | AUROC:0.8468\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.000366\n",
      "Train | 64/64 | Loss:0.2821 | MainLoss:0.1172 | Alpha:0.4728 | SPLoss:0.3360 | CLSLoss:0.0125 | top1:95.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0983 | MainLoss:0.0983 | SPLoss:0.3421 | CLSLoss:0.0126 | top1:96.5826 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.0672 | MainLoss:1.0672 | SPLoss:0.3421 | CLSLoss:0.0126 | top1:67.3718 | AUROC:0.8278\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.000365\n",
      "Train | 64/64 | Loss:0.2615 | MainLoss:0.1029 | Alpha:0.4726 | SPLoss:0.3228 | CLSLoss:0.0126 | top1:96.2159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1926 | MainLoss:0.1926 | SPLoss:0.3737 | CLSLoss:0.0127 | top1:93.2056 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.9086 | MainLoss:0.9086 | SPLoss:0.3737 | CLSLoss:0.0127 | top1:72.5769 | AUROC:0.8453\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.000365\n",
      "Train | 64/64 | Loss:0.2997 | MainLoss:0.1113 | Alpha:0.4764 | SPLoss:0.3833 | CLSLoss:0.0126 | top1:95.9873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1382 | MainLoss:0.1382 | SPLoss:0.3906 | CLSLoss:0.0119 | top1:95.2150 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.9027 | MainLoss:0.9027 | SPLoss:0.3906 | CLSLoss:0.0119 | top1:70.0385 | AUROC:0.8263\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.000365\n",
      "Train | 64/64 | Loss:0.2926 | MainLoss:0.1211 | Alpha:0.4730 | SPLoss:0.3511 | CLSLoss:0.0117 | top1:95.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0940 | MainLoss:0.0940 | SPLoss:0.4668 | CLSLoss:0.0114 | top1:96.1963 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.9793 | MainLoss:0.9793 | SPLoss:0.4668 | CLSLoss:0.0114 | top1:67.7436 | AUROC:0.8346\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.000364\n",
      "Train | 64/64 | Loss:0.2567 | MainLoss:0.1081 | Alpha:0.4714 | SPLoss:0.3032 | CLSLoss:0.0120 | top1:95.8984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1874 | MainLoss:0.1874 | SPLoss:0.3597 | CLSLoss:0.0124 | top1:93.0685 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.9404 | MainLoss:0.9404 | SPLoss:0.3597 | CLSLoss:0.0124 | top1:70.6154 | AUROC:0.8339\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.000364\n",
      "Train | 64/64 | Loss:0.2508 | MainLoss:0.1037 | Alpha:0.4772 | SPLoss:0.2961 | CLSLoss:0.0121 | top1:96.3175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1637 | MainLoss:0.1637 | SPLoss:0.3505 | CLSLoss:0.0124 | top1:94.2741 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.9353 | MainLoss:0.9353 | SPLoss:0.3505 | CLSLoss:0.0124 | top1:70.6410 | AUROC:0.8359\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.000364\n",
      "Train | 64/64 | Loss:0.2925 | MainLoss:0.1019 | Alpha:0.4790 | SPLoss:0.3847 | CLSLoss:0.0125 | top1:96.0127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1256 | MainLoss:0.1256 | SPLoss:0.4343 | CLSLoss:0.0127 | top1:95.2710 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:1.0001 | MainLoss:1.0001 | SPLoss:0.4343 | CLSLoss:0.0127 | top1:68.5513 | AUROC:0.8295\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.000363\n",
      "Train | 64/64 | Loss:0.2760 | MainLoss:0.1078 | Alpha:0.4766 | SPLoss:0.3401 | CLSLoss:0.0126 | top1:95.7841 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1872 | MainLoss:0.1872 | SPLoss:0.3495 | CLSLoss:0.0127 | top1:93.1215 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.8283 | MainLoss:0.8283 | SPLoss:0.3495 | CLSLoss:0.0127 | top1:72.2179 | AUROC:0.8308\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.000363\n",
      "Train | 64/64 | Loss:0.2749 | MainLoss:0.1102 | Alpha:0.4754 | SPLoss:0.3337 | CLSLoss:0.0127 | top1:95.8730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1122 | MainLoss:0.1122 | SPLoss:0.3728 | CLSLoss:0.0124 | top1:95.6947 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:1.0004 | MainLoss:1.0004 | SPLoss:0.3728 | CLSLoss:0.0124 | top1:68.9103 | AUROC:0.8143\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.000363\n",
      "Train | 64/64 | Loss:0.2417 | MainLoss:0.0894 | Alpha:0.4775 | SPLoss:0.3061 | CLSLoss:0.0126 | top1:96.8635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2412 | MainLoss:0.2412 | SPLoss:0.3047 | CLSLoss:0.0129 | top1:92.0249 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.7828 | MainLoss:0.7828 | SPLoss:0.3047 | CLSLoss:0.0129 | top1:74.3333 | AUROC:0.8412\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.000362\n",
      "Train | 64/64 | Loss:0.2493 | MainLoss:0.0984 | Alpha:0.4770 | SPLoss:0.3039 | CLSLoss:0.0124 | top1:96.2667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1276 | MainLoss:0.1276 | SPLoss:0.2638 | CLSLoss:0.0127 | top1:95.8287 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1755 | MainLoss:1.1755 | SPLoss:0.2638 | CLSLoss:0.0127 | top1:67.7051 | AUROC:0.7991\n",
      "\n",
      "Epoch: [212 | 1000] LR: 0.000362\n",
      "Train | 64/64 | Loss:0.2884 | MainLoss:0.1078 | Alpha:0.4765 | SPLoss:0.3673 | CLSLoss:0.0118 | top1:95.8730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2349 | MainLoss:0.2349 | SPLoss:0.4661 | CLSLoss:0.0113 | top1:91.4642 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.7806 | MainLoss:0.7806 | SPLoss:0.4661 | CLSLoss:0.0113 | top1:73.0513 | AUROC:0.8352\n",
      "\n",
      "Epoch: [213 | 1000] LR: 0.000361\n",
      "Train | 64/64 | Loss:0.2531 | MainLoss:0.0973 | Alpha:0.4721 | SPLoss:0.3183 | CLSLoss:0.0116 | top1:96.3302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1785 | MainLoss:0.1785 | SPLoss:0.3224 | CLSLoss:0.0119 | top1:93.5545 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.8788 | MainLoss:0.8788 | SPLoss:0.3224 | CLSLoss:0.0119 | top1:72.3718 | AUROC:0.8419\n",
      "\n",
      "Epoch: [214 | 1000] LR: 0.000361\n",
      "Train | 64/64 | Loss:0.2658 | MainLoss:0.1031 | Alpha:0.4760 | SPLoss:0.3299 | CLSLoss:0.0117 | top1:96.4063 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1433 | MainLoss:0.1433 | SPLoss:0.3588 | CLSLoss:0.0115 | top1:94.9564 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.0320 | MainLoss:1.0320 | SPLoss:0.3588 | CLSLoss:0.0115 | top1:69.0128 | AUROC:0.7992\n",
      "\n",
      "Epoch: [215 | 1000] LR: 0.000361\n",
      "Train | 64/64 | Loss:0.2585 | MainLoss:0.0954 | Alpha:0.4771 | SPLoss:0.3303 | CLSLoss:0.0114 | top1:96.3683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1746 | MainLoss:0.1746 | SPLoss:0.2964 | CLSLoss:0.0117 | top1:94.0000 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9111 | MainLoss:0.9111 | SPLoss:0.2964 | CLSLoss:0.0117 | top1:72.5641 | AUROC:0.8289\n",
      "\n",
      "Epoch: [216 | 1000] LR: 0.000360\n",
      "Train | 64/64 | Loss:0.2530 | MainLoss:0.0924 | Alpha:0.4803 | SPLoss:0.3230 | CLSLoss:0.0116 | top1:96.4825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1738 | MainLoss:0.1738 | SPLoss:0.3191 | CLSLoss:0.0117 | top1:93.6791 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.8829 | MainLoss:0.8829 | SPLoss:0.3191 | CLSLoss:0.0117 | top1:72.4615 | AUROC:0.8260\n",
      "\n",
      "Epoch: [217 | 1000] LR: 0.000360\n",
      "Train | 64/64 | Loss:0.2456 | MainLoss:0.0938 | Alpha:0.4795 | SPLoss:0.3045 | CLSLoss:0.0118 | top1:96.3810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1237 | MainLoss:0.1237 | SPLoss:0.3914 | CLSLoss:0.0123 | top1:95.4798 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:0.9567 | MainLoss:0.9567 | SPLoss:0.3914 | CLSLoss:0.0123 | top1:70.7564 | AUROC:0.8356\n",
      "\n",
      "Epoch: [218 | 1000] LR: 0.000360\n",
      "Train | 64/64 | Loss:0.2396 | MainLoss:0.0844 | Alpha:0.4803 | SPLoss:0.3110 | CLSLoss:0.0122 | top1:96.7873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1411 | MainLoss:0.1411 | SPLoss:0.3987 | CLSLoss:0.0124 | top1:94.6231 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.9924 | MainLoss:0.9924 | SPLoss:0.3987 | CLSLoss:0.0124 | top1:69.8205 | AUROC:0.8446\n",
      "\n",
      "Epoch: [219 | 1000] LR: 0.000359\n",
      "Train | 64/64 | Loss:0.2640 | MainLoss:0.0951 | Alpha:0.4797 | SPLoss:0.3397 | CLSLoss:0.0121 | top1:96.3810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0927 | MainLoss:0.0927 | SPLoss:0.4220 | CLSLoss:0.0119 | top1:96.4953 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:1.1352 | MainLoss:1.1352 | SPLoss:0.4220 | CLSLoss:0.0119 | top1:66.2821 | AUROC:0.8269\n",
      "\n",
      "Epoch: [220 | 1000] LR: 0.000359\n",
      "Train | 64/64 | Loss:0.2687 | MainLoss:0.0998 | Alpha:0.4759 | SPLoss:0.3434 | CLSLoss:0.0119 | top1:96.3048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0909 | MainLoss:0.0909 | SPLoss:0.3249 | CLSLoss:0.0122 | top1:96.6168 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:1.0537 | MainLoss:1.0537 | SPLoss:0.3249 | CLSLoss:0.0122 | top1:67.9103 | AUROC:0.8335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [221 | 1000] LR: 0.000358\n",
      "Train | 64/64 | Loss:0.2355 | MainLoss:0.0882 | Alpha:0.4791 | SPLoss:0.2953 | CLSLoss:0.0121 | top1:96.8762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1385 | MainLoss:0.1385 | SPLoss:0.3218 | CLSLoss:0.0121 | top1:95.2181 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:1.0940 | MainLoss:1.0940 | SPLoss:0.3218 | CLSLoss:0.0121 | top1:68.8205 | AUROC:0.8120\n",
      "\n",
      "Epoch: [222 | 1000] LR: 0.000358\n",
      "Train | 64/64 | Loss:0.2556 | MainLoss:0.0850 | Alpha:0.4827 | SPLoss:0.3420 | CLSLoss:0.0119 | top1:97.0032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0615 | MainLoss:0.0615 | SPLoss:0.3901 | CLSLoss:0.0118 | top1:97.8162 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2159 | MainLoss:1.2159 | SPLoss:0.3901 | CLSLoss:0.0118 | top1:64.8590 | AUROC:0.8081\n",
      "\n",
      "Epoch: [223 | 1000] LR: 0.000358\n",
      "Train | 64/64 | Loss:0.2298 | MainLoss:0.0856 | Alpha:0.4679 | SPLoss:0.2969 | CLSLoss:0.0116 | top1:96.7492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1846 | MainLoss:0.1846 | SPLoss:0.2644 | CLSLoss:0.0120 | top1:93.8629 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.9264 | MainLoss:0.9264 | SPLoss:0.2644 | CLSLoss:0.0120 | top1:72.0385 | AUROC:0.8212\n",
      "\n",
      "Epoch: [224 | 1000] LR: 0.000357\n",
      "Train | 64/64 | Loss:0.2348 | MainLoss:0.0802 | Alpha:0.4828 | SPLoss:0.3079 | CLSLoss:0.0119 | top1:97.2190 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1171 | MainLoss:0.1171 | SPLoss:0.3247 | CLSLoss:0.0121 | top1:96.1713 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:1.0950 | MainLoss:1.0950 | SPLoss:0.3247 | CLSLoss:0.0121 | top1:68.1667 | AUROC:0.8115\n",
      "\n",
      "Epoch: [225 | 1000] LR: 0.000357\n",
      "Train | 64/64 | Loss:0.2398 | MainLoss:0.0837 | Alpha:0.4816 | SPLoss:0.3127 | CLSLoss:0.0116 | top1:97.0794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1099 | MainLoss:0.1099 | SPLoss:0.2757 | CLSLoss:0.0114 | top1:96.2835 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0472 | MainLoss:1.0472 | SPLoss:0.2757 | CLSLoss:0.0114 | top1:68.8462 | AUROC:0.8089\n",
      "\n",
      "Epoch: [226 | 1000] LR: 0.000356\n",
      "Train | 64/64 | Loss:0.2313 | MainLoss:0.0819 | Alpha:0.4832 | SPLoss:0.2978 | CLSLoss:0.0116 | top1:97.1302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0825 | MainLoss:0.0825 | SPLoss:0.3459 | CLSLoss:0.0114 | top1:97.2928 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1833 | MainLoss:1.1833 | SPLoss:0.3459 | CLSLoss:0.0114 | top1:65.7051 | AUROC:0.7998\n",
      "\n",
      "Epoch: [227 | 1000] LR: 0.000356\n",
      "Train | 64/64 | Loss:0.2770 | MainLoss:0.0989 | Alpha:0.4801 | SPLoss:0.3596 | CLSLoss:0.0112 | top1:96.2413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1327 | MainLoss:0.1327 | SPLoss:0.3835 | CLSLoss:0.0109 | top1:95.2991 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9560 | MainLoss:0.9560 | SPLoss:0.3835 | CLSLoss:0.0109 | top1:69.7949 | AUROC:0.8239\n",
      "\n",
      "Epoch: [228 | 1000] LR: 0.000356\n",
      "Train | 64/64 | Loss:0.2298 | MainLoss:0.0841 | Alpha:0.4820 | SPLoss:0.2910 | CLSLoss:0.0111 | top1:96.9397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1727 | MainLoss:0.1727 | SPLoss:0.3013 | CLSLoss:0.0113 | top1:94.0935 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9425 | MainLoss:0.9425 | SPLoss:0.3013 | CLSLoss:0.0113 | top1:70.3077 | AUROC:0.8220\n",
      "\n",
      "Epoch: [229 | 1000] LR: 0.000355\n",
      "Train | 64/64 | Loss:0.2282 | MainLoss:0.0798 | Alpha:0.4840 | SPLoss:0.2948 | CLSLoss:0.0115 | top1:97.1429 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2743 | MainLoss:0.2743 | SPLoss:0.3598 | CLSLoss:0.0115 | top1:90.6293 | AUROC:0.9989\n",
      "Test | 63/64 | Loss:0.8278 | MainLoss:0.8278 | SPLoss:0.3598 | CLSLoss:0.0115 | top1:73.5000 | AUROC:0.8298\n",
      "\n",
      "Epoch: [230 | 1000] LR: 0.000355\n",
      "Train | 64/64 | Loss:0.2213 | MainLoss:0.0762 | Alpha:0.4762 | SPLoss:0.2926 | CLSLoss:0.0116 | top1:97.2318 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1355 | MainLoss:0.1355 | SPLoss:0.3831 | CLSLoss:0.0119 | top1:95.4081 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:1.0843 | MainLoss:1.0843 | SPLoss:0.3831 | CLSLoss:0.0119 | top1:68.2308 | AUROC:0.7996\n",
      "\n",
      "Epoch: [231 | 1000] LR: 0.000355\n",
      "Train | 64/64 | Loss:0.2401 | MainLoss:0.0803 | Alpha:0.4819 | SPLoss:0.3198 | CLSLoss:0.0118 | top1:96.9524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1034 | MainLoss:0.1034 | SPLoss:0.3071 | CLSLoss:0.0118 | top1:96.4922 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1741 | MainLoss:1.1741 | SPLoss:0.3071 | CLSLoss:0.0118 | top1:66.4872 | AUROC:0.7920\n",
      "\n",
      "Epoch: [232 | 1000] LR: 0.000354\n",
      "Train | 64/64 | Loss:0.2405 | MainLoss:0.0789 | Alpha:0.4800 | SPLoss:0.3249 | CLSLoss:0.0116 | top1:97.2190 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1793 | MainLoss:0.1793 | SPLoss:0.3557 | CLSLoss:0.0116 | top1:94.0499 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9326 | MainLoss:0.9326 | SPLoss:0.3557 | CLSLoss:0.0116 | top1:71.0256 | AUROC:0.8064\n",
      "\n",
      "Epoch: [233 | 1000] LR: 0.000354\n",
      "Train | 64/64 | Loss:0.2294 | MainLoss:0.0739 | Alpha:0.4838 | SPLoss:0.3097 | CLSLoss:0.0118 | top1:97.4222 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1968 | MainLoss:0.1968 | SPLoss:0.4046 | CLSLoss:0.0122 | top1:93.3209 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.9425 | MainLoss:0.9425 | SPLoss:0.4046 | CLSLoss:0.0122 | top1:71.5256 | AUROC:0.8220\n",
      "\n",
      "Epoch: [234 | 1000] LR: 0.000353\n",
      "Train | 64/64 | Loss:0.2658 | MainLoss:0.0850 | Alpha:0.4831 | SPLoss:0.3625 | CLSLoss:0.0117 | top1:96.8508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1661 | MainLoss:0.1661 | SPLoss:0.4089 | CLSLoss:0.0110 | top1:94.0093 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.8917 | MainLoss:0.8917 | SPLoss:0.4089 | CLSLoss:0.0110 | top1:71.1795 | AUROC:0.8251\n",
      "\n",
      "Epoch: [235 | 1000] LR: 0.000353\n",
      "Train | 64/64 | Loss:0.2126 | MainLoss:0.0729 | Alpha:0.4828 | SPLoss:0.2778 | CLSLoss:0.0114 | top1:97.3333 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1103 | MainLoss:0.1103 | SPLoss:0.2962 | CLSLoss:0.0114 | top1:96.2274 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1436 | MainLoss:1.1436 | SPLoss:0.2962 | CLSLoss:0.0114 | top1:66.6026 | AUROC:0.8129\n",
      "\n",
      "Epoch: [236 | 1000] LR: 0.000352\n",
      "Train | 64/64 | Loss:0.2347 | MainLoss:0.0739 | Alpha:0.4846 | SPLoss:0.3206 | CLSLoss:0.0114 | top1:97.4603 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1515 | MainLoss:0.1515 | SPLoss:0.2913 | CLSLoss:0.0117 | top1:94.9657 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0620 | MainLoss:1.0620 | SPLoss:0.2913 | CLSLoss:0.0117 | top1:69.6923 | AUROC:0.8068\n",
      "\n",
      "Epoch: [237 | 1000] LR: 0.000352\n",
      "Train | 64/64 | Loss:0.2343 | MainLoss:0.0727 | Alpha:0.4836 | SPLoss:0.3225 | CLSLoss:0.0116 | top1:97.4603 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1348 | MainLoss:0.1348 | SPLoss:0.3337 | CLSLoss:0.0113 | top1:95.2617 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1493 | MainLoss:1.1493 | SPLoss:0.3337 | CLSLoss:0.0113 | top1:68.0128 | AUROC:0.7986\n",
      "\n",
      "Epoch: [238 | 1000] LR: 0.000352\n",
      "Train | 64/64 | Loss:0.2347 | MainLoss:0.0731 | Alpha:0.4854 | SPLoss:0.3218 | CLSLoss:0.0114 | top1:97.5365 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1484 | MainLoss:0.1484 | SPLoss:0.3404 | CLSLoss:0.0111 | top1:94.8474 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0600 | MainLoss:1.0600 | SPLoss:0.3404 | CLSLoss:0.0111 | top1:68.6795 | AUROC:0.8083\n",
      "\n",
      "Epoch: [239 | 1000] LR: 0.000351\n",
      "Train | 64/64 | Loss:0.2227 | MainLoss:0.0737 | Alpha:0.4852 | SPLoss:0.2961 | CLSLoss:0.0108 | top1:97.4476 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1114 | MainLoss:0.1114 | SPLoss:0.2675 | CLSLoss:0.0108 | top1:96.4268 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1436 | MainLoss:1.1436 | SPLoss:0.2675 | CLSLoss:0.0108 | top1:66.6923 | AUROC:0.8170\n",
      "\n",
      "Epoch: [240 | 1000] LR: 0.000351\n",
      "Train | 64/64 | Loss:0.2325 | MainLoss:0.0691 | Alpha:0.4840 | SPLoss:0.3265 | CLSLoss:0.0112 | top1:97.4349 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2103 | MainLoss:0.2103 | SPLoss:0.3212 | CLSLoss:0.0114 | top1:93.1994 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.9419 | MainLoss:0.9419 | SPLoss:0.3212 | CLSLoss:0.0114 | top1:71.9615 | AUROC:0.8151\n",
      "\n",
      "Epoch: [241 | 1000] LR: 0.000350\n",
      "Train | 64/64 | Loss:0.2166 | MainLoss:0.0739 | Alpha:0.4838 | SPLoss:0.2837 | CLSLoss:0.0111 | top1:97.2190 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1075 | MainLoss:0.1075 | SPLoss:0.2952 | CLSLoss:0.0111 | top1:96.1090 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1398 | MainLoss:1.1398 | SPLoss:0.2952 | CLSLoss:0.0111 | top1:67.1410 | AUROC:0.7849\n",
      "\n",
      "Epoch: [242 | 1000] LR: 0.000350\n",
      "Train | 64/64 | Loss:0.2151 | MainLoss:0.0716 | Alpha:0.4846 | SPLoss:0.2846 | CLSLoss:0.0111 | top1:97.3587 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0840 | MainLoss:0.0840 | SPLoss:0.3624 | CLSLoss:0.0110 | top1:97.0935 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1245 | MainLoss:1.1245 | SPLoss:0.3624 | CLSLoss:0.0110 | top1:66.6026 | AUROC:0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [243 | 1000] LR: 0.000350\n",
      "Train | 64/64 | Loss:0.2193 | MainLoss:0.0721 | Alpha:0.4805 | SPLoss:0.2953 | CLSLoss:0.0112 | top1:97.4095 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.2450 | MainLoss:0.2450 | SPLoss:0.3332 | CLSLoss:0.0112 | top1:91.4673 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.8389 | MainLoss:0.8389 | SPLoss:0.3332 | CLSLoss:0.0112 | top1:72.9103 | AUROC:0.8204\n",
      "\n",
      "Epoch: [244 | 1000] LR: 0.000349\n",
      "Train | 64/64 | Loss:0.2282 | MainLoss:0.0793 | Alpha:0.4775 | SPLoss:0.3004 | CLSLoss:0.0110 | top1:97.1810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1858 | MainLoss:0.1858 | SPLoss:0.3420 | CLSLoss:0.0107 | top1:93.0561 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:0.8859 | MainLoss:0.8859 | SPLoss:0.3420 | CLSLoss:0.0107 | top1:71.2308 | AUROC:0.8192\n",
      "\n",
      "Epoch: [245 | 1000] LR: 0.000349\n",
      "Train | 64/64 | Loss:0.2109 | MainLoss:0.0736 | Alpha:0.4813 | SPLoss:0.2744 | CLSLoss:0.0110 | top1:97.1937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1509 | MainLoss:0.1509 | SPLoss:0.3214 | CLSLoss:0.0113 | top1:95.0374 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:0.9849 | MainLoss:0.9849 | SPLoss:0.3214 | CLSLoss:0.0113 | top1:70.1154 | AUROC:0.8226\n",
      "\n",
      "Epoch: [246 | 1000] LR: 0.000348\n",
      "Train | 64/64 | Loss:0.2159 | MainLoss:0.0644 | Alpha:0.4864 | SPLoss:0.3001 | CLSLoss:0.0115 | top1:97.7905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1881 | MainLoss:0.1881 | SPLoss:0.3754 | CLSLoss:0.0115 | top1:93.3520 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.9589 | MainLoss:0.9589 | SPLoss:0.3754 | CLSLoss:0.0115 | top1:70.3718 | AUROC:0.8265\n",
      "\n",
      "Epoch: [247 | 1000] LR: 0.000348\n",
      "Train | 64/64 | Loss:0.1978 | MainLoss:0.0610 | Alpha:0.4849 | SPLoss:0.2706 | CLSLoss:0.0117 | top1:97.8413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1641 | MainLoss:0.1641 | SPLoss:0.2035 | CLSLoss:0.0119 | top1:94.7446 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:1.1079 | MainLoss:1.1079 | SPLoss:0.2035 | CLSLoss:0.0119 | top1:69.8205 | AUROC:0.8136\n",
      "\n",
      "Epoch: [248 | 1000] LR: 0.000348\n",
      "Train | 64/64 | Loss:0.2268 | MainLoss:0.0710 | Alpha:0.4850 | SPLoss:0.3096 | CLSLoss:0.0113 | top1:97.4984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1908 | MainLoss:0.1908 | SPLoss:0.3062 | CLSLoss:0.0112 | top1:93.7632 | AUROC:0.9994\n",
      "Test | 63/64 | Loss:0.9087 | MainLoss:0.9087 | SPLoss:0.3062 | CLSLoss:0.0112 | top1:72.1154 | AUROC:0.8300\n",
      "\n",
      "Epoch: [249 | 1000] LR: 0.000347\n",
      "Train | 64/64 | Loss:0.2060 | MainLoss:0.0667 | Alpha:0.4843 | SPLoss:0.2766 | CLSLoss:0.0109 | top1:97.5365 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1626 | MainLoss:0.1626 | SPLoss:0.3365 | CLSLoss:0.0110 | top1:94.4673 | AUROC:0.9993\n",
      "Test | 63/64 | Loss:1.0692 | MainLoss:1.0692 | SPLoss:0.3365 | CLSLoss:0.0110 | top1:69.0256 | AUROC:0.7999\n",
      "\n",
      "Epoch: [250 | 1000] LR: 0.000347\n",
      "Train | 64/64 | Loss:0.1926 | MainLoss:0.0589 | Alpha:0.4856 | SPLoss:0.2637 | CLSLoss:0.0111 | top1:97.9683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1774 | MainLoss:0.1774 | SPLoss:0.3044 | CLSLoss:0.0114 | top1:94.2617 | AUROC:0.9992\n",
      "Test | 63/64 | Loss:0.9366 | MainLoss:0.9366 | SPLoss:0.3044 | CLSLoss:0.0114 | top1:71.1923 | AUROC:0.8241\n",
      "\n",
      "Epoch: [251 | 1000] LR: 0.000346\n",
      "Train | 64/64 | Loss:0.1968 | MainLoss:0.0632 | Alpha:0.4824 | SPLoss:0.2656 | CLSLoss:0.0113 | top1:97.7524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1301 | MainLoss:0.1301 | SPLoss:0.3368 | CLSLoss:0.0111 | top1:95.6511 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0286 | MainLoss:1.0286 | SPLoss:0.3368 | CLSLoss:0.0111 | top1:69.6026 | AUROC:0.8104\n",
      "\n",
      "Epoch: [252 | 1000] LR: 0.000035\n",
      "Train | 64/64 | Loss:0.0651 | MainLoss:0.0511 | Alpha:0.4863 | SPLoss:0.0177 | CLSLoss:0.0111 | top1:98.2476 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1199 | MainLoss:0.1199 | SPLoss:0.0272 | CLSLoss:0.0112 | top1:95.9564 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0736 | MainLoss:1.0736 | SPLoss:0.0272 | CLSLoss:0.0112 | top1:68.9359 | AUROC:0.8020\n",
      "\n",
      "Epoch: [253 | 1000] LR: 0.000035\n",
      "Train | 64/64 | Loss:0.0544 | MainLoss:0.0423 | Alpha:0.4894 | SPLoss:0.0136 | CLSLoss:0.0112 | top1:98.5651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1194 | MainLoss:0.1194 | SPLoss:0.0267 | CLSLoss:0.0113 | top1:95.9595 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1131 | MainLoss:1.1131 | SPLoss:0.0267 | CLSLoss:0.0113 | top1:68.5000 | AUROC:0.7977\n",
      "\n",
      "Epoch: [254 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0675 | MainLoss:0.0539 | Alpha:0.4865 | SPLoss:0.0165 | CLSLoss:0.0113 | top1:98.1460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1584 | MainLoss:0.1584 | SPLoss:0.0255 | CLSLoss:0.0112 | top1:94.6822 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0106 | MainLoss:1.0106 | SPLoss:0.0255 | CLSLoss:0.0112 | top1:70.5256 | AUROC:0.8023\n",
      "\n",
      "Epoch: [255 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0543 | MainLoss:0.0411 | Alpha:0.4893 | SPLoss:0.0157 | CLSLoss:0.0113 | top1:98.6286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1172 | MainLoss:0.1172 | SPLoss:0.0287 | CLSLoss:0.0114 | top1:96.0093 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1432 | MainLoss:1.1432 | SPLoss:0.0287 | CLSLoss:0.0114 | top1:68.0000 | AUROC:0.7890\n",
      "\n",
      "Epoch: [256 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0501 | MainLoss:0.0384 | Alpha:0.4906 | SPLoss:0.0125 | CLSLoss:0.0114 | top1:98.7048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1458 | MainLoss:0.1458 | SPLoss:0.0185 | CLSLoss:0.0115 | top1:95.2461 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1025 | MainLoss:1.1025 | SPLoss:0.0185 | CLSLoss:0.0115 | top1:69.3974 | AUROC:0.7927\n",
      "\n",
      "Epoch: [257 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0562 | MainLoss:0.0420 | Alpha:0.4902 | SPLoss:0.0175 | CLSLoss:0.0115 | top1:98.4889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1311 | MainLoss:0.1311 | SPLoss:0.0259 | CLSLoss:0.0115 | top1:95.7290 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1476 | MainLoss:1.1476 | SPLoss:0.0259 | CLSLoss:0.0115 | top1:68.4744 | AUROC:0.7908\n",
      "\n",
      "Epoch: [258 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0585 | MainLoss:0.0447 | Alpha:0.4894 | SPLoss:0.0168 | CLSLoss:0.0115 | top1:98.4381 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1306 | MainLoss:0.1306 | SPLoss:0.0210 | CLSLoss:0.0115 | top1:95.8131 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1616 | MainLoss:1.1616 | SPLoss:0.0210 | CLSLoss:0.0115 | top1:68.2436 | AUROC:0.7844\n",
      "\n",
      "Epoch: [259 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0518 | MainLoss:0.0384 | Alpha:0.4905 | SPLoss:0.0157 | CLSLoss:0.0115 | top1:98.6921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1055 | MainLoss:0.1055 | SPLoss:0.0278 | CLSLoss:0.0115 | top1:96.5794 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2078 | MainLoss:1.2078 | SPLoss:0.0278 | CLSLoss:0.0115 | top1:67.4487 | AUROC:0.7851\n",
      "\n",
      "Epoch: [260 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0546 | MainLoss:0.0412 | Alpha:0.4893 | SPLoss:0.0159 | CLSLoss:0.0115 | top1:98.4889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1181 | MainLoss:0.1181 | SPLoss:0.0187 | CLSLoss:0.0115 | top1:96.2305 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1730 | MainLoss:1.1730 | SPLoss:0.0187 | CLSLoss:0.0115 | top1:68.3077 | AUROC:0.7863\n",
      "\n",
      "Epoch: [261 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0508 | MainLoss:0.0373 | Alpha:0.4905 | SPLoss:0.0160 | CLSLoss:0.0115 | top1:98.6921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1377 | MainLoss:0.1377 | SPLoss:0.0258 | CLSLoss:0.0115 | top1:95.5265 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1139 | MainLoss:1.1139 | SPLoss:0.0258 | CLSLoss:0.0115 | top1:69.7821 | AUROC:0.7920\n",
      "\n",
      "Epoch: [262 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0573 | MainLoss:0.0433 | Alpha:0.4895 | SPLoss:0.0169 | CLSLoss:0.0115 | top1:98.4889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1385 | MainLoss:0.1385 | SPLoss:0.0238 | CLSLoss:0.0114 | top1:95.4019 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0893 | MainLoss:1.0893 | SPLoss:0.0238 | CLSLoss:0.0114 | top1:69.7821 | AUROC:0.7934\n",
      "\n",
      "Epoch: [263 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0510 | MainLoss:0.0376 | Alpha:0.4908 | SPLoss:0.0157 | CLSLoss:0.0115 | top1:98.8064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1277 | MainLoss:0.1277 | SPLoss:0.0241 | CLSLoss:0.0115 | top1:95.8847 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1402 | MainLoss:1.1402 | SPLoss:0.0241 | CLSLoss:0.0115 | top1:68.9744 | AUROC:0.7883\n",
      "\n",
      "Epoch: [264 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0535 | MainLoss:0.0407 | Alpha:0.4901 | SPLoss:0.0147 | CLSLoss:0.0115 | top1:98.7302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1462 | MainLoss:0.1462 | SPLoss:0.0218 | CLSLoss:0.0114 | top1:95.3520 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0927 | MainLoss:1.0927 | SPLoss:0.0218 | CLSLoss:0.0114 | top1:69.7821 | AUROC:0.7904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [265 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0548 | MainLoss:0.0415 | Alpha:0.4897 | SPLoss:0.0157 | CLSLoss:0.0114 | top1:98.6159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1141 | MainLoss:0.1141 | SPLoss:0.0205 | CLSLoss:0.0113 | top1:96.2274 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1682 | MainLoss:1.1682 | SPLoss:0.0205 | CLSLoss:0.0113 | top1:68.0000 | AUROC:0.7855\n",
      "\n",
      "Epoch: [266 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0576 | MainLoss:0.0439 | Alpha:0.4893 | SPLoss:0.0169 | CLSLoss:0.0113 | top1:98.4635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1148 | MainLoss:0.1148 | SPLoss:0.0210 | CLSLoss:0.0113 | top1:96.0312 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1570 | MainLoss:1.1570 | SPLoss:0.0210 | CLSLoss:0.0113 | top1:67.9487 | AUROC:0.7866\n",
      "\n",
      "Epoch: [267 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0439 | MainLoss:0.0316 | Alpha:0.4922 | SPLoss:0.0136 | CLSLoss:0.0113 | top1:98.9968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1236 | MainLoss:0.1236 | SPLoss:0.0155 | CLSLoss:0.0114 | top1:96.0156 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1805 | MainLoss:1.1805 | SPLoss:0.0155 | CLSLoss:0.0114 | top1:68.1154 | AUROC:0.7813\n",
      "\n",
      "Epoch: [268 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0610 | MainLoss:0.0469 | Alpha:0.4885 | SPLoss:0.0175 | CLSLoss:0.0113 | top1:98.5397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1359 | MainLoss:0.1359 | SPLoss:0.0276 | CLSLoss:0.0112 | top1:95.6822 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1464 | MainLoss:1.1464 | SPLoss:0.0276 | CLSLoss:0.0112 | top1:68.3974 | AUROC:0.7822\n",
      "\n",
      "Epoch: [269 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0512 | MainLoss:0.0369 | Alpha:0.4910 | SPLoss:0.0178 | CLSLoss:0.0112 | top1:98.8698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1326 | MainLoss:0.1326 | SPLoss:0.0202 | CLSLoss:0.0112 | top1:95.7944 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1695 | MainLoss:1.1695 | SPLoss:0.0202 | CLSLoss:0.0112 | top1:68.3462 | AUROC:0.7756\n",
      "\n",
      "Epoch: [270 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0491 | MainLoss:0.0358 | Alpha:0.4913 | SPLoss:0.0158 | CLSLoss:0.0113 | top1:98.7810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1392 | MainLoss:0.1392 | SPLoss:0.0264 | CLSLoss:0.0113 | top1:95.6199 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1507 | MainLoss:1.1507 | SPLoss:0.0264 | CLSLoss:0.0113 | top1:68.5641 | AUROC:0.7821\n",
      "\n",
      "Epoch: [271 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0522 | MainLoss:0.0387 | Alpha:0.4907 | SPLoss:0.0164 | CLSLoss:0.0112 | top1:98.7302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1175 | MainLoss:0.1175 | SPLoss:0.0216 | CLSLoss:0.0112 | top1:96.1838 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1687 | MainLoss:1.1687 | SPLoss:0.0216 | CLSLoss:0.0112 | top1:68.0256 | AUROC:0.7795\n",
      "\n",
      "Epoch: [272 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0512 | MainLoss:0.0372 | Alpha:0.4911 | SPLoss:0.0172 | CLSLoss:0.0112 | top1:98.7175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1032 | MainLoss:0.1032 | SPLoss:0.0270 | CLSLoss:0.0112 | top1:96.5888 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2470 | MainLoss:1.2470 | SPLoss:0.0270 | CLSLoss:0.0112 | top1:66.7436 | AUROC:0.7708\n",
      "\n",
      "Epoch: [273 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0512 | MainLoss:0.0375 | Alpha:0.4904 | SPLoss:0.0168 | CLSLoss:0.0112 | top1:98.7810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1131 | MainLoss:0.1131 | SPLoss:0.0268 | CLSLoss:0.0111 | top1:96.3053 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1693 | MainLoss:1.1693 | SPLoss:0.0268 | CLSLoss:0.0111 | top1:67.7179 | AUROC:0.7795\n",
      "\n",
      "Epoch: [274 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0451 | MainLoss:0.0327 | Alpha:0.4917 | SPLoss:0.0140 | CLSLoss:0.0112 | top1:98.8825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1501 | MainLoss:0.1501 | SPLoss:0.0258 | CLSLoss:0.0112 | top1:95.3146 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1387 | MainLoss:1.1387 | SPLoss:0.0258 | CLSLoss:0.0112 | top1:68.5256 | AUROC:0.7761\n",
      "\n",
      "Epoch: [275 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0624 | MainLoss:0.0475 | Alpha:0.4881 | SPLoss:0.0195 | CLSLoss:0.0111 | top1:98.4762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1344 | MainLoss:0.1344 | SPLoss:0.0326 | CLSLoss:0.0110 | top1:95.7009 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.1358 | MainLoss:1.1358 | SPLoss:0.0326 | CLSLoss:0.0110 | top1:68.1154 | AUROC:0.7792\n",
      "\n",
      "Epoch: [276 | 1000] LR: 0.000034\n",
      "Train | 64/64 | Loss:0.0436 | MainLoss:0.0320 | Alpha:0.4921 | SPLoss:0.0127 | CLSLoss:0.0110 | top1:99.0984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1275 | MainLoss:0.1275 | SPLoss:0.0157 | CLSLoss:0.0111 | top1:95.9190 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1825 | MainLoss:1.1825 | SPLoss:0.0157 | CLSLoss:0.0111 | top1:67.5513 | AUROC:0.7734\n",
      "\n",
      "Epoch: [277 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0428 | MainLoss:0.0310 | Alpha:0.4924 | SPLoss:0.0129 | CLSLoss:0.0111 | top1:98.8825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1073 | MainLoss:0.1073 | SPLoss:0.0179 | CLSLoss:0.0112 | top1:96.5327 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2707 | MainLoss:1.2707 | SPLoss:0.0179 | CLSLoss:0.0112 | top1:66.4744 | AUROC:0.7681\n",
      "\n",
      "Epoch: [278 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0510 | MainLoss:0.0358 | Alpha:0.4910 | SPLoss:0.0199 | CLSLoss:0.0111 | top1:98.8064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0988 | MainLoss:0.0988 | SPLoss:0.0258 | CLSLoss:0.0112 | top1:96.8536 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2944 | MainLoss:1.2944 | SPLoss:0.0258 | CLSLoss:0.0112 | top1:66.0128 | AUROC:0.7651\n",
      "\n",
      "Epoch: [279 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0609 | MainLoss:0.0459 | Alpha:0.4881 | SPLoss:0.0197 | CLSLoss:0.0110 | top1:98.5397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1620 | MainLoss:0.1620 | SPLoss:0.0305 | CLSLoss:0.0110 | top1:94.9065 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.0996 | MainLoss:1.0996 | SPLoss:0.0305 | CLSLoss:0.0110 | top1:69.3333 | AUROC:0.7787\n",
      "\n",
      "Epoch: [280 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0536 | MainLoss:0.0382 | Alpha:0.4906 | SPLoss:0.0203 | CLSLoss:0.0110 | top1:98.6794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1293 | MainLoss:0.1293 | SPLoss:0.0243 | CLSLoss:0.0110 | top1:95.8505 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1747 | MainLoss:1.1747 | SPLoss:0.0243 | CLSLoss:0.0110 | top1:68.0769 | AUROC:0.7698\n",
      "\n",
      "Epoch: [281 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0487 | MainLoss:0.0358 | Alpha:0.4914 | SPLoss:0.0153 | CLSLoss:0.0110 | top1:98.9714 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1209 | MainLoss:0.1209 | SPLoss:0.0300 | CLSLoss:0.0110 | top1:96.0405 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1845 | MainLoss:1.1845 | SPLoss:0.0300 | CLSLoss:0.0110 | top1:67.9615 | AUROC:0.7727\n",
      "\n",
      "Epoch: [282 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0511 | MainLoss:0.0377 | Alpha:0.4909 | SPLoss:0.0164 | CLSLoss:0.0110 | top1:98.7683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0845 | MainLoss:0.0845 | SPLoss:0.0274 | CLSLoss:0.0109 | top1:97.1651 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3020 | MainLoss:1.3020 | SPLoss:0.0274 | CLSLoss:0.0109 | top1:65.3205 | AUROC:0.7656\n",
      "\n",
      "Epoch: [283 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0409 | MainLoss:0.0285 | Alpha:0.4920 | SPLoss:0.0142 | CLSLoss:0.0110 | top1:99.0476 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1092 | MainLoss:0.1092 | SPLoss:0.0191 | CLSLoss:0.0111 | top1:96.5047 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2702 | MainLoss:1.2702 | SPLoss:0.0191 | CLSLoss:0.0111 | top1:66.8590 | AUROC:0.7645\n",
      "\n",
      "Epoch: [284 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0494 | MainLoss:0.0338 | Alpha:0.4918 | SPLoss:0.0207 | CLSLoss:0.0111 | top1:98.8571 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1279 | MainLoss:0.1279 | SPLoss:0.0213 | CLSLoss:0.0111 | top1:96.0187 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2281 | MainLoss:1.2281 | SPLoss:0.0213 | CLSLoss:0.0111 | top1:67.9103 | AUROC:0.7680\n",
      "\n",
      "Epoch: [285 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0418 | MainLoss:0.0289 | Alpha:0.4929 | SPLoss:0.0152 | CLSLoss:0.0111 | top1:99.0984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0998 | MainLoss:0.0998 | SPLoss:0.0266 | CLSLoss:0.0111 | top1:96.7695 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2874 | MainLoss:1.2874 | SPLoss:0.0266 | CLSLoss:0.0111 | top1:66.3077 | AUROC:0.7668\n",
      "\n",
      "Epoch: [286 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0519 | MainLoss:0.0370 | Alpha:0.4906 | SPLoss:0.0194 | CLSLoss:0.0110 | top1:98.8191 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1163 | MainLoss:0.1163 | SPLoss:0.0322 | CLSLoss:0.0110 | top1:96.1526 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1658 | MainLoss:1.1658 | SPLoss:0.0322 | CLSLoss:0.0110 | top1:68.6282 | AUROC:0.7807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [287 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0410 | MainLoss:0.0285 | Alpha:0.4929 | SPLoss:0.0143 | CLSLoss:0.0110 | top1:99.1238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1250 | MainLoss:0.1250 | SPLoss:0.0180 | CLSLoss:0.0111 | top1:96.0685 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2300 | MainLoss:1.2300 | SPLoss:0.0180 | CLSLoss:0.0111 | top1:67.9615 | AUROC:0.7726\n",
      "\n",
      "Epoch: [288 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0410 | MainLoss:0.0284 | Alpha:0.4933 | SPLoss:0.0145 | CLSLoss:0.0111 | top1:99.0984 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1291 | MainLoss:0.1291 | SPLoss:0.0207 | CLSLoss:0.0111 | top1:96.0280 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1809 | MainLoss:1.1809 | SPLoss:0.0207 | CLSLoss:0.0111 | top1:68.9103 | AUROC:0.7799\n",
      "\n",
      "Epoch: [289 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0495 | MainLoss:0.0335 | Alpha:0.4916 | SPLoss:0.0215 | CLSLoss:0.0111 | top1:98.8698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1368 | MainLoss:0.1368 | SPLoss:0.0271 | CLSLoss:0.0110 | top1:95.6168 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1800 | MainLoss:1.1800 | SPLoss:0.0271 | CLSLoss:0.0110 | top1:68.5769 | AUROC:0.7759\n",
      "\n",
      "Epoch: [290 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0470 | MainLoss:0.0330 | Alpha:0.4919 | SPLoss:0.0174 | CLSLoss:0.0109 | top1:98.8698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1306 | MainLoss:0.1306 | SPLoss:0.0220 | CLSLoss:0.0109 | top1:95.6916 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1772 | MainLoss:1.1772 | SPLoss:0.0220 | CLSLoss:0.0109 | top1:68.3846 | AUROC:0.7745\n",
      "\n",
      "Epoch: [291 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0413 | MainLoss:0.0283 | Alpha:0.4933 | SPLoss:0.0154 | CLSLoss:0.0110 | top1:99.0349 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1775 | MainLoss:0.1775 | SPLoss:0.0231 | CLSLoss:0.0110 | top1:94.4393 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1054 | MainLoss:1.1054 | SPLoss:0.0231 | CLSLoss:0.0110 | top1:70.2051 | AUROC:0.7791\n",
      "\n",
      "Epoch: [292 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0435 | MainLoss:0.0301 | Alpha:0.4919 | SPLoss:0.0163 | CLSLoss:0.0110 | top1:99.0730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1352 | MainLoss:0.1352 | SPLoss:0.0249 | CLSLoss:0.0111 | top1:95.6293 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1777 | MainLoss:1.1777 | SPLoss:0.0249 | CLSLoss:0.0111 | top1:68.9359 | AUROC:0.7728\n",
      "\n",
      "Epoch: [293 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0457 | MainLoss:0.0315 | Alpha:0.4924 | SPLoss:0.0178 | CLSLoss:0.0110 | top1:98.9460 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1106 | MainLoss:0.1106 | SPLoss:0.0204 | CLSLoss:0.0110 | top1:96.4922 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2783 | MainLoss:1.2783 | SPLoss:0.0204 | CLSLoss:0.0110 | top1:67.4615 | AUROC:0.7611\n",
      "\n",
      "Epoch: [294 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0406 | MainLoss:0.0278 | Alpha:0.4932 | SPLoss:0.0148 | CLSLoss:0.0111 | top1:99.1111 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1346 | MainLoss:0.1346 | SPLoss:0.0281 | CLSLoss:0.0111 | top1:95.7103 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1914 | MainLoss:1.1914 | SPLoss:0.0281 | CLSLoss:0.0111 | top1:68.5769 | AUROC:0.7741\n",
      "\n",
      "Epoch: [295 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0434 | MainLoss:0.0290 | Alpha:0.4929 | SPLoss:0.0181 | CLSLoss:0.0111 | top1:98.9333 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1171 | MainLoss:0.1171 | SPLoss:0.0222 | CLSLoss:0.0111 | top1:96.2835 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2724 | MainLoss:1.2724 | SPLoss:0.0222 | CLSLoss:0.0111 | top1:67.3077 | AUROC:0.7681\n",
      "\n",
      "Epoch: [296 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0471 | MainLoss:0.0333 | Alpha:0.4916 | SPLoss:0.0171 | CLSLoss:0.0110 | top1:99.0476 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1281 | MainLoss:0.1281 | SPLoss:0.0287 | CLSLoss:0.0109 | top1:95.9252 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2243 | MainLoss:1.2243 | SPLoss:0.0287 | CLSLoss:0.0109 | top1:67.5769 | AUROC:0.7689\n",
      "\n",
      "Epoch: [297 | 1000] LR: 0.000033\n",
      "Train | 64/64 | Loss:0.0420 | MainLoss:0.0285 | Alpha:0.4932 | SPLoss:0.0163 | CLSLoss:0.0109 | top1:99.1492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1264 | MainLoss:0.1264 | SPLoss:0.0242 | CLSLoss:0.0109 | top1:95.9564 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2145 | MainLoss:1.2145 | SPLoss:0.0242 | CLSLoss:0.0109 | top1:67.9359 | AUROC:0.7694\n",
      "\n",
      "Epoch: [298 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0429 | MainLoss:0.0297 | Alpha:0.4929 | SPLoss:0.0159 | CLSLoss:0.0110 | top1:99.0730 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1262 | MainLoss:0.1262 | SPLoss:0.0291 | CLSLoss:0.0109 | top1:96.0031 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.2389 | MainLoss:1.2389 | SPLoss:0.0291 | CLSLoss:0.0109 | top1:67.3590 | AUROC:0.7706\n",
      "\n",
      "Epoch: [299 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0364 | MainLoss:0.0241 | Alpha:0.4940 | SPLoss:0.0141 | CLSLoss:0.0109 | top1:99.3397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1171 | MainLoss:0.1171 | SPLoss:0.0162 | CLSLoss:0.0109 | top1:96.3271 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2711 | MainLoss:1.2711 | SPLoss:0.0162 | CLSLoss:0.0109 | top1:67.0641 | AUROC:0.7673\n",
      "\n",
      "Epoch: [300 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0341 | MainLoss:0.0215 | Alpha:0.4948 | SPLoss:0.0145 | CLSLoss:0.0110 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1274 | MainLoss:0.1274 | SPLoss:0.0165 | CLSLoss:0.0110 | top1:96.0312 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2695 | MainLoss:1.2695 | SPLoss:0.0165 | CLSLoss:0.0110 | top1:67.2949 | AUROC:0.7637\n",
      "\n",
      "Epoch: [301 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0387 | MainLoss:0.0238 | Alpha:0.4945 | SPLoss:0.0191 | CLSLoss:0.0110 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1223 | MainLoss:0.1223 | SPLoss:0.0228 | CLSLoss:0.0111 | top1:96.0872 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2506 | MainLoss:1.2506 | SPLoss:0.0228 | CLSLoss:0.0111 | top1:67.8205 | AUROC:0.7664\n",
      "\n",
      "Epoch: [302 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0373 | MainLoss:0.0254 | Alpha:0.4937 | SPLoss:0.0131 | CLSLoss:0.0111 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1219 | MainLoss:0.1219 | SPLoss:0.0187 | CLSLoss:0.0111 | top1:96.2586 | AUROC:0.9995\n",
      "Test | 63/64 | Loss:1.2880 | MainLoss:1.2880 | SPLoss:0.0187 | CLSLoss:0.0111 | top1:67.3462 | AUROC:0.7698\n",
      "\n",
      "Epoch: [303 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0493 | MainLoss:0.0335 | Alpha:0.4920 | SPLoss:0.0212 | CLSLoss:0.0110 | top1:98.9333 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1180 | MainLoss:0.1180 | SPLoss:0.0253 | CLSLoss:0.0109 | top1:96.2710 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2767 | MainLoss:1.2767 | SPLoss:0.0253 | CLSLoss:0.0109 | top1:66.9359 | AUROC:0.7665\n",
      "\n",
      "Epoch: [304 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0346 | MainLoss:0.0230 | Alpha:0.4942 | SPLoss:0.0126 | CLSLoss:0.0109 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1189 | MainLoss:0.1189 | SPLoss:0.0171 | CLSLoss:0.0109 | top1:96.3988 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2992 | MainLoss:1.2992 | SPLoss:0.0171 | CLSLoss:0.0109 | top1:67.0513 | AUROC:0.7624\n",
      "\n",
      "Epoch: [305 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0417 | MainLoss:0.0263 | Alpha:0.4935 | SPLoss:0.0204 | CLSLoss:0.0109 | top1:99.2127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1514 | MainLoss:0.1514 | SPLoss:0.0353 | CLSLoss:0.0109 | top1:95.3146 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1738 | MainLoss:1.1738 | SPLoss:0.0353 | CLSLoss:0.0109 | top1:68.9615 | AUROC:0.7803\n",
      "\n",
      "Epoch: [306 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0411 | MainLoss:0.0274 | Alpha:0.4930 | SPLoss:0.0168 | CLSLoss:0.0109 | top1:99.1873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1105 | MainLoss:0.1105 | SPLoss:0.0236 | CLSLoss:0.0109 | top1:96.5670 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.3191 | MainLoss:1.3191 | SPLoss:0.0236 | CLSLoss:0.0109 | top1:66.6538 | AUROC:0.7654\n",
      "\n",
      "Epoch: [307 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0432 | MainLoss:0.0279 | Alpha:0.4932 | SPLoss:0.0203 | CLSLoss:0.0108 | top1:99.0349 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1098 | MainLoss:0.1098 | SPLoss:0.0230 | CLSLoss:0.0108 | top1:96.5763 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2953 | MainLoss:1.2953 | SPLoss:0.0230 | CLSLoss:0.0108 | top1:66.6282 | AUROC:0.7680\n",
      "\n",
      "Epoch: [308 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0328 | MainLoss:0.0206 | Alpha:0.4950 | SPLoss:0.0137 | CLSLoss:0.0109 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1390 | MainLoss:0.1390 | SPLoss:0.0138 | CLSLoss:0.0109 | top1:95.8505 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2551 | MainLoss:1.2551 | SPLoss:0.0138 | CLSLoss:0.0109 | top1:67.7308 | AUROC:0.7678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [309 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0406 | MainLoss:0.0265 | Alpha:0.4937 | SPLoss:0.0176 | CLSLoss:0.0109 | top1:99.1873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1427 | MainLoss:0.1427 | SPLoss:0.0245 | CLSLoss:0.0109 | top1:95.7196 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2173 | MainLoss:1.2173 | SPLoss:0.0245 | CLSLoss:0.0109 | top1:68.3333 | AUROC:0.7686\n",
      "\n",
      "Epoch: [310 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0433 | MainLoss:0.0284 | Alpha:0.4930 | SPLoss:0.0193 | CLSLoss:0.0108 | top1:99.1873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1171 | MainLoss:0.1171 | SPLoss:0.0257 | CLSLoss:0.0107 | top1:96.3956 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2544 | MainLoss:1.2544 | SPLoss:0.0257 | CLSLoss:0.0107 | top1:67.2821 | AUROC:0.7708\n",
      "\n",
      "Epoch: [311 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0399 | MainLoss:0.0267 | Alpha:0.4935 | SPLoss:0.0160 | CLSLoss:0.0107 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1179 | MainLoss:0.1179 | SPLoss:0.0236 | CLSLoss:0.0107 | top1:96.4860 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2701 | MainLoss:1.2701 | SPLoss:0.0236 | CLSLoss:0.0107 | top1:67.2179 | AUROC:0.7638\n",
      "\n",
      "Epoch: [312 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0379 | MainLoss:0.0249 | Alpha:0.4940 | SPLoss:0.0155 | CLSLoss:0.0107 | top1:99.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1099 | MainLoss:0.1099 | SPLoss:0.0233 | CLSLoss:0.0107 | top1:96.6667 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3146 | MainLoss:1.3146 | SPLoss:0.0233 | CLSLoss:0.0107 | top1:66.2692 | AUROC:0.7580\n",
      "\n",
      "Epoch: [313 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0401 | MainLoss:0.0255 | Alpha:0.4938 | SPLoss:0.0188 | CLSLoss:0.0107 | top1:99.1746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1281 | MainLoss:0.1281 | SPLoss:0.0205 | CLSLoss:0.0107 | top1:96.1371 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2568 | MainLoss:1.2568 | SPLoss:0.0205 | CLSLoss:0.0107 | top1:67.3974 | AUROC:0.7642\n",
      "\n",
      "Epoch: [314 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0406 | MainLoss:0.0265 | Alpha:0.4936 | SPLoss:0.0180 | CLSLoss:0.0107 | top1:99.1746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1241 | MainLoss:0.1241 | SPLoss:0.0232 | CLSLoss:0.0106 | top1:96.2150 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2883 | MainLoss:1.2883 | SPLoss:0.0232 | CLSLoss:0.0106 | top1:66.8077 | AUROC:0.7606\n",
      "\n",
      "Epoch: [315 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0399 | MainLoss:0.0261 | Alpha:0.4935 | SPLoss:0.0174 | CLSLoss:0.0106 | top1:99.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1616 | MainLoss:0.1616 | SPLoss:0.0251 | CLSLoss:0.0106 | top1:95.1184 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1559 | MainLoss:1.1559 | SPLoss:0.0251 | CLSLoss:0.0106 | top1:69.2051 | AUROC:0.7753\n",
      "\n",
      "Epoch: [316 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0461 | MainLoss:0.0311 | Alpha:0.4923 | SPLoss:0.0197 | CLSLoss:0.0106 | top1:98.9841 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1495 | MainLoss:0.1495 | SPLoss:0.0263 | CLSLoss:0.0105 | top1:95.2555 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1670 | MainLoss:1.1670 | SPLoss:0.0263 | CLSLoss:0.0105 | top1:69.0385 | AUROC:0.7699\n",
      "\n",
      "Epoch: [317 | 1000] LR: 0.000032\n",
      "Train | 64/64 | Loss:0.0367 | MainLoss:0.0234 | Alpha:0.4942 | SPLoss:0.0164 | CLSLoss:0.0105 | top1:99.2635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1542 | MainLoss:0.1542 | SPLoss:0.0202 | CLSLoss:0.0106 | top1:95.1651 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1672 | MainLoss:1.1672 | SPLoss:0.0202 | CLSLoss:0.0106 | top1:68.8974 | AUROC:0.7696\n",
      "\n",
      "Epoch: [318 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0452 | MainLoss:0.0309 | Alpha:0.4919 | SPLoss:0.0183 | CLSLoss:0.0105 | top1:99.0603 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1258 | MainLoss:0.1258 | SPLoss:0.0298 | CLSLoss:0.0105 | top1:95.9844 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1757 | MainLoss:1.1757 | SPLoss:0.0298 | CLSLoss:0.0105 | top1:68.2949 | AUROC:0.7718\n",
      "\n",
      "Epoch: [319 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0370 | MainLoss:0.0239 | Alpha:0.4938 | SPLoss:0.0161 | CLSLoss:0.0105 | top1:99.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1091 | MainLoss:0.1091 | SPLoss:0.0247 | CLSLoss:0.0106 | top1:96.6075 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2823 | MainLoss:1.2823 | SPLoss:0.0247 | CLSLoss:0.0106 | top1:67.2179 | AUROC:0.7640\n",
      "\n",
      "Epoch: [320 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0512 | MainLoss:0.0344 | Alpha:0.4920 | SPLoss:0.0237 | CLSLoss:0.0106 | top1:98.7556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0781 | MainLoss:0.0781 | SPLoss:0.0347 | CLSLoss:0.0105 | top1:97.4455 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3146 | MainLoss:1.3146 | SPLoss:0.0347 | CLSLoss:0.0105 | top1:65.7308 | AUROC:0.7655\n",
      "\n",
      "Epoch: [321 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0461 | MainLoss:0.0332 | Alpha:0.4910 | SPLoss:0.0158 | CLSLoss:0.0105 | top1:98.9968 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1073 | MainLoss:0.1073 | SPLoss:0.0233 | CLSLoss:0.0104 | top1:96.4206 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1692 | MainLoss:1.1692 | SPLoss:0.0233 | CLSLoss:0.0104 | top1:67.4359 | AUROC:0.7705\n",
      "\n",
      "Epoch: [322 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0405 | MainLoss:0.0283 | Alpha:0.4929 | SPLoss:0.0143 | CLSLoss:0.0103 | top1:99.2254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1232 | MainLoss:0.1232 | SPLoss:0.0219 | CLSLoss:0.0104 | top1:95.9844 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1788 | MainLoss:1.1788 | SPLoss:0.0219 | CLSLoss:0.0104 | top1:68.2564 | AUROC:0.7693\n",
      "\n",
      "Epoch: [323 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0405 | MainLoss:0.0274 | Alpha:0.4933 | SPLoss:0.0161 | CLSLoss:0.0104 | top1:99.1492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1064 | MainLoss:0.1064 | SPLoss:0.0246 | CLSLoss:0.0104 | top1:96.5078 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2479 | MainLoss:1.2479 | SPLoss:0.0246 | CLSLoss:0.0104 | top1:66.8333 | AUROC:0.7651\n",
      "\n",
      "Epoch: [324 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0412 | MainLoss:0.0281 | Alpha:0.4932 | SPLoss:0.0162 | CLSLoss:0.0104 | top1:99.0857 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1202 | MainLoss:0.1202 | SPLoss:0.0208 | CLSLoss:0.0104 | top1:96.0966 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2368 | MainLoss:1.2368 | SPLoss:0.0208 | CLSLoss:0.0104 | top1:67.0513 | AUROC:0.7627\n",
      "\n",
      "Epoch: [325 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0404 | MainLoss:0.0266 | Alpha:0.4935 | SPLoss:0.0174 | CLSLoss:0.0104 | top1:99.1619 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1156 | MainLoss:0.1156 | SPLoss:0.0165 | CLSLoss:0.0104 | top1:96.4143 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3005 | MainLoss:1.3005 | SPLoss:0.0165 | CLSLoss:0.0104 | top1:66.2949 | AUROC:0.7577\n",
      "\n",
      "Epoch: [326 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0440 | MainLoss:0.0299 | Alpha:0.4927 | SPLoss:0.0183 | CLSLoss:0.0104 | top1:99.1238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0868 | MainLoss:0.0868 | SPLoss:0.0286 | CLSLoss:0.0104 | top1:97.2368 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3378 | MainLoss:1.3378 | SPLoss:0.0286 | CLSLoss:0.0104 | top1:65.0000 | AUROC:0.7579\n",
      "\n",
      "Epoch: [327 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0397 | MainLoss:0.0264 | Alpha:0.4927 | SPLoss:0.0168 | CLSLoss:0.0104 | top1:99.1492 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1626 | MainLoss:0.1626 | SPLoss:0.0257 | CLSLoss:0.0104 | top1:94.7726 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1178 | MainLoss:1.1178 | SPLoss:0.0257 | CLSLoss:0.0104 | top1:68.8974 | AUROC:0.7752\n",
      "\n",
      "Epoch: [328 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0394 | MainLoss:0.0272 | Alpha:0.4924 | SPLoss:0.0144 | CLSLoss:0.0104 | top1:99.1746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1078 | MainLoss:0.1078 | SPLoss:0.0211 | CLSLoss:0.0104 | top1:96.5296 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2581 | MainLoss:1.2581 | SPLoss:0.0211 | CLSLoss:0.0104 | top1:66.6538 | AUROC:0.7650\n",
      "\n",
      "Epoch: [329 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0341 | MainLoss:0.0221 | Alpha:0.4947 | SPLoss:0.0138 | CLSLoss:0.0105 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1156 | MainLoss:0.1156 | SPLoss:0.0143 | CLSLoss:0.0105 | top1:96.3551 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2957 | MainLoss:1.2957 | SPLoss:0.0143 | CLSLoss:0.0105 | top1:66.4487 | AUROC:0.7545\n",
      "\n",
      "Epoch: [330 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0310 | MainLoss:0.0198 | Alpha:0.4951 | SPLoss:0.0120 | CLSLoss:0.0106 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1029 | MainLoss:0.1029 | SPLoss:0.0195 | CLSLoss:0.0106 | top1:96.7850 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3776 | MainLoss:1.3776 | SPLoss:0.0195 | CLSLoss:0.0106 | top1:64.9744 | AUROC:0.7484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [331 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0369 | MainLoss:0.0243 | Alpha:0.4940 | SPLoss:0.0149 | CLSLoss:0.0106 | top1:99.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1144 | MainLoss:0.1144 | SPLoss:0.0254 | CLSLoss:0.0106 | top1:96.4174 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3117 | MainLoss:1.3117 | SPLoss:0.0254 | CLSLoss:0.0106 | top1:65.7949 | AUROC:0.7586\n",
      "\n",
      "Epoch: [332 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0354 | MainLoss:0.0228 | Alpha:0.4948 | SPLoss:0.0149 | CLSLoss:0.0106 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1369 | MainLoss:0.1369 | SPLoss:0.0275 | CLSLoss:0.0106 | top1:95.7539 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2462 | MainLoss:1.2462 | SPLoss:0.0275 | CLSLoss:0.0106 | top1:67.2564 | AUROC:0.7646\n",
      "\n",
      "Epoch: [333 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0385 | MainLoss:0.0262 | Alpha:0.4933 | SPLoss:0.0142 | CLSLoss:0.0106 | top1:99.2254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0884 | MainLoss:0.0884 | SPLoss:0.0283 | CLSLoss:0.0106 | top1:97.1900 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3632 | MainLoss:1.3632 | SPLoss:0.0283 | CLSLoss:0.0106 | top1:65.0897 | AUROC:0.7583\n",
      "\n",
      "Epoch: [334 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0352 | MainLoss:0.0228 | Alpha:0.4942 | SPLoss:0.0143 | CLSLoss:0.0106 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1121 | MainLoss:0.1121 | SPLoss:0.0206 | CLSLoss:0.0106 | top1:96.4548 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2694 | MainLoss:1.2694 | SPLoss:0.0206 | CLSLoss:0.0106 | top1:67.2821 | AUROC:0.7634\n",
      "\n",
      "Epoch: [335 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0422 | MainLoss:0.0294 | Alpha:0.4928 | SPLoss:0.0155 | CLSLoss:0.0105 | top1:99.0857 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1039 | MainLoss:0.1039 | SPLoss:0.0268 | CLSLoss:0.0104 | top1:96.6106 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.3113 | MainLoss:1.3113 | SPLoss:0.0268 | CLSLoss:0.0104 | top1:66.3590 | AUROC:0.7573\n",
      "\n",
      "Epoch: [336 | 1000] LR: 0.000031\n",
      "Train | 64/64 | Loss:0.0485 | MainLoss:0.0325 | Alpha:0.4921 | SPLoss:0.0220 | CLSLoss:0.0103 | top1:99.0095 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1215 | MainLoss:0.1215 | SPLoss:0.0195 | CLSLoss:0.0103 | top1:96.0872 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2461 | MainLoss:1.2461 | SPLoss:0.0195 | CLSLoss:0.0103 | top1:67.1667 | AUROC:0.7620\n",
      "\n",
      "Epoch: [337 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0331 | MainLoss:0.0221 | Alpha:0.4945 | SPLoss:0.0120 | CLSLoss:0.0103 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1271 | MainLoss:0.1271 | SPLoss:0.0189 | CLSLoss:0.0103 | top1:96.0467 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2433 | MainLoss:1.2433 | SPLoss:0.0189 | CLSLoss:0.0103 | top1:67.6667 | AUROC:0.7666\n",
      "\n",
      "Epoch: [338 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0349 | MainLoss:0.0231 | Alpha:0.4943 | SPLoss:0.0135 | CLSLoss:0.0104 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1259 | MainLoss:0.1259 | SPLoss:0.0209 | CLSLoss:0.0103 | top1:96.0218 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2221 | MainLoss:1.2221 | SPLoss:0.0209 | CLSLoss:0.0103 | top1:67.7564 | AUROC:0.7696\n",
      "\n",
      "Epoch: [339 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0415 | MainLoss:0.0263 | Alpha:0.4937 | SPLoss:0.0203 | CLSLoss:0.0104 | top1:99.0476 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1259 | MainLoss:0.1259 | SPLoss:0.0248 | CLSLoss:0.0104 | top1:95.9813 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2158 | MainLoss:1.2158 | SPLoss:0.0248 | CLSLoss:0.0104 | top1:67.8718 | AUROC:0.7733\n",
      "\n",
      "Epoch: [340 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0392 | MainLoss:0.0262 | Alpha:0.4936 | SPLoss:0.0159 | CLSLoss:0.0103 | top1:99.2381 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1011 | MainLoss:0.1011 | SPLoss:0.0206 | CLSLoss:0.0103 | top1:96.7383 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3174 | MainLoss:1.3174 | SPLoss:0.0206 | CLSLoss:0.0103 | top1:66.6410 | AUROC:0.7648\n",
      "\n",
      "Epoch: [341 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0356 | MainLoss:0.0231 | Alpha:0.4945 | SPLoss:0.0152 | CLSLoss:0.0102 | top1:99.1619 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1057 | MainLoss:0.1057 | SPLoss:0.0212 | CLSLoss:0.0103 | top1:96.6168 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2955 | MainLoss:1.2955 | SPLoss:0.0212 | CLSLoss:0.0103 | top1:66.8846 | AUROC:0.7655\n",
      "\n",
      "Epoch: [342 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0327 | MainLoss:0.0201 | Alpha:0.4949 | SPLoss:0.0152 | CLSLoss:0.0103 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1439 | MainLoss:0.1439 | SPLoss:0.0214 | CLSLoss:0.0104 | top1:95.5794 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2164 | MainLoss:1.2164 | SPLoss:0.0214 | CLSLoss:0.0104 | top1:68.6538 | AUROC:0.7674\n",
      "\n",
      "Epoch: [343 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0435 | MainLoss:0.0288 | Alpha:0.4924 | SPLoss:0.0195 | CLSLoss:0.0103 | top1:99.0222 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1656 | MainLoss:0.1656 | SPLoss:0.0274 | CLSLoss:0.0103 | top1:94.6916 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1485 | MainLoss:1.1485 | SPLoss:0.0274 | CLSLoss:0.0103 | top1:69.4615 | AUROC:0.7723\n",
      "\n",
      "Epoch: [344 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0361 | MainLoss:0.0234 | Alpha:0.4939 | SPLoss:0.0153 | CLSLoss:0.0103 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1069 | MainLoss:0.1069 | SPLoss:0.0233 | CLSLoss:0.0103 | top1:96.5919 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3370 | MainLoss:1.3370 | SPLoss:0.0233 | CLSLoss:0.0103 | top1:66.3205 | AUROC:0.7584\n",
      "\n",
      "Epoch: [345 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0381 | MainLoss:0.0251 | Alpha:0.4936 | SPLoss:0.0160 | CLSLoss:0.0103 | top1:99.2635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1424 | MainLoss:0.1424 | SPLoss:0.0202 | CLSLoss:0.0102 | top1:95.4268 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2214 | MainLoss:1.2214 | SPLoss:0.0202 | CLSLoss:0.0102 | top1:67.9872 | AUROC:0.7683\n",
      "\n",
      "Epoch: [346 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0378 | MainLoss:0.0257 | Alpha:0.4935 | SPLoss:0.0141 | CLSLoss:0.0103 | top1:99.1873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1398 | MainLoss:0.1398 | SPLoss:0.0230 | CLSLoss:0.0102 | top1:95.4424 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1681 | MainLoss:1.1681 | SPLoss:0.0230 | CLSLoss:0.0102 | top1:68.5000 | AUROC:0.7713\n",
      "\n",
      "Epoch: [347 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0330 | MainLoss:0.0215 | Alpha:0.4947 | SPLoss:0.0129 | CLSLoss:0.0102 | top1:99.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1298 | MainLoss:0.1298 | SPLoss:0.0193 | CLSLoss:0.0102 | top1:95.8318 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2165 | MainLoss:1.2165 | SPLoss:0.0193 | CLSLoss:0.0102 | top1:68.0769 | AUROC:0.7704\n",
      "\n",
      "Epoch: [348 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0280 | MainLoss:0.0172 | Alpha:0.4956 | SPLoss:0.0115 | CLSLoss:0.0103 | top1:99.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1333 | MainLoss:0.1333 | SPLoss:0.0130 | CLSLoss:0.0104 | top1:95.8100 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2447 | MainLoss:1.2447 | SPLoss:0.0130 | CLSLoss:0.0104 | top1:68.4231 | AUROC:0.7654\n",
      "\n",
      "Epoch: [349 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0337 | MainLoss:0.0209 | Alpha:0.4950 | SPLoss:0.0155 | CLSLoss:0.0104 | top1:99.2381 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1225 | MainLoss:0.1225 | SPLoss:0.0237 | CLSLoss:0.0104 | top1:96.1745 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3076 | MainLoss:1.3076 | SPLoss:0.0237 | CLSLoss:0.0104 | top1:67.2436 | AUROC:0.7624\n",
      "\n",
      "Epoch: [350 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0452 | MainLoss:0.0301 | Alpha:0.4925 | SPLoss:0.0205 | CLSLoss:0.0102 | top1:99.0095 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0842 | MainLoss:0.0842 | SPLoss:0.0288 | CLSLoss:0.0101 | top1:97.2181 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3821 | MainLoss:1.3821 | SPLoss:0.0288 | CLSLoss:0.0101 | top1:64.7692 | AUROC:0.7581\n",
      "\n",
      "Epoch: [351 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0397 | MainLoss:0.0271 | Alpha:0.4928 | SPLoss:0.0154 | CLSLoss:0.0101 | top1:99.1238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1064 | MainLoss:0.1064 | SPLoss:0.0179 | CLSLoss:0.0101 | top1:96.4766 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2910 | MainLoss:1.2910 | SPLoss:0.0179 | CLSLoss:0.0101 | top1:66.4744 | AUROC:0.7631\n",
      "\n",
      "Epoch: [352 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0404 | MainLoss:0.0279 | Alpha:0.4932 | SPLoss:0.0152 | CLSLoss:0.0100 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1035 | MainLoss:0.1035 | SPLoss:0.0211 | CLSLoss:0.0100 | top1:96.6231 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3279 | MainLoss:1.3279 | SPLoss:0.0211 | CLSLoss:0.0100 | top1:65.8077 | AUROC:0.7591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [353 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0388 | MainLoss:0.0249 | Alpha:0.4940 | SPLoss:0.0180 | CLSLoss:0.0100 | top1:99.1746 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1177 | MainLoss:0.1177 | SPLoss:0.0195 | CLSLoss:0.0100 | top1:96.1682 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2399 | MainLoss:1.2399 | SPLoss:0.0195 | CLSLoss:0.0100 | top1:67.6795 | AUROC:0.7704\n",
      "\n",
      "Epoch: [354 | 1000] LR: 0.000030\n",
      "Train | 64/64 | Loss:0.0381 | MainLoss:0.0247 | Alpha:0.4944 | SPLoss:0.0171 | CLSLoss:0.0100 | top1:99.1619 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1118 | MainLoss:0.1118 | SPLoss:0.0213 | CLSLoss:0.0100 | top1:96.2056 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2514 | MainLoss:1.2514 | SPLoss:0.0213 | CLSLoss:0.0100 | top1:67.3205 | AUROC:0.7656\n",
      "\n",
      "Epoch: [355 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0381 | MainLoss:0.0243 | Alpha:0.4942 | SPLoss:0.0178 | CLSLoss:0.0100 | top1:99.1619 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1487 | MainLoss:0.1487 | SPLoss:0.0196 | CLSLoss:0.0100 | top1:95.0561 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1722 | MainLoss:1.1722 | SPLoss:0.0196 | CLSLoss:0.0100 | top1:69.0513 | AUROC:0.7704\n",
      "\n",
      "Epoch: [356 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0326 | MainLoss:0.0210 | Alpha:0.4943 | SPLoss:0.0134 | CLSLoss:0.0100 | top1:99.4413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1122 | MainLoss:0.1122 | SPLoss:0.0177 | CLSLoss:0.0101 | top1:96.3676 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2669 | MainLoss:1.2669 | SPLoss:0.0177 | CLSLoss:0.0101 | top1:67.3974 | AUROC:0.7616\n",
      "\n",
      "Epoch: [357 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0380 | MainLoss:0.0259 | Alpha:0.4938 | SPLoss:0.0146 | CLSLoss:0.0101 | top1:99.2127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0997 | MainLoss:0.0997 | SPLoss:0.0241 | CLSLoss:0.0100 | top1:96.7539 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2724 | MainLoss:1.2724 | SPLoss:0.0241 | CLSLoss:0.0100 | top1:66.2949 | AUROC:0.7568\n",
      "\n",
      "Epoch: [358 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0378 | MainLoss:0.0249 | Alpha:0.4940 | SPLoss:0.0162 | CLSLoss:0.0100 | top1:99.0222 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1133 | MainLoss:0.1133 | SPLoss:0.0231 | CLSLoss:0.0099 | top1:96.2617 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2564 | MainLoss:1.2564 | SPLoss:0.0231 | CLSLoss:0.0099 | top1:66.8333 | AUROC:0.7578\n",
      "\n",
      "Epoch: [359 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0298 | MainLoss:0.0184 | Alpha:0.4955 | SPLoss:0.0128 | CLSLoss:0.0100 | top1:99.4921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1122 | MainLoss:0.1122 | SPLoss:0.0163 | CLSLoss:0.0101 | top1:96.4361 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3386 | MainLoss:1.3386 | SPLoss:0.0163 | CLSLoss:0.0101 | top1:66.4359 | AUROC:0.7536\n",
      "\n",
      "Epoch: [360 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0351 | MainLoss:0.0226 | Alpha:0.4948 | SPLoss:0.0152 | CLSLoss:0.0100 | top1:99.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1312 | MainLoss:0.1312 | SPLoss:0.0211 | CLSLoss:0.0100 | top1:95.8100 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2981 | MainLoss:1.2981 | SPLoss:0.0211 | CLSLoss:0.0100 | top1:66.8462 | AUROC:0.7535\n",
      "\n",
      "Epoch: [361 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0315 | MainLoss:0.0201 | Alpha:0.4949 | SPLoss:0.0130 | CLSLoss:0.0100 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1175 | MainLoss:0.1175 | SPLoss:0.0192 | CLSLoss:0.0101 | top1:96.1838 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3027 | MainLoss:1.3027 | SPLoss:0.0192 | CLSLoss:0.0101 | top1:66.8974 | AUROC:0.7589\n",
      "\n",
      "Epoch: [362 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0324 | MainLoss:0.0212 | Alpha:0.4949 | SPLoss:0.0126 | CLSLoss:0.0101 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1265 | MainLoss:0.1265 | SPLoss:0.0207 | CLSLoss:0.0101 | top1:95.9377 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2742 | MainLoss:1.2742 | SPLoss:0.0207 | CLSLoss:0.0101 | top1:67.3462 | AUROC:0.7612\n",
      "\n",
      "Epoch: [363 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0375 | MainLoss:0.0254 | Alpha:0.4938 | SPLoss:0.0144 | CLSLoss:0.0101 | top1:99.2127 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1049 | MainLoss:0.1049 | SPLoss:0.0266 | CLSLoss:0.0100 | top1:96.5608 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3071 | MainLoss:1.3071 | SPLoss:0.0266 | CLSLoss:0.0100 | top1:66.8718 | AUROC:0.7643\n",
      "\n",
      "Epoch: [364 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0364 | MainLoss:0.0235 | Alpha:0.4942 | SPLoss:0.0161 | CLSLoss:0.0100 | top1:99.1873 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1432 | MainLoss:0.1432 | SPLoss:0.0255 | CLSLoss:0.0099 | top1:95.1464 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1859 | MainLoss:1.1859 | SPLoss:0.0255 | CLSLoss:0.0099 | top1:68.1282 | AUROC:0.7719\n",
      "\n",
      "Epoch: [365 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0361 | MainLoss:0.0239 | Alpha:0.4940 | SPLoss:0.0148 | CLSLoss:0.0099 | top1:99.2254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1223 | MainLoss:0.1223 | SPLoss:0.0174 | CLSLoss:0.0100 | top1:95.9065 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2628 | MainLoss:1.2628 | SPLoss:0.0174 | CLSLoss:0.0100 | top1:67.8462 | AUROC:0.7722\n",
      "\n",
      "Epoch: [366 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0324 | MainLoss:0.0209 | Alpha:0.4946 | SPLoss:0.0134 | CLSLoss:0.0100 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1180 | MainLoss:0.1180 | SPLoss:0.0169 | CLSLoss:0.0100 | top1:96.0499 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2564 | MainLoss:1.2564 | SPLoss:0.0169 | CLSLoss:0.0100 | top1:67.9615 | AUROC:0.7690\n",
      "\n",
      "Epoch: [367 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0341 | MainLoss:0.0223 | Alpha:0.4945 | SPLoss:0.0139 | CLSLoss:0.0100 | top1:99.2889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0826 | MainLoss:0.0826 | SPLoss:0.0243 | CLSLoss:0.0100 | top1:97.1776 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.4052 | MainLoss:1.4052 | SPLoss:0.0243 | CLSLoss:0.0100 | top1:65.2051 | AUROC:0.7571\n",
      "\n",
      "Epoch: [368 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0325 | MainLoss:0.0201 | Alpha:0.4945 | SPLoss:0.0151 | CLSLoss:0.0099 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1022 | MainLoss:0.1022 | SPLoss:0.0175 | CLSLoss:0.0100 | top1:96.6449 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3157 | MainLoss:1.3157 | SPLoss:0.0175 | CLSLoss:0.0100 | top1:66.7179 | AUROC:0.7598\n",
      "\n",
      "Epoch: [369 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0289 | MainLoss:0.0188 | Alpha:0.4955 | SPLoss:0.0104 | CLSLoss:0.0100 | top1:99.4413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1082 | MainLoss:0.1082 | SPLoss:0.0210 | CLSLoss:0.0100 | top1:96.4922 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3240 | MainLoss:1.3240 | SPLoss:0.0210 | CLSLoss:0.0100 | top1:66.7308 | AUROC:0.7588\n",
      "\n",
      "Epoch: [370 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0368 | MainLoss:0.0244 | Alpha:0.4943 | SPLoss:0.0151 | CLSLoss:0.0099 | top1:99.2254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1176 | MainLoss:0.1176 | SPLoss:0.0206 | CLSLoss:0.0099 | top1:96.0810 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2706 | MainLoss:1.2706 | SPLoss:0.0206 | CLSLoss:0.0099 | top1:67.4231 | AUROC:0.7673\n",
      "\n",
      "Epoch: [371 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0345 | MainLoss:0.0218 | Alpha:0.4949 | SPLoss:0.0157 | CLSLoss:0.0099 | top1:99.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1145 | MainLoss:0.1145 | SPLoss:0.0187 | CLSLoss:0.0099 | top1:96.2741 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2727 | MainLoss:1.2727 | SPLoss:0.0187 | CLSLoss:0.0099 | top1:67.3590 | AUROC:0.7629\n",
      "\n",
      "Epoch: [372 | 1000] LR: 0.000029\n",
      "Train | 64/64 | Loss:0.0328 | MainLoss:0.0202 | Alpha:0.4950 | SPLoss:0.0156 | CLSLoss:0.0099 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0825 | MainLoss:0.0825 | SPLoss:0.0195 | CLSLoss:0.0099 | top1:97.3271 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.4314 | MainLoss:1.4314 | SPLoss:0.0195 | CLSLoss:0.0099 | top1:64.7821 | AUROC:0.7515\n",
      "\n",
      "Epoch: [373 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0360 | MainLoss:0.0233 | Alpha:0.4947 | SPLoss:0.0157 | CLSLoss:0.0099 | top1:99.2635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1231 | MainLoss:0.1231 | SPLoss:0.0229 | CLSLoss:0.0098 | top1:95.9907 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2780 | MainLoss:1.2780 | SPLoss:0.0229 | CLSLoss:0.0098 | top1:67.4487 | AUROC:0.7635\n",
      "\n",
      "Epoch: [374 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0291 | MainLoss:0.0187 | Alpha:0.4951 | SPLoss:0.0113 | CLSLoss:0.0098 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1052 | MainLoss:0.1052 | SPLoss:0.0152 | CLSLoss:0.0098 | top1:96.5763 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3327 | MainLoss:1.3327 | SPLoss:0.0152 | CLSLoss:0.0098 | top1:66.7051 | AUROC:0.7622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [375 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0311 | MainLoss:0.0195 | Alpha:0.4953 | SPLoss:0.0136 | CLSLoss:0.0098 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1247 | MainLoss:0.1247 | SPLoss:0.0186 | CLSLoss:0.0098 | top1:96.0405 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2730 | MainLoss:1.2730 | SPLoss:0.0186 | CLSLoss:0.0098 | top1:67.6923 | AUROC:0.7645\n",
      "\n",
      "Epoch: [376 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0345 | MainLoss:0.0226 | Alpha:0.4944 | SPLoss:0.0143 | CLSLoss:0.0098 | top1:99.3397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1045 | MainLoss:0.1045 | SPLoss:0.0206 | CLSLoss:0.0098 | top1:96.6667 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2956 | MainLoss:1.2956 | SPLoss:0.0206 | CLSLoss:0.0098 | top1:67.4359 | AUROC:0.7643\n",
      "\n",
      "Epoch: [377 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0325 | MainLoss:0.0216 | Alpha:0.4946 | SPLoss:0.0123 | CLSLoss:0.0098 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1353 | MainLoss:0.1353 | SPLoss:0.0174 | CLSLoss:0.0097 | top1:95.7539 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2149 | MainLoss:1.2149 | SPLoss:0.0174 | CLSLoss:0.0097 | top1:68.4359 | AUROC:0.7678\n",
      "\n",
      "Epoch: [378 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0321 | MainLoss:0.0204 | Alpha:0.4950 | SPLoss:0.0138 | CLSLoss:0.0097 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1331 | MainLoss:0.1331 | SPLoss:0.0180 | CLSLoss:0.0097 | top1:95.7726 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2411 | MainLoss:1.2411 | SPLoss:0.0180 | CLSLoss:0.0097 | top1:68.3718 | AUROC:0.7632\n",
      "\n",
      "Epoch: [379 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0305 | MainLoss:0.0192 | Alpha:0.4954 | SPLoss:0.0131 | CLSLoss:0.0098 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1367 | MainLoss:0.1367 | SPLoss:0.0174 | CLSLoss:0.0098 | top1:95.6729 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2193 | MainLoss:1.2193 | SPLoss:0.0174 | CLSLoss:0.0098 | top1:68.5513 | AUROC:0.7687\n",
      "\n",
      "Epoch: [380 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0367 | MainLoss:0.0233 | Alpha:0.4942 | SPLoss:0.0173 | CLSLoss:0.0097 | top1:99.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1083 | MainLoss:0.1083 | SPLoss:0.0202 | CLSLoss:0.0097 | top1:96.5608 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2902 | MainLoss:1.2902 | SPLoss:0.0202 | CLSLoss:0.0097 | top1:67.1667 | AUROC:0.7629\n",
      "\n",
      "Epoch: [381 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0400 | MainLoss:0.0266 | Alpha:0.4935 | SPLoss:0.0175 | CLSLoss:0.0096 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1108 | MainLoss:0.1108 | SPLoss:0.0215 | CLSLoss:0.0095 | top1:96.3520 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2620 | MainLoss:1.2620 | SPLoss:0.0215 | CLSLoss:0.0095 | top1:66.9487 | AUROC:0.7626\n",
      "\n",
      "Epoch: [382 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0270 | MainLoss:0.0176 | Alpha:0.4958 | SPLoss:0.0094 | CLSLoss:0.0096 | top1:99.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0972 | MainLoss:0.0972 | SPLoss:0.0145 | CLSLoss:0.0097 | top1:96.8598 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3180 | MainLoss:1.3180 | SPLoss:0.0145 | CLSLoss:0.0097 | top1:66.5385 | AUROC:0.7629\n",
      "\n",
      "Epoch: [383 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0328 | MainLoss:0.0212 | Alpha:0.4949 | SPLoss:0.0137 | CLSLoss:0.0096 | top1:99.3778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0932 | MainLoss:0.0932 | SPLoss:0.0197 | CLSLoss:0.0096 | top1:97.0093 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3140 | MainLoss:1.3140 | SPLoss:0.0197 | CLSLoss:0.0096 | top1:66.5385 | AUROC:0.7638\n",
      "\n",
      "Epoch: [384 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0266 | MainLoss:0.0173 | Alpha:0.4957 | SPLoss:0.0091 | CLSLoss:0.0097 | top1:99.4921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1285 | MainLoss:0.1285 | SPLoss:0.0136 | CLSLoss:0.0097 | top1:95.9688 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2417 | MainLoss:1.2417 | SPLoss:0.0136 | CLSLoss:0.0097 | top1:67.9103 | AUROC:0.7674\n",
      "\n",
      "Epoch: [385 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0301 | MainLoss:0.0180 | Alpha:0.4955 | SPLoss:0.0148 | CLSLoss:0.0097 | top1:99.3778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1131 | MainLoss:0.1131 | SPLoss:0.0210 | CLSLoss:0.0097 | top1:96.4299 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3162 | MainLoss:1.3162 | SPLoss:0.0210 | CLSLoss:0.0097 | top1:66.9615 | AUROC:0.7651\n",
      "\n",
      "Epoch: [386 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0385 | MainLoss:0.0258 | Alpha:0.4938 | SPLoss:0.0163 | CLSLoss:0.0096 | top1:99.1238 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0905 | MainLoss:0.0905 | SPLoss:0.0299 | CLSLoss:0.0096 | top1:97.0031 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3218 | MainLoss:1.3218 | SPLoss:0.0299 | CLSLoss:0.0096 | top1:65.8462 | AUROC:0.7635\n",
      "\n",
      "Epoch: [387 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0300 | MainLoss:0.0179 | Alpha:0.4959 | SPLoss:0.0148 | CLSLoss:0.0096 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1127 | MainLoss:0.1127 | SPLoss:0.0144 | CLSLoss:0.0096 | top1:96.5265 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3105 | MainLoss:1.3105 | SPLoss:0.0144 | CLSLoss:0.0096 | top1:66.8333 | AUROC:0.7570\n",
      "\n",
      "Epoch: [388 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0274 | MainLoss:0.0166 | Alpha:0.4961 | SPLoss:0.0120 | CLSLoss:0.0097 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0993 | MainLoss:0.0993 | SPLoss:0.0196 | CLSLoss:0.0097 | top1:96.9439 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3894 | MainLoss:1.3894 | SPLoss:0.0196 | CLSLoss:0.0097 | top1:65.7692 | AUROC:0.7560\n",
      "\n",
      "Epoch: [389 | 1000] LR: 0.000028\n",
      "Train | 64/64 | Loss:0.0366 | MainLoss:0.0229 | Alpha:0.4945 | SPLoss:0.0181 | CLSLoss:0.0097 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1065 | MainLoss:0.1065 | SPLoss:0.0176 | CLSLoss:0.0096 | top1:96.6822 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3450 | MainLoss:1.3450 | SPLoss:0.0176 | CLSLoss:0.0096 | top1:66.4231 | AUROC:0.7557\n",
      "\n",
      "Epoch: [390 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0365 | MainLoss:0.0225 | Alpha:0.4948 | SPLoss:0.0185 | CLSLoss:0.0096 | top1:99.2254 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1330 | MainLoss:0.1330 | SPLoss:0.0222 | CLSLoss:0.0096 | top1:95.6978 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2378 | MainLoss:1.2378 | SPLoss:0.0222 | CLSLoss:0.0096 | top1:67.7949 | AUROC:0.7645\n",
      "\n",
      "Epoch: [391 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0360 | MainLoss:0.0227 | Alpha:0.4942 | SPLoss:0.0174 | CLSLoss:0.0095 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1177 | MainLoss:0.1177 | SPLoss:0.0239 | CLSLoss:0.0095 | top1:96.1589 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2884 | MainLoss:1.2884 | SPLoss:0.0239 | CLSLoss:0.0095 | top1:66.6282 | AUROC:0.7623\n",
      "\n",
      "Epoch: [392 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0329 | MainLoss:0.0204 | Alpha:0.4950 | SPLoss:0.0157 | CLSLoss:0.0096 | top1:99.2000 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1516 | MainLoss:0.1516 | SPLoss:0.0170 | CLSLoss:0.0096 | top1:95.2118 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.2149 | MainLoss:1.2149 | SPLoss:0.0170 | CLSLoss:0.0096 | top1:68.2179 | AUROC:0.7675\n",
      "\n",
      "Epoch: [393 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0314 | MainLoss:0.0198 | Alpha:0.4948 | SPLoss:0.0138 | CLSLoss:0.0096 | top1:99.3778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1448 | MainLoss:0.1448 | SPLoss:0.0215 | CLSLoss:0.0095 | top1:95.4891 | AUROC:0.9996\n",
      "Test | 63/64 | Loss:1.1916 | MainLoss:1.1916 | SPLoss:0.0215 | CLSLoss:0.0095 | top1:68.4744 | AUROC:0.7697\n",
      "\n",
      "Epoch: [394 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0306 | MainLoss:0.0200 | Alpha:0.4948 | SPLoss:0.0118 | CLSLoss:0.0095 | top1:99.4032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1105 | MainLoss:0.1105 | SPLoss:0.0170 | CLSLoss:0.0095 | top1:96.5358 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2839 | MainLoss:1.2839 | SPLoss:0.0170 | CLSLoss:0.0095 | top1:66.6282 | AUROC:0.7650\n",
      "\n",
      "Epoch: [395 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0320 | MainLoss:0.0207 | Alpha:0.4950 | SPLoss:0.0134 | CLSLoss:0.0094 | top1:99.3778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1048 | MainLoss:0.1048 | SPLoss:0.0132 | CLSLoss:0.0095 | top1:96.7850 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3283 | MainLoss:1.3283 | SPLoss:0.0132 | CLSLoss:0.0095 | top1:66.2051 | AUROC:0.7599\n",
      "\n",
      "Epoch: [396 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0341 | MainLoss:0.0213 | Alpha:0.4947 | SPLoss:0.0163 | CLSLoss:0.0094 | top1:99.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1048 | MainLoss:0.1048 | SPLoss:0.0182 | CLSLoss:0.0094 | top1:96.7227 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3326 | MainLoss:1.3326 | SPLoss:0.0182 | CLSLoss:0.0094 | top1:66.4103 | AUROC:0.7559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [397 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0294 | MainLoss:0.0186 | Alpha:0.4958 | SPLoss:0.0122 | CLSLoss:0.0094 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0871 | MainLoss:0.0871 | SPLoss:0.0233 | CLSLoss:0.0094 | top1:97.2212 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3725 | MainLoss:1.3725 | SPLoss:0.0233 | CLSLoss:0.0094 | top1:65.8974 | AUROC:0.7548\n",
      "\n",
      "Epoch: [398 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0354 | MainLoss:0.0232 | Alpha:0.4942 | SPLoss:0.0153 | CLSLoss:0.0094 | top1:99.2889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1025 | MainLoss:0.1025 | SPLoss:0.0213 | CLSLoss:0.0093 | top1:96.6822 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3033 | MainLoss:1.3033 | SPLoss:0.0213 | CLSLoss:0.0093 | top1:66.8974 | AUROC:0.7623\n",
      "\n",
      "Epoch: [399 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0301 | MainLoss:0.0193 | Alpha:0.4953 | SPLoss:0.0125 | CLSLoss:0.0093 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0643 | MainLoss:0.0643 | SPLoss:0.0235 | CLSLoss:0.0093 | top1:97.8629 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.5209 | MainLoss:1.5209 | SPLoss:0.0235 | CLSLoss:0.0093 | top1:63.6538 | AUROC:0.7481\n",
      "\n",
      "Epoch: [400 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0329 | MainLoss:0.0207 | Alpha:0.4935 | SPLoss:0.0153 | CLSLoss:0.0093 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1193 | MainLoss:0.1193 | SPLoss:0.0166 | CLSLoss:0.0093 | top1:96.2087 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2599 | MainLoss:1.2599 | SPLoss:0.0166 | CLSLoss:0.0093 | top1:67.2692 | AUROC:0.7666\n",
      "\n",
      "Epoch: [401 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0318 | MainLoss:0.0200 | Alpha:0.4953 | SPLoss:0.0144 | CLSLoss:0.0093 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1129 | MainLoss:0.1129 | SPLoss:0.0215 | CLSLoss:0.0093 | top1:96.3022 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2114 | MainLoss:1.2114 | SPLoss:0.0215 | CLSLoss:0.0093 | top1:67.9487 | AUROC:0.7745\n",
      "\n",
      "Epoch: [402 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0336 | MainLoss:0.0228 | Alpha:0.4942 | SPLoss:0.0126 | CLSLoss:0.0093 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1127 | MainLoss:0.1127 | SPLoss:0.0193 | CLSLoss:0.0093 | top1:96.2586 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2000 | MainLoss:1.2000 | SPLoss:0.0193 | CLSLoss:0.0093 | top1:68.1282 | AUROC:0.7747\n",
      "\n",
      "Epoch: [403 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0319 | MainLoss:0.0218 | Alpha:0.4947 | SPLoss:0.0112 | CLSLoss:0.0093 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1085 | MainLoss:0.1085 | SPLoss:0.0204 | CLSLoss:0.0093 | top1:96.3489 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2105 | MainLoss:1.2105 | SPLoss:0.0204 | CLSLoss:0.0093 | top1:67.5641 | AUROC:0.7710\n",
      "\n",
      "Epoch: [404 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0263 | MainLoss:0.0174 | Alpha:0.4958 | SPLoss:0.0087 | CLSLoss:0.0093 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1099 | MainLoss:0.1099 | SPLoss:0.0125 | CLSLoss:0.0094 | top1:96.4081 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.1973 | MainLoss:1.1973 | SPLoss:0.0125 | CLSLoss:0.0094 | top1:68.1538 | AUROC:0.7757\n",
      "\n",
      "Epoch: [405 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0316 | MainLoss:0.0209 | Alpha:0.4946 | SPLoss:0.0122 | CLSLoss:0.0094 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1190 | MainLoss:0.1190 | SPLoss:0.0161 | CLSLoss:0.0094 | top1:96.1682 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2020 | MainLoss:1.2020 | SPLoss:0.0161 | CLSLoss:0.0094 | top1:67.9231 | AUROC:0.7714\n",
      "\n",
      "Epoch: [406 | 1000] LR: 0.000027\n",
      "Train | 64/64 | Loss:0.0288 | MainLoss:0.0179 | Alpha:0.4955 | SPLoss:0.0126 | CLSLoss:0.0094 | top1:99.3397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1046 | MainLoss:0.1046 | SPLoss:0.0160 | CLSLoss:0.0094 | top1:96.6916 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3071 | MainLoss:1.3071 | SPLoss:0.0160 | CLSLoss:0.0094 | top1:66.7436 | AUROC:0.7614\n",
      "\n",
      "Epoch: [407 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0310 | MainLoss:0.0193 | Alpha:0.4955 | SPLoss:0.0140 | CLSLoss:0.0095 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1059 | MainLoss:0.1059 | SPLoss:0.0175 | CLSLoss:0.0094 | top1:96.6542 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2969 | MainLoss:1.2969 | SPLoss:0.0175 | CLSLoss:0.0094 | top1:66.8974 | AUROC:0.7635\n",
      "\n",
      "Epoch: [408 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0340 | MainLoss:0.0218 | Alpha:0.4947 | SPLoss:0.0153 | CLSLoss:0.0094 | top1:99.2508 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1362 | MainLoss:0.1362 | SPLoss:0.0265 | CLSLoss:0.0094 | top1:95.5857 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1822 | MainLoss:1.1822 | SPLoss:0.0265 | CLSLoss:0.0094 | top1:68.7564 | AUROC:0.7723\n",
      "\n",
      "Epoch: [409 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0239 | MainLoss:0.0155 | Alpha:0.4963 | SPLoss:0.0075 | CLSLoss:0.0094 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1299 | MainLoss:0.1299 | SPLoss:0.0136 | CLSLoss:0.0094 | top1:95.7757 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2055 | MainLoss:1.2055 | SPLoss:0.0136 | CLSLoss:0.0094 | top1:68.7692 | AUROC:0.7729\n",
      "\n",
      "Epoch: [410 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0315 | MainLoss:0.0206 | Alpha:0.4951 | SPLoss:0.0124 | CLSLoss:0.0094 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1178 | MainLoss:0.1178 | SPLoss:0.0199 | CLSLoss:0.0094 | top1:96.1028 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2139 | MainLoss:1.2139 | SPLoss:0.0199 | CLSLoss:0.0094 | top1:68.5897 | AUROC:0.7745\n",
      "\n",
      "Epoch: [411 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0254 | MainLoss:0.0161 | Alpha:0.4961 | SPLoss:0.0094 | CLSLoss:0.0094 | top1:99.5048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1157 | MainLoss:0.1157 | SPLoss:0.0110 | CLSLoss:0.0095 | top1:96.3022 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2781 | MainLoss:1.2781 | SPLoss:0.0110 | CLSLoss:0.0095 | top1:67.9487 | AUROC:0.7649\n",
      "\n",
      "Epoch: [412 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0287 | MainLoss:0.0177 | Alpha:0.4958 | SPLoss:0.0127 | CLSLoss:0.0095 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0822 | MainLoss:0.0822 | SPLoss:0.0178 | CLSLoss:0.0095 | top1:97.3863 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3916 | MainLoss:1.3916 | SPLoss:0.0178 | CLSLoss:0.0095 | top1:66.6538 | AUROC:0.7584\n",
      "\n",
      "Epoch: [413 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0303 | MainLoss:0.0179 | Alpha:0.4955 | SPLoss:0.0156 | CLSLoss:0.0094 | top1:99.4032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1097 | MainLoss:0.1097 | SPLoss:0.0162 | CLSLoss:0.0094 | top1:96.4953 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2654 | MainLoss:1.2654 | SPLoss:0.0162 | CLSLoss:0.0094 | top1:68.3077 | AUROC:0.7662\n",
      "\n",
      "Epoch: [414 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0306 | MainLoss:0.0197 | Alpha:0.4952 | SPLoss:0.0126 | CLSLoss:0.0094 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1066 | MainLoss:0.1066 | SPLoss:0.0213 | CLSLoss:0.0094 | top1:96.5608 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2620 | MainLoss:1.2620 | SPLoss:0.0213 | CLSLoss:0.0094 | top1:68.2692 | AUROC:0.7645\n",
      "\n",
      "Epoch: [415 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0276 | MainLoss:0.0174 | Alpha:0.4956 | SPLoss:0.0112 | CLSLoss:0.0093 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1298 | MainLoss:0.1298 | SPLoss:0.0151 | CLSLoss:0.0093 | top1:95.8162 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2079 | MainLoss:1.2079 | SPLoss:0.0151 | CLSLoss:0.0093 | top1:68.8718 | AUROC:0.7663\n",
      "\n",
      "Epoch: [416 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0316 | MainLoss:0.0193 | Alpha:0.4949 | SPLoss:0.0155 | CLSLoss:0.0093 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1267 | MainLoss:0.1267 | SPLoss:0.0193 | CLSLoss:0.0093 | top1:96.0561 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2051 | MainLoss:1.2051 | SPLoss:0.0193 | CLSLoss:0.0093 | top1:68.5256 | AUROC:0.7684\n",
      "\n",
      "Epoch: [417 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0321 | MainLoss:0.0218 | Alpha:0.4946 | SPLoss:0.0115 | CLSLoss:0.0092 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0963 | MainLoss:0.0963 | SPLoss:0.0194 | CLSLoss:0.0092 | top1:96.9408 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3290 | MainLoss:1.3290 | SPLoss:0.0194 | CLSLoss:0.0092 | top1:66.7821 | AUROC:0.7601\n",
      "\n",
      "Epoch: [418 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0322 | MainLoss:0.0202 | Alpha:0.4950 | SPLoss:0.0150 | CLSLoss:0.0092 | top1:99.2762 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1006 | MainLoss:0.1006 | SPLoss:0.0211 | CLSLoss:0.0091 | top1:96.7414 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2892 | MainLoss:1.2892 | SPLoss:0.0211 | CLSLoss:0.0091 | top1:66.8974 | AUROC:0.7618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [419 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0290 | MainLoss:0.0184 | Alpha:0.4954 | SPLoss:0.0123 | CLSLoss:0.0091 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1076 | MainLoss:0.1076 | SPLoss:0.0165 | CLSLoss:0.0092 | top1:96.5358 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2780 | MainLoss:1.2780 | SPLoss:0.0165 | CLSLoss:0.0092 | top1:67.1282 | AUROC:0.7607\n",
      "\n",
      "Epoch: [420 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0272 | MainLoss:0.0174 | Alpha:0.4958 | SPLoss:0.0108 | CLSLoss:0.0092 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1069 | MainLoss:0.1069 | SPLoss:0.0130 | CLSLoss:0.0092 | top1:96.6729 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2987 | MainLoss:1.2987 | SPLoss:0.0130 | CLSLoss:0.0092 | top1:67.2821 | AUROC:0.7601\n",
      "\n",
      "Epoch: [421 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0263 | MainLoss:0.0165 | Alpha:0.4958 | SPLoss:0.0107 | CLSLoss:0.0092 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1462 | MainLoss:0.1462 | SPLoss:0.0174 | CLSLoss:0.0092 | top1:95.5826 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2100 | MainLoss:1.2100 | SPLoss:0.0174 | CLSLoss:0.0092 | top1:68.8462 | AUROC:0.7654\n",
      "\n",
      "Epoch: [422 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0278 | MainLoss:0.0173 | Alpha:0.4958 | SPLoss:0.0121 | CLSLoss:0.0092 | top1:99.4794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1033 | MainLoss:0.1033 | SPLoss:0.0158 | CLSLoss:0.0091 | top1:96.8442 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2888 | MainLoss:1.2888 | SPLoss:0.0158 | CLSLoss:0.0091 | top1:67.0513 | AUROC:0.7625\n",
      "\n",
      "Epoch: [423 | 1000] LR: 0.000026\n",
      "Train | 64/64 | Loss:0.0254 | MainLoss:0.0154 | Alpha:0.4963 | SPLoss:0.0111 | CLSLoss:0.0092 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0699 | MainLoss:0.0699 | SPLoss:0.0193 | CLSLoss:0.0092 | top1:97.8474 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.4578 | MainLoss:1.4578 | SPLoss:0.0193 | CLSLoss:0.0092 | top1:64.7692 | AUROC:0.7508\n",
      "\n",
      "Epoch: [424 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0335 | MainLoss:0.0199 | Alpha:0.4944 | SPLoss:0.0182 | CLSLoss:0.0091 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1508 | MainLoss:0.1508 | SPLoss:0.0315 | CLSLoss:0.0091 | top1:95.2523 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1500 | MainLoss:1.1500 | SPLoss:0.0315 | CLSLoss:0.0091 | top1:69.1667 | AUROC:0.7708\n",
      "\n",
      "Epoch: [425 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0293 | MainLoss:0.0181 | Alpha:0.4942 | SPLoss:0.0136 | CLSLoss:0.0091 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1080 | MainLoss:0.1080 | SPLoss:0.0138 | CLSLoss:0.0091 | top1:96.6106 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2658 | MainLoss:1.2658 | SPLoss:0.0138 | CLSLoss:0.0091 | top1:67.3077 | AUROC:0.7643\n",
      "\n",
      "Epoch: [426 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0275 | MainLoss:0.0179 | Alpha:0.4956 | SPLoss:0.0102 | CLSLoss:0.0091 | top1:99.4286 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0951 | MainLoss:0.0951 | SPLoss:0.0166 | CLSLoss:0.0092 | top1:97.0031 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3093 | MainLoss:1.3093 | SPLoss:0.0166 | CLSLoss:0.0092 | top1:66.8333 | AUROC:0.7643\n",
      "\n",
      "Epoch: [427 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0229 | MainLoss:0.0130 | Alpha:0.4970 | SPLoss:0.0108 | CLSLoss:0.0091 | top1:99.6190 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1146 | MainLoss:0.1146 | SPLoss:0.0097 | CLSLoss:0.0092 | top1:96.4268 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2834 | MainLoss:1.2834 | SPLoss:0.0097 | CLSLoss:0.0092 | top1:67.5641 | AUROC:0.7658\n",
      "\n",
      "Epoch: [428 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0329 | MainLoss:0.0215 | Alpha:0.4945 | SPLoss:0.0139 | CLSLoss:0.0091 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1068 | MainLoss:0.1068 | SPLoss:0.0228 | CLSLoss:0.0091 | top1:96.5950 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2535 | MainLoss:1.2535 | SPLoss:0.0228 | CLSLoss:0.0091 | top1:67.7179 | AUROC:0.7718\n",
      "\n",
      "Epoch: [429 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0337 | MainLoss:0.0220 | Alpha:0.4948 | SPLoss:0.0146 | CLSLoss:0.0090 | top1:99.2635 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1114 | MainLoss:0.1114 | SPLoss:0.0171 | CLSLoss:0.0090 | top1:96.3520 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2279 | MainLoss:1.2279 | SPLoss:0.0171 | CLSLoss:0.0090 | top1:67.9744 | AUROC:0.7718\n",
      "\n",
      "Epoch: [430 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0272 | MainLoss:0.0185 | Alpha:0.4955 | SPLoss:0.0087 | CLSLoss:0.0090 | top1:99.4794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1285 | MainLoss:0.1285 | SPLoss:0.0193 | CLSLoss:0.0090 | top1:95.7165 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1862 | MainLoss:1.1862 | SPLoss:0.0193 | CLSLoss:0.0090 | top1:68.5128 | AUROC:0.7764\n",
      "\n",
      "Epoch: [431 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0246 | MainLoss:0.0153 | Alpha:0.4963 | SPLoss:0.0098 | CLSLoss:0.0090 | top1:99.5048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1330 | MainLoss:0.1330 | SPLoss:0.0140 | CLSLoss:0.0091 | top1:95.6760 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.1937 | MainLoss:1.1937 | SPLoss:0.0140 | CLSLoss:0.0091 | top1:68.2692 | AUROC:0.7767\n",
      "\n",
      "Epoch: [432 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0298 | MainLoss:0.0196 | Alpha:0.4953 | SPLoss:0.0116 | CLSLoss:0.0090 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1401 | MainLoss:0.1401 | SPLoss:0.0109 | CLSLoss:0.0090 | top1:95.5576 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1637 | MainLoss:1.1637 | SPLoss:0.0109 | CLSLoss:0.0090 | top1:68.8590 | AUROC:0.7780\n",
      "\n",
      "Epoch: [433 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0262 | MainLoss:0.0166 | Alpha:0.4954 | SPLoss:0.0104 | CLSLoss:0.0090 | top1:99.5175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1216 | MainLoss:0.1216 | SPLoss:0.0120 | CLSLoss:0.0090 | top1:96.2523 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2290 | MainLoss:1.2290 | SPLoss:0.0120 | CLSLoss:0.0090 | top1:68.1154 | AUROC:0.7728\n",
      "\n",
      "Epoch: [434 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0248 | MainLoss:0.0161 | Alpha:0.4960 | SPLoss:0.0085 | CLSLoss:0.0091 | top1:99.5556 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1693 | MainLoss:0.1693 | SPLoss:0.0208 | CLSLoss:0.0091 | top1:94.7757 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1301 | MainLoss:1.1301 | SPLoss:0.0208 | CLSLoss:0.0091 | top1:69.7051 | AUROC:0.7801\n",
      "\n",
      "Epoch: [435 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0275 | MainLoss:0.0164 | Alpha:0.4954 | SPLoss:0.0133 | CLSLoss:0.0091 | top1:99.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1068 | MainLoss:0.1068 | SPLoss:0.0168 | CLSLoss:0.0091 | top1:96.6417 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2980 | MainLoss:1.2980 | SPLoss:0.0168 | CLSLoss:0.0091 | top1:67.2179 | AUROC:0.7673\n",
      "\n",
      "Epoch: [436 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0248 | MainLoss:0.0163 | Alpha:0.4961 | SPLoss:0.0080 | CLSLoss:0.0091 | top1:99.4413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1391 | MainLoss:0.1391 | SPLoss:0.0131 | CLSLoss:0.0091 | top1:95.6231 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2228 | MainLoss:1.2228 | SPLoss:0.0131 | CLSLoss:0.0091 | top1:68.2692 | AUROC:0.7688\n",
      "\n",
      "Epoch: [437 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0324 | MainLoss:0.0210 | Alpha:0.4948 | SPLoss:0.0142 | CLSLoss:0.0090 | top1:99.3778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1072 | MainLoss:0.1072 | SPLoss:0.0162 | CLSLoss:0.0090 | top1:96.6324 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2689 | MainLoss:1.2689 | SPLoss:0.0162 | CLSLoss:0.0090 | top1:67.3333 | AUROC:0.7718\n",
      "\n",
      "Epoch: [438 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0317 | MainLoss:0.0208 | Alpha:0.4950 | SPLoss:0.0131 | CLSLoss:0.0089 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0915 | MainLoss:0.0915 | SPLoss:0.0200 | CLSLoss:0.0089 | top1:97.1682 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3225 | MainLoss:1.3225 | SPLoss:0.0200 | CLSLoss:0.0089 | top1:66.1410 | AUROC:0.7609\n",
      "\n",
      "Epoch: [439 | 1000] LR: 0.000025\n",
      "Train | 64/64 | Loss:0.0272 | MainLoss:0.0176 | Alpha:0.4956 | SPLoss:0.0105 | CLSLoss:0.0089 | top1:99.4794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1088 | MainLoss:0.1088 | SPLoss:0.0146 | CLSLoss:0.0089 | top1:96.6449 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2520 | MainLoss:1.2520 | SPLoss:0.0146 | CLSLoss:0.0089 | top1:67.2821 | AUROC:0.7663\n",
      "\n",
      "Epoch: [440 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0308 | MainLoss:0.0193 | Alpha:0.4954 | SPLoss:0.0143 | CLSLoss:0.0089 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0760 | MainLoss:0.0760 | SPLoss:0.0159 | CLSLoss:0.0089 | top1:97.6542 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.4028 | MainLoss:1.4028 | SPLoss:0.0159 | CLSLoss:0.0089 | top1:65.3205 | AUROC:0.7575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [441 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0250 | MainLoss:0.0162 | Alpha:0.4953 | SPLoss:0.0087 | CLSLoss:0.0090 | top1:99.5175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0924 | MainLoss:0.0924 | SPLoss:0.0156 | CLSLoss:0.0090 | top1:97.1869 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3494 | MainLoss:1.3494 | SPLoss:0.0156 | CLSLoss:0.0090 | top1:65.9487 | AUROC:0.7606\n",
      "\n",
      "Epoch: [442 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0291 | MainLoss:0.0180 | Alpha:0.4956 | SPLoss:0.0134 | CLSLoss:0.0089 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1008 | MainLoss:0.1008 | SPLoss:0.0153 | CLSLoss:0.0089 | top1:96.9439 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3384 | MainLoss:1.3384 | SPLoss:0.0153 | CLSLoss:0.0089 | top1:65.9359 | AUROC:0.7589\n",
      "\n",
      "Epoch: [443 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0267 | MainLoss:0.0167 | Alpha:0.4961 | SPLoss:0.0114 | CLSLoss:0.0089 | top1:99.5048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1055 | MainLoss:0.1055 | SPLoss:0.0144 | CLSLoss:0.0089 | top1:96.8224 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3172 | MainLoss:1.3172 | SPLoss:0.0144 | CLSLoss:0.0089 | top1:66.0769 | AUROC:0.7616\n",
      "\n",
      "Epoch: [444 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0314 | MainLoss:0.0208 | Alpha:0.4950 | SPLoss:0.0126 | CLSLoss:0.0088 | top1:99.4032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1289 | MainLoss:0.1289 | SPLoss:0.0221 | CLSLoss:0.0088 | top1:96.0997 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2042 | MainLoss:1.2042 | SPLoss:0.0221 | CLSLoss:0.0088 | top1:67.9359 | AUROC:0.7684\n",
      "\n",
      "Epoch: [445 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0267 | MainLoss:0.0183 | Alpha:0.4955 | SPLoss:0.0083 | CLSLoss:0.0087 | top1:99.5302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1246 | MainLoss:0.1246 | SPLoss:0.0142 | CLSLoss:0.0087 | top1:96.0592 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2161 | MainLoss:1.2161 | SPLoss:0.0142 | CLSLoss:0.0087 | top1:67.4103 | AUROC:0.7645\n",
      "\n",
      "Epoch: [446 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0251 | MainLoss:0.0160 | Alpha:0.4959 | SPLoss:0.0096 | CLSLoss:0.0087 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1287 | MainLoss:0.1287 | SPLoss:0.0157 | CLSLoss:0.0088 | top1:95.9346 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2025 | MainLoss:1.2025 | SPLoss:0.0157 | CLSLoss:0.0088 | top1:68.0000 | AUROC:0.7712\n",
      "\n",
      "Epoch: [447 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0273 | MainLoss:0.0174 | Alpha:0.4958 | SPLoss:0.0110 | CLSLoss:0.0088 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1366 | MainLoss:0.1366 | SPLoss:0.0167 | CLSLoss:0.0088 | top1:95.6604 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.1484 | MainLoss:1.1484 | SPLoss:0.0167 | CLSLoss:0.0088 | top1:68.5897 | AUROC:0.7761\n",
      "\n",
      "Epoch: [448 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0268 | MainLoss:0.0175 | Alpha:0.4956 | SPLoss:0.0099 | CLSLoss:0.0088 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1131 | MainLoss:0.1131 | SPLoss:0.0113 | CLSLoss:0.0089 | top1:96.4704 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2514 | MainLoss:1.2514 | SPLoss:0.0113 | CLSLoss:0.0089 | top1:67.5897 | AUROC:0.7645\n",
      "\n",
      "Epoch: [449 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0259 | MainLoss:0.0162 | Alpha:0.4961 | SPLoss:0.0107 | CLSLoss:0.0089 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0973 | MainLoss:0.0973 | SPLoss:0.0174 | CLSLoss:0.0089 | top1:96.8131 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2941 | MainLoss:1.2941 | SPLoss:0.0174 | CLSLoss:0.0089 | top1:66.8205 | AUROC:0.7610\n",
      "\n",
      "Epoch: [450 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0230 | MainLoss:0.0146 | Alpha:0.4963 | SPLoss:0.0081 | CLSLoss:0.0089 | top1:99.6318 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1218 | MainLoss:0.1218 | SPLoss:0.0107 | CLSLoss:0.0089 | top1:96.2243 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2824 | MainLoss:1.2824 | SPLoss:0.0107 | CLSLoss:0.0089 | top1:67.3846 | AUROC:0.7593\n",
      "\n",
      "Epoch: [451 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0297 | MainLoss:0.0190 | Alpha:0.4954 | SPLoss:0.0127 | CLSLoss:0.0089 | top1:99.3143 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1161 | MainLoss:0.1161 | SPLoss:0.0178 | CLSLoss:0.0089 | top1:96.2741 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2584 | MainLoss:1.2584 | SPLoss:0.0178 | CLSLoss:0.0089 | top1:67.5897 | AUROC:0.7620\n",
      "\n",
      "Epoch: [452 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0318 | MainLoss:0.0205 | Alpha:0.4950 | SPLoss:0.0140 | CLSLoss:0.0088 | top1:99.3397 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1219 | MainLoss:0.1219 | SPLoss:0.0153 | CLSLoss:0.0088 | top1:96.0685 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2336 | MainLoss:1.2336 | SPLoss:0.0153 | CLSLoss:0.0088 | top1:67.7564 | AUROC:0.7651\n",
      "\n",
      "Epoch: [453 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0289 | MainLoss:0.0189 | Alpha:0.4952 | SPLoss:0.0112 | CLSLoss:0.0088 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1371 | MainLoss:0.1371 | SPLoss:0.0183 | CLSLoss:0.0088 | top1:95.5670 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2267 | MainLoss:1.2267 | SPLoss:0.0183 | CLSLoss:0.0088 | top1:67.6410 | AUROC:0.7613\n",
      "\n",
      "Epoch: [454 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0295 | MainLoss:0.0198 | Alpha:0.4953 | SPLoss:0.0107 | CLSLoss:0.0088 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1130 | MainLoss:0.1130 | SPLoss:0.0183 | CLSLoss:0.0087 | top1:96.3209 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2648 | MainLoss:1.2648 | SPLoss:0.0183 | CLSLoss:0.0087 | top1:67.0128 | AUROC:0.7601\n",
      "\n",
      "Epoch: [455 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0285 | MainLoss:0.0188 | Alpha:0.4955 | SPLoss:0.0109 | CLSLoss:0.0088 | top1:99.3905 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0991 | MainLoss:0.0991 | SPLoss:0.0165 | CLSLoss:0.0088 | top1:96.7819 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2965 | MainLoss:1.2965 | SPLoss:0.0165 | CLSLoss:0.0088 | top1:66.4359 | AUROC:0.7555\n",
      "\n",
      "Epoch: [456 | 1000] LR: 0.000024\n",
      "Train | 64/64 | Loss:0.0220 | MainLoss:0.0137 | Alpha:0.4964 | SPLoss:0.0078 | CLSLoss:0.0088 | top1:99.5810 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1169 | MainLoss:0.1169 | SPLoss:0.0121 | CLSLoss:0.0088 | top1:96.2710 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2406 | MainLoss:1.2406 | SPLoss:0.0121 | CLSLoss:0.0088 | top1:67.1667 | AUROC:0.7608\n",
      "\n",
      "Epoch: [457 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0230 | MainLoss:0.0143 | Alpha:0.4966 | SPLoss:0.0088 | CLSLoss:0.0088 | top1:99.5683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1013 | MainLoss:0.1013 | SPLoss:0.0116 | CLSLoss:0.0089 | top1:96.8037 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3463 | MainLoss:1.3463 | SPLoss:0.0116 | CLSLoss:0.0089 | top1:65.9487 | AUROC:0.7533\n",
      "\n",
      "Epoch: [458 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0232 | MainLoss:0.0144 | Alpha:0.4964 | SPLoss:0.0088 | CLSLoss:0.0089 | top1:99.5048 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1164 | MainLoss:0.1164 | SPLoss:0.0170 | CLSLoss:0.0089 | top1:96.3489 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3232 | MainLoss:1.3232 | SPLoss:0.0170 | CLSLoss:0.0089 | top1:66.5897 | AUROC:0.7534\n",
      "\n",
      "Epoch: [459 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0307 | MainLoss:0.0216 | Alpha:0.4948 | SPLoss:0.0095 | CLSLoss:0.0089 | top1:99.3524 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1288 | MainLoss:0.1288 | SPLoss:0.0192 | CLSLoss:0.0088 | top1:95.8941 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2469 | MainLoss:1.2469 | SPLoss:0.0192 | CLSLoss:0.0088 | top1:67.3462 | AUROC:0.7581\n",
      "\n",
      "Epoch: [460 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0304 | MainLoss:0.0197 | Alpha:0.4952 | SPLoss:0.0130 | CLSLoss:0.0087 | top1:99.3016 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1132 | MainLoss:0.1132 | SPLoss:0.0180 | CLSLoss:0.0087 | top1:96.3209 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2265 | MainLoss:1.2265 | SPLoss:0.0180 | CLSLoss:0.0087 | top1:67.6667 | AUROC:0.7635\n",
      "\n",
      "Epoch: [461 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0299 | MainLoss:0.0196 | Alpha:0.4953 | SPLoss:0.0121 | CLSLoss:0.0087 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1253 | MainLoss:0.1253 | SPLoss:0.0136 | CLSLoss:0.0087 | top1:95.9782 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.1932 | MainLoss:1.1932 | SPLoss:0.0136 | CLSLoss:0.0087 | top1:68.0513 | AUROC:0.7651\n",
      "\n",
      "Epoch: [462 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0278 | MainLoss:0.0185 | Alpha:0.4954 | SPLoss:0.0100 | CLSLoss:0.0087 | top1:99.4540 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1265 | MainLoss:0.1265 | SPLoss:0.0160 | CLSLoss:0.0087 | top1:95.9782 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2310 | MainLoss:1.2310 | SPLoss:0.0160 | CLSLoss:0.0087 | top1:67.6667 | AUROC:0.7617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [463 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0264 | MainLoss:0.0180 | Alpha:0.4957 | SPLoss:0.0083 | CLSLoss:0.0087 | top1:99.4794 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1369 | MainLoss:0.1369 | SPLoss:0.0128 | CLSLoss:0.0086 | top1:95.6293 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.1834 | MainLoss:1.1834 | SPLoss:0.0128 | CLSLoss:0.0086 | top1:67.7821 | AUROC:0.7608\n",
      "\n",
      "Epoch: [464 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0274 | MainLoss:0.0179 | Alpha:0.4953 | SPLoss:0.0106 | CLSLoss:0.0086 | top1:99.5175 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0978 | MainLoss:0.0978 | SPLoss:0.0142 | CLSLoss:0.0086 | top1:96.8567 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2750 | MainLoss:1.2750 | SPLoss:0.0142 | CLSLoss:0.0086 | top1:66.9103 | AUROC:0.7643\n",
      "\n",
      "Epoch: [465 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0235 | MainLoss:0.0143 | Alpha:0.4965 | SPLoss:0.0099 | CLSLoss:0.0087 | top1:99.6064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1088 | MainLoss:0.1088 | SPLoss:0.0118 | CLSLoss:0.0087 | top1:96.5826 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2385 | MainLoss:1.2385 | SPLoss:0.0118 | CLSLoss:0.0087 | top1:67.6538 | AUROC:0.7678\n",
      "\n",
      "Epoch: [466 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0194 | MainLoss:0.0117 | Alpha:0.4971 | SPLoss:0.0069 | CLSLoss:0.0087 | top1:99.6825 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0894 | MainLoss:0.0894 | SPLoss:0.0100 | CLSLoss:0.0088 | top1:97.2835 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3439 | MainLoss:1.3439 | SPLoss:0.0100 | CLSLoss:0.0088 | top1:66.6410 | AUROC:0.7593\n",
      "\n",
      "Epoch: [467 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0236 | MainLoss:0.0136 | Alpha:0.4966 | SPLoss:0.0113 | CLSLoss:0.0088 | top1:99.5302 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1189 | MainLoss:0.1189 | SPLoss:0.0143 | CLSLoss:0.0088 | top1:96.3209 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2810 | MainLoss:1.2810 | SPLoss:0.0143 | CLSLoss:0.0088 | top1:67.3846 | AUROC:0.7627\n",
      "\n",
      "Epoch: [468 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0194 | MainLoss:0.0110 | Alpha:0.4973 | SPLoss:0.0082 | CLSLoss:0.0088 | top1:99.6698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1171 | MainLoss:0.1171 | SPLoss:0.0096 | CLSLoss:0.0088 | top1:96.3676 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3194 | MainLoss:1.3194 | SPLoss:0.0096 | CLSLoss:0.0088 | top1:67.2949 | AUROC:0.7592\n",
      "\n",
      "Epoch: [469 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0306 | MainLoss:0.0200 | Alpha:0.4952 | SPLoss:0.0126 | CLSLoss:0.0088 | top1:99.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1184 | MainLoss:0.1184 | SPLoss:0.0157 | CLSLoss:0.0087 | top1:96.3240 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2885 | MainLoss:1.2885 | SPLoss:0.0157 | CLSLoss:0.0087 | top1:66.9487 | AUROC:0.7612\n",
      "\n",
      "Epoch: [470 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0277 | MainLoss:0.0178 | Alpha:0.4959 | SPLoss:0.0113 | CLSLoss:0.0087 | top1:99.3778 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1085 | MainLoss:0.1085 | SPLoss:0.0193 | CLSLoss:0.0086 | top1:96.6511 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2927 | MainLoss:1.2927 | SPLoss:0.0193 | CLSLoss:0.0086 | top1:66.8974 | AUROC:0.7616\n",
      "\n",
      "Epoch: [471 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0262 | MainLoss:0.0168 | Alpha:0.4960 | SPLoss:0.0102 | CLSLoss:0.0086 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1032 | MainLoss:0.1032 | SPLoss:0.0150 | CLSLoss:0.0086 | top1:96.7227 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3109 | MainLoss:1.3109 | SPLoss:0.0150 | CLSLoss:0.0086 | top1:66.4103 | AUROC:0.7558\n",
      "\n",
      "Epoch: [472 | 1000] LR: 0.000023\n",
      "Train | 64/64 | Loss:0.0261 | MainLoss:0.0169 | Alpha:0.4959 | SPLoss:0.0100 | CLSLoss:0.0086 | top1:99.4413 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1414 | MainLoss:0.1414 | SPLoss:0.0162 | CLSLoss:0.0086 | top1:95.5389 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2073 | MainLoss:1.2073 | SPLoss:0.0162 | CLSLoss:0.0086 | top1:67.8718 | AUROC:0.7646\n",
      "\n",
      "Epoch: [473 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0224 | MainLoss:0.0142 | Alpha:0.4963 | SPLoss:0.0079 | CLSLoss:0.0086 | top1:99.5429 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1108 | MainLoss:0.1108 | SPLoss:0.0128 | CLSLoss:0.0086 | top1:96.5483 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3143 | MainLoss:1.3143 | SPLoss:0.0128 | CLSLoss:0.0086 | top1:66.6795 | AUROC:0.7588\n",
      "\n",
      "Epoch: [474 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0225 | MainLoss:0.0146 | Alpha:0.4965 | SPLoss:0.0072 | CLSLoss:0.0086 | top1:99.5429 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0933 | MainLoss:0.0933 | SPLoss:0.0127 | CLSLoss:0.0086 | top1:97.0810 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3861 | MainLoss:1.3861 | SPLoss:0.0127 | CLSLoss:0.0086 | top1:65.8205 | AUROC:0.7531\n",
      "\n",
      "Epoch: [475 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0233 | MainLoss:0.0138 | Alpha:0.4965 | SPLoss:0.0103 | CLSLoss:0.0086 | top1:99.6064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1100 | MainLoss:0.1100 | SPLoss:0.0137 | CLSLoss:0.0087 | top1:96.5421 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3618 | MainLoss:1.3618 | SPLoss:0.0137 | CLSLoss:0.0087 | top1:66.4615 | AUROC:0.7536\n",
      "\n",
      "Epoch: [476 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0272 | MainLoss:0.0175 | Alpha:0.4958 | SPLoss:0.0109 | CLSLoss:0.0086 | top1:99.4159 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1294 | MainLoss:0.1294 | SPLoss:0.0200 | CLSLoss:0.0086 | top1:95.8910 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2944 | MainLoss:1.2944 | SPLoss:0.0200 | CLSLoss:0.0086 | top1:66.9231 | AUROC:0.7512\n",
      "\n",
      "Epoch: [477 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0222 | MainLoss:0.0138 | Alpha:0.4964 | SPLoss:0.0083 | CLSLoss:0.0086 | top1:99.6064 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1157 | MainLoss:0.1157 | SPLoss:0.0120 | CLSLoss:0.0086 | top1:96.3115 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3264 | MainLoss:1.3264 | SPLoss:0.0120 | CLSLoss:0.0086 | top1:66.1154 | AUROC:0.7492\n",
      "\n",
      "Epoch: [478 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0316 | MainLoss:0.0197 | Alpha:0.4954 | SPLoss:0.0155 | CLSLoss:0.0086 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0909 | MainLoss:0.0909 | SPLoss:0.0212 | CLSLoss:0.0086 | top1:97.0997 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3881 | MainLoss:1.3881 | SPLoss:0.0212 | CLSLoss:0.0086 | top1:65.4231 | AUROC:0.7440\n",
      "\n",
      "Epoch: [479 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0262 | MainLoss:0.0173 | Alpha:0.4958 | SPLoss:0.0095 | CLSLoss:0.0086 | top1:99.4032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0805 | MainLoss:0.0805 | SPLoss:0.0172 | CLSLoss:0.0086 | top1:97.4330 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.4032 | MainLoss:1.4032 | SPLoss:0.0172 | CLSLoss:0.0086 | top1:64.8718 | AUROC:0.7464\n",
      "\n",
      "Epoch: [480 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0238 | MainLoss:0.0146 | Alpha:0.4963 | SPLoss:0.0101 | CLSLoss:0.0086 | top1:99.5937 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1126 | MainLoss:0.1126 | SPLoss:0.0128 | CLSLoss:0.0086 | top1:96.4891 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.2869 | MainLoss:1.2869 | SPLoss:0.0128 | CLSLoss:0.0086 | top1:66.8205 | AUROC:0.7558\n",
      "\n",
      "Epoch: [481 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0210 | MainLoss:0.0134 | Alpha:0.4968 | SPLoss:0.0066 | CLSLoss:0.0086 | top1:99.5683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1004 | MainLoss:0.1004 | SPLoss:0.0133 | CLSLoss:0.0087 | top1:96.8411 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3411 | MainLoss:1.3411 | SPLoss:0.0133 | CLSLoss:0.0087 | top1:66.4231 | AUROC:0.7584\n",
      "\n",
      "Epoch: [482 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0317 | MainLoss:0.0215 | Alpha:0.4949 | SPLoss:0.0121 | CLSLoss:0.0086 | top1:99.2889 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0690 | MainLoss:0.0690 | SPLoss:0.0173 | CLSLoss:0.0085 | top1:97.8037 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.4732 | MainLoss:1.4732 | SPLoss:0.0173 | CLSLoss:0.0085 | top1:64.1026 | AUROC:0.7488\n",
      "\n",
      "Epoch: [483 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0244 | MainLoss:0.0146 | Alpha:0.4956 | SPLoss:0.0112 | CLSLoss:0.0085 | top1:99.5429 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0960 | MainLoss:0.0960 | SPLoss:0.0107 | CLSLoss:0.0085 | top1:97.0405 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3527 | MainLoss:1.3527 | SPLoss:0.0107 | CLSLoss:0.0085 | top1:66.2179 | AUROC:0.7574\n",
      "\n",
      "Epoch: [484 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0276 | MainLoss:0.0177 | Alpha:0.4955 | SPLoss:0.0115 | CLSLoss:0.0085 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0933 | MainLoss:0.0933 | SPLoss:0.0177 | CLSLoss:0.0085 | top1:97.0717 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3546 | MainLoss:1.3546 | SPLoss:0.0177 | CLSLoss:0.0085 | top1:66.0128 | AUROC:0.7566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [485 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0212 | MainLoss:0.0135 | Alpha:0.4967 | SPLoss:0.0071 | CLSLoss:0.0085 | top1:99.5683 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0941 | MainLoss:0.0941 | SPLoss:0.0150 | CLSLoss:0.0085 | top1:97.0685 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3693 | MainLoss:1.3693 | SPLoss:0.0150 | CLSLoss:0.0085 | top1:65.8077 | AUROC:0.7595\n",
      "\n",
      "Epoch: [486 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0262 | MainLoss:0.0164 | Alpha:0.4959 | SPLoss:0.0114 | CLSLoss:0.0085 | top1:99.4667 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0665 | MainLoss:0.0665 | SPLoss:0.0156 | CLSLoss:0.0085 | top1:97.9844 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.4842 | MainLoss:1.4842 | SPLoss:0.0156 | CLSLoss:0.0085 | top1:64.3718 | AUROC:0.7481\n",
      "\n",
      "Epoch: [487 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0278 | MainLoss:0.0174 | Alpha:0.4950 | SPLoss:0.0126 | CLSLoss:0.0085 | top1:99.3651 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1211 | MainLoss:0.1211 | SPLoss:0.0197 | CLSLoss:0.0085 | top1:96.1994 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.2626 | MainLoss:1.2626 | SPLoss:0.0197 | CLSLoss:0.0085 | top1:66.5256 | AUROC:0.7565\n",
      "\n",
      "Epoch: [488 | 1000] LR: 0.000022\n",
      "Train | 64/64 | Loss:0.0298 | MainLoss:0.0193 | Alpha:0.4951 | SPLoss:0.0127 | CLSLoss:0.0085 | top1:99.4032 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1058 | MainLoss:0.1058 | SPLoss:0.0116 | CLSLoss:0.0084 | top1:96.6511 | AUROC:0.9997\n",
      "Test | 63/64 | Loss:1.3023 | MainLoss:1.3023 | SPLoss:0.0116 | CLSLoss:0.0084 | top1:66.2436 | AUROC:0.7559\n",
      "\n",
      "Epoch: [489 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0196 | MainLoss:0.0121 | Alpha:0.4968 | SPLoss:0.0067 | CLSLoss:0.0085 | top1:99.6698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1153 | MainLoss:0.1153 | SPLoss:0.0108 | CLSLoss:0.0085 | top1:96.4579 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3312 | MainLoss:1.3312 | SPLoss:0.0108 | CLSLoss:0.0085 | top1:66.6154 | AUROC:0.7517\n",
      "\n",
      "Epoch: [490 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0308 | MainLoss:0.0194 | Alpha:0.4954 | SPLoss:0.0145 | CLSLoss:0.0085 | top1:99.3270 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.1141 | MainLoss:0.1141 | SPLoss:0.0150 | CLSLoss:0.0084 | top1:96.4548 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3050 | MainLoss:1.3050 | SPLoss:0.0150 | CLSLoss:0.0084 | top1:66.6923 | AUROC:0.7543\n",
      "\n",
      "Epoch: [491 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0188 | MainLoss:0.0114 | Alpha:0.4971 | SPLoss:0.0064 | CLSLoss:0.0085 | top1:99.6698 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0954 | MainLoss:0.0954 | SPLoss:0.0161 | CLSLoss:0.0085 | top1:97.0405 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3630 | MainLoss:1.3630 | SPLoss:0.0161 | CLSLoss:0.0085 | top1:66.4359 | AUROC:0.7596\n",
      "\n",
      "Epoch: [492 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0218 | MainLoss:0.0125 | Alpha:0.4971 | SPLoss:0.0104 | CLSLoss:0.0085 | top1:99.6190 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0945 | MainLoss:0.0945 | SPLoss:0.0139 | CLSLoss:0.0085 | top1:97.0966 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3738 | MainLoss:1.3738 | SPLoss:0.0139 | CLSLoss:0.0085 | top1:66.0769 | AUROC:0.7547\n",
      "\n",
      "Epoch: [493 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0188 | MainLoss:0.0107 | Alpha:0.4973 | SPLoss:0.0078 | CLSLoss:0.0085 | top1:99.6952 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0999 | MainLoss:0.0999 | SPLoss:0.0097 | CLSLoss:0.0085 | top1:96.9813 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3903 | MainLoss:1.3903 | SPLoss:0.0097 | CLSLoss:0.0085 | top1:66.1538 | AUROC:0.7567\n",
      "\n",
      "Epoch: [494 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0240 | MainLoss:0.0149 | Alpha:0.4963 | SPLoss:0.0098 | CLSLoss:0.0085 | top1:99.5429 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0945 | MainLoss:0.0945 | SPLoss:0.0146 | CLSLoss:0.0085 | top1:97.1246 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3914 | MainLoss:1.3914 | SPLoss:0.0146 | CLSLoss:0.0085 | top1:65.8077 | AUROC:0.7572\n",
      "\n",
      "Epoch: [495 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0264 | MainLoss:0.0168 | Alpha:0.4959 | SPLoss:0.0108 | CLSLoss:0.0084 | top1:99.4921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0899 | MainLoss:0.0899 | SPLoss:0.0159 | CLSLoss:0.0084 | top1:97.2274 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3850 | MainLoss:1.3850 | SPLoss:0.0159 | CLSLoss:0.0084 | top1:65.6538 | AUROC:0.7543\n",
      "\n",
      "Epoch: [496 | 1000] LR: 0.000021\n",
      "Train | 64/64 | Loss:0.0246 | MainLoss:0.0161 | Alpha:0.4962 | SPLoss:0.0086 | CLSLoss:0.0084 | top1:99.4921 | AUROC:0.0000\n",
      "Test | 257/64 | Loss:0.0995 | MainLoss:0.0995 | SPLoss:0.0150 | CLSLoss:0.0084 | top1:96.9065 | AUROC:0.9998\n",
      "Test | 63/64 | Loss:1.3373 | MainLoss:1.3373 | SPLoss:0.0150 | CLSLoss:0.0084 | top1:66.2051 | AUROC:0.7534\n",
      "\n",
      "Epoch: [497 | 1000] LR: 0.000021\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
