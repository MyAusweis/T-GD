{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = './log/pggan/128/b1_2/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b1' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 300\n",
    "start_epoch = 0\n",
    "train_batch = 190\n",
    "test_batch = 190\n",
    "lr = 0.1\n",
    "schedule = [20, 75, 125, 175]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b1_2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.02, 0.02), scale=(0.98, 1.02), shear=2, fillcolor=(124,117,104)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 6.52M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    bar = Bar('Processing', max=len(train_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} '.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(train_loader),\n",
    "                    data=data_time.val,\n",
    "                    bt=batch_time.val,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "                 batch=batch_idx+1, size=len(train_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(val_loader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:} | top1: {top1:}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(val_loader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,)\n",
    "        bar.next()\n",
    "    print('{batch}/{size} Data:{data:.3f} | Batch:{bt:.3f} | Total:{total:} | ETA:{eta:} | Loss:{loss:} | top1:{tp1:}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), data=data_time.val, bt=batch_time.val, total=bar.elapsed_td, eta=bar.eta_td, loss=losses.avg, tp1=top1.avg))\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [190 | 300] LR: 0.000073\n",
      "1/676 Data:0.768 | Batch:5.641 | Total:0:00:05 | ETA:1:03:29 | Loss:0.14894811809062958 | top1:93.68421173095703\n",
      "11/676 Data:0.001 | Batch:0.665 | Total:0:00:12 | ETA:0:13:12 | Loss:0.11345696246082132 | top1:95.2153091430664\n",
      "21/676 Data:0.001 | Batch:0.666 | Total:0:00:19 | ETA:0:07:15 | Loss:0.11436259249846141 | top1:95.43859100341797\n",
      "31/676 Data:0.002 | Batch:0.657 | Total:0:00:25 | ETA:0:07:08 | Loss:0.11889133506244229 | top1:95.07640075683594\n",
      "41/676 Data:0.003 | Batch:0.660 | Total:0:00:32 | ETA:0:06:59 | Loss:0.12211005934854834 | top1:94.83953857421875\n",
      "51/676 Data:0.002 | Batch:0.667 | Total:0:00:39 | ETA:0:06:52 | Loss:0.12064347766778048 | top1:94.89164733886719\n",
      "61/676 Data:0.001 | Batch:0.664 | Total:0:00:45 | ETA:0:06:50 | Loss:0.12412355205074685 | top1:94.7195816040039\n",
      "71/676 Data:0.002 | Batch:0.663 | Total:0:00:52 | ETA:0:06:42 | Loss:0.12249374484092417 | top1:94.76649475097656\n",
      "81/676 Data:0.001 | Batch:0.662 | Total:0:00:58 | ETA:0:06:36 | Loss:0.12266828368107478 | top1:94.76283264160156\n",
      "91/676 Data:0.003 | Batch:0.675 | Total:0:01:05 | ETA:0:06:28 | Loss:0.12318609925089302 | top1:94.7484130859375\n",
      "101/676 Data:0.002 | Batch:0.665 | Total:0:01:12 | ETA:0:06:24 | Loss:0.12589803662630591 | top1:94.59614562988281\n",
      "111/676 Data:0.001 | Batch:0.665 | Total:0:01:18 | ETA:0:06:16 | Loss:0.1254288038691959 | top1:94.63726806640625\n",
      "121/676 Data:0.003 | Batch:0.662 | Total:0:01:25 | ETA:0:06:09 | Loss:0.12509125643525243 | top1:94.66725158691406\n",
      "131/676 Data:0.002 | Batch:0.660 | Total:0:01:32 | ETA:0:06:02 | Loss:0.1259980320589233 | top1:94.64041900634766\n",
      "141/676 Data:0.001 | Batch:0.666 | Total:0:01:38 | ETA:0:05:55 | Loss:0.12606272756630646 | top1:94.65845489501953\n",
      "151/676 Data:0.002 | Batch:0.665 | Total:0:01:45 | ETA:0:05:50 | Loss:0.12593825136786266 | top1:94.6775894165039\n",
      "161/676 Data:0.003 | Batch:0.665 | Total:0:01:52 | ETA:0:05:42 | Loss:0.12597653954665852 | top1:94.68781280517578\n",
      "171/676 Data:0.001 | Batch:0.662 | Total:0:01:58 | ETA:0:05:36 | Loss:0.12591211959632517 | top1:94.69683074951172\n",
      "181/676 Data:0.002 | Batch:0.662 | Total:0:02:05 | ETA:0:05:29 | Loss:0.1262508100979236 | top1:94.68740844726562\n",
      "191/676 Data:0.002 | Batch:0.662 | Total:0:02:11 | ETA:0:05:21 | Loss:0.12694839626082574 | top1:94.67070770263672\n",
      "201/676 Data:0.002 | Batch:0.655 | Total:0:02:18 | ETA:0:05:14 | Loss:0.1263345473663724 | top1:94.68971252441406\n",
      "211/676 Data:0.002 | Batch:0.662 | Total:0:02:25 | ETA:0:05:07 | Loss:0.12630023916750724 | top1:94.70441436767578\n",
      "221/676 Data:0.002 | Batch:0.661 | Total:0:02:31 | ETA:0:05:01 | Loss:0.12606173130047268 | top1:94.72254943847656\n",
      "231/676 Data:0.002 | Batch:0.662 | Total:0:02:38 | ETA:0:04:54 | Loss:0.1258643219868342 | top1:94.73001098632812\n",
      "241/676 Data:0.002 | Batch:0.662 | Total:0:02:45 | ETA:0:04:49 | Loss:0.12529015621456369 | top1:94.78269958496094\n",
      "251/676 Data:0.002 | Batch:0.662 | Total:0:02:51 | ETA:0:04:42 | Loss:0.12533688853936367 | top1:94.79345703125\n",
      "261/676 Data:0.001 | Batch:0.663 | Total:0:02:58 | ETA:0:04:35 | Loss:0.12486101187394497 | top1:94.82556915283203\n",
      "271/676 Data:0.002 | Batch:0.664 | Total:0:03:04 | ETA:0:04:30 | Loss:0.12495913749691305 | top1:94.81258392333984\n",
      "281/676 Data:0.002 | Batch:0.661 | Total:0:03:11 | ETA:0:04:22 | Loss:0.12504196005253604 | top1:94.83611297607422\n",
      "291/676 Data:0.001 | Batch:0.662 | Total:0:03:18 | ETA:0:04:16 | Loss:0.12467402593581538 | top1:94.82366180419922\n",
      "301/676 Data:0.001 | Batch:0.661 | Total:0:03:24 | ETA:0:04:08 | Loss:0.12470654378400689 | top1:94.843505859375\n",
      "311/676 Data:0.002 | Batch:0.662 | Total:0:03:31 | ETA:0:04:02 | Loss:0.12476520007925401 | top1:94.84683990478516\n",
      "321/676 Data:0.002 | Batch:0.664 | Total:0:03:37 | ETA:0:03:55 | Loss:0.12498740617452753 | top1:94.8302993774414\n",
      "331/676 Data:0.002 | Batch:0.663 | Total:0:03:44 | ETA:0:03:50 | Loss:0.1250262286997994 | top1:94.82747650146484\n",
      "341/676 Data:0.002 | Batch:0.660 | Total:0:03:51 | ETA:0:03:42 | Loss:0.12487686643939563 | top1:94.82635498046875\n",
      "351/676 Data:0.001 | Batch:0.663 | Total:0:03:57 | ETA:0:03:36 | Loss:0.12508590699855418 | top1:94.82530975341797\n",
      "361/676 Data:0.001 | Batch:0.662 | Total:0:04:04 | ETA:0:03:29 | Loss:0.12514066006833496 | top1:94.81556701660156\n",
      "371/676 Data:0.002 | Batch:0.661 | Total:0:04:11 | ETA:0:03:23 | Loss:0.12571895098429164 | top1:94.79784393310547\n",
      "381/676 Data:0.003 | Batch:0.670 | Total:0:04:17 | ETA:0:03:16 | Loss:0.12563365709593916 | top1:94.80177307128906\n",
      "391/676 Data:0.002 | Batch:0.663 | Total:0:04:24 | ETA:0:03:09 | Loss:0.12515859820348832 | top1:94.82164001464844\n",
      "401/676 Data:0.002 | Batch:0.651 | Total:0:04:31 | ETA:0:03:02 | Loss:0.12541044791440417 | top1:94.81427764892578\n",
      "411/676 Data:0.001 | Batch:0.660 | Total:0:04:37 | ETA:0:02:56 | Loss:0.12532937905576688 | top1:94.8098373413086\n",
      "421/676 Data:0.001 | Batch:0.663 | Total:0:04:44 | ETA:0:02:49 | Loss:0.12531402728336724 | top1:94.81685638427734\n",
      "431/676 Data:0.001 | Batch:0.661 | Total:0:04:50 | ETA:0:02:43 | Loss:0.12515759099871265 | top1:94.82843017578125\n",
      "441/676 Data:0.002 | Batch:0.665 | Total:0:04:57 | ETA:0:02:36 | Loss:0.1250587193265794 | top1:94.82515716552734\n",
      "451/676 Data:0.001 | Batch:0.661 | Total:0:05:04 | ETA:0:02:30 | Loss:0.1254070003386876 | top1:94.80219268798828\n",
      "461/676 Data:0.001 | Batch:0.662 | Total:0:05:10 | ETA:0:02:23 | Loss:0.12513708909912896 | top1:94.81790161132812\n",
      "471/676 Data:0.001 | Batch:0.662 | Total:0:05:17 | ETA:0:02:16 | Loss:0.12485365694558798 | top1:94.82958984375\n",
      "481/676 Data:0.001 | Batch:0.667 | Total:0:05:24 | ETA:0:02:10 | Loss:0.12498304867447042 | top1:94.83203887939453\n",
      "491/676 Data:0.001 | Batch:0.660 | Total:0:05:30 | ETA:0:02:03 | Loss:0.12477943289061427 | top1:94.83653259277344\n",
      "501/676 Data:0.002 | Batch:0.663 | Total:0:05:37 | ETA:0:01:56 | Loss:0.12484367136767287 | top1:94.84504699707031\n",
      "511/676 Data:0.002 | Batch:0.660 | Total:0:05:43 | ETA:0:01:50 | Loss:0.12476672522300844 | top1:94.84910583496094\n",
      "521/676 Data:0.002 | Batch:0.665 | Total:0:05:50 | ETA:0:01:43 | Loss:0.12504101556059039 | top1:94.83382415771484\n",
      "531/676 Data:0.002 | Batch:0.661 | Total:0:05:57 | ETA:0:01:36 | Loss:0.12477800518751368 | top1:94.85281372070312\n",
      "541/676 Data:0.002 | Batch:0.661 | Total:0:06:03 | ETA:0:01:30 | Loss:0.12486157894768027 | top1:94.84385681152344\n",
      "551/676 Data:0.001 | Batch:0.664 | Total:0:06:10 | ETA:0:01:23 | Loss:0.12505818883839406 | top1:94.8352279663086\n",
      "561/676 Data:0.003 | Batch:0.662 | Total:0:06:16 | ETA:0:01:17 | Loss:0.12543692978334725 | top1:94.81471252441406\n",
      "571/676 Data:0.002 | Batch:0.661 | Total:0:06:23 | ETA:0:01:10 | Loss:0.1254261486858599 | top1:94.81150817871094\n",
      "581/676 Data:0.002 | Batch:0.661 | Total:0:06:30 | ETA:0:01:03 | Loss:0.1257941457739488 | top1:94.79029083251953\n",
      "591/676 Data:0.002 | Batch:0.661 | Total:0:06:36 | ETA:0:00:57 | Loss:0.12615017153562225 | top1:94.77336120605469\n",
      "601/676 Data:0.002 | Batch:0.660 | Total:0:06:43 | ETA:0:00:50 | Loss:0.1262288151894651 | top1:94.76399230957031\n",
      "611/676 Data:0.002 | Batch:0.661 | Total:0:06:50 | ETA:0:00:44 | Loss:0.12621303578860046 | top1:94.77129364013672\n",
      "621/676 Data:0.002 | Batch:0.662 | Total:0:06:56 | ETA:0:00:37 | Loss:0.12627470858647052 | top1:94.76396179199219\n",
      "631/676 Data:0.003 | Batch:0.661 | Total:0:07:03 | ETA:0:00:30 | Loss:0.1265762558180349 | top1:94.74435424804688\n",
      "641/676 Data:0.001 | Batch:0.660 | Total:0:07:09 | ETA:0:00:24 | Loss:0.12668475154758244 | top1:94.74176788330078\n",
      "651/676 Data:0.002 | Batch:0.662 | Total:0:07:16 | ETA:0:00:17 | Loss:0.1269832407331778 | top1:94.73360443115234\n",
      "661/676 Data:0.002 | Batch:0.658 | Total:0:07:23 | ETA:0:00:10 | Loss:0.12679396802489046 | top1:94.74162292480469\n",
      "671/676 Data:0.004 | Batch:0.674 | Total:0:07:29 | ETA:0:00:04 | Loss:0.12714842203366064 | top1:94.72586059570312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cutz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 Data:0.002 | Batch:1.415 | Total:0:00:32 | ETA:0:00:00 | Loss:0.048176510560955864 | top1:98.17134094238281\n",
      "\n",
      "Epoch: [191 | 300] LR: 0.000017\n",
      "1/676 Data:0.775 | Batch:1.686 | Total:0:00:01 | ETA:0:19:00 | Loss:0.1032135933637619 | top1:96.84210968017578\n",
      "11/676 Data:0.002 | Batch:0.664 | Total:0:00:08 | ETA:0:08:50 | Loss:0.11978998441587795 | top1:95.55023956298828\n",
      "21/676 Data:0.003 | Batch:0.660 | Total:0:00:15 | ETA:0:07:13 | Loss:0.11676709353923798 | top1:95.41352844238281\n",
      "31/676 Data:0.002 | Batch:0.664 | Total:0:00:21 | ETA:0:07:07 | Loss:0.11599493723723196 | top1:95.39898681640625\n",
      "41/676 Data:0.002 | Batch:0.661 | Total:0:00:28 | ETA:0:07:02 | Loss:0.11721923129587639 | top1:95.37869262695312\n",
      "51/676 Data:0.002 | Batch:0.661 | Total:0:00:35 | ETA:0:06:56 | Loss:0.11902291678330477 | top1:95.22187805175781\n",
      "61/676 Data:0.002 | Batch:0.663 | Total:0:00:41 | ETA:0:06:48 | Loss:0.11885816237477005 | top1:95.25452423095703\n",
      "71/676 Data:0.003 | Batch:0.670 | Total:0:00:48 | ETA:0:06:42 | Loss:0.1188571282045942 | top1:95.22608947753906\n",
      "81/676 Data:0.002 | Batch:0.664 | Total:0:00:55 | ETA:0:06:36 | Loss:0.12114328771461676 | top1:95.11370849609375\n",
      "91/676 Data:0.002 | Batch:0.662 | Total:0:01:01 | ETA:0:06:29 | Loss:0.12185889801808766 | top1:95.09542846679688\n",
      "101/676 Data:0.001 | Batch:0.664 | Total:0:01:08 | ETA:0:06:22 | Loss:0.12258976070892692 | top1:95.03387451171875\n",
      "111/676 Data:0.002 | Batch:0.656 | Total:0:01:14 | ETA:0:06:14 | Loss:0.12332038849860698 | top1:94.99288940429688\n",
      "121/676 Data:0.002 | Batch:0.660 | Total:0:01:21 | ETA:0:06:06 | Loss:0.12244140787804422 | top1:95.03697204589844\n",
      "131/676 Data:0.002 | Batch:0.662 | Total:0:01:28 | ETA:0:06:00 | Loss:0.12287687527314398 | top1:95.00602722167969\n",
      "141/676 Data:0.001 | Batch:0.665 | Total:0:01:34 | ETA:0:05:54 | Loss:0.12289459839568916 | top1:94.9869384765625\n",
      "151/676 Data:0.002 | Batch:0.662 | Total:0:01:41 | ETA:0:05:47 | Loss:0.12251944612983047 | top1:94.97734832763672\n",
      "161/676 Data:0.002 | Batch:0.660 | Total:0:01:47 | ETA:0:05:41 | Loss:0.1224288655826764 | top1:94.98856353759766\n",
      "171/676 Data:0.002 | Batch:0.660 | Total:0:01:54 | ETA:0:05:34 | Loss:0.12197125053893752 | top1:95.00769805908203\n",
      "181/676 Data:0.002 | Batch:0.658 | Total:0:02:01 | ETA:0:05:28 | Loss:0.12293320939521105 | top1:94.97237396240234\n",
      "191/676 Data:0.002 | Batch:0.661 | Total:0:02:07 | ETA:0:05:21 | Loss:0.12279029553317275 | top1:94.96831512451172\n",
      "201/676 Data:0.002 | Batch:0.660 | Total:0:02:14 | ETA:0:05:15 | Loss:0.1231406647989999 | top1:94.96988677978516\n",
      "211/676 Data:0.002 | Batch:0.659 | Total:0:02:20 | ETA:0:05:08 | Loss:0.1234743740250714 | top1:94.96632385253906\n",
      "221/676 Data:0.001 | Batch:0.664 | Total:0:02:27 | ETA:0:05:02 | Loss:0.12362521822770796 | top1:94.9368896484375\n",
      "231/676 Data:0.002 | Batch:0.661 | Total:0:02:34 | ETA:0:04:56 | Loss:0.1233421235438033 | top1:94.92595672607422\n",
      "241/676 Data:0.002 | Batch:0.664 | Total:0:02:40 | ETA:0:04:49 | Loss:0.12355868209199787 | top1:94.92247009277344\n",
      "251/676 Data:0.002 | Batch:0.666 | Total:0:02:47 | ETA:0:04:42 | Loss:0.12513758147142798 | top1:94.87104034423828\n",
      "261/676 Data:0.002 | Batch:0.628 | Total:0:02:53 | ETA:0:03:58 | Loss:0.12516692611906263 | top1:94.87800598144531\n",
      "271/676 Data:0.002 | Batch:0.631 | Total:0:02:59 | ETA:0:04:15 | Loss:0.12479239754461274 | top1:94.89026641845703\n",
      "281/676 Data:0.002 | Batch:0.628 | Total:0:03:05 | ETA:0:04:09 | Loss:0.12459134663127071 | top1:94.88106536865234\n",
      "291/676 Data:0.002 | Batch:0.620 | Total:0:03:12 | ETA:0:04:04 | Loss:0.12474304440197666 | top1:94.86344909667969\n",
      "301/676 Data:0.002 | Batch:0.501 | Total:0:03:18 | ETA:0:03:53 | Loss:0.1248791381419695 | top1:94.85574340820312\n",
      "311/676 Data:0.001 | Batch:0.654 | Total:0:03:24 | ETA:0:03:26 | Loss:0.12461899068817449 | top1:94.85699462890625\n",
      "321/676 Data:0.002 | Batch:0.662 | Total:0:03:30 | ETA:0:03:55 | Loss:0.12466260796627524 | top1:94.84505462646484\n",
      "331/676 Data:0.002 | Batch:0.668 | Total:0:03:37 | ETA:0:03:48 | Loss:0.12477547192546536 | top1:94.84973907470703\n",
      "341/676 Data:0.002 | Batch:0.660 | Total:0:03:43 | ETA:0:03:42 | Loss:0.12448671295190138 | top1:94.8726577758789\n",
      "351/676 Data:0.003 | Batch:0.664 | Total:0:03:50 | ETA:0:03:35 | Loss:0.12488355395248812 | top1:94.86129760742188\n",
      "361/676 Data:0.002 | Batch:0.661 | Total:0:03:57 | ETA:0:03:29 | Loss:0.12498223499867064 | top1:94.8666000366211\n",
      "371/676 Data:0.002 | Batch:0.659 | Total:0:04:03 | ETA:0:03:22 | Loss:0.12481508591787192 | top1:94.8687744140625\n",
      "381/676 Data:0.002 | Batch:0.660 | Total:0:04:10 | ETA:0:03:15 | Loss:0.1252799264969319 | top1:94.85978698730469\n",
      "391/676 Data:0.002 | Batch:0.659 | Total:0:04:16 | ETA:0:03:09 | Loss:0.1251291806530922 | top1:94.86337280273438\n",
      "401/676 Data:0.002 | Batch:0.661 | Total:0:04:23 | ETA:0:03:02 | Loss:0.12566189649061965 | top1:94.83528137207031\n",
      "411/676 Data:0.003 | Batch:0.663 | Total:0:04:30 | ETA:0:02:56 | Loss:0.12585850786009845 | top1:94.82008361816406\n",
      "421/676 Data:0.002 | Batch:0.660 | Total:0:04:36 | ETA:0:02:50 | Loss:0.12590349841677095 | top1:94.82185363769531\n",
      "431/676 Data:0.002 | Batch:0.660 | Total:0:04:43 | ETA:0:02:42 | Loss:0.1261626878653906 | top1:94.81011199951172\n",
      "441/676 Data:0.002 | Batch:0.660 | Total:0:04:50 | ETA:0:02:36 | Loss:0.1261635958692249 | top1:94.81083679199219\n",
      "451/676 Data:0.002 | Batch:0.661 | Total:0:04:56 | ETA:0:02:29 | Loss:0.12599388261360764 | top1:94.81969451904297\n",
      "461/676 Data:0.002 | Batch:0.663 | Total:0:05:03 | ETA:0:02:23 | Loss:0.12620513140347153 | top1:94.80534362792969\n",
      "471/676 Data:0.002 | Batch:0.654 | Total:0:05:09 | ETA:0:02:16 | Loss:0.1264025548002396 | top1:94.80165100097656\n",
      "481/676 Data:0.002 | Batch:0.663 | Total:0:05:16 | ETA:0:02:09 | Loss:0.12621680325722 | top1:94.80467987060547\n",
      "491/676 Data:0.002 | Batch:0.669 | Total:0:05:23 | ETA:0:02:03 | Loss:0.12626429251916782 | top1:94.796875\n",
      "501/676 Data:0.002 | Batch:0.659 | Total:0:05:29 | ETA:0:01:56 | Loss:0.12610746584758312 | top1:94.80827331542969\n",
      "511/676 Data:0.002 | Batch:0.656 | Total:0:05:36 | ETA:0:01:49 | Loss:0.1260264188474055 | top1:94.82026672363281\n",
      "521/676 Data:0.002 | Batch:0.660 | Total:0:05:42 | ETA:0:01:43 | Loss:0.12605109339626416 | top1:94.82068634033203\n",
      "531/676 Data:0.002 | Batch:0.659 | Total:0:05:49 | ETA:0:01:36 | Loss:0.12576514413043605 | top1:94.83992767333984\n",
      "541/676 Data:0.002 | Batch:0.660 | Total:0:05:56 | ETA:0:01:29 | Loss:0.12609737590230768 | top1:94.82440185546875\n",
      "551/676 Data:0.002 | Batch:0.660 | Total:0:06:02 | ETA:0:01:23 | Loss:0.1259306462552608 | top1:94.83332061767578\n",
      "561/676 Data:0.002 | Batch:0.660 | Total:0:06:09 | ETA:0:01:16 | Loss:0.1256637732280873 | top1:94.84473419189453\n",
      "571/676 Data:0.002 | Batch:0.666 | Total:0:06:15 | ETA:0:01:10 | Loss:0.12557486992087966 | top1:94.85482788085938\n",
      "581/676 Data:0.001 | Batch:0.668 | Total:0:06:22 | ETA:0:01:04 | Loss:0.12556151223085218 | top1:94.85189056396484\n",
      "591/676 Data:0.003 | Batch:0.667 | Total:0:06:29 | ETA:0:00:57 | Loss:0.12566969416997759 | top1:94.8481674194336\n",
      "601/676 Data:0.002 | Batch:0.659 | Total:0:06:35 | ETA:0:00:50 | Loss:0.12576407088143457 | top1:94.84280395507812\n",
      "611/676 Data:0.002 | Batch:0.660 | Total:0:06:42 | ETA:0:00:43 | Loss:0.12584949009815527 | top1:94.84537506103516\n",
      "621/676 Data:0.002 | Batch:0.661 | Total:0:06:48 | ETA:0:00:37 | Loss:0.12591475372513136 | top1:94.83854675292969\n",
      "631/676 Data:0.002 | Batch:0.661 | Total:0:06:55 | ETA:0:00:30 | Loss:0.12567906360335282 | top1:94.85111999511719\n",
      "641/676 Data:0.003 | Batch:0.662 | Total:0:07:02 | ETA:0:00:24 | Loss:0.12585788712998075 | top1:94.85015106201172\n",
      "651/676 Data:0.002 | Batch:0.674 | Total:0:07:08 | ETA:0:00:17 | Loss:0.1258197848194389 | top1:94.85244750976562\n",
      "661/676 Data:0.002 | Batch:0.661 | Total:0:07:15 | ETA:0:00:10 | Loss:0.12587601231599901 | top1:94.8507080078125\n",
      "671/676 Data:0.002 | Batch:0.663 | Total:0:07:22 | ETA:0:00:04 | Loss:0.12570436686366218 | top1:94.85136413574219\n",
      "169/169 Data:0.002 | Batch:0.168 | Total:0:00:31 | ETA:0:00:00 | Loss:0.04949812750938433 | top1:98.11526489257812\n",
      "\n",
      "Epoch: [192 | 300] LR: 0.000024\n",
      "1/676 Data:0.830 | Batch:1.499 | Total:0:00:01 | ETA:0:16:54 | Loss:0.11652650684118271 | top1:94.73684692382812\n",
      "11/676 Data:0.002 | Batch:0.660 | Total:0:00:08 | ETA:0:08:17 | Loss:0.1202878552404317 | top1:94.88037872314453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/676 Data:0.002 | Batch:0.663 | Total:0:00:14 | ETA:0:07:14 | Loss:0.12610845799957002 | top1:94.46115112304688\n",
      "31/676 Data:0.002 | Batch:0.663 | Total:0:00:21 | ETA:0:07:08 | Loss:0.11917218049207041 | top1:94.8896484375\n",
      "41/676 Data:0.002 | Batch:0.661 | Total:0:00:27 | ETA:0:07:00 | Loss:0.12301543654828537 | top1:94.801025390625\n",
      "51/676 Data:0.002 | Batch:0.661 | Total:0:00:34 | ETA:0:06:53 | Loss:0.1227696588372483 | top1:94.81940460205078\n",
      "61/676 Data:0.002 | Batch:0.658 | Total:0:00:41 | ETA:0:06:46 | Loss:0.12124164695622491 | top1:94.93528747558594\n",
      "71/676 Data:0.002 | Batch:0.660 | Total:0:00:47 | ETA:0:06:40 | Loss:0.12257842572642044 | top1:94.877685546875\n",
      "81/676 Data:0.002 | Batch:0.660 | Total:0:00:54 | ETA:0:06:35 | Loss:0.12366642701772997 | top1:94.80831146240234\n",
      "91/676 Data:0.002 | Batch:0.663 | Total:0:01:01 | ETA:0:06:28 | Loss:0.1245055031645429 | top1:94.76576232910156\n",
      "101/676 Data:0.002 | Batch:0.659 | Total:0:01:07 | ETA:0:06:21 | Loss:0.12346474700930095 | top1:94.86711883544922\n",
      "111/676 Data:0.002 | Batch:0.668 | Total:0:01:14 | ETA:0:06:14 | Loss:0.12359608105711035 | top1:94.87908935546875\n",
      "121/676 Data:0.002 | Batch:0.660 | Total:0:01:20 | ETA:0:06:09 | Loss:0.1244393672701741 | top1:94.85863494873047\n",
      "131/676 Data:0.003 | Batch:0.657 | Total:0:01:27 | ETA:0:06:01 | Loss:0.12517428318507798 | top1:94.86139678955078\n",
      "141/676 Data:0.002 | Batch:0.662 | Total:0:01:34 | ETA:0:05:54 | Loss:0.1259264128005251 | top1:94.81896209716797\n",
      "151/676 Data:0.002 | Batch:0.661 | Total:0:01:40 | ETA:0:05:48 | Loss:0.125812699315169 | top1:94.82049560546875\n",
      "161/676 Data:0.001 | Batch:0.664 | Total:0:01:47 | ETA:0:05:42 | Loss:0.12639381407950975 | top1:94.82837677001953\n",
      "171/676 Data:0.003 | Batch:0.660 | Total:0:01:53 | ETA:0:05:36 | Loss:0.1267843427278145 | top1:94.79840087890625\n",
      "181/676 Data:0.001 | Batch:0.660 | Total:0:02:00 | ETA:0:05:28 | Loss:0.12642927558382572 | top1:94.82989501953125\n",
      "191/676 Data:0.002 | Batch:0.664 | Total:0:02:07 | ETA:0:05:21 | Loss:0.12636031787744992 | top1:94.81675720214844\n",
      "201/676 Data:0.002 | Batch:0.660 | Total:0:02:13 | ETA:0:05:15 | Loss:0.1259730797057128 | top1:94.83110809326172\n",
      "211/676 Data:0.002 | Batch:0.661 | Total:0:02:20 | ETA:0:05:08 | Loss:0.12561645010989425 | top1:94.85408020019531\n",
      "221/676 Data:0.000 | Batch:0.659 | Total:0:02:27 | ETA:0:05:02 | Loss:0.1259377937747073 | top1:94.83209991455078\n",
      "231/676 Data:0.002 | Batch:0.660 | Total:0:02:33 | ETA:0:04:56 | Loss:0.1261508067610202 | top1:94.8120346069336\n",
      "241/676 Data:0.002 | Batch:0.664 | Total:0:02:40 | ETA:0:04:48 | Loss:0.12630820343912388 | top1:94.80672454833984\n",
      "251/676 Data:0.002 | Batch:0.662 | Total:0:02:46 | ETA:0:04:42 | Loss:0.12610741513243234 | top1:94.79975128173828\n",
      "261/676 Data:0.002 | Batch:0.663 | Total:0:02:53 | ETA:0:04:36 | Loss:0.12591483562442535 | top1:94.80540466308594\n",
      "271/676 Data:0.002 | Batch:0.663 | Total:0:03:00 | ETA:0:04:29 | Loss:0.12563687517579192 | top1:94.82229614257812\n",
      "281/676 Data:0.002 | Batch:0.663 | Total:0:03:06 | ETA:0:04:22 | Loss:0.12575089670319997 | top1:94.82487487792969\n",
      "291/676 Data:0.002 | Batch:0.663 | Total:0:03:13 | ETA:0:04:16 | Loss:0.1256680267501328 | top1:94.84355163574219\n",
      "301/676 Data:0.002 | Batch:0.666 | Total:0:03:20 | ETA:0:04:09 | Loss:0.12567718204519282 | top1:94.84525299072266\n",
      "311/676 Data:0.002 | Batch:0.665 | Total:0:03:26 | ETA:0:04:03 | Loss:0.12594780552281828 | top1:94.83499908447266\n",
      "321/676 Data:0.002 | Batch:0.663 | Total:0:03:33 | ETA:0:03:56 | Loss:0.12567179922792027 | top1:94.8385009765625\n",
      "331/676 Data:0.002 | Batch:0.659 | Total:0:03:39 | ETA:0:03:49 | Loss:0.12600231142517662 | top1:94.81634521484375\n",
      "341/676 Data:0.002 | Batch:0.662 | Total:0:03:46 | ETA:0:03:42 | Loss:0.12650053905558026 | top1:94.80011749267578\n",
      "351/676 Data:0.002 | Batch:0.664 | Total:0:03:53 | ETA:0:03:36 | Loss:0.1267373979494952 | top1:94.80132293701172\n",
      "361/676 Data:0.002 | Batch:0.660 | Total:0:03:59 | ETA:0:03:29 | Loss:0.12619793537870008 | top1:94.81703186035156\n",
      "371/676 Data:0.002 | Batch:0.660 | Total:0:04:06 | ETA:0:03:22 | Loss:0.12700944234217917 | top1:94.78507232666016\n",
      "381/676 Data:0.002 | Batch:0.662 | Total:0:04:13 | ETA:0:03:15 | Loss:0.1269346564089063 | top1:94.77275848388672\n",
      "391/676 Data:0.002 | Batch:0.659 | Total:0:04:19 | ETA:0:03:09 | Loss:0.12737788861174412 | top1:94.75837707519531\n",
      "401/676 Data:0.002 | Batch:0.662 | Total:0:04:26 | ETA:0:03:03 | Loss:0.12732349441124316 | top1:94.76309204101562\n",
      "411/676 Data:0.002 | Batch:0.661 | Total:0:04:32 | ETA:0:02:56 | Loss:0.12692096446008577 | top1:94.79447174072266\n",
      "421/676 Data:0.002 | Batch:0.662 | Total:0:04:39 | ETA:0:02:49 | Loss:0.12694429224821563 | top1:94.7831039428711\n",
      "431/676 Data:0.002 | Batch:0.660 | Total:0:04:46 | ETA:0:02:42 | Loss:0.12753712300323555 | top1:94.75393676757812\n",
      "441/676 Data:0.002 | Batch:0.657 | Total:0:04:52 | ETA:0:02:36 | Loss:0.12781868078439684 | top1:94.74758911132812\n",
      "451/676 Data:0.002 | Batch:0.660 | Total:0:04:59 | ETA:0:02:29 | Loss:0.1278514949865061 | top1:94.74034118652344\n",
      "461/676 Data:0.002 | Batch:0.665 | Total:0:05:05 | ETA:0:02:23 | Loss:0.12792245433574903 | top1:94.72999572753906\n",
      "471/676 Data:0.002 | Batch:0.660 | Total:0:05:12 | ETA:0:02:16 | Loss:0.1276870349109679 | top1:94.74913787841797\n",
      "481/676 Data:0.002 | Batch:0.660 | Total:0:05:19 | ETA:0:02:09 | Loss:0.12779836805068778 | top1:94.74668884277344\n",
      "491/676 Data:0.003 | Batch:0.662 | Total:0:05:25 | ETA:0:02:03 | Loss:0.12750002396598367 | top1:94.7625732421875\n",
      "501/676 Data:0.001 | Batch:0.663 | Total:0:05:32 | ETA:0:01:56 | Loss:0.12737565046685662 | top1:94.774658203125\n",
      "511/676 Data:0.002 | Batch:0.660 | Total:0:05:39 | ETA:0:01:50 | Loss:0.127337601356602 | top1:94.78318786621094\n",
      "521/676 Data:0.003 | Batch:0.663 | Total:0:05:45 | ETA:0:01:43 | Loss:0.12731219369796554 | top1:94.78128814697266\n",
      "531/676 Data:0.002 | Batch:0.661 | Total:0:05:52 | ETA:0:01:37 | Loss:0.12709193558146276 | top1:94.7913589477539\n",
      "541/676 Data:0.002 | Batch:0.663 | Total:0:05:58 | ETA:0:01:30 | Loss:0.1269330487633038 | top1:94.80299377441406\n",
      "551/676 Data:0.002 | Batch:0.661 | Total:0:06:05 | ETA:0:01:23 | Loss:0.12716516725948848 | top1:94.78842163085938\n",
      "561/676 Data:0.002 | Batch:0.660 | Total:0:06:12 | ETA:0:01:17 | Loss:0.12699968808076598 | top1:94.79032135009766\n",
      "571/676 Data:0.002 | Batch:0.661 | Total:0:06:18 | ETA:0:01:10 | Loss:0.126889738768026 | top1:94.79491424560547\n",
      "581/676 Data:0.002 | Batch:0.662 | Total:0:06:25 | ETA:0:01:03 | Loss:0.12677449892674172 | top1:94.80659484863281\n",
      "591/676 Data:0.002 | Batch:0.664 | Total:0:06:31 | ETA:0:00:57 | Loss:0.12670655986656593 | top1:94.80719757080078\n",
      "601/676 Data:0.003 | Batch:0.664 | Total:0:06:38 | ETA:0:00:50 | Loss:0.1269306799276002 | top1:94.78675842285156\n",
      "611/676 Data:0.002 | Batch:0.664 | Total:0:06:45 | ETA:0:00:44 | Loss:0.12681769796093825 | top1:94.78766632080078\n",
      "621/676 Data:0.002 | Batch:0.662 | Total:0:06:51 | ETA:0:00:37 | Loss:0.12662015495980802 | top1:94.80464935302734\n",
      "631/676 Data:0.002 | Batch:0.663 | Total:0:06:58 | ETA:0:00:30 | Loss:0.12662169966026077 | top1:94.79606628417969\n",
      "641/676 Data:0.002 | Batch:0.664 | Total:0:07:05 | ETA:0:00:24 | Loss:0.12681228441423076 | top1:94.7943115234375\n",
      "651/676 Data:0.002 | Batch:0.667 | Total:0:07:11 | ETA:0:00:17 | Loss:0.12674927191319554 | top1:94.79505157470703\n",
      "661/676 Data:0.002 | Batch:0.663 | Total:0:07:18 | ETA:0:00:10 | Loss:0.12689279533257644 | top1:94.79338073730469\n",
      "671/676 Data:0.002 | Batch:0.669 | Total:0:07:25 | ETA:0:00:04 | Loss:0.1269117141789278 | top1:94.79096221923828\n",
      "169/169 Data:0.002 | Batch:0.169 | Total:0:00:31 | ETA:0:00:00 | Loss:0.04810814531200864 | top1:98.1744613647461\n",
      "\n",
      "Epoch: [193 | 300] LR: 0.000031\n",
      "1/676 Data:0.836 | Batch:1.510 | Total:0:00:01 | ETA:0:17:01 | Loss:0.11234953254461288 | top1:94.73684692382812\n",
      "11/676 Data:0.002 | Batch:0.667 | Total:0:00:08 | ETA:0:08:17 | Loss:0.11301772567358884 | top1:95.11961364746094\n",
      "21/676 Data:0.002 | Batch:0.662 | Total:0:00:14 | ETA:0:07:14 | Loss:0.12332745889822642 | top1:94.96240234375\n",
      "31/676 Data:0.002 | Batch:0.660 | Total:0:00:21 | ETA:0:07:07 | Loss:0.1289281835479121 | top1:94.6519546508789\n",
      "41/676 Data:0.002 | Batch:0.658 | Total:0:00:27 | ETA:0:07:00 | Loss:0.12870486462261618 | top1:94.72400665283203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/676 Data:0.002 | Batch:0.660 | Total:0:00:34 | ETA:0:06:54 | Loss:0.12979373014440723 | top1:94.55108642578125\n",
      "61/676 Data:0.001 | Batch:0.662 | Total:0:00:41 | ETA:0:06:48 | Loss:0.12833000719547272 | top1:94.65918731689453\n",
      "71/676 Data:0.002 | Batch:0.659 | Total:0:00:47 | ETA:0:06:41 | Loss:0.12812491369919038 | top1:94.64047241210938\n",
      "81/676 Data:0.003 | Batch:0.658 | Total:0:00:54 | ETA:0:06:34 | Loss:0.12730689650332486 | top1:94.61988067626953\n",
      "91/676 Data:0.002 | Batch:0.662 | Total:0:01:01 | ETA:0:06:28 | Loss:0.1286635419333374 | top1:94.52284240722656\n",
      "101/676 Data:0.002 | Batch:0.660 | Total:0:01:07 | ETA:0:06:21 | Loss:0.12682508754700716 | top1:94.61177825927734\n",
      "111/676 Data:0.002 | Batch:0.656 | Total:0:01:14 | ETA:0:06:12 | Loss:0.12642028655957532 | top1:94.65149688720703\n",
      "121/676 Data:0.002 | Batch:0.671 | Total:0:01:20 | ETA:0:06:07 | Loss:0.12687122375388776 | top1:94.67594909667969\n",
      "131/676 Data:0.002 | Batch:0.660 | Total:0:01:27 | ETA:0:06:01 | Loss:0.12675838209404292 | top1:94.72077178955078\n",
      "141/676 Data:0.002 | Batch:0.658 | Total:0:01:34 | ETA:0:05:54 | Loss:0.1272826430698236 | top1:94.69204711914062\n",
      "151/676 Data:0.002 | Batch:0.659 | Total:0:01:40 | ETA:0:05:47 | Loss:0.12713573984061646 | top1:94.73336029052734\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
