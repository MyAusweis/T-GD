{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained =  './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.01\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_star/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'star/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.010000\n",
      "Train | 20/20 | Loss:0.9564 | MainLoss:0.8625 | Alpha:0.0491 | SPLoss:0.0466 | CLSLoss:1.8637 | top1:54.7000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6788 | MainLoss:0.6788 | SPLoss:0.0679 | CLSLoss:1.8296 | top1:57.8375 | AUROC:0.6108\n",
      "Test | 161/20 | Loss:0.0473 | MainLoss:0.0473 | SPLoss:0.0679 | CLSLoss:1.8296 | top1:99.9065 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.013000\n",
      "Train | 20/20 | Loss:0.7721 | MainLoss:0.6842 | Alpha:0.0467 | SPLoss:0.0774 | CLSLoss:1.8028 | top1:56.0750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6658 | MainLoss:0.6658 | SPLoss:0.0842 | CLSLoss:1.7760 | top1:59.1579 | AUROC:0.6310\n",
      "Test | 161/20 | Loss:0.0706 | MainLoss:0.0706 | SPLoss:0.0842 | CLSLoss:1.7760 | top1:99.9159 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:0.7605 | MainLoss:0.6734 | Alpha:0.0475 | SPLoss:0.0873 | CLSLoss:1.7466 | top1:57.6000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6584 | MainLoss:0.6584 | SPLoss:0.0924 | CLSLoss:1.7163 | top1:60.8093 | AUROC:0.6527\n",
      "Test | 161/20 | Loss:0.0786 | MainLoss:0.0786 | SPLoss:0.0924 | CLSLoss:1.7163 | top1:99.8754 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.019000\n",
      "Train | 20/20 | Loss:0.7468 | MainLoss:0.6596 | Alpha:0.0490 | SPLoss:0.0941 | CLSLoss:1.6833 | top1:60.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6446 | MainLoss:0.6446 | SPLoss:0.1008 | CLSLoss:1.6487 | top1:62.6999 | AUROC:0.6797\n",
      "Test | 161/20 | Loss:0.0678 | MainLoss:0.0678 | SPLoss:0.1008 | CLSLoss:1.6487 | top1:99.8131 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.022000\n",
      "Train | 20/20 | Loss:0.7258 | MainLoss:0.6455 | Alpha:0.0466 | SPLoss:0.1077 | CLSLoss:1.6146 | top1:62.1500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6229 | MainLoss:0.6229 | SPLoss:0.1186 | CLSLoss:1.5779 | top1:64.7838 | AUROC:0.7127\n",
      "Test | 161/20 | Loss:0.0570 | MainLoss:0.0570 | SPLoss:0.1186 | CLSLoss:1.5779 | top1:99.6044 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.025000\n",
      "Train | 20/20 | Loss:0.6991 | MainLoss:0.6204 | Alpha:0.0469 | SPLoss:0.1351 | CLSLoss:1.5415 | top1:64.6500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5941 | MainLoss:0.5941 | SPLoss:0.1527 | CLSLoss:1.5012 | top1:67.6180 | AUROC:0.7477\n",
      "Test | 161/20 | Loss:0.0516 | MainLoss:0.0516 | SPLoss:0.1527 | CLSLoss:1.5012 | top1:99.4081 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.028000\n",
      "Train | 20/20 | Loss:0.6828 | MainLoss:0.6044 | Alpha:0.0479 | SPLoss:0.1777 | CLSLoss:1.4595 | top1:66.8000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5651 | MainLoss:0.5651 | SPLoss:0.2100 | CLSLoss:1.4122 | top1:70.3670 | AUROC:0.7781\n",
      "Test | 161/20 | Loss:0.0633 | MainLoss:0.0633 | SPLoss:0.2100 | CLSLoss:1.4122 | top1:99.2430 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.031000\n",
      "Train | 20/20 | Loss:0.6552 | MainLoss:0.5726 | Alpha:0.0515 | SPLoss:0.2350 | CLSLoss:1.3693 | top1:69.7750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5376 | MainLoss:0.5376 | SPLoss:0.2665 | CLSLoss:1.3198 | top1:72.4378 | AUROC:0.8040\n",
      "Test | 161/20 | Loss:0.0476 | MainLoss:0.0476 | SPLoss:0.2665 | CLSLoss:1.3198 | top1:99.2648 | AUROC:1.0000\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.034000\n",
      "Train | 20/20 | Loss:0.6370 | MainLoss:0.5599 | Alpha:0.0489 | SPLoss:0.3039 | CLSLoss:1.2716 | top1:71.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5142 | MainLoss:0.5142 | SPLoss:0.3437 | CLSLoss:1.2203 | top1:74.2824 | AUROC:0.8242\n",
      "Test | 161/20 | Loss:0.0705 | MainLoss:0.0705 | SPLoss:0.3437 | CLSLoss:1.2203 | top1:98.8006 | AUROC:1.0000\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.037000\n",
      "Train | 20/20 | Loss:0.6193 | MainLoss:0.5433 | Alpha:0.0486 | SPLoss:0.3880 | CLSLoss:1.1762 | top1:71.8500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4955 | MainLoss:0.4955 | SPLoss:0.4360 | CLSLoss:1.1268 | top1:75.3047 | AUROC:0.8408\n",
      "Test | 161/20 | Loss:0.0651 | MainLoss:0.0651 | SPLoss:0.4360 | CLSLoss:1.1268 | top1:98.5950 | AUROC:0.9999\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.6102 | MainLoss:0.5267 | Alpha:0.0540 | SPLoss:0.4717 | CLSLoss:1.0751 | top1:74.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5029 | MainLoss:0.5029 | SPLoss:0.5108 | CLSLoss:1.0221 | top1:74.6068 | AUROC:0.8526\n",
      "Test | 161/20 | Loss:0.0601 | MainLoss:0.0601 | SPLoss:0.5108 | CLSLoss:1.0221 | top1:98.8879 | AUROC:0.9998\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5753 | MainLoss:0.5004 | Alpha:0.0489 | SPLoss:0.5479 | CLSLoss:0.9845 | top1:75.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4594 | MainLoss:0.4594 | SPLoss:0.5882 | CLSLoss:0.9377 | top1:78.0406 | AUROC:0.8645\n",
      "Test | 161/20 | Loss:0.0861 | MainLoss:0.0861 | SPLoss:0.5882 | CLSLoss:0.9377 | top1:97.7041 | AUROC:0.9998\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5749 | MainLoss:0.4978 | Alpha:0.0507 | SPLoss:0.6240 | CLSLoss:0.8951 | top1:75.4750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4759 | MainLoss:0.4759 | SPLoss:0.6661 | CLSLoss:0.8574 | top1:76.7071 | AUROC:0.8660\n",
      "Test | 161/20 | Loss:0.1385 | MainLoss:0.1385 | SPLoss:0.6661 | CLSLoss:0.8574 | top1:94.3271 | AUROC:0.9996\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5512 | MainLoss:0.4746 | Alpha:0.0503 | SPLoss:0.7020 | CLSLoss:0.8205 | top1:77.1250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4818 | MainLoss:0.4818 | SPLoss:0.7452 | CLSLoss:0.7857 | top1:77.2084 | AUROC:0.8758\n",
      "Test | 161/20 | Loss:0.0647 | MainLoss:0.0647 | SPLoss:0.7452 | CLSLoss:0.7857 | top1:98.2617 | AUROC:0.9994\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.039999\n",
      "Train | 20/20 | Loss:0.5446 | MainLoss:0.4693 | Alpha:0.0491 | SPLoss:0.7810 | CLSLoss:0.7516 | top1:77.1250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4637 | MainLoss:0.4637 | SPLoss:0.8232 | CLSLoss:0.7227 | top1:78.2864 | AUROC:0.8822\n",
      "Test | 161/20 | Loss:0.0775 | MainLoss:0.0775 | SPLoss:0.8232 | CLSLoss:0.7227 | top1:97.5608 | AUROC:0.9991\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.039998\n",
      "Train | 20/20 | Loss:0.5438 | MainLoss:0.4686 | Alpha:0.0486 | SPLoss:0.8598 | CLSLoss:0.6896 | top1:77.2750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4410 | MainLoss:0.4410 | SPLoss:0.9040 | CLSLoss:0.6588 | top1:79.3250 | AUROC:0.8866\n",
      "Test | 161/20 | Loss:0.0985 | MainLoss:0.0985 | SPLoss:0.9040 | CLSLoss:0.6588 | top1:97.0810 | AUROC:0.9988\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.039998\n",
      "Train | 20/20 | Loss:0.5257 | MainLoss:0.4468 | Alpha:0.0506 | SPLoss:0.9251 | CLSLoss:0.6360 | top1:78.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4422 | MainLoss:0.4422 | SPLoss:0.9556 | CLSLoss:0.6115 | top1:79.4233 | AUROC:0.8861\n",
      "Test | 161/20 | Loss:0.0988 | MainLoss:0.0988 | SPLoss:0.9556 | CLSLoss:0.6115 | top1:96.4330 | AUROC:0.9988\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.039996\n",
      "Train | 20/20 | Loss:0.5216 | MainLoss:0.4474 | Alpha:0.0475 | SPLoss:0.9793 | CLSLoss:0.5841 | top1:78.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4151 | MainLoss:0.4151 | SPLoss:1.0187 | CLSLoss:0.5644 | top1:80.6062 | AUROC:0.8921\n",
      "Test | 161/20 | Loss:0.1332 | MainLoss:0.1332 | SPLoss:1.0187 | CLSLoss:0.5644 | top1:94.4050 | AUROC:0.9989\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.039995\n",
      "Train | 20/20 | Loss:0.5138 | MainLoss:0.4386 | Alpha:0.0475 | SPLoss:1.0428 | CLSLoss:0.5420 | top1:79.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4072 | MainLoss:0.4072 | SPLoss:1.0749 | CLSLoss:0.5168 | top1:81.1861 | AUROC:0.8946\n",
      "Test | 161/20 | Loss:0.1333 | MainLoss:0.1333 | SPLoss:1.0749 | CLSLoss:0.5168 | top1:94.7290 | AUROC:0.9987\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.039994\n",
      "Train | 20/20 | Loss:0.5184 | MainLoss:0.4405 | Alpha:0.0489 | SPLoss:1.0963 | CLSLoss:0.4969 | top1:79.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4037 | MainLoss:0.4037 | SPLoss:1.1282 | CLSLoss:0.4749 | top1:81.3270 | AUROC:0.8975\n",
      "Test | 161/20 | Loss:0.1510 | MainLoss:0.1510 | SPLoss:1.1282 | CLSLoss:0.4749 | top1:93.9190 | AUROC:0.9988\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.039992\n",
      "Train | 20/20 | Loss:0.5094 | MainLoss:0.4330 | Alpha:0.0473 | SPLoss:1.1569 | CLSLoss:0.4575 | top1:79.2750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4062 | MainLoss:0.4062 | SPLoss:1.1830 | CLSLoss:0.4415 | top1:81.4024 | AUROC:0.8993\n",
      "Test | 161/20 | Loss:0.1243 | MainLoss:0.1243 | SPLoss:1.1830 | CLSLoss:0.4415 | top1:95.4486 | AUROC:0.9979\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.039990\n",
      "Train | 20/20 | Loss:0.5177 | MainLoss:0.4351 | Alpha:0.0509 | SPLoss:1.2029 | CLSLoss:0.4227 | top1:79.2250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4087 | MainLoss:0.4087 | SPLoss:1.2333 | CLSLoss:0.4052 | top1:81.2320 | AUROC:0.8949\n",
      "Test | 161/20 | Loss:0.1394 | MainLoss:0.1394 | SPLoss:1.2333 | CLSLoss:0.4052 | top1:94.6542 | AUROC:0.9982\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.039988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.4913 | MainLoss:0.4112 | Alpha:0.0487 | SPLoss:1.2523 | CLSLoss:0.3944 | top1:81.0000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4243 | MainLoss:0.4243 | SPLoss:1.2790 | CLSLoss:0.3653 | top1:80.3309 | AUROC:0.8947\n",
      "Test | 161/20 | Loss:0.1811 | MainLoss:0.1811 | SPLoss:1.2790 | CLSLoss:0.3653 | top1:92.9844 | AUROC:0.9985\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.039986\n",
      "Train | 20/20 | Loss:0.4795 | MainLoss:0.3968 | Alpha:0.0497 | SPLoss:1.3008 | CLSLoss:0.3638 | top1:81.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3921 | MainLoss:0.3921 | SPLoss:1.3237 | CLSLoss:0.3524 | top1:82.1298 | AUROC:0.9041\n",
      "Test | 161/20 | Loss:0.1428 | MainLoss:0.1428 | SPLoss:1.3237 | CLSLoss:0.3524 | top1:94.1994 | AUROC:0.9980\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.039983\n",
      "Train | 20/20 | Loss:0.5628 | MainLoss:0.4251 | Alpha:0.0497 | SPLoss:2.4514 | CLSLoss:0.3389 | top1:79.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4344 | MainLoss:0.4344 | SPLoss:13.6591 | CLSLoss:0.3206 | top1:79.2202 | AUROC:0.9039\n",
      "Test | 161/20 | Loss:0.2318 | MainLoss:0.2318 | SPLoss:13.6591 | CLSLoss:0.3206 | top1:89.2181 | AUROC:0.9958\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.039981\n",
      "Train | 20/20 | Loss:1.0513 | MainLoss:0.3969 | Alpha:0.0484 | SPLoss:13.1961 | CLSLoss:0.3178 | top1:81.5000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4714 | MainLoss:0.4714 | SPLoss:12.7166 | CLSLoss:0.3039 | top1:77.5524 | AUROC:0.9009\n",
      "Test | 161/20 | Loss:0.2505 | MainLoss:0.2505 | SPLoss:12.7166 | CLSLoss:0.3039 | top1:88.1589 | AUROC:0.9965\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.039978\n",
      "Train | 20/20 | Loss:0.9638 | MainLoss:0.3798 | Alpha:0.0463 | SPLoss:12.2960 | CLSLoss:0.3044 | top1:82.8500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4073 | MainLoss:0.4073 | SPLoss:11.8981 | CLSLoss:0.2950 | top1:81.3696 | AUROC:0.9064\n",
      "Test | 161/20 | Loss:0.2163 | MainLoss:0.2163 | SPLoss:11.8981 | CLSLoss:0.2950 | top1:90.3178 | AUROC:0.9956\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.039975\n",
      "Train | 20/20 | Loss:0.9808 | MainLoss:0.4065 | Alpha:0.0487 | SPLoss:11.4834 | CLSLoss:0.2873 | top1:82.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3841 | MainLoss:0.3841 | SPLoss:11.0715 | CLSLoss:0.2796 | top1:82.5557 | AUROC:0.9080\n",
      "Test | 161/20 | Loss:0.1492 | MainLoss:0.1492 | SPLoss:11.0715 | CLSLoss:0.2796 | top1:94.4237 | AUROC:0.9965\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.039971\n",
      "Train | 20/20 | Loss:0.9394 | MainLoss:0.3806 | Alpha:0.0509 | SPLoss:10.7290 | CLSLoss:0.2777 | top1:82.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3830 | MainLoss:0.3830 | SPLoss:10.2820 | CLSLoss:0.2682 | top1:82.7556 | AUROC:0.9098\n",
      "Test | 161/20 | Loss:0.1452 | MainLoss:0.1452 | SPLoss:10.2820 | CLSLoss:0.2682 | top1:94.5327 | AUROC:0.9958\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.039968\n",
      "Train | 20/20 | Loss:0.8463 | MainLoss:0.3692 | Alpha:0.0466 | SPLoss:9.9688 | CLSLoss:0.2648 | top1:83.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3815 | MainLoss:0.3815 | SPLoss:9.6343 | CLSLoss:0.2564 | top1:82.9030 | AUROC:0.9117\n",
      "Test | 161/20 | Loss:0.1587 | MainLoss:0.1587 | SPLoss:9.6343 | CLSLoss:0.2564 | top1:93.9065 | AUROC:0.9958\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.039964\n",
      "Train | 20/20 | Loss:0.8635 | MainLoss:0.3857 | Alpha:0.0499 | SPLoss:9.3432 | CLSLoss:0.2528 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3852 | MainLoss:0.3852 | SPLoss:8.9784 | CLSLoss:0.2527 | top1:82.7851 | AUROC:0.9125\n",
      "Test | 161/20 | Loss:0.1895 | MainLoss:0.1895 | SPLoss:8.9784 | CLSLoss:0.2527 | top1:92.4206 | AUROC:0.9962\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.039961\n",
      "Train | 20/20 | Loss:0.8388 | MainLoss:0.3736 | Alpha:0.0521 | SPLoss:8.6517 | CLSLoss:0.2462 | top1:83.8250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3768 | MainLoss:0.3768 | SPLoss:8.3512 | CLSLoss:0.2414 | top1:83.1684 | AUROC:0.9131\n",
      "Test | 161/20 | Loss:0.1512 | MainLoss:0.1512 | SPLoss:8.3512 | CLSLoss:0.2414 | top1:94.2243 | AUROC:0.9957\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.039956\n",
      "Train | 20/20 | Loss:0.7698 | MainLoss:0.3792 | Alpha:0.0467 | SPLoss:8.1301 | CLSLoss:0.2355 | top1:83.0000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3766 | MainLoss:0.3766 | SPLoss:7.8888 | CLSLoss:0.2313 | top1:83.2733 | AUROC:0.9133\n",
      "Test | 161/20 | Loss:0.2014 | MainLoss:0.2014 | SPLoss:7.8888 | CLSLoss:0.2313 | top1:91.3894 | AUROC:0.9951\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.039952\n",
      "Train | 20/20 | Loss:0.7647 | MainLoss:0.3705 | Alpha:0.0499 | SPLoss:7.6641 | CLSLoss:0.2301 | top1:83.0000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4165 | MainLoss:0.4165 | SPLoss:7.4079 | CLSLoss:0.2318 | top1:81.5826 | AUROC:0.9128\n",
      "Test | 161/20 | Loss:0.1433 | MainLoss:0.1433 | SPLoss:7.4079 | CLSLoss:0.2318 | top1:94.6604 | AUROC:0.9938\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.039948\n",
      "Train | 20/20 | Loss:0.7248 | MainLoss:0.3766 | Alpha:0.0469 | SPLoss:7.1831 | CLSLoss:0.2279 | top1:82.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4141 | MainLoss:0.4141 | SPLoss:7.0045 | CLSLoss:0.2191 | top1:81.1042 | AUROC:0.9123\n",
      "Test | 161/20 | Loss:0.2737 | MainLoss:0.2737 | SPLoss:7.0045 | CLSLoss:0.2191 | top1:87.6542 | AUROC:0.9927\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.039943\n",
      "Train | 20/20 | Loss:0.6982 | MainLoss:0.3498 | Alpha:0.0496 | SPLoss:6.7842 | CLSLoss:0.2224 | top1:84.7000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3847 | MainLoss:0.3847 | SPLoss:6.5906 | CLSLoss:0.2250 | top1:83.2864 | AUROC:0.9130\n",
      "Test | 161/20 | Loss:0.1739 | MainLoss:0.1739 | SPLoss:6.5906 | CLSLoss:0.2250 | top1:93.1932 | AUROC:0.9935\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.039938\n",
      "Train | 20/20 | Loss:0.6847 | MainLoss:0.3631 | Alpha:0.0484 | SPLoss:6.4276 | CLSLoss:0.2177 | top1:84.1750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3819 | MainLoss:0.3819 | SPLoss:6.2447 | CLSLoss:0.2178 | top1:83.1193 | AUROC:0.9131\n",
      "Test | 161/20 | Loss:0.1780 | MainLoss:0.1780 | SPLoss:6.2447 | CLSLoss:0.2178 | top1:92.9502 | AUROC:0.9930\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.039933\n",
      "Train | 20/20 | Loss:0.6635 | MainLoss:0.3529 | Alpha:0.0493 | SPLoss:6.0913 | CLSLoss:0.2177 | top1:84.2250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6111 | MainLoss:0.6111 | SPLoss:5.9077 | CLSLoss:0.2157 | top1:72.8899 | AUROC:0.9053\n",
      "Test | 161/20 | Loss:0.4420 | MainLoss:0.4420 | SPLoss:5.9077 | CLSLoss:0.2157 | top1:79.6449 | AUROC:0.9898\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.039928\n",
      "Train | 20/20 | Loss:0.6648 | MainLoss:0.3722 | Alpha:0.0489 | SPLoss:5.7679 | CLSLoss:0.2120 | top1:83.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4252 | MainLoss:0.4252 | SPLoss:5.5901 | CLSLoss:0.2146 | top1:81.1697 | AUROC:0.9123\n",
      "Test | 161/20 | Loss:0.2365 | MainLoss:0.2365 | SPLoss:5.5901 | CLSLoss:0.2146 | top1:89.9595 | AUROC:0.9933\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.039923\n",
      "Train | 20/20 | Loss:0.6387 | MainLoss:0.3632 | Alpha:0.0486 | SPLoss:5.4523 | CLSLoss:0.2108 | top1:83.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3790 | MainLoss:0.3790 | SPLoss:5.3154 | CLSLoss:0.2120 | top1:83.2602 | AUROC:0.9149\n",
      "Test | 161/20 | Loss:0.1680 | MainLoss:0.1680 | SPLoss:5.3154 | CLSLoss:0.2120 | top1:93.4081 | AUROC:0.9927\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.039917\n",
      "Train | 20/20 | Loss:0.5919 | MainLoss:0.3467 | Alpha:0.0451 | SPLoss:5.2253 | CLSLoss:0.2149 | top1:84.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3839 | MainLoss:0.3839 | SPLoss:5.1111 | CLSLoss:0.2143 | top1:83.4862 | AUROC:0.9134\n",
      "Test | 161/20 | Loss:0.1870 | MainLoss:0.1870 | SPLoss:5.1111 | CLSLoss:0.2143 | top1:92.3489 | AUROC:0.9919\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.039911\n",
      "Train | 20/20 | Loss:0.5998 | MainLoss:0.3485 | Alpha:0.0483 | SPLoss:4.9910 | CLSLoss:0.2106 | top1:84.4000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3989 | MainLoss:0.3989 | SPLoss:4.8772 | CLSLoss:0.2139 | top1:82.5360 | AUROC:0.9163\n",
      "Test | 161/20 | Loss:0.2497 | MainLoss:0.2497 | SPLoss:4.8772 | CLSLoss:0.2139 | top1:89.2492 | AUROC:0.9918\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.039905\n",
      "Train | 20/20 | Loss:0.6218 | MainLoss:0.3709 | Alpha:0.0504 | SPLoss:4.7780 | CLSLoss:0.2055 | top1:82.8750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4419 | MainLoss:0.4419 | SPLoss:4.6561 | CLSLoss:0.2051 | top1:80.2261 | AUROC:0.9142\n",
      "Test | 161/20 | Loss:0.1368 | MainLoss:0.1368 | SPLoss:4.6561 | CLSLoss:0.2051 | top1:95.3333 | AUROC:0.9920\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.039899\n",
      "Train | 20/20 | Loss:0.5735 | MainLoss:0.3476 | Alpha:0.0473 | SPLoss:4.5687 | CLSLoss:0.2083 | top1:84.5500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3862 | MainLoss:0.3862 | SPLoss:4.4854 | CLSLoss:0.2097 | top1:82.6409 | AUROC:0.9159\n",
      "Test | 161/20 | Loss:0.2387 | MainLoss:0.2387 | SPLoss:4.4854 | CLSLoss:0.2097 | top1:89.6542 | AUROC:0.9916\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.039893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.5615 | MainLoss:0.3458 | Alpha:0.0462 | SPLoss:4.4609 | CLSLoss:0.2066 | top1:84.4000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3739 | MainLoss:0.3739 | SPLoss:4.3749 | CLSLoss:0.2104 | top1:83.5419 | AUROC:0.9179\n",
      "Test | 161/20 | Loss:0.2154 | MainLoss:0.2154 | SPLoss:4.3749 | CLSLoss:0.2104 | top1:90.8692 | AUROC:0.9914\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.039886\n",
      "Train | 20/20 | Loss:0.5838 | MainLoss:0.3745 | Alpha:0.0462 | SPLoss:4.3273 | CLSLoss:0.2009 | top1:83.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3755 | MainLoss:0.3755 | SPLoss:4.2517 | CLSLoss:0.2016 | top1:83.3028 | AUROC:0.9154\n",
      "Test | 161/20 | Loss:0.1921 | MainLoss:0.1921 | SPLoss:4.2517 | CLSLoss:0.2016 | top1:92.3396 | AUROC:0.9908\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.039879\n",
      "Train | 20/20 | Loss:0.5541 | MainLoss:0.3398 | Alpha:0.0489 | SPLoss:4.1809 | CLSLoss:0.2042 | top1:85.7000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3722 | MainLoss:0.3722 | SPLoss:4.1279 | CLSLoss:0.1937 | top1:83.4502 | AUROC:0.9165\n",
      "Test | 161/20 | Loss:0.1771 | MainLoss:0.1771 | SPLoss:4.1279 | CLSLoss:0.1937 | top1:93.3209 | AUROC:0.9909\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.039872\n",
      "Train | 20/20 | Loss:0.5625 | MainLoss:0.3352 | Alpha:0.0537 | SPLoss:4.0282 | CLSLoss:0.1994 | top1:85.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3957 | MainLoss:0.3957 | SPLoss:3.9530 | CLSLoss:0.1978 | top1:82.2281 | AUROC:0.9177\n",
      "Test | 161/20 | Loss:0.1733 | MainLoss:0.1733 | SPLoss:3.9530 | CLSLoss:0.1978 | top1:93.5732 | AUROC:0.9878\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.039865\n",
      "Train | 20/20 | Loss:0.5434 | MainLoss:0.3403 | Alpha:0.0497 | SPLoss:3.8863 | CLSLoss:0.2000 | top1:85.3750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4001 | MainLoss:0.4001 | SPLoss:3.8131 | CLSLoss:0.1953 | top1:81.9790 | AUROC:0.9185\n",
      "Test | 161/20 | Loss:0.2791 | MainLoss:0.2791 | SPLoss:3.8131 | CLSLoss:0.1953 | top1:87.5234 | AUROC:0.9908\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.039858\n",
      "Train | 20/20 | Loss:0.5353 | MainLoss:0.3430 | Alpha:0.0485 | SPLoss:3.7632 | CLSLoss:0.1989 | top1:85.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3939 | MainLoss:0.3939 | SPLoss:3.7054 | CLSLoss:0.1960 | top1:82.4738 | AUROC:0.9176\n",
      "Test | 161/20 | Loss:0.2760 | MainLoss:0.2760 | SPLoss:3.7054 | CLSLoss:0.1960 | top1:87.8411 | AUROC:0.9919\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.039850\n",
      "Train | 20/20 | Loss:0.5111 | MainLoss:0.3284 | Alpha:0.0474 | SPLoss:3.6575 | CLSLoss:0.1991 | top1:85.6500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3860 | MainLoss:0.3860 | SPLoss:3.6026 | CLSLoss:0.2049 | top1:83.2896 | AUROC:0.9155\n",
      "Test | 161/20 | Loss:0.1623 | MainLoss:0.1623 | SPLoss:3.6026 | CLSLoss:0.2049 | top1:93.7165 | AUROC:0.9914\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.003984\n",
      "Train | 20/20 | Loss:0.4795 | MainLoss:0.2977 | Alpha:0.0478 | SPLoss:3.5960 | CLSLoss:0.2057 | top1:87.7500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3763 | MainLoss:0.3763 | SPLoss:3.5895 | CLSLoss:0.2064 | top1:83.7287 | AUROC:0.9175\n",
      "Test | 161/20 | Loss:0.1831 | MainLoss:0.1831 | SPLoss:3.5895 | CLSLoss:0.2064 | top1:92.7165 | AUROC:0.9913\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.003983\n",
      "Train | 20/20 | Loss:0.4772 | MainLoss:0.2949 | Alpha:0.0481 | SPLoss:3.5836 | CLSLoss:0.2070 | top1:87.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3751 | MainLoss:0.3751 | SPLoss:3.5769 | CLSLoss:0.2071 | top1:83.8925 | AUROC:0.9180\n",
      "Test | 161/20 | Loss:0.1939 | MainLoss:0.1939 | SPLoss:3.5769 | CLSLoss:0.2071 | top1:92.1838 | AUROC:0.9914\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.003983\n",
      "Train | 20/20 | Loss:0.4888 | MainLoss:0.3087 | Alpha:0.0476 | SPLoss:3.5718 | CLSLoss:0.2072 | top1:86.9000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3723 | MainLoss:0.3723 | SPLoss:3.5661 | CLSLoss:0.2074 | top1:83.9220 | AUROC:0.9184\n",
      "Test | 161/20 | Loss:0.1969 | MainLoss:0.1969 | SPLoss:3.5661 | CLSLoss:0.2074 | top1:91.9720 | AUROC:0.9912\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.003982\n",
      "Train | 20/20 | Loss:0.4824 | MainLoss:0.3043 | Alpha:0.0472 | SPLoss:3.5627 | CLSLoss:0.2076 | top1:86.9750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3711 | MainLoss:0.3711 | SPLoss:3.5555 | CLSLoss:0.2080 | top1:83.9548 | AUROC:0.9193\n",
      "Test | 161/20 | Loss:0.1984 | MainLoss:0.1984 | SPLoss:3.5555 | CLSLoss:0.2080 | top1:91.8941 | AUROC:0.9915\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.003981\n",
      "Train | 20/20 | Loss:0.4799 | MainLoss:0.3016 | Alpha:0.0474 | SPLoss:3.5512 | CLSLoss:0.2081 | top1:87.3250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3724 | MainLoss:0.3724 | SPLoss:3.5459 | CLSLoss:0.2089 | top1:83.9646 | AUROC:0.9193\n",
      "Test | 161/20 | Loss:0.1881 | MainLoss:0.1881 | SPLoss:3.5459 | CLSLoss:0.2089 | top1:92.4424 | AUROC:0.9916\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.003980\n",
      "Train | 20/20 | Loss:0.4596 | MainLoss:0.2839 | Alpha:0.0469 | SPLoss:3.5397 | CLSLoss:0.2096 | top1:88.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3727 | MainLoss:0.3727 | SPLoss:3.5324 | CLSLoss:0.2106 | top1:84.0826 | AUROC:0.9201\n",
      "Test | 161/20 | Loss:0.1973 | MainLoss:0.1973 | SPLoss:3.5324 | CLSLoss:0.2106 | top1:91.9595 | AUROC:0.9918\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.003979\n",
      "Train | 20/20 | Loss:0.4731 | MainLoss:0.2909 | Alpha:0.0487 | SPLoss:3.5258 | CLSLoss:0.2108 | top1:88.0750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3732 | MainLoss:0.3732 | SPLoss:3.5194 | CLSLoss:0.2110 | top1:84.0662 | AUROC:0.9196\n",
      "Test | 161/20 | Loss:0.1992 | MainLoss:0.1992 | SPLoss:3.5194 | CLSLoss:0.2110 | top1:91.9346 | AUROC:0.9920\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.003978\n",
      "Train | 20/20 | Loss:0.4506 | MainLoss:0.2744 | Alpha:0.0473 | SPLoss:3.5141 | CLSLoss:0.2118 | top1:88.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3756 | MainLoss:0.3756 | SPLoss:3.5101 | CLSLoss:0.2127 | top1:84.0727 | AUROC:0.9199\n",
      "Test | 161/20 | Loss:0.1850 | MainLoss:0.1850 | SPLoss:3.5101 | CLSLoss:0.2127 | top1:92.6199 | AUROC:0.9922\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.003977\n",
      "Train | 20/20 | Loss:0.4691 | MainLoss:0.2903 | Alpha:0.0481 | SPLoss:3.5048 | CLSLoss:0.2130 | top1:87.4750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3748 | MainLoss:0.3748 | SPLoss:3.4977 | CLSLoss:0.2130 | top1:84.0564 | AUROC:0.9204\n",
      "Test | 161/20 | Loss:0.1868 | MainLoss:0.1868 | SPLoss:3.4977 | CLSLoss:0.2130 | top1:92.5358 | AUROC:0.9921\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.003976\n",
      "Train | 20/20 | Loss:0.4747 | MainLoss:0.2920 | Alpha:0.0493 | SPLoss:3.4936 | CLSLoss:0.2133 | top1:88.1000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3732 | MainLoss:0.3732 | SPLoss:3.4873 | CLSLoss:0.2128 | top1:84.0924 | AUROC:0.9204\n",
      "Test | 161/20 | Loss:0.1881 | MainLoss:0.1881 | SPLoss:3.4873 | CLSLoss:0.2128 | top1:92.4579 | AUROC:0.9919\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.003975\n",
      "Train | 20/20 | Loss:0.4782 | MainLoss:0.2929 | Alpha:0.0502 | SPLoss:3.4797 | CLSLoss:0.2132 | top1:87.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3707 | MainLoss:0.3707 | SPLoss:3.4713 | CLSLoss:0.2130 | top1:84.2136 | AUROC:0.9210\n",
      "Test | 161/20 | Loss:0.1992 | MainLoss:0.1992 | SPLoss:3.4713 | CLSLoss:0.2130 | top1:91.9065 | AUROC:0.9922\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.003974\n",
      "Train | 20/20 | Loss:0.4619 | MainLoss:0.2900 | Alpha:0.0467 | SPLoss:3.4664 | CLSLoss:0.2133 | top1:87.4750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3729 | MainLoss:0.3729 | SPLoss:3.4619 | CLSLoss:0.2138 | top1:84.0695 | AUROC:0.9208\n",
      "Test | 161/20 | Loss:0.1854 | MainLoss:0.1854 | SPLoss:3.4619 | CLSLoss:0.2138 | top1:92.5981 | AUROC:0.9922\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.003973\n",
      "Train | 20/20 | Loss:0.4769 | MainLoss:0.2960 | Alpha:0.0493 | SPLoss:3.4568 | CLSLoss:0.2139 | top1:87.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3720 | MainLoss:0.3720 | SPLoss:3.4502 | CLSLoss:0.2140 | top1:84.0924 | AUROC:0.9211\n",
      "Test | 161/20 | Loss:0.2131 | MainLoss:0.2131 | SPLoss:3.4502 | CLSLoss:0.2140 | top1:91.2991 | AUROC:0.9918\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.003972\n",
      "Train | 20/20 | Loss:0.4644 | MainLoss:0.2842 | Alpha:0.0493 | SPLoss:3.4443 | CLSLoss:0.2144 | top1:88.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3705 | MainLoss:0.3705 | SPLoss:3.4370 | CLSLoss:0.2148 | top1:84.1547 | AUROC:0.9214\n",
      "Test | 161/20 | Loss:0.1946 | MainLoss:0.1946 | SPLoss:3.4370 | CLSLoss:0.2148 | top1:92.2305 | AUROC:0.9918\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.003971\n",
      "Train | 20/20 | Loss:0.4794 | MainLoss:0.2874 | Alpha:0.0527 | SPLoss:3.4298 | CLSLoss:0.2149 | top1:87.5000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3697 | MainLoss:0.3697 | SPLoss:3.4236 | CLSLoss:0.2151 | top1:84.1678 | AUROC:0.9209\n",
      "Test | 161/20 | Loss:0.2005 | MainLoss:0.2005 | SPLoss:3.4236 | CLSLoss:0.2151 | top1:91.9502 | AUROC:0.9922\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.003970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.4472 | MainLoss:0.2742 | Alpha:0.0476 | SPLoss:3.4181 | CLSLoss:0.2158 | top1:88.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3713 | MainLoss:0.3713 | SPLoss:3.4138 | CLSLoss:0.2165 | top1:84.2071 | AUROC:0.9210\n",
      "Test | 161/20 | Loss:0.2008 | MainLoss:0.2008 | SPLoss:3.4138 | CLSLoss:0.2165 | top1:91.9470 | AUROC:0.9918\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.003969\n",
      "Train | 20/20 | Loss:0.4707 | MainLoss:0.2895 | Alpha:0.0499 | SPLoss:3.4093 | CLSLoss:0.2165 | top1:87.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3717 | MainLoss:0.3717 | SPLoss:3.4046 | CLSLoss:0.2168 | top1:84.1874 | AUROC:0.9213\n",
      "Test | 161/20 | Loss:0.2118 | MainLoss:0.2118 | SPLoss:3.4046 | CLSLoss:0.2168 | top1:91.4019 | AUROC:0.9919\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.003968\n",
      "Train | 20/20 | Loss:0.4610 | MainLoss:0.2821 | Alpha:0.0495 | SPLoss:3.3993 | CLSLoss:0.2172 | top1:88.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3738 | MainLoss:0.3738 | SPLoss:3.3955 | CLSLoss:0.2171 | top1:84.1317 | AUROC:0.9205\n",
      "Test | 161/20 | Loss:0.1894 | MainLoss:0.1894 | SPLoss:3.3955 | CLSLoss:0.2171 | top1:92.4579 | AUROC:0.9922\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.003967\n",
      "Train | 20/20 | Loss:0.4407 | MainLoss:0.2703 | Alpha:0.0472 | SPLoss:3.3900 | CLSLoss:0.2177 | top1:89.0000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3740 | MainLoss:0.3740 | SPLoss:3.3852 | CLSLoss:0.2182 | top1:84.1252 | AUROC:0.9210\n",
      "Test | 161/20 | Loss:0.1927 | MainLoss:0.1927 | SPLoss:3.3852 | CLSLoss:0.2182 | top1:92.2835 | AUROC:0.9924\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.003966\n",
      "Train | 20/20 | Loss:0.4347 | MainLoss:0.2670 | Alpha:0.0466 | SPLoss:3.3812 | CLSLoss:0.2193 | top1:88.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3779 | MainLoss:0.3779 | SPLoss:3.3765 | CLSLoss:0.2194 | top1:84.1678 | AUROC:0.9208\n",
      "Test | 161/20 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:3.3765 | CLSLoss:0.2194 | top1:90.9159 | AUROC:0.9918\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.003965\n",
      "Train | 20/20 | Loss:0.4525 | MainLoss:0.2753 | Alpha:0.0493 | SPLoss:3.3731 | CLSLoss:0.2194 | top1:89.1750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3755 | MainLoss:0.3755 | SPLoss:3.3661 | CLSLoss:0.2195 | top1:84.1547 | AUROC:0.9210\n",
      "Test | 161/20 | Loss:0.1969 | MainLoss:0.1969 | SPLoss:3.3661 | CLSLoss:0.2195 | top1:92.1371 | AUROC:0.9924\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.003963\n",
      "Train | 20/20 | Loss:0.4509 | MainLoss:0.2793 | Alpha:0.0479 | SPLoss:3.3630 | CLSLoss:0.2198 | top1:88.3750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3762 | MainLoss:0.3762 | SPLoss:3.3573 | CLSLoss:0.2198 | top1:84.1186 | AUROC:0.9208\n",
      "Test | 161/20 | Loss:0.2049 | MainLoss:0.2049 | SPLoss:3.3573 | CLSLoss:0.2198 | top1:91.6978 | AUROC:0.9924\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.003962\n",
      "Train | 20/20 | Loss:0.4661 | MainLoss:0.2857 | Alpha:0.0505 | SPLoss:3.3560 | CLSLoss:0.2195 | top1:87.5750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3761 | MainLoss:0.3761 | SPLoss:3.3495 | CLSLoss:0.2194 | top1:84.1481 | AUROC:0.9206\n",
      "Test | 161/20 | Loss:0.2104 | MainLoss:0.2104 | SPLoss:3.3495 | CLSLoss:0.2194 | top1:91.4424 | AUROC:0.9921\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.003961\n",
      "Train | 20/20 | Loss:0.4373 | MainLoss:0.2726 | Alpha:0.0462 | SPLoss:3.3454 | CLSLoss:0.2197 | top1:88.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3751 | MainLoss:0.3751 | SPLoss:3.3436 | CLSLoss:0.2203 | top1:84.1547 | AUROC:0.9209\n",
      "Test | 161/20 | Loss:0.1998 | MainLoss:0.1998 | SPLoss:3.3436 | CLSLoss:0.2203 | top1:91.9346 | AUROC:0.9921\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.003960\n",
      "Train | 20/20 | Loss:0.4445 | MainLoss:0.2764 | Alpha:0.0472 | SPLoss:3.3383 | CLSLoss:0.2203 | top1:88.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3780 | MainLoss:0.3780 | SPLoss:3.3353 | CLSLoss:0.2202 | top1:83.9974 | AUROC:0.9208\n",
      "Test | 161/20 | Loss:0.1765 | MainLoss:0.1765 | SPLoss:3.3353 | CLSLoss:0.2202 | top1:93.0997 | AUROC:0.9927\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.003958\n",
      "Train | 20/20 | Loss:0.4677 | MainLoss:0.2890 | Alpha:0.0504 | SPLoss:3.3299 | CLSLoss:0.2199 | top1:88.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3724 | MainLoss:0.3724 | SPLoss:3.3237 | CLSLoss:0.2199 | top1:84.2136 | AUROC:0.9216\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
