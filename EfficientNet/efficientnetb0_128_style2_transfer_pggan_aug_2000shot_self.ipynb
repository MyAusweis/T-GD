{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style2/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 250\n",
    "test_batch = 250\n",
    "lr = 0.001\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.0\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/b0/to_pggan/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style2/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.001000\n",
      "Train | 32/32 | Loss:2.2028 | MainLoss:2.2028 | Alpha:0.0218 | SPLoss:0.0140 | CLSLoss:3.3868 | top1:48.5935 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.8428 | MainLoss:0.8428 | SPLoss:0.0346 | CLSLoss:3.3666 | top1:45.9938 | AUROC:0.4651\n",
      "Test | 32/32 | Loss:0.2589 | MainLoss:0.2589 | SPLoss:0.0346 | CLSLoss:3.3666 | top1:92.9103 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.001300\n",
      "Train | 32/32 | Loss:0.7818 | MainLoss:0.7818 | Alpha:0.3041 | SPLoss:0.0007 | CLSLoss:3.3631 | top1:48.9677 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.7166 | MainLoss:0.7166 | SPLoss:0.0017 | CLSLoss:3.3604 | top1:52.4798 | AUROC:0.5432\n",
      "Test | 32/32 | Loss:0.3242 | MainLoss:0.3242 | SPLoss:0.0017 | CLSLoss:3.3604 | top1:85.2179 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.001600\n",
      "Train | 32/32 | Loss:0.7339 | MainLoss:0.7339 | Alpha:0.3264 | SPLoss:0.0002 | CLSLoss:3.3584 | top1:50.9935 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6913 | MainLoss:0.6913 | SPLoss:0.0006 | CLSLoss:3.3566 | top1:56.8131 | AUROC:0.5845\n",
      "Test | 32/32 | Loss:0.3536 | MainLoss:0.3536 | SPLoss:0.0006 | CLSLoss:3.3566 | top1:75.7051 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.001900\n",
      "Train | 32/32 | Loss:0.7099 | MainLoss:0.7099 | Alpha:0.3310 | SPLoss:0.0001 | CLSLoss:3.3553 | top1:52.5290 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6841 | MainLoss:0.6841 | SPLoss:0.0004 | CLSLoss:3.3540 | top1:57.0312 | AUROC:0.6068\n",
      "Test | 32/32 | Loss:0.3283 | MainLoss:0.3283 | SPLoss:0.0004 | CLSLoss:3.3540 | top1:82.1026 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.002200\n",
      "Train | 32/32 | Loss:0.7009 | MainLoss:0.7009 | Alpha:0.3325 | SPLoss:0.0001 | CLSLoss:3.3530 | top1:52.3742 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6758 | MainLoss:0.6758 | SPLoss:0.0003 | CLSLoss:3.3521 | top1:59.1994 | AUROC:0.6286\n",
      "Test | 32/32 | Loss:0.3401 | MainLoss:0.3401 | SPLoss:0.0003 | CLSLoss:3.3521 | top1:78.0385 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.002500\n",
      "Train | 32/32 | Loss:0.6959 | MainLoss:0.6959 | Alpha:0.3343 | SPLoss:0.0001 | CLSLoss:3.3512 | top1:53.1097 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6685 | MainLoss:0.6685 | SPLoss:0.0003 | CLSLoss:3.3506 | top1:60.9128 | AUROC:0.6499\n",
      "Test | 32/32 | Loss:0.3625 | MainLoss:0.3625 | SPLoss:0.0003 | CLSLoss:3.3506 | top1:71.3333 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.002800\n",
      "Train | 32/32 | Loss:0.6901 | MainLoss:0.6901 | Alpha:0.3350 | SPLoss:0.0001 | CLSLoss:3.3501 | top1:54.2194 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6637 | MainLoss:0.6637 | SPLoss:0.0002 | CLSLoss:3.3497 | top1:62.2648 | AUROC:0.6682\n",
      "Test | 32/32 | Loss:0.3531 | MainLoss:0.3531 | SPLoss:0.0002 | CLSLoss:3.3497 | top1:73.6923 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.003100\n",
      "Train | 32/32 | Loss:0.6892 | MainLoss:0.6892 | Alpha:0.3351 | SPLoss:0.0001 | CLSLoss:3.3494 | top1:54.7484 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6574 | MainLoss:0.6574 | SPLoss:0.0003 | CLSLoss:3.3490 | top1:63.5794 | AUROC:0.6896\n",
      "Test | 32/32 | Loss:0.3713 | MainLoss:0.3713 | SPLoss:0.0003 | CLSLoss:3.3490 | top1:68.5641 | AUROC:0.9999\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.003400\n",
      "Train | 32/32 | Loss:0.6820 | MainLoss:0.6820 | Alpha:0.3365 | SPLoss:0.0001 | CLSLoss:3.3490 | top1:55.9355 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6501 | MainLoss:0.6501 | SPLoss:0.0003 | CLSLoss:3.3490 | top1:64.4424 | AUROC:0.7134\n",
      "Test | 32/32 | Loss:0.3934 | MainLoss:0.3934 | SPLoss:0.0003 | CLSLoss:3.3490 | top1:63.5897 | AUROC:0.9997\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.003700\n",
      "Train | 32/32 | Loss:0.6760 | MainLoss:0.6760 | Alpha:0.3372 | SPLoss:0.0001 | CLSLoss:3.3492 | top1:57.4710 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6430 | MainLoss:0.6430 | SPLoss:0.0004 | CLSLoss:3.3496 | top1:64.2866 | AUROC:0.7400\n",
      "Test | 32/32 | Loss:0.4309 | MainLoss:0.4309 | SPLoss:0.0004 | CLSLoss:3.3496 | top1:58.8718 | AUROC:0.9994\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.6699 | MainLoss:0.6699 | Alpha:0.3375 | SPLoss:0.0002 | CLSLoss:3.3501 | top1:59.2000 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6255 | MainLoss:0.6255 | SPLoss:0.0005 | CLSLoss:3.3508 | top1:69.2960 | AUROC:0.7696\n",
      "Test | 32/32 | Loss:0.3917 | MainLoss:0.3917 | SPLoss:0.0005 | CLSLoss:3.3508 | top1:64.9231 | AUROC:0.9992\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.6672 | MainLoss:0.6672 | Alpha:0.3390 | SPLoss:0.0002 | CLSLoss:3.3513 | top1:59.7290 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.6083 | MainLoss:0.6083 | SPLoss:0.0005 | CLSLoss:3.3520 | top1:72.3645 | AUROC:0.8032\n",
      "Test | 32/32 | Loss:0.3952 | MainLoss:0.3952 | SPLoss:0.0005 | CLSLoss:3.3520 | top1:64.7051 | AUROC:0.9989\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.6543 | MainLoss:0.6543 | Alpha:0.3419 | SPLoss:0.0002 | CLSLoss:3.3530 | top1:61.9484 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.5869 | MainLoss:0.5869 | SPLoss:0.0007 | CLSLoss:3.3543 | top1:75.4922 | AUROC:0.8363\n",
      "Test | 32/32 | Loss:0.3705 | MainLoss:0.3705 | SPLoss:0.0007 | CLSLoss:3.3543 | top1:69.8846 | AUROC:0.9987\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.6447 | MainLoss:0.6447 | Alpha:0.3433 | SPLoss:0.0003 | CLSLoss:3.3555 | top1:63.8064 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.5671 | MainLoss:0.5671 | SPLoss:0.0010 | CLSLoss:3.3570 | top1:75.2960 | AUROC:0.8711\n",
      "Test | 32/32 | Loss:0.3371 | MainLoss:0.3371 | SPLoss:0.0010 | CLSLoss:3.3570 | top1:77.0897 | AUROC:0.9983\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.6222 | MainLoss:0.6222 | Alpha:0.3462 | SPLoss:0.0005 | CLSLoss:3.3587 | top1:66.5419 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.4966 | MainLoss:0.4966 | SPLoss:0.0015 | CLSLoss:3.3609 | top1:82.0530 | AUROC:0.9093\n",
      "Test | 32/32 | Loss:0.4904 | MainLoss:0.4904 | SPLoss:0.0015 | CLSLoss:3.3609 | top1:59.3974 | AUROC:0.9959\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.6004 | MainLoss:0.6004 | Alpha:0.3522 | SPLoss:0.0006 | CLSLoss:3.3626 | top1:68.2323 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.4401 | MainLoss:0.4401 | SPLoss:0.0017 | CLSLoss:3.3647 | top1:86.4424 | AUROC:0.9398\n",
      "Test | 32/32 | Loss:0.4589 | MainLoss:0.4589 | SPLoss:0.0017 | CLSLoss:3.3647 | top1:63.4744 | AUROC:0.9936\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.5824 | MainLoss:0.5824 | Alpha:0.3595 | SPLoss:0.0006 | CLSLoss:3.3657 | top1:69.8064 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.3839 | MainLoss:0.3839 | SPLoss:0.0017 | CLSLoss:3.3672 | top1:89.3209 | AUROC:0.9639\n",
      "Test | 32/32 | Loss:0.4711 | MainLoss:0.4711 | SPLoss:0.0017 | CLSLoss:3.3672 | top1:63.2179 | AUROC:0.9898\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.5509 | MainLoss:0.5509 | Alpha:0.3670 | SPLoss:0.0006 | CLSLoss:3.3688 | top1:73.4193 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.3305 | MainLoss:0.3305 | SPLoss:0.0016 | CLSLoss:3.3692 | top1:91.6698 | AUROC:0.9772\n",
      "Test | 32/32 | Loss:0.5126 | MainLoss:0.5126 | SPLoss:0.0016 | CLSLoss:3.3692 | top1:61.2949 | AUROC:0.9831\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.004000\n",
      "Train | 32/32 | Loss:0.5376 | MainLoss:0.5376 | Alpha:0.3710 | SPLoss:0.0005 | CLSLoss:3.3699 | top1:73.9871 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.3470 | MainLoss:0.3470 | SPLoss:0.0013 | CLSLoss:3.3706 | top1:88.9065 | AUROC:0.9836\n",
      "Test | 32/32 | Loss:0.4174 | MainLoss:0.4174 | SPLoss:0.0013 | CLSLoss:3.3706 | top1:68.1795 | AUROC:0.9789\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.003999\n",
      "Train | 32/32 | Loss:0.5089 | MainLoss:0.5089 | Alpha:0.3738 | SPLoss:0.0004 | CLSLoss:3.3722 | top1:75.1355 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2852 | MainLoss:0.2852 | SPLoss:0.0010 | CLSLoss:3.3732 | top1:93.3302 | AUROC:0.9888\n",
      "Test | 32/32 | Loss:0.5087 | MainLoss:0.5087 | SPLoss:0.0010 | CLSLoss:3.3732 | top1:61.2308 | AUROC:0.9637\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.003999\n",
      "Train | 32/32 | Loss:0.4980 | MainLoss:0.4980 | Alpha:0.3800 | SPLoss:0.0003 | CLSLoss:3.3738 | top1:76.5161 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2445 | MainLoss:0.2445 | SPLoss:0.0009 | CLSLoss:3.3747 | top1:95.7944 | AUROC:0.9921\n",
      "Test | 32/32 | Loss:0.6005 | MainLoss:0.6005 | SPLoss:0.0009 | CLSLoss:3.3747 | top1:55.8333 | AUROC:0.9399\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.003999\n",
      "Train | 32/32 | Loss:0.4816 | MainLoss:0.4816 | Alpha:0.3839 | SPLoss:0.0002 | CLSLoss:3.3759 | top1:77.0064 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2318 | MainLoss:0.2318 | SPLoss:0.0006 | CLSLoss:3.3770 | top1:96.0966 | AUROC:0.9932\n",
      "Test | 32/32 | Loss:0.6094 | MainLoss:0.6094 | SPLoss:0.0006 | CLSLoss:3.3770 | top1:55.3846 | AUROC:0.9213\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.003999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:0.4853 | MainLoss:0.4853 | Alpha:0.3829 | SPLoss:0.0002 | CLSLoss:3.3777 | top1:77.0452 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2173 | MainLoss:0.2173 | SPLoss:0.0005 | CLSLoss:3.3783 | top1:96.7726 | AUROC:0.9946\n",
      "Test | 32/32 | Loss:0.6441 | MainLoss:0.6441 | SPLoss:0.0005 | CLSLoss:3.3783 | top1:53.5256 | AUROC:0.8974\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.003999\n",
      "Train | 32/32 | Loss:0.4685 | MainLoss:0.4685 | Alpha:0.3867 | SPLoss:0.0001 | CLSLoss:3.3788 | top1:77.7935 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2469 | MainLoss:0.2469 | SPLoss:0.0004 | CLSLoss:3.3799 | top1:95.6729 | AUROC:0.9950\n",
      "Test | 32/32 | Loss:0.5588 | MainLoss:0.5588 | SPLoss:0.0004 | CLSLoss:3.3799 | top1:56.0641 | AUROC:0.8897\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.003998\n",
      "Train | 32/32 | Loss:0.4642 | MainLoss:0.4642 | Alpha:0.3876 | SPLoss:0.0002 | CLSLoss:3.3801 | top1:78.0774 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1978 | MainLoss:0.1978 | SPLoss:0.0004 | CLSLoss:3.3805 | top1:97.3645 | AUROC:0.9959\n",
      "Test | 32/32 | Loss:0.7112 | MainLoss:0.7112 | SPLoss:0.0004 | CLSLoss:3.3805 | top1:50.9231 | AUROC:0.8444\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.003998\n",
      "Train | 32/32 | Loss:0.4543 | MainLoss:0.4543 | Alpha:0.3896 | SPLoss:0.0001 | CLSLoss:3.3816 | top1:78.6839 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2453 | MainLoss:0.2453 | SPLoss:0.0004 | CLSLoss:3.3822 | top1:96.2648 | AUROC:0.9960\n",
      "Test | 32/32 | Loss:0.5776 | MainLoss:0.5776 | SPLoss:0.0004 | CLSLoss:3.3822 | top1:53.5513 | AUROC:0.8488\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.003998\n",
      "Train | 32/32 | Loss:0.4523 | MainLoss:0.4523 | Alpha:0.3896 | SPLoss:0.0001 | CLSLoss:3.3829 | top1:79.0064 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1829 | MainLoss:0.1829 | SPLoss:0.0003 | CLSLoss:3.3833 | top1:97.6978 | AUROC:0.9966\n",
      "Test | 32/32 | Loss:0.7686 | MainLoss:0.7686 | SPLoss:0.0003 | CLSLoss:3.3833 | top1:48.9744 | AUROC:0.7878\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.003997\n",
      "Train | 32/32 | Loss:0.4490 | MainLoss:0.4490 | Alpha:0.3897 | SPLoss:0.0001 | CLSLoss:3.3842 | top1:79.0064 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1966 | MainLoss:0.1966 | SPLoss:0.0003 | CLSLoss:3.3851 | top1:97.6760 | AUROC:0.9969\n",
      "Test | 32/32 | Loss:0.7050 | MainLoss:0.7050 | SPLoss:0.0003 | CLSLoss:3.3851 | top1:49.3718 | AUROC:0.7828\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.003997\n",
      "Train | 32/32 | Loss:0.4488 | MainLoss:0.4488 | Alpha:0.3915 | SPLoss:0.0001 | CLSLoss:3.3852 | top1:79.0839 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1939 | MainLoss:0.1939 | SPLoss:0.0002 | CLSLoss:3.3861 | top1:97.7477 | AUROC:0.9972\n",
      "Test | 32/32 | Loss:0.7096 | MainLoss:0.7096 | SPLoss:0.0002 | CLSLoss:3.3861 | top1:49.0897 | AUROC:0.7714\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.003997\n",
      "Train | 32/32 | Loss:0.4417 | MainLoss:0.4417 | Alpha:0.3932 | SPLoss:0.0001 | CLSLoss:3.3862 | top1:79.6129 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1890 | MainLoss:0.1890 | SPLoss:0.0002 | CLSLoss:3.3869 | top1:97.9720 | AUROC:0.9973\n",
      "Test | 32/32 | Loss:0.7319 | MainLoss:0.7319 | SPLoss:0.0002 | CLSLoss:3.3869 | top1:48.3590 | AUROC:0.7484\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.003996\n",
      "Train | 32/32 | Loss:0.4461 | MainLoss:0.4461 | Alpha:0.3936 | SPLoss:0.0001 | CLSLoss:3.3871 | top1:79.2258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1598 | MainLoss:0.1598 | SPLoss:0.0002 | CLSLoss:3.3876 | top1:98.1277 | AUROC:0.9975\n",
      "Test | 32/32 | Loss:0.8337 | MainLoss:0.8337 | SPLoss:0.0002 | CLSLoss:3.3876 | top1:48.1410 | AUROC:0.7131\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.003996\n",
      "Train | 32/32 | Loss:0.4527 | MainLoss:0.4527 | Alpha:0.3895 | SPLoss:0.0001 | CLSLoss:3.3871 | top1:79.1097 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1917 | MainLoss:0.1917 | SPLoss:0.0002 | CLSLoss:3.3870 | top1:98.1215 | AUROC:0.9976\n",
      "Test | 32/32 | Loss:0.7348 | MainLoss:0.7348 | SPLoss:0.0002 | CLSLoss:3.3870 | top1:48.1410 | AUROC:0.7139\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.003996\n",
      "Train | 32/32 | Loss:0.4499 | MainLoss:0.4499 | Alpha:0.3915 | SPLoss:0.0001 | CLSLoss:3.3875 | top1:78.9548 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1982 | MainLoss:0.1982 | SPLoss:0.0002 | CLSLoss:3.3879 | top1:98.0312 | AUROC:0.9977\n",
      "Test | 32/32 | Loss:0.7148 | MainLoss:0.7148 | SPLoss:0.0002 | CLSLoss:3.3879 | top1:48.1410 | AUROC:0.7097\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.003995\n",
      "Train | 32/32 | Loss:0.4357 | MainLoss:0.4357 | Alpha:0.3944 | SPLoss:0.0001 | CLSLoss:3.3887 | top1:79.8710 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1698 | MainLoss:0.1698 | SPLoss:0.0001 | CLSLoss:3.3890 | top1:98.2679 | AUROC:0.9979\n",
      "Test | 32/32 | Loss:0.7947 | MainLoss:0.7947 | SPLoss:0.0001 | CLSLoss:3.3890 | top1:48.2051 | AUROC:0.6803\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.003995\n",
      "Train | 32/32 | Loss:0.4364 | MainLoss:0.4364 | Alpha:0.3932 | SPLoss:0.0001 | CLSLoss:3.3894 | top1:79.8194 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1978 | MainLoss:0.1978 | SPLoss:0.0002 | CLSLoss:3.3901 | top1:98.0654 | AUROC:0.9980\n",
      "Test | 32/32 | Loss:0.7154 | MainLoss:0.7154 | SPLoss:0.0002 | CLSLoss:3.3901 | top1:48.3333 | AUROC:0.6910\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.003994\n",
      "Train | 32/32 | Loss:0.4343 | MainLoss:0.4343 | Alpha:0.3946 | SPLoss:0.0001 | CLSLoss:3.3904 | top1:79.9484 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.2133 | MainLoss:0.2133 | SPLoss:0.0001 | CLSLoss:3.3908 | top1:97.9346 | AUROC:0.9981\n",
      "Test | 32/32 | Loss:0.6897 | MainLoss:0.6897 | SPLoss:0.0001 | CLSLoss:3.3908 | top1:47.5385 | AUROC:0.6854\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.003994\n",
      "Train | 32/32 | Loss:0.4441 | MainLoss:0.4441 | Alpha:0.3928 | SPLoss:0.0001 | CLSLoss:3.3909 | top1:79.1355 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1603 | MainLoss:0.1603 | SPLoss:0.0002 | CLSLoss:3.3913 | top1:98.3178 | AUROC:0.9982\n",
      "Test | 32/32 | Loss:0.8149 | MainLoss:0.8149 | SPLoss:0.0002 | CLSLoss:3.3913 | top1:48.5897 | AUROC:0.6471\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.003993\n",
      "Train | 32/32 | Loss:0.4384 | MainLoss:0.4384 | Alpha:0.3930 | SPLoss:0.0001 | CLSLoss:3.3913 | top1:79.5871 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1618 | MainLoss:0.1618 | SPLoss:0.0001 | CLSLoss:3.3918 | top1:98.4891 | AUROC:0.9983\n",
      "Test | 32/32 | Loss:0.8341 | MainLoss:0.8341 | SPLoss:0.0001 | CLSLoss:3.3918 | top1:47.5769 | AUROC:0.6113\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.003993\n",
      "Train | 32/32 | Loss:0.4357 | MainLoss:0.4357 | Alpha:0.3928 | SPLoss:0.0001 | CLSLoss:3.3922 | top1:79.5226 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1630 | MainLoss:0.1630 | SPLoss:0.0001 | CLSLoss:3.3927 | top1:98.4611 | AUROC:0.9984\n",
      "Test | 32/32 | Loss:0.8241 | MainLoss:0.8241 | SPLoss:0.0001 | CLSLoss:3.3927 | top1:47.5385 | AUROC:0.6064\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.003992\n",
      "Train | 32/32 | Loss:0.4372 | MainLoss:0.4372 | Alpha:0.3932 | SPLoss:0.0001 | CLSLoss:3.3926 | top1:79.6258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1684 | MainLoss:0.1684 | SPLoss:0.0001 | CLSLoss:3.3932 | top1:98.4299 | AUROC:0.9984\n",
      "Test | 32/32 | Loss:0.7993 | MainLoss:0.7993 | SPLoss:0.0001 | CLSLoss:3.3932 | top1:48.0513 | AUROC:0.6150\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.003992\n",
      "Train | 32/32 | Loss:0.4315 | MainLoss:0.4315 | Alpha:0.3947 | SPLoss:0.0001 | CLSLoss:3.3932 | top1:80.0516 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1822 | MainLoss:0.1822 | SPLoss:0.0001 | CLSLoss:3.3938 | top1:98.4579 | AUROC:0.9985\n",
      "Test | 32/32 | Loss:0.7699 | MainLoss:0.7699 | SPLoss:0.0001 | CLSLoss:3.3938 | top1:46.9744 | AUROC:0.6113\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.003991\n",
      "Train | 32/32 | Loss:0.4336 | MainLoss:0.4336 | Alpha:0.3945 | SPLoss:0.0000 | CLSLoss:3.3941 | top1:79.5484 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1779 | MainLoss:0.1779 | SPLoss:0.0001 | CLSLoss:3.3946 | top1:98.5389 | AUROC:0.9986\n",
      "Test | 32/32 | Loss:0.7921 | MainLoss:0.7921 | SPLoss:0.0001 | CLSLoss:3.3946 | top1:45.8205 | AUROC:0.5867\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.003991\n",
      "Train | 32/32 | Loss:0.4373 | MainLoss:0.4373 | Alpha:0.3947 | SPLoss:1.5243 | CLSLoss:3.3947 | top1:79.6258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1788 | MainLoss:0.1788 | SPLoss:4.7270 | CLSLoss:3.3951 | top1:98.5981 | AUROC:0.9986\n",
      "Test | 32/32 | Loss:0.7878 | MainLoss:0.7878 | SPLoss:4.7270 | CLSLoss:3.3951 | top1:47.3590 | AUROC:0.5874\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.003990\n",
      "Train | 32/32 | Loss:0.4365 | MainLoss:0.4365 | Alpha:0.3935 | SPLoss:0.0001 | CLSLoss:3.3957 | top1:79.7548 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1969 | MainLoss:0.1969 | SPLoss:0.0002 | CLSLoss:3.3957 | top1:98.6511 | AUROC:0.9986\n",
      "Test | 32/32 | Loss:0.7712 | MainLoss:0.7712 | SPLoss:0.0002 | CLSLoss:3.3957 | top1:44.1667 | AUROC:0.5606\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.003989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:0.4282 | MainLoss:0.4282 | Alpha:0.3956 | SPLoss:0.0001 | CLSLoss:3.3967 | top1:80.1032 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1969 | MainLoss:0.1969 | SPLoss:0.0001 | CLSLoss:3.3970 | top1:98.5296 | AUROC:0.9987\n",
      "Test | 32/32 | Loss:0.7603 | MainLoss:0.7603 | SPLoss:0.0001 | CLSLoss:3.3970 | top1:45.2179 | AUROC:0.5679\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.003989\n",
      "Train | 32/32 | Loss:0.4408 | MainLoss:0.4408 | Alpha:0.3932 | SPLoss:0.0001 | CLSLoss:3.3975 | top1:79.4193 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1983 | MainLoss:0.1983 | SPLoss:0.0001 | CLSLoss:3.3974 | top1:98.5888 | AUROC:0.9988\n",
      "Test | 32/32 | Loss:0.7595 | MainLoss:0.7595 | SPLoss:0.0001 | CLSLoss:3.3974 | top1:45.1923 | AUROC:0.5606\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.003988\n",
      "Train | 32/32 | Loss:0.4337 | MainLoss:0.4337 | Alpha:0.3946 | SPLoss:0.0001 | CLSLoss:3.3979 | top1:80.3742 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1678 | MainLoss:0.1678 | SPLoss:0.0001 | CLSLoss:3.3982 | top1:98.7944 | AUROC:0.9988\n",
      "Test | 32/32 | Loss:0.8342 | MainLoss:0.8342 | SPLoss:0.0001 | CLSLoss:3.3982 | top1:46.5256 | AUROC:0.5275\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.003987\n",
      "Train | 32/32 | Loss:0.4313 | MainLoss:0.4313 | Alpha:0.3941 | SPLoss:0.0001 | CLSLoss:3.3989 | top1:79.8839 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1612 | MainLoss:0.1612 | SPLoss:0.0002 | CLSLoss:3.3995 | top1:98.7726 | AUROC:0.9989\n",
      "Test | 32/32 | Loss:0.8404 | MainLoss:0.8404 | SPLoss:0.0002 | CLSLoss:3.3995 | top1:46.9872 | AUROC:0.5318\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.003987\n",
      "Train | 32/32 | Loss:0.4243 | MainLoss:0.4243 | Alpha:0.3967 | SPLoss:0.0001 | CLSLoss:3.3997 | top1:80.4645 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1613 | MainLoss:0.1613 | SPLoss:0.0001 | CLSLoss:3.4003 | top1:98.7944 | AUROC:0.9989\n",
      "Test | 32/32 | Loss:0.8483 | MainLoss:0.8483 | SPLoss:0.0001 | CLSLoss:3.4003 | top1:45.8846 | AUROC:0.5129\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.003986\n",
      "Train | 32/32 | Loss:0.4271 | MainLoss:0.4271 | Alpha:0.3958 | SPLoss:0.0000 | CLSLoss:3.4006 | top1:80.3484 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1722 | MainLoss:0.1722 | SPLoss:0.0001 | CLSLoss:3.4005 | top1:98.8505 | AUROC:0.9989\n",
      "Test | 32/32 | Loss:0.8335 | MainLoss:0.8335 | SPLoss:0.0001 | CLSLoss:3.4005 | top1:44.8205 | AUROC:0.5000\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.003985\n",
      "Train | 32/32 | Loss:0.4269 | MainLoss:0.4269 | Alpha:0.3962 | SPLoss:0.0001 | CLSLoss:3.4011 | top1:80.4129 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1482 | MainLoss:0.1482 | SPLoss:0.0001 | CLSLoss:3.4015 | top1:98.9315 | AUROC:0.9989\n",
      "Test | 32/32 | Loss:0.9018 | MainLoss:0.9018 | SPLoss:0.0001 | CLSLoss:3.4015 | top1:45.0000 | AUROC:0.4742\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4199 | MainLoss:0.4199 | Alpha:0.3961 | SPLoss:0.0000 | CLSLoss:3.4015 | top1:80.5935 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1660 | MainLoss:0.1660 | SPLoss:0.0000 | CLSLoss:3.4016 | top1:98.8411 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8499 | MainLoss:0.8499 | SPLoss:0.0000 | CLSLoss:3.4016 | top1:44.2179 | AUROC:0.4900\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4241 | MainLoss:0.4241 | Alpha:0.3966 | SPLoss:0.0000 | CLSLoss:3.4016 | top1:80.2968 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1659 | MainLoss:0.1659 | SPLoss:0.0000 | CLSLoss:3.4016 | top1:98.8474 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8509 | MainLoss:0.8509 | SPLoss:0.0000 | CLSLoss:3.4016 | top1:44.2308 | AUROC:0.4881\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4243 | MainLoss:0.4243 | Alpha:0.3962 | SPLoss:0.0000 | CLSLoss:3.4016 | top1:80.4000 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:0.0000 | CLSLoss:3.4017 | top1:98.8474 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8460 | MainLoss:0.8460 | SPLoss:0.0000 | CLSLoss:3.4017 | top1:44.1154 | AUROC:0.4879\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4296 | MainLoss:0.4296 | Alpha:0.3947 | SPLoss:0.0000 | CLSLoss:3.4017 | top1:80.0258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1750 | MainLoss:0.1750 | SPLoss:0.0000 | CLSLoss:3.4018 | top1:98.7850 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8259 | MainLoss:0.8259 | SPLoss:0.0000 | CLSLoss:3.4018 | top1:43.8846 | AUROC:0.4990\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4224 | MainLoss:0.4224 | Alpha:0.3969 | SPLoss:0.0000 | CLSLoss:3.4019 | top1:80.2065 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1731 | MainLoss:0.1731 | SPLoss:0.0000 | CLSLoss:3.4020 | top1:98.8100 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8298 | MainLoss:0.8298 | SPLoss:0.0000 | CLSLoss:3.4020 | top1:44.1026 | AUROC:0.4983\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4295 | MainLoss:0.4295 | Alpha:0.3949 | SPLoss:0.0000 | CLSLoss:3.4020 | top1:79.8710 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1719 | MainLoss:0.1719 | SPLoss:0.0000 | CLSLoss:3.4020 | top1:98.8224 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8339 | MainLoss:0.8339 | SPLoss:0.0000 | CLSLoss:3.4020 | top1:44.1410 | AUROC:0.4949\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4221 | MainLoss:0.4221 | Alpha:0.3970 | SPLoss:0.0000 | CLSLoss:3.4021 | top1:80.4129 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1681 | MainLoss:0.1681 | SPLoss:0.0000 | CLSLoss:3.4021 | top1:98.8598 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8433 | MainLoss:0.8433 | SPLoss:0.0000 | CLSLoss:3.4021 | top1:44.3462 | AUROC:0.4902\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4222 | MainLoss:0.4222 | Alpha:0.3966 | SPLoss:0.0000 | CLSLoss:3.4022 | top1:80.5677 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1707 | MainLoss:0.1707 | SPLoss:0.0000 | CLSLoss:3.4023 | top1:98.8255 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8358 | MainLoss:0.8358 | SPLoss:0.0000 | CLSLoss:3.4023 | top1:44.1154 | AUROC:0.4952\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4288 | MainLoss:0.4288 | Alpha:0.3955 | SPLoss:0.0000 | CLSLoss:3.4023 | top1:79.9484 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1709 | MainLoss:0.1709 | SPLoss:0.0000 | CLSLoss:3.4023 | top1:98.8255 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8357 | MainLoss:0.8357 | SPLoss:0.0000 | CLSLoss:3.4023 | top1:44.1795 | AUROC:0.4938\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4282 | MainLoss:0.4282 | Alpha:0.3953 | SPLoss:0.0000 | CLSLoss:3.4024 | top1:80.1548 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1749 | MainLoss:0.1749 | SPLoss:0.0000 | CLSLoss:3.4024 | top1:98.7850 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8246 | MainLoss:0.8246 | SPLoss:0.0000 | CLSLoss:3.4024 | top1:44.0385 | AUROC:0.4997\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.000398\n",
      "Train | 32/32 | Loss:0.4238 | MainLoss:0.4238 | Alpha:0.3969 | SPLoss:0.0000 | CLSLoss:3.4025 | top1:80.1032 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1728 | MainLoss:0.1728 | SPLoss:0.0000 | CLSLoss:3.4025 | top1:98.8162 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8295 | MainLoss:0.8295 | SPLoss:0.0000 | CLSLoss:3.4025 | top1:44.1795 | AUROC:0.4984\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4218 | MainLoss:0.4218 | Alpha:0.3974 | SPLoss:0.0000 | CLSLoss:3.4026 | top1:80.6194 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1709 | MainLoss:0.1709 | SPLoss:0.0000 | CLSLoss:3.4027 | top1:98.8193 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8334 | MainLoss:0.8334 | SPLoss:0.0000 | CLSLoss:3.4027 | top1:44.3333 | AUROC:0.4961\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4201 | MainLoss:0.4201 | Alpha:0.3970 | SPLoss:0.0000 | CLSLoss:3.4027 | top1:80.4387 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1624 | MainLoss:0.1624 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:98.8723 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8574 | MainLoss:0.8574 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:44.6410 | AUROC:0.4849\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4254 | MainLoss:0.4254 | Alpha:0.3956 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:80.2581 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1710 | MainLoss:0.1710 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:98.8411 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8362 | MainLoss:0.8362 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:44.1026 | AUROC:0.4912\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4286 | MainLoss:0.4286 | Alpha:0.3953 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:80.0000 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1703 | MainLoss:0.1703 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:98.8629 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8386 | MainLoss:0.8386 | SPLoss:0.0000 | CLSLoss:3.4028 | top1:44.1667 | AUROC:0.4885\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.000397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:0.4280 | MainLoss:0.4280 | Alpha:0.3952 | SPLoss:0.0000 | CLSLoss:3.4029 | top1:80.0258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1767 | MainLoss:0.1767 | SPLoss:0.0000 | CLSLoss:3.4029 | top1:98.8131 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8235 | MainLoss:0.8235 | SPLoss:0.0000 | CLSLoss:3.4029 | top1:43.8333 | AUROC:0.4936\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4270 | MainLoss:0.4270 | Alpha:0.3959 | SPLoss:0.0000 | CLSLoss:3.4030 | top1:79.9613 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1712 | MainLoss:0.1712 | SPLoss:0.0000 | CLSLoss:3.4031 | top1:98.8567 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8364 | MainLoss:0.8364 | SPLoss:0.0000 | CLSLoss:3.4030 | top1:44.1154 | AUROC:0.4882\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4262 | MainLoss:0.4262 | Alpha:0.3960 | SPLoss:0.0000 | CLSLoss:3.4031 | top1:80.3355 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1693 | MainLoss:0.1693 | SPLoss:0.0000 | CLSLoss:3.4031 | top1:98.8754 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8422 | MainLoss:0.8422 | SPLoss:0.0000 | CLSLoss:3.4031 | top1:44.1410 | AUROC:0.4833\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4271 | MainLoss:0.4271 | Alpha:0.3955 | SPLoss:0.0000 | CLSLoss:3.4032 | top1:80.3226 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1722 | MainLoss:0.1722 | SPLoss:0.0000 | CLSLoss:3.4032 | top1:98.8442 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8338 | MainLoss:0.8338 | SPLoss:0.0000 | CLSLoss:3.4032 | top1:44.0256 | AUROC:0.4882\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.000397\n",
      "Train | 32/32 | Loss:0.4225 | MainLoss:0.4225 | Alpha:0.3966 | SPLoss:0.0000 | CLSLoss:3.4033 | top1:80.3355 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1707 | MainLoss:0.1707 | SPLoss:0.0000 | CLSLoss:3.4033 | top1:98.8598 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8381 | MainLoss:0.8381 | SPLoss:0.0000 | CLSLoss:3.4033 | top1:44.0769 | AUROC:0.4853\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4229 | MainLoss:0.4229 | Alpha:0.3967 | SPLoss:0.0000 | CLSLoss:3.4034 | top1:80.7226 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1686 | MainLoss:0.1686 | SPLoss:0.0000 | CLSLoss:3.4035 | top1:98.8661 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8430 | MainLoss:0.8430 | SPLoss:0.0000 | CLSLoss:3.4035 | top1:44.1154 | AUROC:0.4845\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4247 | MainLoss:0.4247 | Alpha:0.3963 | SPLoss:0.0000 | CLSLoss:3.4035 | top1:80.3742 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1716 | MainLoss:0.1716 | SPLoss:0.0000 | CLSLoss:3.4035 | top1:98.8598 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8379 | MainLoss:0.8379 | SPLoss:0.0000 | CLSLoss:3.4035 | top1:43.8205 | AUROC:0.4830\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4253 | MainLoss:0.4253 | Alpha:0.3960 | SPLoss:0.0000 | CLSLoss:3.4035 | top1:80.2710 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:0.0000 | CLSLoss:3.4036 | top1:98.8723 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8399 | MainLoss:0.8399 | SPLoss:0.0000 | CLSLoss:3.4036 | top1:43.8846 | AUROC:0.4820\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4260 | MainLoss:0.4260 | Alpha:0.3961 | SPLoss:0.0000 | CLSLoss:3.4036 | top1:80.2452 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1759 | MainLoss:0.1759 | SPLoss:0.0000 | CLSLoss:3.4037 | top1:98.8318 | AUROC:0.9990\n",
      "Test | 32/32 | Loss:0.8267 | MainLoss:0.8267 | SPLoss:0.0000 | CLSLoss:3.4037 | top1:43.6795 | AUROC:0.4873\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4247 | MainLoss:0.4247 | Alpha:0.3964 | SPLoss:0.0000 | CLSLoss:3.4037 | top1:80.2581 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:0.0000 | CLSLoss:3.4038 | top1:98.8474 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8365 | MainLoss:0.8365 | SPLoss:0.0000 | CLSLoss:3.4038 | top1:44.1538 | AUROC:0.4865\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4215 | MainLoss:0.4215 | Alpha:0.3967 | SPLoss:0.0000 | CLSLoss:3.4039 | top1:80.4774 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1741 | MainLoss:0.1741 | SPLoss:0.0000 | CLSLoss:3.4039 | top1:98.8006 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8280 | MainLoss:0.8280 | SPLoss:0.0000 | CLSLoss:3.4039 | top1:43.9487 | AUROC:0.4901\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4308 | MainLoss:0.4308 | Alpha:0.3947 | SPLoss:0.0000 | CLSLoss:3.4039 | top1:79.9097 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1747 | MainLoss:0.1747 | SPLoss:0.0000 | CLSLoss:3.4039 | top1:98.8380 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8283 | MainLoss:0.8283 | SPLoss:0.0000 | CLSLoss:3.4039 | top1:43.8333 | AUROC:0.4880\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.000396\n",
      "Train | 32/32 | Loss:0.4263 | MainLoss:0.4263 | Alpha:0.3962 | SPLoss:0.0000 | CLSLoss:3.4040 | top1:79.9871 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1719 | MainLoss:0.1719 | SPLoss:0.0000 | CLSLoss:3.4040 | top1:98.8629 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8344 | MainLoss:0.8344 | SPLoss:0.0000 | CLSLoss:3.4040 | top1:44.0256 | AUROC:0.4850\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4315 | MainLoss:0.4315 | Alpha:0.3945 | SPLoss:0.0000 | CLSLoss:3.4040 | top1:79.6258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1598 | MainLoss:0.1598 | SPLoss:0.0000 | CLSLoss:3.4040 | top1:98.9502 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8676 | MainLoss:0.8676 | SPLoss:0.0000 | CLSLoss:3.4040 | top1:44.7179 | AUROC:0.4702\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4242 | MainLoss:0.4242 | Alpha:0.3958 | SPLoss:0.0000 | CLSLoss:3.4041 | top1:80.1290 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1712 | MainLoss:0.1712 | SPLoss:0.0000 | CLSLoss:3.4041 | top1:98.8910 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8363 | MainLoss:0.8363 | SPLoss:0.0000 | CLSLoss:3.4041 | top1:44.1667 | AUROC:0.4818\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4251 | MainLoss:0.4251 | Alpha:0.3965 | SPLoss:0.0000 | CLSLoss:3.4042 | top1:80.2710 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1716 | MainLoss:0.1716 | SPLoss:0.0000 | CLSLoss:3.4042 | top1:98.8847 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8356 | MainLoss:0.8356 | SPLoss:0.0000 | CLSLoss:3.4042 | top1:44.1282 | AUROC:0.4826\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4241 | MainLoss:0.4241 | Alpha:0.3964 | SPLoss:0.0000 | CLSLoss:3.4043 | top1:80.3226 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1697 | MainLoss:0.1697 | SPLoss:0.0000 | CLSLoss:3.4043 | top1:98.8879 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8397 | MainLoss:0.8397 | SPLoss:0.0000 | CLSLoss:3.4043 | top1:44.1923 | AUROC:0.4814\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4277 | MainLoss:0.4277 | Alpha:0.3953 | SPLoss:0.0000 | CLSLoss:3.4044 | top1:80.0258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1765 | MainLoss:0.1765 | SPLoss:0.0000 | CLSLoss:3.4044 | top1:98.8287 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8249 | MainLoss:0.8249 | SPLoss:0.0000 | CLSLoss:3.4044 | top1:43.6667 | AUROC:0.4861\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4255 | MainLoss:0.4255 | Alpha:0.3961 | SPLoss:0.0000 | CLSLoss:3.4045 | top1:80.2839 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1774 | MainLoss:0.1774 | SPLoss:0.0000 | CLSLoss:3.4045 | top1:98.8037 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8222 | MainLoss:0.8222 | SPLoss:0.0000 | CLSLoss:3.4045 | top1:43.7179 | AUROC:0.4886\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.000395\n",
      "Train | 32/32 | Loss:0.4259 | MainLoss:0.4259 | Alpha:0.3961 | SPLoss:0.0000 | CLSLoss:3.4046 | top1:80.2323 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1690 | MainLoss:0.1690 | SPLoss:0.0000 | CLSLoss:3.4047 | top1:98.8692 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8408 | MainLoss:0.8408 | SPLoss:0.0000 | CLSLoss:3.4047 | top1:44.1667 | AUROC:0.4822\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.000394\n",
      "Train | 32/32 | Loss:0.4223 | MainLoss:0.4223 | Alpha:0.3968 | SPLoss:0.0000 | CLSLoss:3.4047 | top1:80.4258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:0.0000 | CLSLoss:3.4047 | top1:98.8847 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8432 | MainLoss:0.8432 | SPLoss:0.0000 | CLSLoss:3.4047 | top1:44.1282 | AUROC:0.4795\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.000394\n",
      "Train | 32/32 | Loss:0.4233 | MainLoss:0.4233 | Alpha:0.3967 | SPLoss:0.0000 | CLSLoss:3.4048 | top1:80.2323 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1679 | MainLoss:0.1679 | SPLoss:0.0000 | CLSLoss:3.4048 | top1:98.8816 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8439 | MainLoss:0.8439 | SPLoss:0.0000 | CLSLoss:3.4048 | top1:44.1410 | AUROC:0.4794\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.000394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 32/32 | Loss:0.4247 | MainLoss:0.4247 | Alpha:0.3964 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:80.2323 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1654 | MainLoss:0.1654 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:98.9128 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8508 | MainLoss:0.8508 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:44.4231 | AUROC:0.4749\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.000394\n",
      "Train | 32/32 | Loss:0.4219 | MainLoss:0.4219 | Alpha:0.3970 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:80.6194 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1668 | MainLoss:0.1668 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:98.9128 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8478 | MainLoss:0.8478 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:44.2949 | AUROC:0.4758\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.000394\n",
      "Train | 32/32 | Loss:0.4285 | MainLoss:0.4285 | Alpha:0.3949 | SPLoss:0.0000 | CLSLoss:3.4049 | top1:79.9226 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1723 | MainLoss:0.1723 | SPLoss:0.0000 | CLSLoss:3.4050 | top1:98.8910 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8349 | MainLoss:0.8349 | SPLoss:0.0000 | CLSLoss:3.4050 | top1:43.9231 | AUROC:0.4798\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.000394\n",
      "Train | 32/32 | Loss:0.4247 | MainLoss:0.4247 | Alpha:0.3964 | SPLoss:0.0000 | CLSLoss:3.4050 | top1:80.1935 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1664 | MainLoss:0.1664 | SPLoss:0.0000 | CLSLoss:3.4050 | top1:98.9315 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8498 | MainLoss:0.8498 | SPLoss:0.0000 | CLSLoss:3.4050 | top1:44.2949 | AUROC:0.4726\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.000394\n",
      "Train | 32/32 | Loss:0.4215 | MainLoss:0.4215 | Alpha:0.3967 | SPLoss:0.0000 | CLSLoss:3.4051 | top1:80.4258 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1749 | MainLoss:0.1749 | SPLoss:0.0000 | CLSLoss:3.4052 | top1:98.8754 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8287 | MainLoss:0.8287 | SPLoss:0.0000 | CLSLoss:3.4052 | top1:43.7949 | AUROC:0.4816\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.000393\n",
      "Train | 32/32 | Loss:0.4245 | MainLoss:0.4245 | Alpha:0.3960 | SPLoss:0.0000 | CLSLoss:3.4052 | top1:79.9613 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1726 | MainLoss:0.1726 | SPLoss:0.0000 | CLSLoss:3.4053 | top1:98.8785 | AUROC:0.9992\n",
      "Test | 32/32 | Loss:0.8338 | MainLoss:0.8338 | SPLoss:0.0000 | CLSLoss:3.4053 | top1:43.7949 | AUROC:0.4795\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.000393\n",
      "Train | 32/32 | Loss:0.4275 | MainLoss:0.4275 | Alpha:0.3957 | SPLoss:0.0000 | CLSLoss:3.4053 | top1:79.9226 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1686 | MainLoss:0.1686 | SPLoss:0.0000 | CLSLoss:3.4053 | top1:98.9128 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8447 | MainLoss:0.8447 | SPLoss:0.0000 | CLSLoss:3.4053 | top1:44.0128 | AUROC:0.4737\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.000393\n",
      "Train | 32/32 | Loss:0.4222 | MainLoss:0.4222 | Alpha:0.3968 | SPLoss:0.0000 | CLSLoss:3.4054 | top1:80.3742 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1654 | MainLoss:0.1654 | SPLoss:0.0000 | CLSLoss:3.4054 | top1:98.9533 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8541 | MainLoss:0.8541 | SPLoss:0.0000 | CLSLoss:3.4054 | top1:44.1667 | AUROC:0.4671\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.000393\n",
      "Train | 32/32 | Loss:0.4197 | MainLoss:0.4197 | Alpha:0.3971 | SPLoss:0.0000 | CLSLoss:3.4054 | top1:80.4000 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1702 | MainLoss:0.1702 | SPLoss:0.0000 | CLSLoss:3.4055 | top1:98.9034 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8411 | MainLoss:0.8411 | SPLoss:0.0000 | CLSLoss:3.4055 | top1:43.8974 | AUROC:0.4741\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.000393\n",
      "Train | 32/32 | Loss:0.4216 | MainLoss:0.4216 | Alpha:0.3967 | SPLoss:0.0000 | CLSLoss:3.4056 | top1:80.5032 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1688 | MainLoss:0.1688 | SPLoss:0.0000 | CLSLoss:3.4056 | top1:98.9065 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8436 | MainLoss:0.8436 | SPLoss:0.0000 | CLSLoss:3.4056 | top1:44.0256 | AUROC:0.4737\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.000393\n",
      "Train | 32/32 | Loss:0.4258 | MainLoss:0.4258 | Alpha:0.3962 | SPLoss:0.0000 | CLSLoss:3.4057 | top1:79.9871 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1702 | MainLoss:0.1702 | SPLoss:0.0000 | CLSLoss:3.4057 | top1:98.9034 | AUROC:0.9992\n",
      "Test | 32/32 | Loss:0.8407 | MainLoss:0.8407 | SPLoss:0.0000 | CLSLoss:3.4057 | top1:43.8974 | AUROC:0.4739\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.000392\n",
      "Train | 32/32 | Loss:0.4200 | MainLoss:0.4200 | Alpha:0.3968 | SPLoss:0.0000 | CLSLoss:3.4057 | top1:80.4903 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1689 | MainLoss:0.1689 | SPLoss:0.0000 | CLSLoss:3.4058 | top1:98.9159 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8456 | MainLoss:0.8456 | SPLoss:0.0000 | CLSLoss:3.4058 | top1:43.8846 | AUROC:0.4705\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.000392\n",
      "Train | 32/32 | Loss:0.4269 | MainLoss:0.4269 | Alpha:0.3956 | SPLoss:0.0000 | CLSLoss:3.4058 | top1:80.2452 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1758 | MainLoss:0.1758 | SPLoss:0.0000 | CLSLoss:3.4059 | top1:98.8754 | AUROC:0.9992\n",
      "Test | 32/32 | Loss:0.8277 | MainLoss:0.8277 | SPLoss:0.0000 | CLSLoss:3.4059 | top1:43.6154 | AUROC:0.4795\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.000392\n",
      "Train | 32/32 | Loss:0.4249 | MainLoss:0.4249 | Alpha:0.3962 | SPLoss:0.0000 | CLSLoss:3.4059 | top1:80.1806 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1620 | MainLoss:0.1620 | SPLoss:0.0000 | CLSLoss:3.4059 | top1:98.9688 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8637 | MainLoss:0.8637 | SPLoss:0.0000 | CLSLoss:3.4059 | top1:44.2308 | AUROC:0.4638\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.000392\n",
      "Train | 32/32 | Loss:0.4215 | MainLoss:0.4215 | Alpha:0.3968 | SPLoss:0.0000 | CLSLoss:3.4060 | top1:80.6194 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1694 | MainLoss:0.1694 | SPLoss:0.0000 | CLSLoss:3.4060 | top1:98.9128 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8434 | MainLoss:0.8434 | SPLoss:0.0000 | CLSLoss:3.4060 | top1:43.7821 | AUROC:0.4717\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.000392\n",
      "Train | 32/32 | Loss:0.4246 | MainLoss:0.4246 | Alpha:0.3965 | SPLoss:0.0000 | CLSLoss:3.4060 | top1:80.2323 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1717 | MainLoss:0.1717 | SPLoss:0.0000 | CLSLoss:3.4061 | top1:98.9097 | AUROC:0.9991\n",
      "Test | 32/32 | Loss:0.8401 | MainLoss:0.8401 | SPLoss:0.0000 | CLSLoss:3.4061 | top1:43.4744 | AUROC:0.4717\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.000392\n",
      "Train | 32/32 | Loss:0.4163 | MainLoss:0.4163 | Alpha:0.3981 | SPLoss:0.0000 | CLSLoss:3.4061 | top1:80.7742 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1706 | MainLoss:0.1706 | SPLoss:0.0000 | CLSLoss:3.4062 | top1:98.9159 | AUROC:0.9992\n",
      "Test | 32/32 | Loss:0.8428 | MainLoss:0.8428 | SPLoss:0.0000 | CLSLoss:3.4062 | top1:43.4744 | AUROC:0.4700\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.000391\n",
      "Train | 32/32 | Loss:0.4241 | MainLoss:0.4241 | Alpha:0.3964 | SPLoss:0.0000 | CLSLoss:3.4062 | top1:80.2452 | AUROC:0.0000\n",
      "Test | 129/32 | Loss:0.1701 | MainLoss:0.1701 | SPLoss:0.0000 | CLSLoss:3.4062 | top1:98.9221 | AUROC:0.9992\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
