{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import resnext50_32x4d\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/32x4d/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'resnext32x4d' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 400\n",
    "test_batch = 400\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/32x4d/to_pggan/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = 'fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/32x4d/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "student_model = resnext50_32x4d(pretrained=False, num_classes=2)\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 22.98M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (bn2): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + (sp_alpha*loss_sp) + (sp_alpha*loss_cls)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 20/20 | Loss:1.1180 | MainLoss:0.9747 | Alpha:0.0230 | SPLoss:0.6201 | CLSLoss:5.6217 | top1:52.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6707 | MainLoss:0.6707 | SPLoss:0.8541 | CLSLoss:5.1674 | top1:59.4143 | AUROC:0.6350\n",
      "Test | 39/20 | Loss:0.3558 | MainLoss:0.3558 | SPLoss:0.8541 | CLSLoss:5.1674 | top1:78.9615 | AUROC:0.9991\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 20/20 | Loss:1.4619 | MainLoss:0.6584 | Alpha:0.3377 | SPLoss:0.0132 | CLSLoss:2.3660 | top1:61.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6477 | MainLoss:0.6477 | SPLoss:0.0240 | CLSLoss:0.7219 | top1:66.0966 | AUROC:0.7369\n",
      "Test | 39/20 | Loss:0.3972 | MainLoss:0.3972 | SPLoss:0.0240 | CLSLoss:0.7219 | top1:77.7821 | AUROC:0.9984\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 20/20 | Loss:0.7684 | MainLoss:0.6636 | Alpha:0.3413 | SPLoss:0.0057 | CLSLoss:0.3015 | top1:65.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6631 | MainLoss:0.6631 | SPLoss:0.0108 | CLSLoss:0.0896 | top1:70.8567 | AUROC:0.7976\n",
      "Test | 39/20 | Loss:0.5174 | MainLoss:0.5174 | SPLoss:0.0108 | CLSLoss:0.0896 | top1:75.4231 | AUROC:0.9976\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 20/20 | Loss:0.6910 | MainLoss:0.6737 | Alpha:0.3382 | SPLoss:0.0029 | CLSLoss:0.0484 | top1:66.2750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6675 | MainLoss:0.6675 | SPLoss:0.0060 | CLSLoss:0.0293 | top1:75.6449 | AUROC:0.8407\n",
      "Test | 39/20 | Loss:0.5808 | MainLoss:0.5808 | SPLoss:0.0060 | CLSLoss:0.0293 | top1:70.7692 | AUROC:0.9959\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 20/20 | Loss:0.6813 | MainLoss:0.6696 | Alpha:0.3377 | SPLoss:0.0040 | CLSLoss:0.0304 | top1:68.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6494 | MainLoss:0.6494 | SPLoss:0.0120 | CLSLoss:0.0384 | top1:79.1713 | AUROC:0.8739\n",
      "Test | 39/20 | Loss:0.5632 | MainLoss:0.5632 | SPLoss:0.0120 | CLSLoss:0.0384 | top1:67.4231 | AUROC:0.9925\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 20/20 | Loss:0.6632 | MainLoss:0.6367 | Alpha:0.3408 | SPLoss:0.0194 | CLSLoss:0.0585 | top1:71.7000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.5784 | MainLoss:0.5784 | SPLoss:0.0442 | CLSLoss:0.0723 | top1:83.3240 | AUROC:0.9279\n",
      "Test | 39/20 | Loss:0.5402 | MainLoss:0.5402 | SPLoss:0.0442 | CLSLoss:0.0723 | top1:68.7436 | AUROC:0.9809\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 20/20 | Loss:0.6249 | MainLoss:0.5833 | Alpha:0.3508 | SPLoss:0.0287 | CLSLoss:0.0897 | top1:74.9500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.4613 | MainLoss:0.4613 | SPLoss:0.0571 | CLSLoss:0.1129 | top1:90.0093 | AUROC:0.9706\n",
      "Test | 39/20 | Loss:0.5878 | MainLoss:0.5878 | SPLoss:0.0571 | CLSLoss:0.1129 | top1:63.4359 | AUROC:0.9506\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 20/20 | Loss:0.6058 | MainLoss:0.5579 | Alpha:0.3644 | SPLoss:0.0245 | CLSLoss:0.1071 | top1:75.3250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.4009 | MainLoss:0.4009 | SPLoss:0.0330 | CLSLoss:0.1126 | top1:93.2337 | AUROC:0.9867\n",
      "Test | 39/20 | Loss:0.6415 | MainLoss:0.6415 | SPLoss:0.0330 | CLSLoss:0.1126 | top1:60.6026 | AUROC:0.9007\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 20/20 | Loss:0.5970 | MainLoss:0.5427 | Alpha:0.3701 | SPLoss:0.0320 | CLSLoss:0.1146 | top1:76.3750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.4099 | MainLoss:0.4099 | SPLoss:0.0366 | CLSLoss:0.1085 | top1:92.6480 | AUROC:0.9944\n",
      "Test | 39/20 | Loss:0.6276 | MainLoss:0.6276 | SPLoss:0.0366 | CLSLoss:0.1085 | top1:63.3718 | AUROC:0.8586\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 20/20 | Loss:0.5928 | MainLoss:0.5372 | Alpha:0.3742 | SPLoss:0.0274 | CLSLoss:0.1210 | top1:76.3000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.5717 | MainLoss:0.5717 | SPLoss:0.0618 | CLSLoss:0.1338 | top1:50.0000 | AUROC:0.9955\n",
      "Test | 39/20 | Loss:0.6127 | MainLoss:0.6127 | SPLoss:0.0618 | CLSLoss:0.1338 | top1:50.0000 | AUROC:0.9459\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 20/20 | Loss:0.5646 | MainLoss:0.5083 | Alpha:0.3504 | SPLoss:0.0318 | CLSLoss:0.1288 | top1:76.0500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.3003 | MainLoss:0.3003 | SPLoss:0.0466 | CLSLoss:0.1389 | top1:93.7882 | AUROC:0.9982\n",
      "Test | 39/20 | Loss:0.6381 | MainLoss:0.6381 | SPLoss:0.0466 | CLSLoss:0.1389 | top1:62.2820 | AUROC:0.8548\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 20/20 | Loss:0.5503 | MainLoss:0.4924 | Alpha:0.3863 | SPLoss:0.0272 | CLSLoss:0.1225 | top1:76.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.3866 | MainLoss:0.3866 | SPLoss:0.0420 | CLSLoss:0.0979 | top1:90.5078 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:0.6477 | MainLoss:0.6477 | SPLoss:0.0420 | CLSLoss:0.0979 | top1:67.4231 | AUROC:0.7227\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 20/20 | Loss:0.5765 | MainLoss:0.4786 | Alpha:0.3793 | SPLoss:0.1431 | CLSLoss:0.1155 | top1:79.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2511 | MainLoss:0.2511 | SPLoss:0.0366 | CLSLoss:0.1084 | top1:97.4268 | AUROC:0.9994\n",
      "Test | 39/20 | Loss:0.7041 | MainLoss:0.7041 | SPLoss:0.0366 | CLSLoss:0.1084 | top1:55.5000 | AUROC:0.7807\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 20/20 | Loss:0.5108 | MainLoss:0.4578 | Alpha:0.3925 | SPLoss:0.0269 | CLSLoss:0.1083 | top1:78.6750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2094 | MainLoss:0.2094 | SPLoss:0.0305 | CLSLoss:0.1002 | top1:97.7352 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.7508 | MainLoss:0.7508 | SPLoss:0.0305 | CLSLoss:0.1002 | top1:54.8462 | AUROC:0.8653\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 20/20 | Loss:0.4825 | MainLoss:0.4413 | Alpha:0.3916 | SPLoss:0.0213 | CLSLoss:0.0838 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1984 | MainLoss:0.1984 | SPLoss:0.0220 | CLSLoss:0.0924 | top1:97.8910 | AUROC:0.9992\n",
      "Test | 39/20 | Loss:0.7306 | MainLoss:0.7306 | SPLoss:0.0220 | CLSLoss:0.0924 | top1:54.7564 | AUROC:0.8235\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 20/20 | Loss:0.4724 | MainLoss:0.4337 | Alpha:0.3973 | SPLoss:0.0185 | CLSLoss:0.0790 | top1:79.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2350 | MainLoss:0.2350 | SPLoss:0.0177 | CLSLoss:0.0584 | top1:98.5514 | AUROC:0.9994\n",
      "Test | 39/20 | Loss:0.7089 | MainLoss:0.7089 | SPLoss:0.0177 | CLSLoss:0.0584 | top1:52.9231 | AUROC:0.7718\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 20/20 | Loss:0.4562 | MainLoss:0.4259 | Alpha:0.3951 | SPLoss:0.0118 | CLSLoss:0.0649 | top1:81.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1773 | MainLoss:0.1773 | SPLoss:0.0133 | CLSLoss:0.0693 | top1:98.6293 | AUROC:0.9994\n",
      "Test | 39/20 | Loss:0.7656 | MainLoss:0.7656 | SPLoss:0.0133 | CLSLoss:0.0693 | top1:52.8846 | AUROC:0.7263\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 20/20 | Loss:0.4511 | MainLoss:0.4207 | Alpha:0.3991 | SPLoss:0.0119 | CLSLoss:0.0640 | top1:79.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2838 | MainLoss:0.2838 | SPLoss:0.0242 | CLSLoss:0.0512 | top1:95.5794 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:0.6503 | MainLoss:0.6503 | SPLoss:0.0242 | CLSLoss:0.0512 | top1:60.3974 | AUROC:0.7076\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 20/20 | Loss:0.4597 | MainLoss:0.4250 | Alpha:0.3939 | SPLoss:0.0255 | CLSLoss:0.0627 | top1:81.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1843 | MainLoss:0.1843 | SPLoss:0.0170 | CLSLoss:0.0544 | top1:98.9875 | AUROC:0.9995\n",
      "Test | 39/20 | Loss:0.7535 | MainLoss:0.7535 | SPLoss:0.0170 | CLSLoss:0.0544 | top1:52.2564 | AUROC:0.7569\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 20/20 | Loss:0.4593 | MainLoss:0.4315 | Alpha:0.3983 | SPLoss:0.0120 | CLSLoss:0.0579 | top1:80.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2601 | MainLoss:0.2601 | SPLoss:0.0285 | CLSLoss:0.0568 | top1:97.0499 | AUROC:0.9995\n",
      "Test | 39/20 | Loss:0.6515 | MainLoss:0.6515 | SPLoss:0.0285 | CLSLoss:0.0568 | top1:58.0385 | AUROC:0.8495\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 20/20 | Loss:0.4329 | MainLoss:0.4068 | Alpha:0.3961 | SPLoss:0.0098 | CLSLoss:0.0561 | top1:81.5250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1774 | MainLoss:0.1774 | SPLoss:0.0145 | CLSLoss:0.0512 | top1:99.2025 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.7605 | MainLoss:0.7605 | SPLoss:0.0145 | CLSLoss:0.0512 | top1:51.7179 | AUROC:0.8535\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 20/20 | Loss:0.4385 | MainLoss:0.4149 | Alpha:0.3980 | SPLoss:0.0073 | CLSLoss:0.0520 | top1:81.2000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2232 | MainLoss:0.2232 | SPLoss:0.0072 | CLSLoss:0.0429 | top1:98.7664 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6931 | MainLoss:0.6931 | SPLoss:0.0072 | CLSLoss:0.0429 | top1:53.4359 | AUROC:0.8047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n",
      "Train | 20/20 | Loss:0.4428 | MainLoss:0.4193 | Alpha:0.3976 | SPLoss:0.0093 | CLSLoss:0.0498 | top1:80.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2199 | MainLoss:0.2199 | SPLoss:0.0133 | CLSLoss:0.0345 | top1:98.9720 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6992 | MainLoss:0.6992 | SPLoss:0.0133 | CLSLoss:0.0345 | top1:52.5897 | AUROC:0.8438\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 20/20 | Loss:0.4367 | MainLoss:0.4132 | Alpha:0.3981 | SPLoss:0.0114 | CLSLoss:0.0477 | top1:81.8750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1344 | MainLoss:0.1344 | SPLoss:0.0079 | CLSLoss:0.0602 | top1:99.2897 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.8186 | MainLoss:0.8186 | SPLoss:0.0079 | CLSLoss:0.0602 | top1:51.4872 | AUROC:0.8022\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 20/20 | Loss:0.4568 | MainLoss:0.4297 | Alpha:0.4000 | SPLoss:0.0175 | CLSLoss:0.0506 | top1:80.9000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2373 | MainLoss:0.2373 | SPLoss:0.0528 | CLSLoss:0.0539 | top1:99.1122 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6915 | MainLoss:0.6915 | SPLoss:0.0528 | CLSLoss:0.0539 | top1:52.1282 | AUROC:0.8415\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 20/20 | Loss:0.4310 | MainLoss:0.4087 | Alpha:0.3975 | SPLoss:0.0080 | CLSLoss:0.0482 | top1:81.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2059 | MainLoss:0.2059 | SPLoss:0.0088 | CLSLoss:0.0356 | top1:99.4206 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7219 | MainLoss:0.7219 | SPLoss:0.0088 | CLSLoss:0.0356 | top1:50.9231 | AUROC:0.8520\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 20/20 | Loss:0.4332 | MainLoss:0.4104 | Alpha:0.3986 | SPLoss:0.0104 | CLSLoss:0.0467 | top1:81.3250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2830 | MainLoss:0.2830 | SPLoss:0.0162 | CLSLoss:0.0310 | top1:96.7757 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6485 | MainLoss:0.6485 | SPLoss:0.0162 | CLSLoss:0.0310 | top1:59.0256 | AUROC:0.8589\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 20/20 | Loss:0.4289 | MainLoss:0.4071 | Alpha:0.3937 | SPLoss:0.0111 | CLSLoss:0.0442 | top1:81.2250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2854 | MainLoss:0.2854 | SPLoss:0.0107 | CLSLoss:0.0298 | top1:97.1963 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6519 | MainLoss:0.6519 | SPLoss:0.0107 | CLSLoss:0.0298 | top1:58.1154 | AUROC:0.8795\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 20/20 | Loss:0.4795 | MainLoss:0.4365 | Alpha:0.3941 | SPLoss:0.0467 | CLSLoss:0.0623 | top1:79.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1670 | MainLoss:0.1670 | SPLoss:0.0101 | CLSLoss:0.0555 | top1:98.7601 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7286 | MainLoss:0.7286 | SPLoss:0.0101 | CLSLoss:0.0555 | top1:53.5513 | AUROC:0.8522\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 20/20 | Loss:0.4372 | MainLoss:0.4135 | Alpha:0.4019 | SPLoss:0.0154 | CLSLoss:0.0438 | top1:81.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1475 | MainLoss:0.1475 | SPLoss:0.0207 | CLSLoss:0.0479 | top1:99.4642 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.8011 | MainLoss:0.8011 | SPLoss:0.0207 | CLSLoss:0.0479 | top1:50.7692 | AUROC:0.8581\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 20/20 | Loss:0.4223 | MainLoss:0.4013 | Alpha:0.4003 | SPLoss:0.0096 | CLSLoss:0.0428 | top1:81.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1482 | MainLoss:0.1482 | SPLoss:0.0130 | CLSLoss:0.0596 | top1:98.7850 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.7487 | MainLoss:0.7487 | SPLoss:0.0130 | CLSLoss:0.0596 | top1:53.6795 | AUROC:0.8554\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 20/20 | Loss:0.4312 | MainLoss:0.4082 | Alpha:0.4018 | SPLoss:0.0147 | CLSLoss:0.0427 | top1:81.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2004 | MainLoss:0.2004 | SPLoss:0.0110 | CLSLoss:0.0344 | top1:98.9346 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6924 | MainLoss:0.6924 | SPLoss:0.0110 | CLSLoss:0.0344 | top1:53.1667 | AUROC:0.8591\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 20/20 | Loss:0.4843 | MainLoss:0.4321 | Alpha:0.4015 | SPLoss:0.0559 | CLSLoss:0.0738 | top1:79.7500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1003 | MainLoss:0.1003 | SPLoss:0.0110 | CLSLoss:0.0756 | top1:99.4299 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.9027 | MainLoss:0.9027 | SPLoss:0.0110 | CLSLoss:0.0756 | top1:51.1795 | AUROC:0.8185\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 20/20 | Loss:0.4310 | MainLoss:0.4083 | Alpha:0.3979 | SPLoss:0.0122 | CLSLoss:0.0451 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1486 | MainLoss:0.1486 | SPLoss:0.0304 | CLSLoss:0.0437 | top1:99.4829 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7835 | MainLoss:0.7835 | SPLoss:0.0304 | CLSLoss:0.0437 | top1:51.0256 | AUROC:0.8441\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 20/20 | Loss:0.4278 | MainLoss:0.4095 | Alpha:0.4003 | SPLoss:0.0062 | CLSLoss:0.0394 | top1:81.3000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2722 | MainLoss:0.2722 | SPLoss:0.0231 | CLSLoss:0.0371 | top1:94.7850 | AUROC:0.9996\n",
      "Test | 39/20 | Loss:0.6106 | MainLoss:0.6106 | SPLoss:0.0231 | CLSLoss:0.0371 | top1:63.3590 | AUROC:0.8613\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 20/20 | Loss:0.4397 | MainLoss:0.4063 | Alpha:0.3962 | SPLoss:0.0306 | CLSLoss:0.0537 | top1:81.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2254 | MainLoss:0.2254 | SPLoss:0.0353 | CLSLoss:0.0395 | top1:98.7539 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6802 | MainLoss:0.6802 | SPLoss:0.0353 | CLSLoss:0.0395 | top1:53.2949 | AUROC:0.8584\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 20/20 | Loss:0.4336 | MainLoss:0.4116 | Alpha:0.4002 | SPLoss:0.0137 | CLSLoss:0.0414 | top1:81.7000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1590 | MainLoss:0.1590 | SPLoss:0.0139 | CLSLoss:0.0539 | top1:99.3271 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7409 | MainLoss:0.7409 | SPLoss:0.0139 | CLSLoss:0.0539 | top1:51.4103 | AUROC:0.8922\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 20/20 | Loss:0.4201 | MainLoss:0.3984 | Alpha:0.4030 | SPLoss:0.0109 | CLSLoss:0.0429 | top1:82.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2054 | MainLoss:0.2054 | SPLoss:0.0080 | CLSLoss:0.0346 | top1:99.3053 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6974 | MainLoss:0.6974 | SPLoss:0.0080 | CLSLoss:0.0346 | top1:51.7564 | AUROC:0.8811\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 20/20 | Loss:0.4172 | MainLoss:0.3982 | Alpha:0.4020 | SPLoss:0.0068 | CLSLoss:0.0404 | top1:81.7500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2809 | MainLoss:0.2809 | SPLoss:0.0083 | CLSLoss:0.0231 | top1:98.8692 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6713 | MainLoss:0.6713 | SPLoss:0.0083 | CLSLoss:0.0231 | top1:53.5513 | AUROC:0.8771\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 20/20 | Loss:0.4315 | MainLoss:0.4049 | Alpha:0.3940 | SPLoss:0.0191 | CLSLoss:0.0484 | top1:81.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2048 | MainLoss:0.2048 | SPLoss:0.0098 | CLSLoss:0.0342 | top1:98.9470 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6878 | MainLoss:0.6878 | SPLoss:0.0098 | CLSLoss:0.0342 | top1:53.5128 | AUROC:0.8648\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 20/20 | Loss:0.4278 | MainLoss:0.4077 | Alpha:0.4011 | SPLoss:0.0095 | CLSLoss:0.0406 | top1:81.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1294 | MainLoss:0.1294 | SPLoss:0.0094 | CLSLoss:0.0482 | top1:99.5452 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.8233 | MainLoss:0.8233 | SPLoss:0.0094 | CLSLoss:0.0482 | top1:51.0128 | AUROC:0.8258\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 20/20 | Loss:0.4272 | MainLoss:0.4089 | Alpha:0.4010 | SPLoss:0.0080 | CLSLoss:0.0379 | top1:81.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1495 | MainLoss:0.1495 | SPLoss:0.0088 | CLSLoss:0.0474 | top1:99.4237 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7612 | MainLoss:0.7612 | SPLoss:0.0088 | CLSLoss:0.0474 | top1:51.5769 | AUROC:0.8569\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 20/20 | Loss:0.4094 | MainLoss:0.3887 | Alpha:0.4054 | SPLoss:0.0102 | CLSLoss:0.0409 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1360 | MainLoss:0.1360 | SPLoss:0.0175 | CLSLoss:0.0547 | top1:98.8193 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.7892 | MainLoss:0.7892 | SPLoss:0.0175 | CLSLoss:0.0547 | top1:54.1923 | AUROC:0.8125\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 20/20 | Loss:0.5085 | MainLoss:0.4460 | Alpha:0.4036 | SPLoss:0.0967 | CLSLoss:0.0580 | top1:78.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1953 | MainLoss:0.1953 | SPLoss:0.0507 | CLSLoss:0.0362 | top1:99.4735 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7842 | MainLoss:0.7842 | SPLoss:0.0507 | CLSLoss:0.0362 | top1:50.8718 | AUROC:0.8360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n",
      "Train | 20/20 | Loss:0.4248 | MainLoss:0.4041 | Alpha:0.3943 | SPLoss:0.0089 | CLSLoss:0.0436 | top1:81.8250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2162 | MainLoss:0.2162 | SPLoss:0.0174 | CLSLoss:0.0312 | top1:98.5826 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6634 | MainLoss:0.6634 | SPLoss:0.0174 | CLSLoss:0.0312 | top1:54.7821 | AUROC:0.8875\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 20/20 | Loss:0.4133 | MainLoss:0.3896 | Alpha:0.4040 | SPLoss:0.0170 | CLSLoss:0.0415 | top1:82.2750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2665 | MainLoss:0.2665 | SPLoss:0.0281 | CLSLoss:0.0536 | top1:93.7259 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.5893 | MainLoss:0.5893 | SPLoss:0.0281 | CLSLoss:0.0536 | top1:64.9231 | AUROC:0.8852\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 20/20 | Loss:0.5278 | MainLoss:0.4447 | Alpha:0.3998 | SPLoss:0.1504 | CLSLoss:0.0572 | top1:79.2500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.7761 | MainLoss:0.7761 | SPLoss:2.2823 | CLSLoss:0.1554 | top1:49.9595 | AUROC:0.0009\n",
      "Test | 39/20 | Loss:0.7344 | MainLoss:0.7344 | SPLoss:2.2823 | CLSLoss:0.1554 | top1:50.0000 | AUROC:0.2622\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 20/20 | Loss:0.6175 | MainLoss:0.5612 | Alpha:0.3189 | SPLoss:0.0406 | CLSLoss:0.1362 | top1:73.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.3247 | MainLoss:0.3247 | SPLoss:0.0863 | CLSLoss:0.1589 | top1:98.5358 | AUROC:0.9996\n",
      "Test | 39/20 | Loss:0.6682 | MainLoss:0.6682 | SPLoss:0.0863 | CLSLoss:0.1589 | top1:54.1154 | AUROC:0.8662\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 20/20 | Loss:0.5167 | MainLoss:0.4522 | Alpha:0.3849 | SPLoss:0.0401 | CLSLoss:0.1273 | top1:79.9500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1312 | MainLoss:0.1312 | SPLoss:0.0684 | CLSLoss:0.1437 | top1:99.3832 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:1.0321 | MainLoss:1.0321 | SPLoss:0.0684 | CLSLoss:0.1437 | top1:50.3590 | AUROC:0.7830\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 20/20 | Loss:0.4889 | MainLoss:0.4403 | Alpha:0.3807 | SPLoss:0.0319 | CLSLoss:0.0954 | top1:79.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2718 | MainLoss:0.2718 | SPLoss:0.0387 | CLSLoss:0.0711 | top1:97.7726 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6876 | MainLoss:0.6876 | SPLoss:0.0387 | CLSLoss:0.0711 | top1:56.1154 | AUROC:0.5740\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 20/20 | Loss:0.4728 | MainLoss:0.4282 | Alpha:0.3953 | SPLoss:0.0326 | CLSLoss:0.0801 | top1:79.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1952 | MainLoss:0.1952 | SPLoss:0.0229 | CLSLoss:0.0616 | top1:98.8629 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7076 | MainLoss:0.7076 | SPLoss:0.0229 | CLSLoss:0.0616 | top1:53.2308 | AUROC:0.8199\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 20/20 | Loss:0.4176 | MainLoss:0.3913 | Alpha:0.4023 | SPLoss:0.0010 | CLSLoss:0.0643 | top1:82.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1828 | MainLoss:0.1828 | SPLoss:0.0025 | CLSLoss:0.0647 | top1:98.5296 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7148 | MainLoss:0.7148 | SPLoss:0.0025 | CLSLoss:0.0647 | top1:54.3846 | AUROC:0.7580\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 20/20 | Loss:0.4242 | MainLoss:0.3997 | Alpha:0.4015 | SPLoss:0.0004 | CLSLoss:0.0608 | top1:81.4000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1951 | MainLoss:0.1951 | SPLoss:0.0009 | CLSLoss:0.0555 | top1:98.3645 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7035 | MainLoss:0.7035 | SPLoss:0.0009 | CLSLoss:0.0555 | top1:54.7692 | AUROC:0.7374\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 20/20 | Loss:0.4197 | MainLoss:0.3968 | Alpha:0.4020 | SPLoss:0.0005 | CLSLoss:0.0565 | top1:81.9000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1894 | MainLoss:0.1894 | SPLoss:0.0010 | CLSLoss:0.0533 | top1:98.3863 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7086 | MainLoss:0.7086 | SPLoss:0.0010 | CLSLoss:0.0533 | top1:54.7436 | AUROC:0.7337\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 20/20 | Loss:0.4202 | MainLoss:0.3988 | Alpha:0.4017 | SPLoss:0.0004 | CLSLoss:0.0530 | top1:81.7000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1810 | MainLoss:0.1810 | SPLoss:0.0009 | CLSLoss:0.0520 | top1:98.5576 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7169 | MainLoss:0.7169 | SPLoss:0.0009 | CLSLoss:0.0520 | top1:54.3462 | AUROC:0.7437\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 20/20 | Loss:0.4090 | MainLoss:0.3878 | Alpha:0.4042 | SPLoss:0.0004 | CLSLoss:0.0521 | top1:82.0500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1668 | MainLoss:0.1668 | SPLoss:0.0009 | CLSLoss:0.0541 | top1:98.6168 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7338 | MainLoss:0.7338 | SPLoss:0.0009 | CLSLoss:0.0541 | top1:54.2692 | AUROC:0.7432\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 20/20 | Loss:0.4142 | MainLoss:0.3939 | Alpha:0.4028 | SPLoss:0.0004 | CLSLoss:0.0501 | top1:81.8750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1773 | MainLoss:0.1773 | SPLoss:0.0009 | CLSLoss:0.0478 | top1:98.6106 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7245 | MainLoss:0.7245 | SPLoss:0.0009 | CLSLoss:0.0478 | top1:54.1923 | AUROC:0.7164\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 20/20 | Loss:0.4128 | MainLoss:0.3941 | Alpha:0.4029 | SPLoss:0.0004 | CLSLoss:0.0462 | top1:82.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:0.0008 | CLSLoss:0.0483 | top1:98.6324 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7325 | MainLoss:0.7325 | SPLoss:0.0008 | CLSLoss:0.0483 | top1:54.1795 | AUROC:0.7324\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 20/20 | Loss:0.4109 | MainLoss:0.3922 | Alpha:0.4034 | SPLoss:0.0004 | CLSLoss:0.0460 | top1:82.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1907 | MainLoss:0.1907 | SPLoss:0.0009 | CLSLoss:0.0447 | top1:97.9751 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6998 | MainLoss:0.6998 | SPLoss:0.0009 | CLSLoss:0.0447 | top1:56.1410 | AUROC:0.7140\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 20/20 | Loss:0.4123 | MainLoss:0.3943 | Alpha:0.4027 | SPLoss:0.0004 | CLSLoss:0.0444 | top1:82.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1710 | MainLoss:0.1710 | SPLoss:0.0008 | CLSLoss:0.0434 | top1:98.6044 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7278 | MainLoss:0.7278 | SPLoss:0.0008 | CLSLoss:0.0434 | top1:54.2692 | AUROC:0.7316\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 20/20 | Loss:0.4036 | MainLoss:0.3861 | Alpha:0.4047 | SPLoss:0.0004 | CLSLoss:0.0429 | top1:82.2250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1764 | MainLoss:0.1764 | SPLoss:0.0010 | CLSLoss:0.0440 | top1:98.1558 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7138 | MainLoss:0.7138 | SPLoss:0.0010 | CLSLoss:0.0440 | top1:55.6795 | AUROC:0.7226\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 20/20 | Loss:0.4072 | MainLoss:0.3903 | Alpha:0.4040 | SPLoss:0.0004 | CLSLoss:0.0415 | top1:82.1750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1667 | MainLoss:0.1667 | SPLoss:0.0008 | CLSLoss:0.0421 | top1:98.5078 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7269 | MainLoss:0.7269 | SPLoss:0.0008 | CLSLoss:0.0421 | top1:54.5897 | AUROC:0.7566\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 20/20 | Loss:0.4042 | MainLoss:0.3877 | Alpha:0.4043 | SPLoss:0.0005 | CLSLoss:0.0403 | top1:82.1250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1866 | MainLoss:0.1866 | SPLoss:0.0009 | CLSLoss:0.0379 | top1:98.2492 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6993 | MainLoss:0.6993 | SPLoss:0.0009 | CLSLoss:0.0379 | top1:55.5000 | AUROC:0.7595\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 20/20 | Loss:0.4050 | MainLoss:0.3891 | Alpha:0.4040 | SPLoss:0.0004 | CLSLoss:0.0389 | top1:81.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1817 | MainLoss:0.1817 | SPLoss:0.0008 | CLSLoss:0.0393 | top1:98.0218 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6989 | MainLoss:0.6989 | SPLoss:0.0008 | CLSLoss:0.0393 | top1:56.1795 | AUROC:0.7599\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 20/20 | Loss:0.3984 | MainLoss:0.3815 | Alpha:0.4057 | SPLoss:0.0005 | CLSLoss:0.0412 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1771 | MainLoss:0.1771 | SPLoss:0.0010 | CLSLoss:0.0383 | top1:98.1059 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7032 | MainLoss:0.7032 | SPLoss:0.0010 | CLSLoss:0.0383 | top1:55.9359 | AUROC:0.7688\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 20/20 | Loss:0.4066 | MainLoss:0.3913 | Alpha:0.4037 | SPLoss:0.0005 | CLSLoss:0.0375 | top1:81.7000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1874 | MainLoss:0.1874 | SPLoss:0.0008 | CLSLoss:0.0347 | top1:98.0561 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6934 | MainLoss:0.6934 | SPLoss:0.0008 | CLSLoss:0.0347 | top1:56.0000 | AUROC:0.7701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n",
      "Train | 20/20 | Loss:0.3954 | MainLoss:0.3805 | Alpha:0.4058 | SPLoss:0.0004 | CLSLoss:0.0364 | top1:82.4250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.0010 | CLSLoss:0.0418 | top1:98.4704 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7374 | MainLoss:0.7374 | SPLoss:0.0010 | CLSLoss:0.0418 | top1:54.9359 | AUROC:0.7616\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 20/20 | Loss:0.3994 | MainLoss:0.3835 | Alpha:0.4053 | SPLoss:0.0006 | CLSLoss:0.0386 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1737 | MainLoss:0.1737 | SPLoss:0.0011 | CLSLoss:0.0350 | top1:98.4735 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7103 | MainLoss:0.7103 | SPLoss:0.0011 | CLSLoss:0.0350 | top1:54.9359 | AUROC:0.7807\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 20/20 | Loss:0.4017 | MainLoss:0.3866 | Alpha:0.4043 | SPLoss:0.0007 | CLSLoss:0.0364 | top1:81.6000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1859 | MainLoss:0.1859 | SPLoss:0.0011 | CLSLoss:0.0347 | top1:97.8069 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6867 | MainLoss:0.6867 | SPLoss:0.0011 | CLSLoss:0.0347 | top1:56.7051 | AUROC:0.7842\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 20/20 | Loss:0.3959 | MainLoss:0.3809 | Alpha:0.4057 | SPLoss:0.0005 | CLSLoss:0.0363 | top1:82.3250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2161 | MainLoss:0.2161 | SPLoss:0.0013 | CLSLoss:0.0327 | top1:96.3115 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6511 | MainLoss:0.6511 | SPLoss:0.0013 | CLSLoss:0.0327 | top1:61.1538 | AUROC:0.7667\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 20/20 | Loss:0.3970 | MainLoss:0.3831 | Alpha:0.4041 | SPLoss:0.0008 | CLSLoss:0.0338 | top1:82.2500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1538 | MainLoss:0.1538 | SPLoss:0.0016 | CLSLoss:0.0380 | top1:98.4455 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7359 | MainLoss:0.7359 | SPLoss:0.0016 | CLSLoss:0.0380 | top1:55.0641 | AUROC:0.7768\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 20/20 | Loss:0.3957 | MainLoss:0.3808 | Alpha:0.4057 | SPLoss:0.0007 | CLSLoss:0.0361 | top1:81.9000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1576 | MainLoss:0.1576 | SPLoss:0.0016 | CLSLoss:0.0337 | top1:98.7227 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7336 | MainLoss:0.7336 | SPLoss:0.0016 | CLSLoss:0.0337 | top1:54.2436 | AUROC:0.7958\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 20/20 | Loss:0.3926 | MainLoss:0.3784 | Alpha:0.4060 | SPLoss:0.0006 | CLSLoss:0.0345 | top1:82.6000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1877 | MainLoss:0.1877 | SPLoss:0.0014 | CLSLoss:0.0348 | top1:97.1682 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.6766 | MainLoss:0.6766 | SPLoss:0.0014 | CLSLoss:0.0348 | top1:59.0385 | AUROC:0.7761\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 20/20 | Loss:0.3912 | MainLoss:0.3769 | Alpha:0.4071 | SPLoss:0.0009 | CLSLoss:0.0342 | top1:82.9000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1568 | MainLoss:0.1568 | SPLoss:0.0015 | CLSLoss:0.0337 | top1:98.5327 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7302 | MainLoss:0.7302 | SPLoss:0.0015 | CLSLoss:0.0337 | top1:55.0769 | AUROC:0.7832\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 20/20 | Loss:0.3849 | MainLoss:0.3710 | Alpha:0.4083 | SPLoss:0.0006 | CLSLoss:0.0335 | top1:83.1250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1599 | MainLoss:0.1599 | SPLoss:0.0014 | CLSLoss:0.0369 | top1:97.8723 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7210 | MainLoss:0.7210 | SPLoss:0.0014 | CLSLoss:0.0369 | top1:56.7564 | AUROC:0.7630\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 20/20 | Loss:0.3962 | MainLoss:0.3823 | Alpha:0.4059 | SPLoss:0.0009 | CLSLoss:0.0334 | top1:82.0750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1816 | MainLoss:0.1816 | SPLoss:0.0014 | CLSLoss:0.0336 | top1:97.1651 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6818 | MainLoss:0.6818 | SPLoss:0.0014 | CLSLoss:0.0336 | top1:58.7949 | AUROC:0.7876\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 20/20 | Loss:0.3983 | MainLoss:0.3848 | Alpha:0.4055 | SPLoss:0.0011 | CLSLoss:0.0322 | top1:81.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1832 | MainLoss:0.1832 | SPLoss:0.0015 | CLSLoss:0.0300 | top1:97.6480 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6808 | MainLoss:0.6808 | SPLoss:0.0015 | CLSLoss:0.0300 | top1:57.9103 | AUROC:0.7990\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 20/20 | Loss:0.3951 | MainLoss:0.3822 | Alpha:0.4056 | SPLoss:0.0007 | CLSLoss:0.0311 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1839 | MainLoss:0.1839 | SPLoss:0.0013 | CLSLoss:0.0309 | top1:97.3770 | AUROC:0.9997\n",
      "Test | 39/20 | Loss:0.6810 | MainLoss:0.6810 | SPLoss:0.0013 | CLSLoss:0.0309 | top1:58.3846 | AUROC:0.7940\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 20/20 | Loss:0.3856 | MainLoss:0.3720 | Alpha:0.4087 | SPLoss:0.0014 | CLSLoss:0.0320 | top1:83.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1486 | MainLoss:0.1486 | SPLoss:0.0023 | CLSLoss:0.0342 | top1:98.4330 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7453 | MainLoss:0.7453 | SPLoss:0.0023 | CLSLoss:0.0342 | top1:55.3590 | AUROC:0.7789\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 20/20 | Loss:0.3853 | MainLoss:0.3712 | Alpha:0.4079 | SPLoss:0.0010 | CLSLoss:0.0334 | top1:83.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1699 | MainLoss:0.1699 | SPLoss:0.0014 | CLSLoss:0.0305 | top1:97.9875 | AUROC:0.9998\n",
      "Test | 39/20 | Loss:0.7035 | MainLoss:0.7035 | SPLoss:0.0014 | CLSLoss:0.0305 | top1:56.4872 | AUROC:0.7932\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
