{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style2/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 250\n",
    "test_batch = 250\n",
    "lr = 0.1\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/b0/to_pggan/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style2/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 200, gamma=0.1, last_epoch=-1)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=100, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, ac=arc.avg))\n",
    "    return (losses.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, ac=arc.avg))\n",
    "    return (losses.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 16/16 | Loss:0.9536 | MainLoss:0.9310 | Alpha:0.0275 | SPLoss:0.8322 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6927 | MainLoss:0.6927 | SPLoss:0.9268 | CLSLoss:0.0000 | AUROC:0.5754\n",
      "Test | 31/16 | Loss:0.7149 | MainLoss:0.7149 | SPLoss:0.9268 | CLSLoss:0.0000 | AUROC:0.0225\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.103000\n",
      "Train | 16/16 | Loss:0.7200 | MainLoss:0.6956 | Alpha:0.0274 | SPLoss:0.8872 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6927 | MainLoss:0.6927 | SPLoss:0.8524 | CLSLoss:0.0000 | AUROC:0.5588\n",
      "Test | 31/16 | Loss:0.7051 | MainLoss:0.7051 | SPLoss:0.8524 | CLSLoss:0.0000 | AUROC:0.1297\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.106000\n",
      "Train | 16/16 | Loss:0.7168 | MainLoss:0.6952 | Alpha:0.0265 | SPLoss:0.8195 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6933 | MainLoss:0.6933 | SPLoss:0.7790 | CLSLoss:0.0000 | AUROC:0.5641\n",
      "Test | 31/16 | Loss:0.7048 | MainLoss:0.7048 | SPLoss:0.7790 | CLSLoss:0.0000 | AUROC:0.2240\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.109000\n",
      "Train | 16/16 | Loss:0.7152 | MainLoss:0.6944 | Alpha:0.0278 | SPLoss:0.7461 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6946 | MainLoss:0.6946 | SPLoss:0.7234 | CLSLoss:0.0000 | AUROC:0.5658\n",
      "Test | 31/16 | Loss:0.7015 | MainLoss:0.7015 | SPLoss:0.7234 | CLSLoss:0.0000 | AUROC:0.3676\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.112000\n",
      "Train | 16/16 | Loss:0.7120 | MainLoss:0.6940 | Alpha:0.0262 | SPLoss:0.6897 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6916 | MainLoss:0.6916 | SPLoss:0.6622 | CLSLoss:0.0000 | AUROC:0.5711\n",
      "Test | 31/16 | Loss:0.6933 | MainLoss:0.6933 | SPLoss:0.6622 | CLSLoss:0.0000 | AUROC:0.5823\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.115000\n",
      "Train | 16/16 | Loss:0.7118 | MainLoss:0.6939 | Alpha:0.0283 | SPLoss:0.6333 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6923 | MainLoss:0.6923 | SPLoss:0.6086 | CLSLoss:0.0000 | AUROC:0.5883\n",
      "Test | 31/16 | Loss:0.6899 | MainLoss:0.6899 | SPLoss:0.6086 | CLSLoss:0.0000 | AUROC:0.7044\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.118000\n",
      "Train | 16/16 | Loss:0.7078 | MainLoss:0.6925 | Alpha:0.0263 | SPLoss:0.5802 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6902 | MainLoss:0.6902 | SPLoss:0.5522 | CLSLoss:0.0000 | AUROC:0.6359\n",
      "Test | 31/16 | Loss:0.6375 | MainLoss:0.6375 | SPLoss:0.5522 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.121000\n",
      "Train | 16/16 | Loss:0.7055 | MainLoss:0.6906 | Alpha:0.0281 | SPLoss:0.5305 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6799 | MainLoss:0.6799 | SPLoss:0.5156 | CLSLoss:0.0000 | AUROC:0.7042\n",
      "Test | 31/16 | Loss:0.6482 | MainLoss:0.6482 | SPLoss:0.5156 | CLSLoss:0.0000 | AUROC:0.9858\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.124000\n",
      "Train | 16/16 | Loss:0.6961 | MainLoss:0.6827 | Alpha:0.0269 | SPLoss:0.4959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5966 | MainLoss:0.5966 | SPLoss:0.4900 | CLSLoss:0.0000 | AUROC:0.8893\n",
      "Test | 31/16 | Loss:0.5176 | MainLoss:0.5176 | SPLoss:0.4900 | CLSLoss:0.0000 | AUROC:0.9769\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.127000\n",
      "Train | 16/16 | Loss:0.6252 | MainLoss:0.6039 | Alpha:0.0264 | SPLoss:0.8577 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2436 | MainLoss:0.2436 | SPLoss:2.0451 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "Test | 31/16 | Loss:0.7316 | MainLoss:0.7316 | SPLoss:2.0451 | CLSLoss:0.0000 | AUROC:0.6780\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.130000\n",
      "Train | 16/16 | Loss:0.5413 | MainLoss:0.4860 | Alpha:0.0279 | SPLoss:1.9807 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1905 | MainLoss:0.1905 | SPLoss:1.9258 | CLSLoss:0.0000 | AUROC:0.9975\n",
      "Test | 31/16 | Loss:0.6982 | MainLoss:0.6982 | SPLoss:1.9258 | CLSLoss:0.0000 | AUROC:0.9041\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.133000\n",
      "Train | 16/16 | Loss:0.5486 | MainLoss:0.4743 | Alpha:0.0270 | SPLoss:2.9127 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1673 | MainLoss:0.1673 | SPLoss:5.6516 | CLSLoss:0.0000 | AUROC:0.9980\n",
      "Test | 31/16 | Loss:0.8407 | MainLoss:0.8407 | SPLoss:5.6516 | CLSLoss:0.0000 | AUROC:0.9412\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.136000\n",
      "Train | 16/16 | Loss:0.5904 | MainLoss:0.4466 | Alpha:0.0267 | SPLoss:5.3838 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1541 | MainLoss:0.1541 | SPLoss:5.1163 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.7110 | MainLoss:0.7110 | SPLoss:5.1163 | CLSLoss:0.0000 | AUROC:0.9740\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.139000\n",
      "Train | 16/16 | Loss:0.5834 | MainLoss:0.4563 | Alpha:0.0260 | SPLoss:4.8632 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1653 | MainLoss:0.1653 | SPLoss:4.6737 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.6500 | MainLoss:0.6500 | SPLoss:4.6737 | CLSLoss:0.0000 | AUROC:0.9716\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.142000\n",
      "Train | 16/16 | Loss:0.5616 | MainLoss:0.4450 | Alpha:0.0260 | SPLoss:4.4740 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2096 | MainLoss:0.2096 | SPLoss:4.2519 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5379 | MainLoss:0.5379 | SPLoss:4.2519 | CLSLoss:0.0000 | AUROC:0.9837\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.145000\n",
      "Train | 16/16 | Loss:0.5408 | MainLoss:0.4303 | Alpha:0.0275 | SPLoss:4.0024 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1514 | MainLoss:0.1514 | SPLoss:3.7896 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.6964 | MainLoss:0.6964 | SPLoss:3.7896 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.148000\n",
      "Train | 16/16 | Loss:0.5244 | MainLoss:0.4255 | Alpha:0.0275 | SPLoss:3.6057 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1745 | MainLoss:0.1745 | SPLoss:3.4000 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6140 | MainLoss:0.6140 | SPLoss:3.4000 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.151000\n",
      "Train | 16/16 | Loss:0.5261 | MainLoss:0.4353 | Alpha:0.0281 | SPLoss:3.2272 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2705 | MainLoss:0.2705 | SPLoss:3.0466 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.3964 | MainLoss:0.3964 | SPLoss:3.0466 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.154000\n",
      "Train | 16/16 | Loss:0.5146 | MainLoss:0.4354 | Alpha:0.0270 | SPLoss:2.9408 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1738 | MainLoss:0.1738 | SPLoss:2.8602 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6754 | MainLoss:0.6754 | SPLoss:2.8602 | CLSLoss:0.0000 | AUROC:0.9905\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.157000\n",
      "Train | 16/16 | Loss:0.5132 | MainLoss:0.4364 | Alpha:0.0282 | SPLoss:2.7188 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2816 | MainLoss:0.2816 | SPLoss:2.5486 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.3739 | MainLoss:0.3739 | SPLoss:2.5486 | CLSLoss:0.0000 | AUROC:0.9923\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.160000\n",
      "Train | 16/16 | Loss:0.5046 | MainLoss:0.4379 | Alpha:0.0269 | SPLoss:2.4851 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2373 | MainLoss:0.2373 | SPLoss:2.3345 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4605 | MainLoss:0.4605 | SPLoss:2.3345 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.163000\n",
      "Train | 16/16 | Loss:0.4832 | MainLoss:0.4226 | Alpha:0.0272 | SPLoss:2.2224 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1709 | MainLoss:0.1709 | SPLoss:2.1291 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.5876 | MainLoss:0.5876 | SPLoss:2.1291 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.166000\n",
      "Train | 16/16 | Loss:0.4983 | MainLoss:0.4426 | Alpha:0.0262 | SPLoss:2.1142 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1616 | MainLoss:0.1616 | SPLoss:2.0340 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.6581 | MainLoss:0.6581 | SPLoss:2.0340 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.169000\n",
      "Train | 16/16 | Loss:0.4796 | MainLoss:0.4267 | Alpha:0.0269 | SPLoss:1.9645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1676 | MainLoss:0.1676 | SPLoss:1.9527 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.7562 | MainLoss:0.7562 | SPLoss:1.9527 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.172000\n",
      "Train | 16/16 | Loss:0.6875 | MainLoss:0.4344 | Alpha:0.0258 | SPLoss:9.8584 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1431 | MainLoss:0.1431 | SPLoss:22.6525 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6775 | MainLoss:0.6775 | SPLoss:22.6525 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.175000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.9971 | MainLoss:0.4173 | Alpha:0.0276 | SPLoss:20.9926 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1942 | MainLoss:0.1942 | SPLoss:19.1745 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5478 | MainLoss:0.5478 | SPLoss:19.1746 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.178000\n",
      "Train | 16/16 | Loss:0.8953 | MainLoss:0.4304 | Alpha:0.0260 | SPLoss:17.8656 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3074 | MainLoss:0.3074 | SPLoss:16.5499 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.5628 | MainLoss:0.5628 | SPLoss:16.5499 | CLSLoss:0.0000 | AUROC:0.8461\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.181000\n",
      "Train | 16/16 | Loss:0.8414 | MainLoss:0.4360 | Alpha:0.0264 | SPLoss:15.3600 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2111 | MainLoss:0.2111 | SPLoss:14.1016 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.5132 | MainLoss:0.5132 | SPLoss:14.1016 | CLSLoss:0.0000 | AUROC:0.9935\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.184000\n",
      "Train | 16/16 | Loss:0.7626 | MainLoss:0.4243 | Alpha:0.0258 | SPLoss:13.0561 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:12.0773 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5099 | MainLoss:0.5099 | SPLoss:12.0773 | CLSLoss:0.0000 | AUROC:0.9870\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.187000\n",
      "Train | 16/16 | Loss:0.7406 | MainLoss:0.4397 | Alpha:0.0268 | SPLoss:11.2870 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2479 | MainLoss:0.2479 | SPLoss:10.3231 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.4223 | MainLoss:0.4223 | SPLoss:10.3231 | CLSLoss:0.0000 | AUROC:0.9846\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.190000\n",
      "Train | 16/16 | Loss:0.6820 | MainLoss:0.4256 | Alpha:0.0268 | SPLoss:9.5687 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1308 | MainLoss:0.1308 | SPLoss:8.7925 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7194 | MainLoss:0.7194 | SPLoss:8.7925 | CLSLoss:0.0000 | AUROC:0.9922\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.193000\n",
      "Train | 16/16 | Loss:0.6537 | MainLoss:0.4295 | Alpha:0.0275 | SPLoss:8.1546 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2784 | MainLoss:0.2784 | SPLoss:7.6088 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4463 | MainLoss:0.4463 | SPLoss:7.6088 | CLSLoss:0.0000 | AUROC:0.9813\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.196000\n",
      "Train | 16/16 | Loss:0.6099 | MainLoss:0.4225 | Alpha:0.0262 | SPLoss:7.1693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1963 | MainLoss:0.1963 | SPLoss:6.5287 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5162 | MainLoss:0.5162 | SPLoss:6.5287 | CLSLoss:0.0000 | AUROC:0.9963\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.199000\n",
      "Train | 16/16 | Loss:0.5970 | MainLoss:0.4238 | Alpha:0.0283 | SPLoss:6.1000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1679 | MainLoss:0.1679 | SPLoss:5.5992 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.5973 | MainLoss:0.5973 | SPLoss:5.5992 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.202000\n",
      "Train | 16/16 | Loss:0.6067 | MainLoss:0.4592 | Alpha:0.0277 | SPLoss:5.3250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1918 | MainLoss:0.1918 | SPLoss:5.2086 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5805 | MainLoss:0.5805 | SPLoss:5.2086 | CLSLoss:0.0000 | AUROC:0.9549\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.205000\n",
      "Train | 16/16 | Loss:0.5565 | MainLoss:0.4237 | Alpha:0.0278 | SPLoss:4.7555 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2022 | MainLoss:0.2022 | SPLoss:4.3282 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.5342 | MainLoss:0.5342 | SPLoss:4.3282 | CLSLoss:0.0000 | AUROC:0.9960\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.208000\n",
      "Train | 16/16 | Loss:0.5299 | MainLoss:0.4191 | Alpha:0.0271 | SPLoss:4.1084 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2080 | MainLoss:0.2080 | SPLoss:3.7567 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5155 | MainLoss:0.5155 | SPLoss:3.7567 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.211000\n",
      "Train | 16/16 | Loss:0.5147 | MainLoss:0.4188 | Alpha:0.0272 | SPLoss:3.5034 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1500 | MainLoss:0.1500 | SPLoss:3.2922 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.6476 | MainLoss:0.6476 | SPLoss:3.2922 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.214000\n",
      "Train | 16/16 | Loss:0.5014 | MainLoss:0.4169 | Alpha:0.0276 | SPLoss:3.0744 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2733 | MainLoss:0.2733 | SPLoss:2.8313 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.3989 | MainLoss:0.3989 | SPLoss:2.8313 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.217000\n",
      "Train | 16/16 | Loss:0.5002 | MainLoss:0.4164 | Alpha:0.0286 | SPLoss:2.9586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1011 | MainLoss:0.1011 | SPLoss:4.2435 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.8111 | MainLoss:0.8111 | SPLoss:4.2435 | CLSLoss:0.0000 | AUROC:0.9965\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.220000\n",
      "Train | 16/16 | Loss:0.5443 | MainLoss:0.4356 | Alpha:0.0268 | SPLoss:4.0488 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1855 | MainLoss:0.1855 | SPLoss:3.8617 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5800 | MainLoss:0.5800 | SPLoss:3.8617 | CLSLoss:0.0000 | AUROC:0.9673\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.223000\n",
      "Train | 16/16 | Loss:0.5193 | MainLoss:0.4224 | Alpha:0.0274 | SPLoss:3.5237 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2249 | MainLoss:0.2249 | SPLoss:3.1888 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.4743 | MainLoss:0.4743 | SPLoss:3.1888 | CLSLoss:0.0000 | AUROC:0.9968\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.226000\n",
      "Train | 16/16 | Loss:0.5042 | MainLoss:0.4222 | Alpha:0.0273 | SPLoss:2.9947 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1741 | MainLoss:0.1741 | SPLoss:2.8867 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.5790 | MainLoss:0.5790 | SPLoss:2.8867 | CLSLoss:0.0000 | AUROC:0.9912\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.229000\n",
      "Train | 16/16 | Loss:0.9616 | MainLoss:0.5935 | Alpha:0.0274 | SPLoss:13.0907 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5201 | MainLoss:0.5201 | SPLoss:31.1258 | CLSLoss:0.0000 | AUROC:0.9975\n",
      "Test | 31/16 | Loss:0.7043 | MainLoss:0.7043 | SPLoss:31.1258 | CLSLoss:0.0000 | AUROC:0.5136\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.232000\n",
      "Train | 16/16 | Loss:1.2955 | MainLoss:0.5089 | Alpha:0.0280 | SPLoss:28.1896 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2592 | MainLoss:0.2592 | SPLoss:25.0187 | CLSLoss:0.0000 | AUROC:0.9963\n",
      "Test | 31/16 | Loss:0.7659 | MainLoss:0.7659 | SPLoss:25.0187 | CLSLoss:0.0000 | AUROC:0.3224\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.235000\n",
      "Train | 16/16 | Loss:1.0904 | MainLoss:0.4510 | Alpha:0.0283 | SPLoss:22.7015 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2177 | MainLoss:0.2177 | SPLoss:19.9035 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.7633 | MainLoss:0.7633 | SPLoss:19.9035 | CLSLoss:0.0000 | AUROC:0.4024\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.238000\n",
      "Train | 16/16 | Loss:1.2607 | MainLoss:0.6957 | Alpha:0.0273 | SPLoss:20.8378 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6881 | MainLoss:0.6881 | SPLoss:27.0855 | CLSLoss:0.0000 | AUROC:0.7741\n",
      "Test | 31/16 | Loss:0.7017 | MainLoss:0.7017 | SPLoss:27.0855 | CLSLoss:0.0000 | AUROC:0.1586\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.241000\n",
      "Train | 16/16 | Loss:1.3540 | MainLoss:0.6695 | Alpha:0.0282 | SPLoss:24.3027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5690 | MainLoss:0.5690 | SPLoss:21.3540 | CLSLoss:0.0000 | AUROC:0.9533\n",
      "Test | 31/16 | Loss:0.7153 | MainLoss:0.7153 | SPLoss:21.3539 | CLSLoss:0.0000 | AUROC:0.4265\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.244000\n",
      "Train | 16/16 | Loss:1.1163 | MainLoss:0.5713 | Alpha:0.0284 | SPLoss:19.3357 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4183 | MainLoss:0.4183 | SPLoss:16.9621 | CLSLoss:0.0000 | AUROC:0.9941\n",
      "Test | 31/16 | Loss:0.7604 | MainLoss:0.7604 | SPLoss:16.9621 | CLSLoss:0.0000 | AUROC:0.5088\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.247000\n",
      "Train | 16/16 | Loss:0.9129 | MainLoss:0.4816 | Alpha:0.0281 | SPLoss:15.4424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2013 | MainLoss:0.2013 | SPLoss:13.6310 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.8602 | MainLoss:0.8602 | SPLoss:13.6310 | CLSLoss:0.0000 | AUROC:0.3663\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.7647 | MainLoss:0.4478 | Alpha:0.0256 | SPLoss:12.2905 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1306 | MainLoss:0.1306 | SPLoss:11.0187 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.9202 | MainLoss:0.9202 | SPLoss:11.0187 | CLSLoss:0.0000 | AUROC:0.4247\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.253000\n",
      "Train | 16/16 | Loss:0.8021 | MainLoss:0.5159 | Alpha:0.0276 | SPLoss:10.3459 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1890 | MainLoss:0.1890 | SPLoss:9.3145 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.8038 | MainLoss:0.8038 | SPLoss:9.3145 | CLSLoss:0.0000 | AUROC:0.4420\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.256000\n",
      "Train | 16/16 | Loss:0.6693 | MainLoss:0.4434 | Alpha:0.0270 | SPLoss:8.3797 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2265 | MainLoss:0.2265 | SPLoss:7.4109 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7533 | MainLoss:0.7533 | SPLoss:7.4109 | CLSLoss:0.0000 | AUROC:0.4434\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.259000\n",
      "Train | 16/16 | Loss:0.6250 | MainLoss:0.4383 | Alpha:0.0279 | SPLoss:6.7133 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1853 | MainLoss:0.1853 | SPLoss:5.9488 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.8055 | MainLoss:0.8055 | SPLoss:5.9488 | CLSLoss:0.0000 | AUROC:0.3034\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.262000\n",
      "Train | 16/16 | Loss:0.6001 | MainLoss:0.4521 | Alpha:0.0272 | SPLoss:5.4192 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1831 | MainLoss:0.1831 | SPLoss:4.9712 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7982 | MainLoss:0.7982 | SPLoss:4.9712 | CLSLoss:0.0000 | AUROC:0.4758\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.265000\n",
      "Train | 16/16 | Loss:0.6004 | MainLoss:0.4709 | Alpha:0.0281 | SPLoss:4.6049 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1639 | MainLoss:0.1639 | SPLoss:4.3334 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.8476 | MainLoss:0.8476 | SPLoss:4.3334 | CLSLoss:0.0000 | AUROC:0.3008\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.268000\n",
      "Train | 16/16 | Loss:0.5646 | MainLoss:0.4490 | Alpha:0.0294 | SPLoss:3.9557 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1940 | MainLoss:0.1940 | SPLoss:3.4642 | CLSLoss:0.0000 | AUROC:0.9998\n",
      "Test | 31/16 | Loss:0.8047 | MainLoss:0.8047 | SPLoss:3.4642 | CLSLoss:0.0000 | AUROC:0.2725\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.271000\n",
      "Train | 16/16 | Loss:0.5273 | MainLoss:0.4408 | Alpha:0.0265 | SPLoss:3.2650 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1713 | MainLoss:0.1713 | SPLoss:2.9686 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.8270 | MainLoss:0.8270 | SPLoss:2.9686 | CLSLoss:0.0000 | AUROC:0.3477\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.274000\n",
      "Train | 16/16 | Loss:0.6306 | MainLoss:0.5219 | Alpha:0.0278 | SPLoss:3.9091 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:3.9167 | CLSLoss:0.0000 | AUROC:0.9998\n",
      "Test | 31/16 | Loss:0.8534 | MainLoss:0.8534 | SPLoss:3.9167 | CLSLoss:0.0000 | AUROC:0.3407\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.277000\n",
      "Train | 16/16 | Loss:0.5400 | MainLoss:0.4421 | Alpha:0.0277 | SPLoss:3.5527 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1483 | MainLoss:0.1483 | SPLoss:3.1307 | CLSLoss:0.0000 | AUROC:0.9998\n",
      "Test | 31/16 | Loss:0.9241 | MainLoss:0.9241 | SPLoss:3.1307 | CLSLoss:0.0000 | AUROC:0.2244\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.280000\n",
      "Train | 16/16 | Loss:0.9996 | MainLoss:0.4776 | Alpha:0.0271 | SPLoss:19.8012 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2364 | MainLoss:0.2364 | SPLoss:32.0693 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7540 | MainLoss:0.7540 | SPLoss:32.0693 | CLSLoss:0.0000 | AUROC:0.4442\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.283000\n",
      "Train | 16/16 | Loss:1.2408 | MainLoss:0.4436 | Alpha:0.0282 | SPLoss:28.1985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1923 | MainLoss:0.1923 | SPLoss:24.3469 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.7954 | MainLoss:0.7954 | SPLoss:24.3469 | CLSLoss:0.0000 | AUROC:0.3859\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.286000\n",
      "Train | 16/16 | Loss:1.2143 | MainLoss:0.5205 | Alpha:0.0268 | SPLoss:25.6856 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.9217 | MainLoss:0.9217 | SPLoss:33.1147 | CLSLoss:0.0000 | AUROC:0.7807\n",
      "Test | 31/16 | Loss:1.0492 | MainLoss:1.0492 | SPLoss:33.1147 | CLSLoss:0.0000 | AUROC:0.4456\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.289000\n",
      "Train | 16/16 | Loss:1.3304 | MainLoss:0.5255 | Alpha:0.0272 | SPLoss:29.6490 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1713 | MainLoss:0.1713 | SPLoss:25.4599 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.8520 | MainLoss:0.8520 | SPLoss:25.4599 | CLSLoss:0.0000 | AUROC:0.3385\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.292000\n",
      "Train | 16/16 | Loss:1.1037 | MainLoss:0.4713 | Alpha:0.0282 | SPLoss:22.4146 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2582 | MainLoss:0.2582 | SPLoss:19.4200 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.7504 | MainLoss:0.7504 | SPLoss:19.4199 | CLSLoss:0.0000 | AUROC:0.4360\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.295000\n",
      "Train | 16/16 | Loss:0.8985 | MainLoss:0.4450 | Alpha:0.0264 | SPLoss:17.0730 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1984 | MainLoss:0.1984 | SPLoss:14.8895 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.8088 | MainLoss:0.8088 | SPLoss:14.8895 | CLSLoss:0.0000 | AUROC:0.3838\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.298000\n",
      "Train | 16/16 | Loss:0.7921 | MainLoss:0.4413 | Alpha:0.0265 | SPLoss:13.2556 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1956 | MainLoss:0.1956 | SPLoss:11.5410 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.8045 | MainLoss:0.8045 | SPLoss:11.5410 | CLSLoss:0.0000 | AUROC:0.3715\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.301000\n",
      "Train | 16/16 | Loss:0.7190 | MainLoss:0.4414 | Alpha:0.0261 | SPLoss:10.6589 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2041 | MainLoss:0.2041 | SPLoss:10.0385 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7766 | MainLoss:0.7766 | SPLoss:10.0385 | CLSLoss:0.0000 | AUROC:0.5082\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.304000\n",
      "Train | 16/16 | Loss:0.6898 | MainLoss:0.4446 | Alpha:0.0275 | SPLoss:8.9596 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1991 | MainLoss:0.1991 | SPLoss:7.7357 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.7831 | MainLoss:0.7831 | SPLoss:7.7357 | CLSLoss:0.0000 | AUROC:0.4443\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.307000\n",
      "Train | 16/16 | Loss:0.6195 | MainLoss:0.4346 | Alpha:0.0264 | SPLoss:7.0226 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1690 | MainLoss:0.1690 | SPLoss:6.1457 | CLSLoss:0.0000 | AUROC:0.9998\n",
      "Test | 31/16 | Loss:0.8059 | MainLoss:0.8059 | SPLoss:6.1457 | CLSLoss:0.0000 | AUROC:0.5429\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.310000\n",
      "Train | 16/16 | Loss:0.9970 | MainLoss:0.6740 | Alpha:0.0263 | SPLoss:11.8269 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6727 | MainLoss:0.6727 | SPLoss:17.3552 | CLSLoss:0.0000 | AUROC:0.9050\n",
      "Test | 31/16 | Loss:0.6952 | MainLoss:0.6952 | SPLoss:17.3552 | CLSLoss:0.0000 | AUROC:0.4819\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.313000\n",
      "Train | 16/16 | Loss:1.0410 | MainLoss:0.6242 | Alpha:0.0276 | SPLoss:15.1279 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3430 | MainLoss:0.3430 | SPLoss:12.8259 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.7276 | MainLoss:0.7276 | SPLoss:12.8259 | CLSLoss:0.0000 | AUROC:0.5045\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.316000\n",
      "Train | 16/16 | Loss:0.7882 | MainLoss:0.4824 | Alpha:0.0272 | SPLoss:11.2579 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2055 | MainLoss:0.2055 | SPLoss:9.5571 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7913 | MainLoss:0.7913 | SPLoss:9.5571 | CLSLoss:0.0000 | AUROC:0.4897\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.319000\n",
      "Train | 16/16 | Loss:0.7372 | MainLoss:0.4944 | Alpha:0.0283 | SPLoss:8.5344 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2124 | MainLoss:0.2124 | SPLoss:7.4520 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.7733 | MainLoss:0.7733 | SPLoss:7.4520 | CLSLoss:0.0000 | AUROC:0.5333\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.322000\n",
      "Train | 16/16 | Loss:0.6337 | MainLoss:0.4559 | Alpha:0.0271 | SPLoss:6.5779 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1739 | MainLoss:0.1739 | SPLoss:5.6516 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.8301 | MainLoss:0.8301 | SPLoss:5.6516 | CLSLoss:0.0000 | AUROC:0.5092\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.325000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6198 | MainLoss:0.4818 | Alpha:0.0264 | SPLoss:5.2055 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1756 | MainLoss:0.1756 | SPLoss:4.7857 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.8279 | MainLoss:0.8279 | SPLoss:4.7857 | CLSLoss:0.0000 | AUROC:0.3775\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.328000\n",
      "Train | 16/16 | Loss:0.6027 | MainLoss:0.4375 | Alpha:0.0278 | SPLoss:5.9840 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1704 | MainLoss:0.1704 | SPLoss:5.8313 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.8393 | MainLoss:0.8393 | SPLoss:5.8313 | CLSLoss:0.0000 | AUROC:0.3804\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.331000\n",
      "Train | 16/16 | Loss:0.5903 | MainLoss:0.4465 | Alpha:0.0277 | SPLoss:5.1654 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2857 | MainLoss:0.2857 | SPLoss:4.6590 | CLSLoss:0.0000 | AUROC:0.9975\n",
      "Test | 31/16 | Loss:0.7331 | MainLoss:0.7331 | SPLoss:4.6590 | CLSLoss:0.0000 | AUROC:0.4188\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.334000\n",
      "Train | 16/16 | Loss:5.8386 | MainLoss:0.4556 | Alpha:0.0283 | SPLoss:170.0710 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1616 | MainLoss:0.1616 | SPLoss:915.2017 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.8418 | MainLoss:0.8418 | SPLoss:915.2022 | CLSLoss:0.0000 | AUROC:0.3423\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.337000\n",
      "Train | 16/16 | Loss:22.0561 | MainLoss:0.4397 | Alpha:0.0274 | SPLoss:784.9752 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2596 | MainLoss:0.2596 | SPLoss:658.6002 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 31/16 | Loss:0.9796 | MainLoss:0.9796 | SPLoss:658.6006 | CLSLoss:0.0000 | AUROC:0.4606\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.340000\n",
      "Train | 16/16 | Loss:15.9984 | MainLoss:0.7858 | Alpha:0.0263 | SPLoss:582.3040 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6904 | MainLoss:0.6904 | SPLoss:487.7163 | CLSLoss:0.0000 | AUROC:0.8158\n",
      "Test | 31/16 | Loss:0.7018 | MainLoss:0.7018 | SPLoss:487.7169 | CLSLoss:0.0000 | AUROC:0.3354\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.343000\n",
      "Train | 16/16 | Loss:12.0748 | MainLoss:0.6054 | Alpha:0.0274 | SPLoss:421.1215 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2199 | MainLoss:0.2199 | SPLoss:349.3868 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7856 | MainLoss:0.7856 | SPLoss:349.3869 | CLSLoss:0.0000 | AUROC:0.5162\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.346000\n",
      "Train | 16/16 | Loss:12.6591 | MainLoss:0.5509 | Alpha:0.0273 | SPLoss:439.6911 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2328 | MainLoss:0.2328 | SPLoss:412.0412 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7353 | MainLoss:0.7353 | SPLoss:412.0417 | CLSLoss:0.0000 | AUROC:0.5879\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.349000\n",
      "Train | 16/16 | Loss:9.6849 | MainLoss:0.4650 | Alpha:0.0259 | SPLoss:351.8578 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1953 | MainLoss:0.1953 | SPLoss:298.4037 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.7796 | MainLoss:0.7796 | SPLoss:298.4034 | CLSLoss:0.0000 | AUROC:0.5946\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.352000\n",
      "Train | 16/16 | Loss:7.4179 | MainLoss:0.4868 | Alpha:0.0271 | SPLoss:253.8192 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3871 | MainLoss:0.3871 | SPLoss:213.4500 | CLSLoss:0.0000 | AUROC:0.9845\n",
      "Test | 31/16 | Loss:0.6944 | MainLoss:0.6944 | SPLoss:213.4500 | CLSLoss:0.0000 | AUROC:0.5824\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.355000\n",
      "Train | 16/16 | Loss:21.8453 | MainLoss:0.6172 | Alpha:0.0262 | SPLoss:816.9197 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1883 | MainLoss:0.1883 | SPLoss:912.6486 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.8610 | MainLoss:0.8610 | SPLoss:912.6499 | CLSLoss:0.0000 | AUROC:0.4763\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.358000\n",
      "Train | 16/16 | Loss:21.8203 | MainLoss:0.4550 | Alpha:0.0274 | SPLoss:777.6412 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1716 | MainLoss:0.1716 | SPLoss:642.9335 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.8312 | MainLoss:0.8312 | SPLoss:642.9338 | CLSLoss:0.0000 | AUROC:0.5380\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.361000\n",
      "Train | 16/16 | Loss:15.3784 | MainLoss:0.4526 | Alpha:0.0272 | SPLoss:550.8495 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2035 | MainLoss:0.2035 | SPLoss:452.3720 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7856 | MainLoss:0.7856 | SPLoss:452.3721 | CLSLoss:0.0000 | AUROC:0.5783\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.364000\n",
      "Train | 16/16 | Loss:10.8827 | MainLoss:0.6289 | Alpha:0.0262 | SPLoss:388.7984 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6951 | MainLoss:0.6951 | SPLoss:344.0479 | CLSLoss:0.0000 | AUROC:0.4458\n",
      "Test | 31/16 | Loss:0.6939 | MainLoss:0.6939 | SPLoss:344.0476 | CLSLoss:0.0000 | AUROC:0.4937\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.367000\n",
      "Train | 16/16 | Loss:8.5433 | MainLoss:0.6971 | Alpha:0.0268 | SPLoss:295.6867 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6711 | MainLoss:0.6711 | SPLoss:241.9885 | CLSLoss:0.0000 | AUROC:0.9375\n",
      "Test | 31/16 | Loss:0.6947 | MainLoss:0.6947 | SPLoss:241.9889 | CLSLoss:0.0000 | AUROC:0.5059\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.370000\n",
      "Train | 16/16 | Loss:6.0234 | MainLoss:0.6472 | Alpha:0.0260 | SPLoss:205.3386 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4672 | MainLoss:0.4672 | SPLoss:171.3632 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 31/16 | Loss:0.7168 | MainLoss:0.7168 | SPLoss:171.3630 | CLSLoss:0.0000 | AUROC:0.4855\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.373000\n",
      "Train | 16/16 | Loss:4.3698 | MainLoss:0.5988 | Alpha:0.0256 | SPLoss:145.9704 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5475 | MainLoss:0.5475 | SPLoss:122.3521 | CLSLoss:0.0000 | AUROC:0.9972\n",
      "Test | 31/16 | Loss:0.7630 | MainLoss:0.7630 | SPLoss:122.3522 | CLSLoss:0.0000 | AUROC:0.5013\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.376000\n",
      "Train | 16/16 | Loss:3.5594 | MainLoss:0.6340 | Alpha:0.0269 | SPLoss:109.5779 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2962 | MainLoss:0.2962 | SPLoss:90.1056 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.7547 | MainLoss:0.7547 | SPLoss:90.1056 | CLSLoss:0.0000 | AUROC:0.5006\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.379000\n",
      "Train | 16/16 | Loss:11.8543 | MainLoss:0.4883 | Alpha:0.0265 | SPLoss:415.6195 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2318 | MainLoss:0.2318 | SPLoss:461.5711 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7693 | MainLoss:0.7693 | SPLoss:461.5709 | CLSLoss:0.0000 | AUROC:0.5283\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.382000\n",
      "Train | 16/16 | Loss:10.8851 | MainLoss:0.5421 | Alpha:0.0256 | SPLoss:401.8012 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2683 | MainLoss:0.2683 | SPLoss:374.8429 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.8913 | MainLoss:0.8913 | SPLoss:374.8430 | CLSLoss:0.0000 | AUROC:0.3926\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.385000\n",
      "Train | 16/16 | Loss:9.2886 | MainLoss:0.6271 | Alpha:0.0270 | SPLoss:320.2622 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6945 | MainLoss:0.6945 | SPLoss:266.9740 | CLSLoss:0.0000 | AUROC:0.5002\n",
      "Test | 31/16 | Loss:0.6951 | MainLoss:0.6951 | SPLoss:266.9740 | CLSLoss:0.0000 | AUROC:0.4215\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.388000\n",
      "Train | 16/16 | Loss:6.6703 | MainLoss:0.5825 | Alpha:0.0270 | SPLoss:225.8262 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2399 | MainLoss:0.2399 | SPLoss:184.1680 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.7942 | MainLoss:0.7942 | SPLoss:184.1679 | CLSLoss:0.0000 | AUROC:0.3388\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.391000\n",
      "Train | 16/16 | Loss:4.6240 | MainLoss:0.5059 | Alpha:0.0262 | SPLoss:159.3118 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2501 | MainLoss:0.2501 | SPLoss:129.0002 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7396 | MainLoss:0.7396 | SPLoss:129.0003 | CLSLoss:0.0000 | AUROC:0.4116\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.394000\n",
      "Train | 16/16 | Loss:25.1485 | MainLoss:0.4544 | Alpha:0.0266 | SPLoss:816.9463 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3056 | MainLoss:0.3056 | SPLoss:3948.4395 | CLSLoss:0.0000 | AUROC:0.9973\n",
      "Test | 31/16 | Loss:0.7272 | MainLoss:0.7272 | SPLoss:3948.4321 | CLSLoss:0.0000 | AUROC:0.7430\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.397000\n",
      "Train | 16/16 | Loss:88.5464 | MainLoss:0.5810 | Alpha:0.0264 | SPLoss:3352.5740 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:2715.7441 | CLSLoss:0.0000 | AUROC:0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 31/16 | Loss:0.7723 | MainLoss:0.7723 | SPLoss:2715.7441 | CLSLoss:0.0000 | AUROC:0.5714\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:61.3824 | MainLoss:0.4740 | Alpha:0.0266 | SPLoss:2272.1147 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2336 | MainLoss:0.2336 | SPLoss:1854.6195 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.7633 | MainLoss:0.7633 | SPLoss:1854.6163 | CLSLoss:0.0000 | AUROC:0.2179\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:41.8196 | MainLoss:0.4990 | Alpha:0.0264 | SPLoss:1552.3323 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2509 | MainLoss:0.2509 | SPLoss:1272.0585 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 31/16 | Loss:0.7575 | MainLoss:0.7575 | SPLoss:1272.0582 | CLSLoss:0.0000 | AUROC:0.4528\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.399999\n",
      "Train | 16/16 | Loss:28.6358 | MainLoss:0.4477 | Alpha:0.0262 | SPLoss:1065.3915 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1447 | MainLoss:0.1447 | SPLoss:873.2485 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.8875 | MainLoss:0.8875 | SPLoss:873.2494 | CLSLoss:0.0000 | AUROC:0.6640\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.399996\n",
      "Train | 16/16 | Loss:19.9696 | MainLoss:0.4570 | Alpha:0.0265 | SPLoss:731.6455 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2687 | MainLoss:0.2687 | SPLoss:598.4587 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.7110 | MainLoss:0.7110 | SPLoss:598.4583 | CLSLoss:0.0000 | AUROC:0.6577\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.399991\n",
      "Train | 16/16 | Loss:13.8141 | MainLoss:0.4639 | Alpha:0.0264 | SPLoss:501.8287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2763 | MainLoss:0.2763 | SPLoss:412.2587 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.7175 | MainLoss:0.7175 | SPLoss:412.2580 | CLSLoss:0.0000 | AUROC:0.5433\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.399984\n",
      "Train | 16/16 | Loss:9.9267 | MainLoss:0.4593 | Alpha:0.0274 | SPLoss:342.7369 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2293 | MainLoss:0.2293 | SPLoss:279.1720 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.7528 | MainLoss:0.7528 | SPLoss:279.1720 | CLSLoss:0.0000 | AUROC:0.4840\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.399975\n",
      "Train | 16/16 | Loss:6.7138 | MainLoss:0.5657 | Alpha:0.0259 | SPLoss:237.0867 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2201 | MainLoss:0.2201 | SPLoss:195.3795 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7827 | MainLoss:0.7827 | SPLoss:195.3795 | CLSLoss:0.0000 | AUROC:0.4514\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.399964\n",
      "Train | 16/16 | Loss:4.9228 | MainLoss:0.4505 | Alpha:0.0273 | SPLoss:165.4450 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2258 | MainLoss:0.2258 | SPLoss:132.7229 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.7524 | MainLoss:0.7524 | SPLoss:132.7228 | CLSLoss:0.0000 | AUROC:0.3956\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.399952\n",
      "Train | 16/16 | Loss:3.4914 | MainLoss:0.4467 | Alpha:0.0273 | SPLoss:112.2581 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1963 | MainLoss:0.1963 | SPLoss:90.0230 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7891 | MainLoss:0.7891 | SPLoss:90.0230 | CLSLoss:0.0000 | AUROC:0.3621\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.399937\n",
      "Train | 16/16 | Loss:2.3954 | MainLoss:0.4385 | Alpha:0.0256 | SPLoss:76.1605 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1660 | MainLoss:0.1660 | SPLoss:62.7586 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.8115 | MainLoss:0.8115 | SPLoss:62.7586 | CLSLoss:0.0000 | AUROC:0.6510\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.399920\n",
      "Train | 16/16 | Loss:1.9066 | MainLoss:0.4720 | Alpha:0.0269 | SPLoss:52.8159 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1959 | MainLoss:0.1959 | SPLoss:43.7877 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7705 | MainLoss:0.7705 | SPLoss:43.7877 | CLSLoss:0.0000 | AUROC:0.6626\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.399901\n",
      "Train | 16/16 | Loss:1.4388 | MainLoss:0.4404 | Alpha:0.0271 | SPLoss:37.0528 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2076 | MainLoss:0.2076 | SPLoss:30.1493 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.7630 | MainLoss:0.7630 | SPLoss:30.1493 | CLSLoss:0.0000 | AUROC:0.4055\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.399881\n",
      "Train | 16/16 | Loss:1.1167 | MainLoss:0.4530 | Alpha:0.0257 | SPLoss:25.5165 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2373 | MainLoss:0.2373 | SPLoss:21.3534 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.6945 | MainLoss:0.6945 | SPLoss:21.3534 | CLSLoss:0.0000 | AUROC:0.8970\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.399858\n",
      "Train | 16/16 | Loss:0.9791 | MainLoss:0.4598 | Alpha:0.0278 | SPLoss:18.6564 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4057 | MainLoss:0.4057 | SPLoss:16.4278 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.6960 | MainLoss:0.6960 | SPLoss:16.4278 | CLSLoss:0.0000 | AUROC:0.7056\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.399833\n",
      "Train | 16/16 | Loss:0.8820 | MainLoss:0.4736 | Alpha:0.0280 | SPLoss:14.4882 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3916 | MainLoss:0.3916 | SPLoss:15.2487 | CLSLoss:0.0000 | AUROC:0.9971\n",
      "Test | 31/16 | Loss:1.2610 | MainLoss:1.2610 | SPLoss:15.2487 | CLSLoss:0.0000 | AUROC:0.3946\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.399807\n",
      "Train | 16/16 | Loss:3.7606 | MainLoss:0.5327 | Alpha:0.0279 | SPLoss:115.0558 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2254 | MainLoss:0.2254 | SPLoss:101.7379 | CLSLoss:0.0000 | AUROC:0.9980\n",
      "Test | 31/16 | Loss:0.7462 | MainLoss:0.7462 | SPLoss:101.7378 | CLSLoss:0.0000 | AUROC:0.6102\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.399778\n",
      "Train | 16/16 | Loss:2.7647 | MainLoss:0.4438 | Alpha:0.0272 | SPLoss:84.9004 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1560 | MainLoss:0.1560 | SPLoss:69.2345 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "Test | 31/16 | Loss:0.8204 | MainLoss:0.8204 | SPLoss:69.2345 | CLSLoss:0.0000 | AUROC:0.6709\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.399747\n",
      "Train | 16/16 | Loss:2.0052 | MainLoss:0.4369 | Alpha:0.0269 | SPLoss:57.8624 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1814 | MainLoss:0.1814 | SPLoss:47.4685 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7630 | MainLoss:0.7630 | SPLoss:47.4686 | CLSLoss:0.0000 | AUROC:0.8945\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.399715\n",
      "Train | 16/16 | Loss:2.2637 | MainLoss:0.4727 | Alpha:0.0272 | SPLoss:65.6985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1300 | MainLoss:0.1300 | SPLoss:64.4100 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.9049 | MainLoss:0.9049 | SPLoss:64.4100 | CLSLoss:0.0000 | AUROC:0.7322\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.399680\n",
      "Train | 16/16 | Loss:2.2716 | MainLoss:0.5554 | Alpha:0.0273 | SPLoss:60.8815 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.7379 | MainLoss:0.7379 | SPLoss:171.7873 | CLSLoss:0.0000 | AUROC:0.0056\n",
      "Test | 31/16 | Loss:0.7014 | MainLoss:0.7014 | SPLoss:171.7872 | CLSLoss:0.0000 | AUROC:0.4862\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.399644\n",
      "Train | 16/16 | Loss:4.6375 | MainLoss:0.6996 | Alpha:0.0267 | SPLoss:146.5045 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5496 | MainLoss:0.5496 | SPLoss:119.9423 | CLSLoss:0.0000 | AUROC:0.9972\n",
      "Test | 31/16 | Loss:0.7019 | MainLoss:0.7019 | SPLoss:119.9425 | CLSLoss:0.0000 | AUROC:0.4934\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.399605\n",
      "Train | 16/16 | Loss:3.2736 | MainLoss:0.5319 | Alpha:0.0271 | SPLoss:101.3562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1951 | MainLoss:0.1951 | SPLoss:82.1584 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7810 | MainLoss:0.7810 | SPLoss:82.1585 | CLSLoss:0.0000 | AUROC:0.4863\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.399565\n",
      "Train | 16/16 | Loss:2.3496 | MainLoss:0.4408 | Alpha:0.0277 | SPLoss:69.2654 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1689 | MainLoss:0.1689 | SPLoss:55.4810 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.8198 | MainLoss:0.8198 | SPLoss:55.4809 | CLSLoss:0.0000 | AUROC:0.5152\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.399523\n",
      "Train | 16/16 | Loss:1.9428 | MainLoss:0.6324 | Alpha:0.0270 | SPLoss:48.4880 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5429 | MainLoss:0.5429 | SPLoss:41.1747 | CLSLoss:0.0000 | AUROC:0.9802\n",
      "Test | 31/16 | Loss:0.7600 | MainLoss:0.7600 | SPLoss:41.1747 | CLSLoss:0.0000 | AUROC:0.4724\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.399478\n",
      "Train | 16/16 | Loss:1.7731 | MainLoss:0.6925 | Alpha:0.0274 | SPLoss:39.2743 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 128/16 | Loss:0.5556 | MainLoss:0.5556 | SPLoss:37.1335 | CLSLoss:0.0000 | AUROC:0.9930\n",
      "Test | 31/16 | Loss:0.7065 | MainLoss:0.7065 | SPLoss:37.1335 | CLSLoss:0.0000 | AUROC:0.4608\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.399432\n",
      "Train | 16/16 | Loss:1.4223 | MainLoss:0.5482 | Alpha:0.0270 | SPLoss:32.3476 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1940 | MainLoss:0.1940 | SPLoss:28.4022 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.8137 | MainLoss:0.8137 | SPLoss:28.4022 | CLSLoss:0.0000 | AUROC:0.4814\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.399383\n",
      "Train | 16/16 | Loss:1.1855 | MainLoss:0.5122 | Alpha:0.0279 | SPLoss:24.4984 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6650 | MainLoss:0.6650 | SPLoss:21.1217 | CLSLoss:0.0000 | AUROC:0.7417\n",
      "Test | 31/16 | Loss:0.6978 | MainLoss:0.6978 | SPLoss:21.1217 | CLSLoss:0.0000 | AUROC:0.4988\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.399333\n",
      "Train | 16/16 | Loss:1.2282 | MainLoss:0.6888 | Alpha:0.0281 | SPLoss:19.1774 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5764 | MainLoss:0.5764 | SPLoss:15.7540 | CLSLoss:0.0000 | AUROC:0.9521\n",
      "Test | 31/16 | Loss:0.6945 | MainLoss:0.6945 | SPLoss:15.7541 | CLSLoss:0.0000 | AUROC:0.5412\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.399281\n",
      "Train | 16/16 | Loss:0.9737 | MainLoss:0.6035 | Alpha:0.0270 | SPLoss:13.6494 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3819 | MainLoss:0.3819 | SPLoss:11.8610 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.7056 | MainLoss:0.7056 | SPLoss:11.8610 | CLSLoss:0.0000 | AUROC:0.5175\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.399227\n",
      "Train | 16/16 | Loss:0.7734 | MainLoss:0.4926 | Alpha:0.0275 | SPLoss:10.2451 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2218 | MainLoss:0.2218 | SPLoss:9.0342 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.9157 | MainLoss:0.9157 | SPLoss:9.0342 | CLSLoss:0.0000 | AUROC:0.5457\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.399171\n",
      "Train | 16/16 | Loss:1.1749 | MainLoss:0.7021 | Alpha:0.0257 | SPLoss:18.8089 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5621 | MainLoss:0.5621 | SPLoss:17.3908 | CLSLoss:0.0000 | AUROC:0.9492\n",
      "Test | 31/16 | Loss:0.7042 | MainLoss:0.7042 | SPLoss:17.3908 | CLSLoss:0.0000 | AUROC:0.5103\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.399112\n",
      "Train | 16/16 | Loss:0.9469 | MainLoss:0.5373 | Alpha:0.0277 | SPLoss:14.7276 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2015 | MainLoss:0.2015 | SPLoss:12.2459 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.7946 | MainLoss:0.7946 | SPLoss:12.2459 | CLSLoss:0.0000 | AUROC:0.5441\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.399052\n",
      "Train | 16/16 | Loss:0.7994 | MainLoss:0.4739 | Alpha:0.0268 | SPLoss:12.1793 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1935 | MainLoss:0.1935 | SPLoss:11.1905 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7690 | MainLoss:0.7690 | SPLoss:11.1905 | CLSLoss:0.0000 | AUROC:0.6505\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.398990\n",
      "Train | 16/16 | Loss:0.7053 | MainLoss:0.4455 | Alpha:0.0273 | SPLoss:9.5548 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1801 | MainLoss:0.1801 | SPLoss:7.8527 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7860 | MainLoss:0.7860 | SPLoss:7.8527 | CLSLoss:0.0000 | AUROC:0.6427\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.398926\n",
      "Train | 16/16 | Loss:0.6388 | MainLoss:0.4524 | Alpha:0.0271 | SPLoss:6.8999 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1588 | MainLoss:0.1588 | SPLoss:5.9684 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.8427 | MainLoss:0.8427 | SPLoss:5.9684 | CLSLoss:0.0000 | AUROC:0.5796\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.398860\n",
      "Train | 16/16 | Loss:0.6020 | MainLoss:0.4569 | Alpha:0.0271 | SPLoss:5.3613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1558 | MainLoss:0.1558 | SPLoss:4.7740 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.9626 | MainLoss:0.9626 | SPLoss:4.7740 | CLSLoss:0.0000 | AUROC:0.4498\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.398792\n",
      "Train | 16/16 | Loss:0.5468 | MainLoss:0.4356 | Alpha:0.0268 | SPLoss:4.1549 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1770 | MainLoss:0.1770 | SPLoss:3.6123 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.8085 | MainLoss:0.8085 | SPLoss:3.6123 | CLSLoss:0.0000 | AUROC:0.4468\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.398722\n",
      "Train | 16/16 | Loss:0.5208 | MainLoss:0.4340 | Alpha:0.0273 | SPLoss:3.1858 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1401 | MainLoss:0.1401 | SPLoss:2.7858 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.8760 | MainLoss:0.8760 | SPLoss:2.7858 | CLSLoss:0.0000 | AUROC:0.6494\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.398650\n",
      "Train | 16/16 | Loss:0.5285 | MainLoss:0.4544 | Alpha:0.0272 | SPLoss:2.7135 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1863 | MainLoss:0.1863 | SPLoss:2.4677 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7760 | MainLoss:0.7760 | SPLoss:2.4677 | CLSLoss:0.0000 | AUROC:0.5397\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.398577\n",
      "Train | 16/16 | Loss:1.1343 | MainLoss:0.6750 | Alpha:0.0272 | SPLoss:17.9657 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4999 | MainLoss:0.4999 | SPLoss:23.7966 | CLSLoss:0.0000 | AUROC:0.9876\n",
      "Test | 31/16 | Loss:0.7044 | MainLoss:0.7044 | SPLoss:23.7966 | CLSLoss:0.0000 | AUROC:0.5004\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.398501\n",
      "Train | 16/16 | Loss:1.2179 | MainLoss:0.6179 | Alpha:0.0261 | SPLoss:23.1509 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4347 | MainLoss:0.4347 | SPLoss:33.5738 | CLSLoss:0.0000 | AUROC:0.9576\n",
      "Test | 31/16 | Loss:0.7242 | MainLoss:0.7242 | SPLoss:33.5738 | CLSLoss:0.0000 | AUROC:0.5123\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.398423\n",
      "Train | 16/16 | Loss:1.3112 | MainLoss:0.4962 | Alpha:0.0270 | SPLoss:30.2879 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2626 | MainLoss:0.2626 | SPLoss:24.8515 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7325 | MainLoss:0.7325 | SPLoss:24.8515 | CLSLoss:0.0000 | AUROC:0.5190\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.398343\n",
      "Train | 16/16 | Loss:1.0182 | MainLoss:0.4482 | Alpha:0.0272 | SPLoss:21.1289 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2141 | MainLoss:0.2141 | SPLoss:17.1658 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7714 | MainLoss:0.7714 | SPLoss:17.1658 | CLSLoss:0.0000 | AUROC:0.4870\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.398262\n",
      "Train | 16/16 | Loss:1.8424 | MainLoss:0.8737 | Alpha:0.0265 | SPLoss:34.8723 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5027 | MainLoss:0.5027 | SPLoss:56.2045 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "Test | 31/16 | Loss:0.8251 | MainLoss:0.8251 | SPLoss:56.2046 | CLSLoss:0.0000 | AUROC:0.5451\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.398178\n",
      "Train | 16/16 | Loss:1.9176 | MainLoss:0.6266 | Alpha:0.0271 | SPLoss:47.8152 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2852 | MainLoss:0.2852 | SPLoss:39.1060 | CLSLoss:0.0000 | AUROC:0.9972\n",
      "Test | 31/16 | Loss:0.8044 | MainLoss:0.8044 | SPLoss:39.1059 | CLSLoss:0.0000 | AUROC:0.4597\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.398092\n",
      "Train | 16/16 | Loss:1.5232 | MainLoss:0.6103 | Alpha:0.0262 | SPLoss:34.5858 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3366 | MainLoss:0.3366 | SPLoss:29.4063 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 31/16 | Loss:0.7959 | MainLoss:0.7959 | SPLoss:29.4064 | CLSLoss:0.0000 | AUROC:0.4536\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.398005\n",
      "Train | 16/16 | Loss:1.6682 | MainLoss:0.4796 | Alpha:0.0262 | SPLoss:44.2227 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3129 | MainLoss:0.3129 | SPLoss:47.0342 | CLSLoss:0.0000 | AUROC:0.9970\n",
      "Test | 31/16 | Loss:0.7129 | MainLoss:0.7129 | SPLoss:47.0342 | CLSLoss:0.0000 | AUROC:0.4978\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.397915\n",
      "Train | 16/16 | Loss:1.5376 | MainLoss:0.4684 | Alpha:0.0268 | SPLoss:39.8067 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1348 | MainLoss:0.1348 | SPLoss:32.7025 | CLSLoss:0.0000 | AUROC:0.9978\n",
      "Test | 31/16 | Loss:0.9222 | MainLoss:0.9222 | SPLoss:32.7025 | CLSLoss:0.0000 | AUROC:0.4558\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.397824\n",
      "Train | 16/16 | Loss:1.3077 | MainLoss:0.5097 | Alpha:0.0288 | SPLoss:27.3863 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6000 | MainLoss:0.6000 | SPLoss:26.4019 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "Test | 31/16 | Loss:0.7237 | MainLoss:0.7237 | SPLoss:26.4019 | CLSLoss:0.0000 | AUROC:0.4487\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.397730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.1474 | MainLoss:0.5351 | Alpha:0.0269 | SPLoss:22.9032 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2267 | MainLoss:0.2267 | SPLoss:18.7347 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 31/16 | Loss:0.7675 | MainLoss:0.7675 | SPLoss:18.7347 | CLSLoss:0.0000 | AUROC:0.4514\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.397635\n",
      "Train | 16/16 | Loss:0.8871 | MainLoss:0.4579 | Alpha:0.0268 | SPLoss:16.0332 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1661 | MainLoss:0.1661 | SPLoss:13.2524 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 31/16 | Loss:0.8729 | MainLoss:0.8729 | SPLoss:13.2524 | CLSLoss:0.0000 | AUROC:0.4221\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.397538\n",
      "Train | 16/16 | Loss:0.7947 | MainLoss:0.4888 | Alpha:0.0266 | SPLoss:11.5859 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1945 | MainLoss:0.1945 | SPLoss:9.6894 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.8007 | MainLoss:0.8007 | SPLoss:9.6894 | CLSLoss:0.0000 | AUROC:0.4432\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.397438\n",
      "Train | 16/16 | Loss:1.9373 | MainLoss:0.5827 | Alpha:0.0269 | SPLoss:48.9836 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6592 | MainLoss:0.6592 | SPLoss:110.8723 | CLSLoss:0.0000 | AUROC:0.9961\n",
      "Test | 31/16 | Loss:0.6948 | MainLoss:0.6948 | SPLoss:110.8724 | CLSLoss:0.0000 | AUROC:0.4684\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.397337\n",
      "Train | 16/16 | Loss:4.1265 | MainLoss:0.6490 | Alpha:0.0278 | SPLoss:125.1544 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4821 | MainLoss:0.4821 | SPLoss:134.9188 | CLSLoss:0.0000 | AUROC:0.9961\n",
      "Test | 31/16 | Loss:0.7035 | MainLoss:0.7035 | SPLoss:134.9186 | CLSLoss:0.0000 | AUROC:0.5374\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.397234\n",
      "Train | 16/16 | Loss:3.8086 | MainLoss:0.6191 | Alpha:0.0276 | SPLoss:115.8426 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4524 | MainLoss:0.4524 | SPLoss:94.9410 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "Test | 31/16 | Loss:0.8424 | MainLoss:0.8424 | SPLoss:94.9411 | CLSLoss:0.0000 | AUROC:0.4979\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.397129\n",
      "Train | 16/16 | Loss:2.7155 | MainLoss:0.4920 | Alpha:0.0279 | SPLoss:78.8064 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2758 | MainLoss:0.2758 | SPLoss:64.2995 | CLSLoss:0.0000 | AUROC:0.9959\n",
      "Test | 31/16 | Loss:0.7859 | MainLoss:0.7859 | SPLoss:64.2996 | CLSLoss:0.0000 | AUROC:0.4664\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.397022\n",
      "Train | 16/16 | Loss:2.0157 | MainLoss:0.4651 | Alpha:0.0287 | SPLoss:54.8065 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2390 | MainLoss:0.2390 | SPLoss:44.3965 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.7674 | MainLoss:0.7674 | SPLoss:44.3966 | CLSLoss:0.0000 | AUROC:0.4663\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.396913\n",
      "Train | 16/16 | Loss:1.4955 | MainLoss:0.4645 | Alpha:0.0274 | SPLoss:37.4204 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2472 | MainLoss:0.2472 | SPLoss:30.8821 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.7799 | MainLoss:0.7799 | SPLoss:30.8821 | CLSLoss:0.0000 | AUROC:0.5041\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.396802\n",
      "Train | 16/16 | Loss:1.4274 | MainLoss:0.6335 | Alpha:0.0288 | SPLoss:27.5895 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2708 | MainLoss:0.2708 | SPLoss:22.4706 | CLSLoss:0.0000 | AUROC:0.9977\n",
      "Test | 31/16 | Loss:0.7393 | MainLoss:0.7393 | SPLoss:22.4706 | CLSLoss:0.0000 | AUROC:0.4868\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.396689\n",
      "Train | 16/16 | Loss:1.0017 | MainLoss:0.4817 | Alpha:0.0269 | SPLoss:19.2180 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2067 | MainLoss:0.2067 | SPLoss:16.0831 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7754 | MainLoss:0.7754 | SPLoss:16.0831 | CLSLoss:0.0000 | AUROC:0.4989\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.396574\n",
      "Train | 16/16 | Loss:0.8205 | MainLoss:0.4423 | Alpha:0.0279 | SPLoss:13.6032 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1699 | MainLoss:0.1699 | SPLoss:11.1128 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.9987 | MainLoss:0.9987 | SPLoss:11.1128 | CLSLoss:0.0000 | AUROC:0.5869\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.396457\n",
      "Train | 16/16 | Loss:1.2560 | MainLoss:0.4602 | Alpha:0.0279 | SPLoss:28.6746 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1781 | MainLoss:0.1781 | SPLoss:26.3258 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7846 | MainLoss:0.7846 | SPLoss:26.3258 | CLSLoss:0.0000 | AUROC:0.6916\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.396339\n",
      "Train | 16/16 | Loss:1.0651 | MainLoss:0.4432 | Alpha:0.0281 | SPLoss:22.3683 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1447 | MainLoss:0.1447 | SPLoss:18.0599 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.9024 | MainLoss:0.9024 | SPLoss:18.0599 | CLSLoss:0.0000 | AUROC:0.4063\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.396218\n",
      "Train | 16/16 | Loss:1.4354 | MainLoss:0.6173 | Alpha:0.0273 | SPLoss:28.9122 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5667 | MainLoss:0.5667 | SPLoss:47.2545 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 31/16 | Loss:0.6967 | MainLoss:0.6967 | SPLoss:47.2544 | CLSLoss:0.0000 | AUROC:0.5083\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.396096\n",
      "Train | 16/16 | Loss:1.6442 | MainLoss:0.5168 | Alpha:0.0273 | SPLoss:41.2701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2372 | MainLoss:0.2372 | SPLoss:34.1939 | CLSLoss:0.0000 | AUROC:0.9972\n",
      "Test | 31/16 | Loss:0.7535 | MainLoss:0.7535 | SPLoss:34.1939 | CLSLoss:0.0000 | AUROC:0.5367\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.395971\n",
      "Train | 16/16 | Loss:1.2285 | MainLoss:0.4479 | Alpha:0.0270 | SPLoss:28.8738 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2092 | MainLoss:0.2092 | SPLoss:23.4960 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7813 | MainLoss:0.7813 | SPLoss:23.4960 | CLSLoss:0.0000 | AUROC:0.6056\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.395845\n",
      "Train | 16/16 | Loss:1.0047 | MainLoss:0.4569 | Alpha:0.0273 | SPLoss:20.0800 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1560 | MainLoss:0.1560 | SPLoss:16.5142 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "Test | 31/16 | Loss:0.8476 | MainLoss:0.8476 | SPLoss:16.5142 | CLSLoss:0.0000 | AUROC:0.6209\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.395716\n",
      "Train | 16/16 | Loss:0.8346 | MainLoss:0.4446 | Alpha:0.0269 | SPLoss:14.4133 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1483 | MainLoss:0.1483 | SPLoss:13.7615 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.9608 | MainLoss:0.9608 | SPLoss:13.7615 | CLSLoss:0.0000 | AUROC:0.6013\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.395586\n",
      "Train | 16/16 | Loss:0.7664 | MainLoss:0.4504 | Alpha:0.0269 | SPLoss:11.7358 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1548 | MainLoss:0.1548 | SPLoss:9.8889 | CLSLoss:0.0000 | AUROC:0.9971\n",
      "Test | 31/16 | Loss:0.8880 | MainLoss:0.8880 | SPLoss:9.8889 | CLSLoss:0.0000 | AUROC:0.5502\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.395454\n",
      "Train | 16/16 | Loss:0.7153 | MainLoss:0.4855 | Alpha:0.0265 | SPLoss:8.7103 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5614 | MainLoss:0.5614 | SPLoss:13.3776 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7126 | MainLoss:0.7126 | SPLoss:13.3776 | CLSLoss:0.0000 | AUROC:0.8147\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.395319\n",
      "Train | 16/16 | Loss:2.4347 | MainLoss:0.7018 | Alpha:0.0284 | SPLoss:59.9608 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3504 | MainLoss:0.3504 | SPLoss:72.4974 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.7200 | MainLoss:0.7200 | SPLoss:72.4974 | CLSLoss:0.0000 | AUROC:0.7768\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.395183\n",
      "Train | 16/16 | Loss:2.1670 | MainLoss:0.5609 | Alpha:0.0257 | SPLoss:63.0904 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2342 | MainLoss:0.2342 | SPLoss:52.0339 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7638 | MainLoss:0.7638 | SPLoss:52.0339 | CLSLoss:0.0000 | AUROC:0.5064\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.395045\n",
      "Train | 16/16 | Loss:1.9788 | MainLoss:0.4840 | Alpha:0.0278 | SPLoss:55.6713 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.9230 | MainLoss:0.9230 | SPLoss:250.1519 | CLSLoss:0.0000 | AUROC:0.7423\n",
      "Test | 31/16 | Loss:0.9258 | MainLoss:0.9258 | SPLoss:250.1515 | CLSLoss:0.0000 | AUROC:0.3592\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.394905\n",
      "Train | 16/16 | Loss:6.4790 | MainLoss:0.6780 | Alpha:0.0269 | SPLoss:216.4488 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5618 | MainLoss:0.5618 | SPLoss:175.6174 | CLSLoss:0.0000 | AUROC:0.9908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 31/16 | Loss:0.7342 | MainLoss:0.7342 | SPLoss:175.6175 | CLSLoss:0.0000 | AUROC:0.5314\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.394763\n",
      "Train | 16/16 | Loss:4.5771 | MainLoss:0.5171 | Alpha:0.0274 | SPLoss:149.0034 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1762 | MainLoss:0.1762 | SPLoss:120.4287 | CLSLoss:0.0000 | AUROC:0.9977\n",
      "Test | 31/16 | Loss:0.9033 | MainLoss:0.9033 | SPLoss:120.4285 | CLSLoss:0.0000 | AUROC:0.5375\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.394620\n",
      "Train | 16/16 | Loss:3.1743 | MainLoss:0.4600 | Alpha:0.0267 | SPLoss:100.9335 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4519 | MainLoss:0.4519 | SPLoss:82.8963 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.7121 | MainLoss:0.7121 | SPLoss:82.8963 | CLSLoss:0.0000 | AUROC:0.5590\n",
      "\n",
      "Epoch: [177 | 1000] LR: 0.394474\n",
      "Train | 16/16 | Loss:2.3289 | MainLoss:0.4836 | Alpha:0.0262 | SPLoss:71.3719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4198 | MainLoss:0.4198 | SPLoss:58.0016 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.7211 | MainLoss:0.7211 | SPLoss:58.0017 | CLSLoss:0.0000 | AUROC:0.5451\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.394326\n",
      "Train | 16/16 | Loss:1.7677 | MainLoss:0.4645 | Alpha:0.0265 | SPLoss:48.6333 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1540 | MainLoss:0.1540 | SPLoss:40.3625 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.9428 | MainLoss:0.9428 | SPLoss:40.3624 | CLSLoss:0.0000 | AUROC:0.5489\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.394177\n",
      "Train | 16/16 | Loss:1.4995 | MainLoss:0.5104 | Alpha:0.0273 | SPLoss:36.2873 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1422 | MainLoss:0.1422 | SPLoss:31.4854 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.9556 | MainLoss:0.9556 | SPLoss:31.4854 | CLSLoss:0.0000 | AUROC:0.4248\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.394025\n",
      "Train | 16/16 | Loss:1.2038 | MainLoss:0.4656 | Alpha:0.0272 | SPLoss:27.0912 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:34.4540 | CLSLoss:0.0000 | AUROC:0.9975\n",
      "Test | 31/16 | Loss:0.7654 | MainLoss:0.7654 | SPLoss:34.4541 | CLSLoss:0.0000 | AUROC:0.4372\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.393872\n",
      "Train | 16/16 | Loss:1.2745 | MainLoss:0.4735 | Alpha:0.0254 | SPLoss:31.6314 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1689 | MainLoss:0.1689 | SPLoss:26.4229 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.8394 | MainLoss:0.8394 | SPLoss:26.4229 | CLSLoss:0.0000 | AUROC:0.3886\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.393717\n",
      "Train | 16/16 | Loss:1.1491 | MainLoss:0.5350 | Alpha:0.0271 | SPLoss:22.5921 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2351 | MainLoss:0.2351 | SPLoss:19.5383 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7677 | MainLoss:0.7677 | SPLoss:19.5383 | CLSLoss:0.0000 | AUROC:0.5372\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.393559\n",
      "Train | 16/16 | Loss:0.9222 | MainLoss:0.4713 | Alpha:0.0268 | SPLoss:16.9625 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1917 | MainLoss:0.1917 | SPLoss:13.8723 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7889 | MainLoss:0.7889 | SPLoss:13.8723 | CLSLoss:0.0000 | AUROC:0.5616\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.393400\n",
      "Train | 16/16 | Loss:1.0583 | MainLoss:0.6740 | Alpha:0.0267 | SPLoss:14.5518 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3927 | MainLoss:0.3927 | SPLoss:14.0103 | CLSLoss:0.0000 | AUROC:0.9971\n",
      "Test | 31/16 | Loss:0.7005 | MainLoss:0.7005 | SPLoss:14.0103 | CLSLoss:0.0000 | AUROC:0.5270\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.393239\n",
      "Train | 16/16 | Loss:679.8250 | MainLoss:0.7423 | Alpha:0.0276 | SPLoss:19713.3926 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6926 | MainLoss:0.6926 | SPLoss:168458.0781 | CLSLoss:0.0000 | AUROC:0.7745\n",
      "Test | 31/16 | Loss:0.6943 | MainLoss:0.6943 | SPLoss:168457.8906 | CLSLoss:0.0000 | AUROC:0.6425\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.393076\n",
      "Train | 16/16 | Loss:3754.6539 | MainLoss:0.6925 | Alpha:0.0263 | SPLoss:142753.2656 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6892 | MainLoss:0.6892 | SPLoss:116449.2031 | CLSLoss:0.0000 | AUROC:0.9495\n",
      "Test | 31/16 | Loss:0.6941 | MainLoss:0.6941 | SPLoss:116449.0312 | CLSLoss:0.0000 | AUROC:0.5268\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.392911\n",
      "Train | 16/16 | Loss:2685.1238 | MainLoss:0.6836 | Alpha:0.0274 | SPLoss:98240.5156 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5961 | MainLoss:0.5961 | SPLoss:79306.8516 | CLSLoss:0.0000 | AUROC:0.9802\n",
      "Test | 31/16 | Loss:0.7565 | MainLoss:0.7565 | SPLoss:79306.9766 | CLSLoss:0.0000 | AUROC:0.3756\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.392745\n",
      "Train | 16/16 | Loss:1698.2886 | MainLoss:0.6769 | Alpha:0.0251 | SPLoss:68214.2812 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4671 | MainLoss:0.4671 | SPLoss:55675.0312 | CLSLoss:0.0000 | AUROC:0.9781\n",
      "Test | 31/16 | Loss:0.7037 | MainLoss:0.7037 | SPLoss:55675.1250 | CLSLoss:0.0000 | AUROC:0.5099\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.392576\n",
      "Train | 16/16 | Loss:1256.4347 | MainLoss:0.6644 | Alpha:0.0267 | SPLoss:47420.1797 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6964 | MainLoss:0.6964 | SPLoss:38282.2422 | CLSLoss:0.0000 | AUROC:0.4374\n",
      "Test | 31/16 | Loss:0.6962 | MainLoss:0.6962 | SPLoss:38282.2422 | CLSLoss:0.0000 | AUROC:0.4450\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.392406\n",
      "Train | 16/16 | Loss:881.2123 | MainLoss:0.6950 | Alpha:0.0274 | SPLoss:32410.1816 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6938 | MainLoss:0.6938 | SPLoss:26031.9375 | CLSLoss:0.0000 | AUROC:0.5423\n",
      "Test | 31/16 | Loss:0.6941 | MainLoss:0.6941 | SPLoss:26031.9531 | CLSLoss:0.0000 | AUROC:0.4720\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.392233\n",
      "Train | 16/16 | Loss:600.8471 | MainLoss:0.6955 | Alpha:0.0274 | SPLoss:21706.8418 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6927 | MainLoss:0.6927 | SPLoss:17723.1445 | CLSLoss:0.0000 | AUROC:0.5579\n",
      "Test | 31/16 | Loss:0.6934 | MainLoss:0.6934 | SPLoss:17723.1660 | CLSLoss:0.0000 | AUROC:0.4307\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.392059\n",
      "Train | 16/16 | Loss:409.2390 | MainLoss:0.6949 | Alpha:0.0274 | SPLoss:14925.7383 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6912 | MainLoss:0.6912 | SPLoss:12056.8281 | CLSLoss:0.0000 | AUROC:0.7505\n",
      "Test | 31/16 | Loss:0.6961 | MainLoss:0.6961 | SPLoss:12056.8213 | CLSLoss:0.0000 | AUROC:0.3901\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.391882\n",
      "Train | 16/16 | Loss:284.6511 | MainLoss:0.6948 | Alpha:0.0281 | SPLoss:10063.0518 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6599 | MainLoss:0.6599 | SPLoss:8121.0195 | CLSLoss:0.0000 | AUROC:0.9732\n",
      "Test | 31/16 | Loss:0.7007 | MainLoss:0.7007 | SPLoss:8121.0088 | CLSLoss:0.0000 | AUROC:0.2293\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.391704\n",
      "Train | 16/16 | Loss:188.2105 | MainLoss:0.7278 | Alpha:0.0274 | SPLoss:6862.9307 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6506 | MainLoss:0.6506 | SPLoss:5563.5898 | CLSLoss:0.0000 | AUROC:0.9922\n",
      "Test | 31/16 | Loss:0.7049 | MainLoss:0.7049 | SPLoss:5563.5981 | CLSLoss:0.0000 | AUROC:0.4922\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.391524\n",
      "Train | 16/16 | Loss:128.6932 | MainLoss:0.7016 | Alpha:0.0274 | SPLoss:4770.1680 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5305 | MainLoss:0.5305 | SPLoss:3791.3938 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "Test | 31/16 | Loss:0.6994 | MainLoss:0.6994 | SPLoss:3791.3979 | CLSLoss:0.0000 | AUROC:0.5885\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.391342\n",
      "Train | 16/16 | Loss:89.2492 | MainLoss:0.5382 | Alpha:0.0279 | SPLoss:3195.7412 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3495 | MainLoss:0.3495 | SPLoss:2565.5012 | CLSLoss:0.0000 | AUROC:0.9966\n",
      "Test | 31/16 | Loss:0.7081 | MainLoss:0.7081 | SPLoss:2565.4985 | CLSLoss:0.0000 | AUROC:0.4252\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.391159\n",
      "Train | 16/16 | Loss:59.1899 | MainLoss:0.4710 | Alpha:0.0272 | SPLoss:2162.6250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2059 | MainLoss:0.2059 | SPLoss:1748.7358 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7713 | MainLoss:0.7713 | SPLoss:1748.7355 | CLSLoss:0.0000 | AUROC:0.3448\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.390973\n",
      "Train | 16/16 | Loss:44.7260 | MainLoss:0.4606 | Alpha:0.0279 | SPLoss:1591.1716 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2314 | MainLoss:0.2314 | SPLoss:1409.8502 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7431 | MainLoss:0.7431 | SPLoss:1409.8494 | CLSLoss:0.0000 | AUROC:0.4370\n",
      "\n",
      "Epoch: [199 | 1000] LR: 0.390785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:33.5783 | MainLoss:0.4522 | Alpha:0.0281 | SPLoss:1167.7124 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2816 | MainLoss:0.2816 | SPLoss:950.5060 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.8581 | MainLoss:0.8581 | SPLoss:950.5074 | CLSLoss:0.0000 | AUROC:0.2494\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.390596\n",
      "Train | 16/16 | Loss:41.1616 | MainLoss:0.6410 | Alpha:0.0273 | SPLoss:1443.4946 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1678 | MainLoss:0.1678 | SPLoss:1391.4980 | CLSLoss:0.0000 | AUROC:0.9962\n",
      "Test | 31/16 | Loss:0.8856 | MainLoss:0.8856 | SPLoss:1391.4963 | CLSLoss:0.0000 | AUROC:0.2964\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.039040\n",
      "Train | 16/16 | Loss:37.3490 | MainLoss:0.4471 | Alpha:0.0270 | SPLoss:1368.0649 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1941 | MainLoss:0.1941 | SPLoss:1340.2672 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.8018 | MainLoss:0.8018 | SPLoss:1340.2676 | CLSLoss:0.0000 | AUROC:0.3368\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.039021\n",
      "Train | 16/16 | Loss:37.4394 | MainLoss:0.4527 | Alpha:0.0281 | SPLoss:1316.5348 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1984 | MainLoss:0.1984 | SPLoss:1288.9264 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.7902 | MainLoss:0.7902 | SPLoss:1288.9241 | CLSLoss:0.0000 | AUROC:0.3444\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.039002\n",
      "Train | 16/16 | Loss:33.5565 | MainLoss:0.4454 | Alpha:0.0261 | SPLoss:1267.2162 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1863 | MainLoss:0.1863 | SPLoss:1243.1575 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.8008 | MainLoss:0.8008 | SPLoss:1243.1577 | CLSLoss:0.0000 | AUROC:0.3761\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.038982\n",
      "Train | 16/16 | Loss:34.0726 | MainLoss:0.4410 | Alpha:0.0275 | SPLoss:1222.2032 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1842 | MainLoss:0.1842 | SPLoss:1196.5386 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.8088 | MainLoss:0.8088 | SPLoss:1196.5394 | CLSLoss:0.0000 | AUROC:0.3291\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.038962\n",
      "Train | 16/16 | Loss:32.6630 | MainLoss:0.4446 | Alpha:0.0274 | SPLoss:1176.1519 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1924 | MainLoss:0.1924 | SPLoss:1151.9301 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.7897 | MainLoss:0.7897 | SPLoss:1151.9301 | CLSLoss:0.0000 | AUROC:0.4115\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.038942\n",
      "Train | 16/16 | Loss:31.6099 | MainLoss:0.4530 | Alpha:0.0275 | SPLoss:1129.8499 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1939 | MainLoss:0.1939 | SPLoss:1108.7534 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7840 | MainLoss:0.7840 | SPLoss:1108.7544 | CLSLoss:0.0000 | AUROC:0.3996\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.038922\n",
      "Train | 16/16 | Loss:29.6262 | MainLoss:0.4374 | Alpha:0.0268 | SPLoss:1091.0974 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1884 | MainLoss:0.1884 | SPLoss:1068.4655 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.7955 | MainLoss:0.7955 | SPLoss:1068.4661 | CLSLoss:0.0000 | AUROC:0.3457\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.038901\n",
      "Train | 16/16 | Loss:29.2182 | MainLoss:0.4473 | Alpha:0.0274 | SPLoss:1050.2964 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1943 | MainLoss:0.1943 | SPLoss:1028.7914 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7767 | MainLoss:0.7767 | SPLoss:1028.7900 | CLSLoss:0.0000 | AUROC:0.4317\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.038881\n",
      "Train | 16/16 | Loss:28.6840 | MainLoss:0.4384 | Alpha:0.0280 | SPLoss:1010.1606 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1819 | MainLoss:0.1819 | SPLoss:989.7057 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7957 | MainLoss:0.7957 | SPLoss:989.7065 | CLSLoss:0.0000 | AUROC:0.3725\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.038860\n",
      "Train | 16/16 | Loss:26.8622 | MainLoss:0.4391 | Alpha:0.0272 | SPLoss:974.0834 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2059 | MainLoss:0.2059 | SPLoss:953.3583 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7598 | MainLoss:0.7598 | SPLoss:953.3579 | CLSLoss:0.0000 | AUROC:0.4264\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.038839\n",
      "Train | 16/16 | Loss:25.6570 | MainLoss:0.4318 | Alpha:0.0269 | SPLoss:936.3840 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1817 | MainLoss:0.1817 | SPLoss:918.3618 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7891 | MainLoss:0.7891 | SPLoss:918.3625 | CLSLoss:0.0000 | AUROC:0.3999\n",
      "\n",
      "Epoch: [212 | 1000] LR: 0.038818\n",
      "Train | 16/16 | Loss:25.1616 | MainLoss:0.4476 | Alpha:0.0274 | SPLoss:902.3973 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1750 | MainLoss:0.1750 | SPLoss:884.2777 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7921 | MainLoss:0.7921 | SPLoss:884.2764 | CLSLoss:0.0000 | AUROC:0.4609\n",
      "\n",
      "Epoch: [213 | 1000] LR: 0.038796\n",
      "Train | 16/16 | Loss:23.9045 | MainLoss:0.4304 | Alpha:0.0270 | SPLoss:869.5294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1788 | MainLoss:0.1788 | SPLoss:851.9340 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7807 | MainLoss:0.7807 | SPLoss:851.9334 | CLSLoss:0.0000 | AUROC:0.5086\n",
      "\n",
      "Epoch: [214 | 1000] LR: 0.038775\n",
      "Train | 16/16 | Loss:23.3411 | MainLoss:0.4362 | Alpha:0.0274 | SPLoss:836.9512 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1855 | MainLoss:0.1855 | SPLoss:820.4405 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7693 | MainLoss:0.7693 | SPLoss:820.4414 | CLSLoss:0.0000 | AUROC:0.6110\n",
      "\n",
      "Epoch: [215 | 1000] LR: 0.038753\n",
      "Train | 16/16 | Loss:21.8915 | MainLoss:0.4307 | Alpha:0.0266 | SPLoss:807.6799 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1766 | MainLoss:0.1766 | SPLoss:790.8421 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7632 | MainLoss:0.7632 | SPLoss:790.8427 | CLSLoss:0.0000 | AUROC:0.6887\n",
      "\n",
      "Epoch: [216 | 1000] LR: 0.038731\n",
      "Train | 16/16 | Loss:21.1763 | MainLoss:0.4302 | Alpha:0.0267 | SPLoss:778.1165 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1732 | MainLoss:0.1732 | SPLoss:762.3840 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7652 | MainLoss:0.7652 | SPLoss:762.3840 | CLSLoss:0.0000 | AUROC:0.7119\n",
      "\n",
      "Epoch: [217 | 1000] LR: 0.038709\n",
      "Train | 16/16 | Loss:21.7260 | MainLoss:0.4304 | Alpha:0.0284 | SPLoss:748.2045 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1646 | MainLoss:0.1646 | SPLoss:733.0187 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7798 | MainLoss:0.7798 | SPLoss:733.0193 | CLSLoss:0.0000 | AUROC:0.7112\n",
      "\n",
      "Epoch: [218 | 1000] LR: 0.038687\n",
      "Train | 16/16 | Loss:20.7237 | MainLoss:0.4392 | Alpha:0.0282 | SPLoss:720.6456 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1831 | MainLoss:0.1831 | SPLoss:705.2377 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7481 | MainLoss:0.7481 | SPLoss:705.2379 | CLSLoss:0.0000 | AUROC:0.7297\n",
      "\n",
      "Epoch: [219 | 1000] LR: 0.038664\n",
      "Train | 16/16 | Loss:19.4319 | MainLoss:0.4385 | Alpha:0.0274 | SPLoss:692.8331 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1680 | MainLoss:0.1680 | SPLoss:679.0482 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7691 | MainLoss:0.7691 | SPLoss:679.0486 | CLSLoss:0.0000 | AUROC:0.7909\n",
      "\n",
      "Epoch: [220 | 1000] LR: 0.038641\n",
      "Train | 16/16 | Loss:19.0485 | MainLoss:0.4311 | Alpha:0.0279 | SPLoss:666.7611 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1868 | MainLoss:0.1868 | SPLoss:653.5033 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7300 | MainLoss:0.7300 | SPLoss:653.5027 | CLSLoss:0.0000 | AUROC:0.8670\n",
      "\n",
      "Epoch: [221 | 1000] LR: 0.038619\n",
      "Train | 16/16 | Loss:18.0889 | MainLoss:0.4316 | Alpha:0.0275 | SPLoss:641.0042 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1550 | MainLoss:0.1550 | SPLoss:629.2825 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7729 | MainLoss:0.7729 | SPLoss:629.2820 | CLSLoss:0.0000 | AUROC:0.8941\n",
      "\n",
      "Epoch: [222 | 1000] LR: 0.038596\n",
      "Train | 16/16 | Loss:17.5332 | MainLoss:0.4317 | Alpha:0.0277 | SPLoss:618.1405 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:605.8873 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7662 | MainLoss:0.7662 | SPLoss:605.8868 | CLSLoss:0.0000 | AUROC:0.8900\n",
      "\n",
      "Epoch: [223 | 1000] LR: 0.038572\n",
      "Train | 16/16 | Loss:15.7876 | MainLoss:0.4348 | Alpha:0.0258 | SPLoss:595.5817 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 128/16 | Loss:0.1693 | MainLoss:0.1693 | SPLoss:584.8630 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7748 | MainLoss:0.7748 | SPLoss:584.8619 | CLSLoss:0.0000 | AUROC:0.8412\n",
      "\n",
      "Epoch: [224 | 1000] LR: 0.038549\n",
      "Train | 16/16 | Loss:16.4625 | MainLoss:0.4269 | Alpha:0.0279 | SPLoss:574.6959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1630 | MainLoss:0.1630 | SPLoss:562.8873 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7627 | MainLoss:0.7627 | SPLoss:562.8869 | CLSLoss:0.0000 | AUROC:0.8980\n",
      "\n",
      "Epoch: [225 | 1000] LR: 0.038525\n",
      "Train | 16/16 | Loss:15.5249 | MainLoss:0.4273 | Alpha:0.0273 | SPLoss:552.8033 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1845 | MainLoss:0.1845 | SPLoss:542.2397 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7085 | MainLoss:0.7085 | SPLoss:542.2390 | CLSLoss:0.0000 | AUROC:0.9419\n",
      "\n",
      "Epoch: [226 | 1000] LR: 0.038502\n",
      "Train | 16/16 | Loss:14.6283 | MainLoss:0.4286 | Alpha:0.0266 | SPLoss:532.9100 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1874 | MainLoss:0.1874 | SPLoss:522.8410 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7136 | MainLoss:0.7136 | SPLoss:522.8420 | CLSLoss:0.0000 | AUROC:0.9157\n",
      "\n",
      "Epoch: [227 | 1000] LR: 0.038478\n",
      "Train | 16/16 | Loss:14.0905 | MainLoss:0.4303 | Alpha:0.0266 | SPLoss:514.4271 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1846 | MainLoss:0.1846 | SPLoss:504.2070 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.7133 | MainLoss:0.7133 | SPLoss:504.2063 | CLSLoss:0.0000 | AUROC:0.9327\n",
      "\n",
      "Epoch: [228 | 1000] LR: 0.038453\n",
      "Train | 16/16 | Loss:13.9294 | MainLoss:0.4302 | Alpha:0.0272 | SPLoss:496.4380 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2005 | MainLoss:0.2005 | SPLoss:486.1791 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6861 | MainLoss:0.6861 | SPLoss:486.1789 | CLSLoss:0.0000 | AUROC:0.9562\n",
      "\n",
      "Epoch: [229 | 1000] LR: 0.038429\n",
      "Train | 16/16 | Loss:13.4852 | MainLoss:0.4202 | Alpha:0.0273 | SPLoss:478.3090 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1989 | MainLoss:0.1989 | SPLoss:468.3669 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6836 | MainLoss:0.6836 | SPLoss:468.3660 | CLSLoss:0.0000 | AUROC:0.9697\n",
      "\n",
      "Epoch: [230 | 1000] LR: 0.038405\n",
      "Train | 16/16 | Loss:12.8957 | MainLoss:0.4227 | Alpha:0.0271 | SPLoss:460.5384 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2070 | MainLoss:0.2070 | SPLoss:451.3630 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6747 | MainLoss:0.6747 | SPLoss:451.3627 | CLSLoss:0.0000 | AUROC:0.9706\n",
      "\n",
      "Epoch: [231 | 1000] LR: 0.038380\n",
      "Train | 16/16 | Loss:12.4937 | MainLoss:0.4304 | Alpha:0.0272 | SPLoss:443.0475 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1816 | MainLoss:0.1816 | SPLoss:434.9212 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7108 | MainLoss:0.7108 | SPLoss:434.9219 | CLSLoss:0.0000 | AUROC:0.9593\n",
      "\n",
      "Epoch: [232 | 1000] LR: 0.038355\n",
      "Train | 16/16 | Loss:12.0622 | MainLoss:0.4272 | Alpha:0.0272 | SPLoss:427.6713 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1741 | MainLoss:0.1741 | SPLoss:419.1010 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7158 | MainLoss:0.7158 | SPLoss:419.1013 | CLSLoss:0.0000 | AUROC:0.9658\n",
      "\n",
      "Epoch: [233 | 1000] LR: 0.038330\n",
      "Train | 16/16 | Loss:11.8053 | MainLoss:0.4333 | Alpha:0.0276 | SPLoss:411.9950 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1426 | MainLoss:0.1426 | SPLoss:403.6670 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7721 | MainLoss:0.7721 | SPLoss:403.6670 | CLSLoss:0.0000 | AUROC:0.9565\n",
      "\n",
      "Epoch: [234 | 1000] LR: 0.038305\n",
      "Train | 16/16 | Loss:10.9786 | MainLoss:0.4257 | Alpha:0.0266 | SPLoss:396.9998 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1599 | MainLoss:0.1599 | SPLoss:389.3147 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.7283 | MainLoss:0.7283 | SPLoss:389.3143 | CLSLoss:0.0000 | AUROC:0.9657\n",
      "\n",
      "Epoch: [235 | 1000] LR: 0.038279\n",
      "Train | 16/16 | Loss:10.5517 | MainLoss:0.4215 | Alpha:0.0265 | SPLoss:383.0557 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2012 | MainLoss:0.2012 | SPLoss:375.5949 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6608 | MainLoss:0.6608 | SPLoss:375.5949 | CLSLoss:0.0000 | AUROC:0.9765\n",
      "\n",
      "Epoch: [236 | 1000] LR: 0.038254\n",
      "Train | 16/16 | Loss:10.3511 | MainLoss:0.4215 | Alpha:0.0269 | SPLoss:368.9630 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2052 | MainLoss:0.2052 | SPLoss:362.1000 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6262 | MainLoss:0.6262 | SPLoss:362.1000 | CLSLoss:0.0000 | AUROC:0.9792\n",
      "\n",
      "Epoch: [237 | 1000] LR: 0.038228\n",
      "Train | 16/16 | Loss:10.1101 | MainLoss:0.4339 | Alpha:0.0272 | SPLoss:355.8249 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2083 | MainLoss:0.2083 | SPLoss:348.9876 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6337 | MainLoss:0.6337 | SPLoss:348.9870 | CLSLoss:0.0000 | AUROC:0.9838\n",
      "\n",
      "Epoch: [238 | 1000] LR: 0.038202\n",
      "Train | 16/16 | Loss:10.0186 | MainLoss:0.4250 | Alpha:0.0280 | SPLoss:342.9779 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2015 | MainLoss:0.2015 | SPLoss:336.0164 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6847 | MainLoss:0.6847 | SPLoss:336.0170 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [239 | 1000] LR: 0.038176\n",
      "Train | 16/16 | Loss:9.5174 | MainLoss:0.4233 | Alpha:0.0275 | SPLoss:329.9464 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1703 | MainLoss:0.1703 | SPLoss:323.6944 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7112 | MainLoss:0.7112 | SPLoss:323.6943 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [240 | 1000] LR: 0.038150\n",
      "Train | 16/16 | Loss:9.3579 | MainLoss:0.4275 | Alpha:0.0281 | SPLoss:318.0983 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2181 | MainLoss:0.2181 | SPLoss:311.6263 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6445 | MainLoss:0.6445 | SPLoss:311.6261 | CLSLoss:0.0000 | AUROC:0.9875\n",
      "\n",
      "Epoch: [241 | 1000] LR: 0.038123\n",
      "Train | 16/16 | Loss:9.0177 | MainLoss:0.4217 | Alpha:0.0280 | SPLoss:307.1388 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1722 | MainLoss:0.1722 | SPLoss:305.3199 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.7193 | MainLoss:0.7193 | SPLoss:305.3199 | CLSLoss:0.0000 | AUROC:0.9867\n",
      "\n",
      "Epoch: [242 | 1000] LR: 0.038097\n",
      "Train | 16/16 | Loss:8.4957 | MainLoss:0.4275 | Alpha:0.0269 | SPLoss:300.2691 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1700 | MainLoss:0.1700 | SPLoss:294.4865 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7013 | MainLoss:0.7013 | SPLoss:294.4866 | CLSLoss:0.0000 | AUROC:0.9875\n",
      "\n",
      "Epoch: [243 | 1000] LR: 0.038070\n",
      "Train | 16/16 | Loss:8.4514 | MainLoss:0.4211 | Alpha:0.0277 | SPLoss:288.8298 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1989 | MainLoss:0.1989 | SPLoss:283.6435 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6569 | MainLoss:0.6569 | SPLoss:283.6430 | CLSLoss:0.0000 | AUROC:0.9831\n",
      "\n",
      "Epoch: [244 | 1000] LR: 0.038043\n",
      "Train | 16/16 | Loss:7.9120 | MainLoss:0.4163 | Alpha:0.0269 | SPLoss:279.2245 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2255 | MainLoss:0.2255 | SPLoss:273.5399 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6055 | MainLoss:0.6055 | SPLoss:273.5399 | CLSLoss:0.0000 | AUROC:0.9846\n",
      "\n",
      "Epoch: [245 | 1000] LR: 0.038015\n",
      "Train | 16/16 | Loss:7.8247 | MainLoss:0.4194 | Alpha:0.0275 | SPLoss:268.5673 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1825 | MainLoss:0.1825 | SPLoss:263.5199 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6614 | MainLoss:0.6614 | SPLoss:263.5195 | CLSLoss:0.0000 | AUROC:0.9849\n",
      "\n",
      "Epoch: [246 | 1000] LR: 0.037988\n",
      "Train | 16/16 | Loss:7.4203 | MainLoss:0.4316 | Alpha:0.0270 | SPLoss:259.0668 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1797 | MainLoss:0.1797 | SPLoss:254.1261 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.6732 | MainLoss:0.6732 | SPLoss:254.1265 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [247 | 1000] LR: 0.037961\n",
      "Train | 16/16 | Loss:7.1510 | MainLoss:0.4272 | Alpha:0.0269 | SPLoss:250.0435 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1908 | MainLoss:0.1908 | SPLoss:245.1001 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6546 | MainLoss:0.6546 | SPLoss:245.1005 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [248 | 1000] LR: 0.037933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:6.7952 | MainLoss:0.4312 | Alpha:0.0264 | SPLoss:241.0647 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1832 | MainLoss:0.1832 | SPLoss:236.5436 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6823 | MainLoss:0.6823 | SPLoss:236.5435 | CLSLoss:0.0000 | AUROC:0.9839\n",
      "\n",
      "Epoch: [249 | 1000] LR: 0.037905\n",
      "Train | 16/16 | Loss:6.4269 | MainLoss:0.4233 | Alpha:0.0258 | SPLoss:232.9474 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1927 | MainLoss:0.1927 | SPLoss:228.5064 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6518 | MainLoss:0.6518 | SPLoss:228.5065 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [250 | 1000] LR: 0.037877\n",
      "Train | 16/16 | Loss:6.4254 | MainLoss:0.4111 | Alpha:0.0268 | SPLoss:224.8648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1680 | MainLoss:0.1680 | SPLoss:220.4465 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6794 | MainLoss:0.6794 | SPLoss:220.4465 | CLSLoss:0.0000 | AUROC:0.9856\n",
      "\n",
      "Epoch: [251 | 1000] LR: 0.037849\n",
      "Train | 16/16 | Loss:6.2574 | MainLoss:0.4225 | Alpha:0.0269 | SPLoss:216.5586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1859 | MainLoss:0.1859 | SPLoss:212.6321 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.6586 | MainLoss:0.6586 | SPLoss:212.6324 | CLSLoss:0.0000 | AUROC:0.9850\n",
      "\n",
      "Epoch: [252 | 1000] LR: 0.037820\n",
      "Train | 16/16 | Loss:6.1598 | MainLoss:0.4231 | Alpha:0.0274 | SPLoss:208.9820 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1783 | MainLoss:0.1783 | SPLoss:204.9375 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6521 | MainLoss:0.6521 | SPLoss:204.9375 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [253 | 1000] LR: 0.037792\n",
      "Train | 16/16 | Loss:5.9189 | MainLoss:0.4215 | Alpha:0.0273 | SPLoss:201.3314 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2295 | MainLoss:0.2295 | SPLoss:197.5925 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5638 | MainLoss:0.5638 | SPLoss:197.5925 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "\n",
      "Epoch: [254 | 1000] LR: 0.037763\n",
      "Train | 16/16 | Loss:5.8572 | MainLoss:0.4254 | Alpha:0.0280 | SPLoss:194.3596 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1935 | MainLoss:0.1935 | SPLoss:190.3382 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6244 | MainLoss:0.6244 | SPLoss:190.3384 | CLSLoss:0.0000 | AUROC:0.9911\n",
      "\n",
      "Epoch: [255 | 1000] LR: 0.037734\n",
      "Train | 16/16 | Loss:5.6871 | MainLoss:0.4174 | Alpha:0.0282 | SPLoss:187.1179 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1513 | MainLoss:0.1513 | SPLoss:183.3120 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7085 | MainLoss:0.7085 | SPLoss:183.3120 | CLSLoss:0.0000 | AUROC:0.9873\n",
      "\n",
      "Epoch: [256 | 1000] LR: 0.037705\n",
      "Train | 16/16 | Loss:5.3398 | MainLoss:0.4160 | Alpha:0.0273 | SPLoss:180.2085 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1690 | MainLoss:0.1690 | SPLoss:176.7421 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6542 | MainLoss:0.6542 | SPLoss:176.7421 | CLSLoss:0.0000 | AUROC:0.9905\n",
      "\n",
      "Epoch: [257 | 1000] LR: 0.037675\n",
      "Train | 16/16 | Loss:5.1342 | MainLoss:0.4245 | Alpha:0.0271 | SPLoss:173.5687 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1777 | MainLoss:0.1777 | SPLoss:170.4807 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6634 | MainLoss:0.6634 | SPLoss:170.4808 | CLSLoss:0.0000 | AUROC:0.9851\n",
      "\n",
      "Epoch: [258 | 1000] LR: 0.037646\n",
      "Train | 16/16 | Loss:5.0351 | MainLoss:0.4143 | Alpha:0.0276 | SPLoss:167.4502 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1533 | MainLoss:0.1533 | SPLoss:164.2980 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.7051 | MainLoss:0.7051 | SPLoss:164.2980 | CLSLoss:0.0000 | AUROC:0.9870\n",
      "\n",
      "Epoch: [259 | 1000] LR: 0.037616\n",
      "Train | 16/16 | Loss:4.8782 | MainLoss:0.4199 | Alpha:0.0276 | SPLoss:161.5497 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1857 | MainLoss:0.1857 | SPLoss:158.4000 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6462 | MainLoss:0.6462 | SPLoss:158.4000 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "\n",
      "Epoch: [260 | 1000] LR: 0.037586\n",
      "Train | 16/16 | Loss:4.7660 | MainLoss:0.4246 | Alpha:0.0279 | SPLoss:155.6828 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1831 | MainLoss:0.1831 | SPLoss:152.6342 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6217 | MainLoss:0.6217 | SPLoss:152.6343 | CLSLoss:0.0000 | AUROC:0.9916\n",
      "\n",
      "Epoch: [261 | 1000] LR: 0.037556\n",
      "Train | 16/16 | Loss:4.4438 | MainLoss:0.4241 | Alpha:0.0268 | SPLoss:150.0339 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1725 | MainLoss:0.1725 | SPLoss:147.2939 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6275 | MainLoss:0.6275 | SPLoss:147.2938 | CLSLoss:0.0000 | AUROC:0.9923\n",
      "\n",
      "Epoch: [262 | 1000] LR: 0.037526\n",
      "Train | 16/16 | Loss:4.4327 | MainLoss:0.4211 | Alpha:0.0277 | SPLoss:144.7553 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1454 | MainLoss:0.1454 | SPLoss:141.9807 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.7067 | MainLoss:0.7067 | SPLoss:141.9805 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "\n",
      "Epoch: [263 | 1000] LR: 0.037496\n",
      "Train | 16/16 | Loss:4.3490 | MainLoss:0.4211 | Alpha:0.0281 | SPLoss:139.5126 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2056 | MainLoss:0.2056 | SPLoss:136.8031 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6132 | MainLoss:0.6132 | SPLoss:136.8031 | CLSLoss:0.0000 | AUROC:0.9930\n",
      "\n",
      "Epoch: [264 | 1000] LR: 0.037465\n",
      "Train | 16/16 | Loss:3.9883 | MainLoss:0.4180 | Alpha:0.0265 | SPLoss:134.6072 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1640 | MainLoss:0.1640 | SPLoss:132.0868 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6683 | MainLoss:0.6683 | SPLoss:132.0867 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "\n",
      "Epoch: [265 | 1000] LR: 0.037435\n",
      "Train | 16/16 | Loss:3.7930 | MainLoss:0.4179 | Alpha:0.0260 | SPLoss:129.9667 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2165 | MainLoss:0.2165 | SPLoss:127.6179 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5486 | MainLoss:0.5486 | SPLoss:127.6177 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "\n",
      "Epoch: [266 | 1000] LR: 0.037404\n",
      "Train | 16/16 | Loss:3.8557 | MainLoss:0.4206 | Alpha:0.0274 | SPLoss:125.3829 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1644 | MainLoss:0.1644 | SPLoss:123.1049 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6623 | MainLoss:0.6623 | SPLoss:123.1048 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "\n",
      "Epoch: [267 | 1000] LR: 0.037373\n",
      "Train | 16/16 | Loss:3.6627 | MainLoss:0.4128 | Alpha:0.0268 | SPLoss:121.1605 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2183 | MainLoss:0.2183 | SPLoss:118.8068 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5363 | MainLoss:0.5363 | SPLoss:118.8067 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "\n",
      "Epoch: [268 | 1000] LR: 0.037341\n",
      "Train | 16/16 | Loss:3.4287 | MainLoss:0.4151 | Alpha:0.0258 | SPLoss:116.9295 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1941 | MainLoss:0.1941 | SPLoss:114.8482 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5788 | MainLoss:0.5788 | SPLoss:114.8483 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "\n",
      "Epoch: [269 | 1000] LR: 0.037310\n",
      "Train | 16/16 | Loss:3.4304 | MainLoss:0.4132 | Alpha:0.0267 | SPLoss:112.8755 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1521 | MainLoss:0.1521 | SPLoss:110.8940 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6751 | MainLoss:0.6751 | SPLoss:110.8939 | CLSLoss:0.0000 | AUROC:0.9935\n",
      "\n",
      "Epoch: [270 | 1000] LR: 0.037278\n",
      "Train | 16/16 | Loss:3.2975 | MainLoss:0.4128 | Alpha:0.0264 | SPLoss:108.9815 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1873 | MainLoss:0.1873 | SPLoss:107.1057 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5973 | MainLoss:0.5973 | SPLoss:107.1056 | CLSLoss:0.0000 | AUROC:0.9913\n",
      "\n",
      "Epoch: [271 | 1000] LR: 0.037247\n",
      "Train | 16/16 | Loss:3.2617 | MainLoss:0.4193 | Alpha:0.0270 | SPLoss:105.3708 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1744 | MainLoss:0.1744 | SPLoss:103.3847 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6283 | MainLoss:0.6283 | SPLoss:103.3847 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "\n",
      "Epoch: [272 | 1000] LR: 0.037215\n",
      "Train | 16/16 | Loss:3.1488 | MainLoss:0.4155 | Alpha:0.0269 | SPLoss:101.7376 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2145 | MainLoss:0.2145 | SPLoss:99.8237 | CLSLoss:0.0000 | AUROC:0.9991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 31/16 | Loss:0.5543 | MainLoss:0.5543 | SPLoss:99.8237 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "\n",
      "Epoch: [273 | 1000] LR: 0.037183\n",
      "Train | 16/16 | Loss:2.9243 | MainLoss:0.4143 | Alpha:0.0255 | SPLoss:98.2752 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1853 | MainLoss:0.1853 | SPLoss:96.5313 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.6059 | MainLoss:0.6059 | SPLoss:96.5315 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "\n",
      "Epoch: [274 | 1000] LR: 0.037151\n",
      "Train | 16/16 | Loss:2.9934 | MainLoss:0.4131 | Alpha:0.0272 | SPLoss:94.9471 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2058 | MainLoss:0.2058 | SPLoss:93.1639 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5668 | MainLoss:0.5668 | SPLoss:93.1637 | CLSLoss:0.0000 | AUROC:0.9934\n",
      "\n",
      "Epoch: [275 | 1000] LR: 0.037118\n",
      "Train | 16/16 | Loss:2.8004 | MainLoss:0.4068 | Alpha:0.0261 | SPLoss:91.7287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2235 | MainLoss:0.2235 | SPLoss:90.0621 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5589 | MainLoss:0.5589 | SPLoss:90.0622 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "\n",
      "Epoch: [276 | 1000] LR: 0.037086\n",
      "Train | 16/16 | Loss:2.8027 | MainLoss:0.3999 | Alpha:0.0271 | SPLoss:88.5860 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2082 | MainLoss:0.2082 | SPLoss:86.9373 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5209 | MainLoss:0.5209 | SPLoss:86.9373 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "\n",
      "Epoch: [277 | 1000] LR: 0.037053\n",
      "Train | 16/16 | Loss:2.6669 | MainLoss:0.4162 | Alpha:0.0263 | SPLoss:85.5247 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:84.0283 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5612 | MainLoss:0.5612 | SPLoss:84.0283 | CLSLoss:0.0000 | AUROC:0.9911\n",
      "\n",
      "Epoch: [278 | 1000] LR: 0.037020\n",
      "Train | 16/16 | Loss:2.7507 | MainLoss:0.4100 | Alpha:0.0273 | SPLoss:85.7893 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2017 | MainLoss:0.2017 | SPLoss:85.6224 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5553 | MainLoss:0.5553 | SPLoss:85.6225 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "\n",
      "Epoch: [279 | 1000] LR: 0.036987\n",
      "Train | 16/16 | Loss:2.5819 | MainLoss:0.3947 | Alpha:0.0259 | SPLoss:84.2048 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1369 | MainLoss:0.1369 | SPLoss:82.7899 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6894 | MainLoss:0.6894 | SPLoss:82.7898 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "\n",
      "Epoch: [280 | 1000] LR: 0.036954\n",
      "Train | 16/16 | Loss:2.6079 | MainLoss:0.4119 | Alpha:0.0270 | SPLoss:81.4428 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1483 | MainLoss:0.1483 | SPLoss:79.9420 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6754 | MainLoss:0.6754 | SPLoss:79.9421 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "\n",
      "Epoch: [281 | 1000] LR: 0.036920\n",
      "Train | 16/16 | Loss:2.5283 | MainLoss:0.4025 | Alpha:0.0270 | SPLoss:78.7132 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2019 | MainLoss:0.2019 | SPLoss:77.1909 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5257 | MainLoss:0.5257 | SPLoss:77.1908 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "\n",
      "Epoch: [282 | 1000] LR: 0.036887\n",
      "Train | 16/16 | Loss:2.4105 | MainLoss:0.4127 | Alpha:0.0263 | SPLoss:76.0015 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2020 | MainLoss:0.2020 | SPLoss:74.6244 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5537 | MainLoss:0.5537 | SPLoss:74.6244 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "\n",
      "Epoch: [283 | 1000] LR: 0.036853\n",
      "Train | 16/16 | Loss:2.4176 | MainLoss:0.4127 | Alpha:0.0273 | SPLoss:73.4146 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1748 | MainLoss:0.1748 | SPLoss:72.0460 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5781 | MainLoss:0.5781 | SPLoss:72.0459 | CLSLoss:0.0000 | AUROC:0.9917\n",
      "\n",
      "Epoch: [284 | 1000] LR: 0.036819\n",
      "Train | 16/16 | Loss:2.3696 | MainLoss:0.4127 | Alpha:0.0276 | SPLoss:70.7906 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1527 | MainLoss:0.1527 | SPLoss:69.5201 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6231 | MainLoss:0.6231 | SPLoss:69.5201 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "\n",
      "Epoch: [285 | 1000] LR: 0.036785\n",
      "Train | 16/16 | Loss:2.2537 | MainLoss:0.4041 | Alpha:0.0270 | SPLoss:68.3849 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1731 | MainLoss:0.1731 | SPLoss:67.1590 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5862 | MainLoss:0.5862 | SPLoss:67.1589 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "\n",
      "Epoch: [286 | 1000] LR: 0.036751\n",
      "Train | 16/16 | Loss:2.2010 | MainLoss:0.4063 | Alpha:0.0272 | SPLoss:66.0693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1950 | MainLoss:0.1950 | SPLoss:64.8554 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5202 | MainLoss:0.5202 | SPLoss:64.8554 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "\n",
      "Epoch: [287 | 1000] LR: 0.036716\n",
      "Train | 16/16 | Loss:2.0929 | MainLoss:0.4084 | Alpha:0.0264 | SPLoss:63.8289 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2121 | MainLoss:0.2121 | SPLoss:62.7010 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.4909 | MainLoss:0.4909 | SPLoss:62.7010 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "\n",
      "Epoch: [288 | 1000] LR: 0.036682\n",
      "Train | 16/16 | Loss:2.1086 | MainLoss:0.4080 | Alpha:0.0276 | SPLoss:61.6253 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1647 | MainLoss:0.1647 | SPLoss:60.5275 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5824 | MainLoss:0.5824 | SPLoss:60.5275 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "\n",
      "Epoch: [289 | 1000] LR: 0.036647\n",
      "Train | 16/16 | Loss:2.0118 | MainLoss:0.4082 | Alpha:0.0269 | SPLoss:59.5698 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1367 | MainLoss:0.1367 | SPLoss:58.5004 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.6621 | MainLoss:0.6621 | SPLoss:58.5004 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "\n",
      "Epoch: [290 | 1000] LR: 0.036612\n",
      "Train | 16/16 | Loss:1.8841 | MainLoss:0.4051 | Alpha:0.0257 | SPLoss:57.6071 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1898 | MainLoss:0.1898 | SPLoss:56.6075 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5451 | MainLoss:0.5451 | SPLoss:56.6075 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "\n",
      "Epoch: [291 | 1000] LR: 0.036577\n",
      "Train | 16/16 | Loss:1.8259 | MainLoss:0.4013 | Alpha:0.0256 | SPLoss:55.8208 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2415 | MainLoss:0.2415 | SPLoss:54.7925 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.4648 | MainLoss:0.4648 | SPLoss:54.7925 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "\n",
      "Epoch: [292 | 1000] LR: 0.036542\n",
      "Train | 16/16 | Loss:1.8814 | MainLoss:0.3910 | Alpha:0.0276 | SPLoss:53.9406 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1511 | MainLoss:0.1511 | SPLoss:52.9150 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.6172 | MainLoss:0.6172 | SPLoss:52.9151 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "\n",
      "Epoch: [293 | 1000] LR: 0.036506\n",
      "Train | 16/16 | Loss:1.8142 | MainLoss:0.4079 | Alpha:0.0270 | SPLoss:52.0884 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:51.1590 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5962 | MainLoss:0.5962 | SPLoss:51.1590 | CLSLoss:0.0000 | AUROC:0.9913\n",
      "\n",
      "Epoch: [294 | 1000] LR: 0.036471\n",
      "Train | 16/16 | Loss:1.7401 | MainLoss:0.3967 | Alpha:0.0267 | SPLoss:50.3361 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1700 | MainLoss:0.1700 | SPLoss:49.4506 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5669 | MainLoss:0.5669 | SPLoss:49.4506 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "\n",
      "Epoch: [295 | 1000] LR: 0.036435\n",
      "Train | 16/16 | Loss:1.7565 | MainLoss:0.3985 | Alpha:0.0279 | SPLoss:48.6461 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1805 | MainLoss:0.1805 | SPLoss:47.7408 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5772 | MainLoss:0.5772 | SPLoss:47.7407 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "\n",
      "Epoch: [296 | 1000] LR: 0.036399\n",
      "Train | 16/16 | Loss:1.6648 | MainLoss:0.3918 | Alpha:0.0271 | SPLoss:46.9881 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1802 | MainLoss:0.1802 | SPLoss:46.1386 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5624 | MainLoss:0.5624 | SPLoss:46.1386 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "\n",
      "Epoch: [297 | 1000] LR: 0.036363\n",
      "Train | 16/16 | Loss:1.6450 | MainLoss:0.3919 | Alpha:0.0276 | SPLoss:45.3906 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 128/16 | Loss:0.2408 | MainLoss:0.2408 | SPLoss:44.5625 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.4298 | MainLoss:0.4298 | SPLoss:44.5625 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [298 | 1000] LR: 0.036327\n",
      "Train | 16/16 | Loss:1.5957 | MainLoss:0.4015 | Alpha:0.0272 | SPLoss:43.8776 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1506 | MainLoss:0.1506 | SPLoss:43.0810 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.6744 | MainLoss:0.6744 | SPLoss:43.0811 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "\n",
      "Epoch: [299 | 1000] LR: 0.036290\n",
      "Train | 16/16 | Loss:1.5467 | MainLoss:0.3925 | Alpha:0.0272 | SPLoss:42.3757 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1390 | MainLoss:0.1390 | SPLoss:41.6290 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.6410 | MainLoss:0.6410 | SPLoss:41.6291 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "\n",
      "Epoch: [300 | 1000] LR: 0.036254\n",
      "Train | 16/16 | Loss:1.5117 | MainLoss:0.3966 | Alpha:0.0272 | SPLoss:40.9521 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1992 | MainLoss:0.1992 | SPLoss:40.2335 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.5078 | MainLoss:0.5078 | SPLoss:40.2336 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "\n",
      "Epoch: [301 | 1000] LR: 0.036217\n",
      "Train | 16/16 | Loss:1.4885 | MainLoss:0.3992 | Alpha:0.0275 | SPLoss:39.6282 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1627 | MainLoss:0.1627 | SPLoss:38.8965 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5855 | MainLoss:0.5855 | SPLoss:38.8965 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [302 | 1000] LR: 0.036180\n",
      "Train | 16/16 | Loss:1.4699 | MainLoss:0.3966 | Alpha:0.0280 | SPLoss:38.2433 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1354 | MainLoss:0.1354 | SPLoss:37.5599 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.6532 | MainLoss:0.6532 | SPLoss:37.5599 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "\n",
      "Epoch: [303 | 1000] LR: 0.036143\n",
      "Train | 16/16 | Loss:1.4194 | MainLoss:0.3911 | Alpha:0.0278 | SPLoss:36.9416 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1876 | MainLoss:0.1876 | SPLoss:36.2959 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.5289 | MainLoss:0.5289 | SPLoss:36.2959 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "\n",
      "Epoch: [304 | 1000] LR: 0.036106\n",
      "Train | 16/16 | Loss:1.3854 | MainLoss:0.4013 | Alpha:0.0276 | SPLoss:35.7048 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:35.0806 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.6219 | MainLoss:0.6219 | SPLoss:35.0806 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "\n",
      "Epoch: [305 | 1000] LR: 0.036069\n",
      "Train | 16/16 | Loss:1.3556 | MainLoss:0.3966 | Alpha:0.0278 | SPLoss:34.5080 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1476 | MainLoss:0.1476 | SPLoss:33.9168 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6229 | MainLoss:0.6229 | SPLoss:33.9168 | CLSLoss:0.0000 | AUROC:0.9910\n",
      "\n",
      "Epoch: [306 | 1000] LR: 0.036031\n",
      "Train | 16/16 | Loss:1.3318 | MainLoss:0.3864 | Alpha:0.0283 | SPLoss:33.3570 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1814 | MainLoss:0.1814 | SPLoss:32.7263 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.5238 | MainLoss:0.5238 | SPLoss:32.7264 | CLSLoss:0.0000 | AUROC:0.9959\n",
      "\n",
      "Epoch: [307 | 1000] LR: 0.035994\n",
      "Train | 16/16 | Loss:1.3007 | MainLoss:0.4048 | Alpha:0.0278 | SPLoss:32.1981 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1881 | MainLoss:0.1881 | SPLoss:31.6448 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.5135 | MainLoss:0.5135 | SPLoss:31.6449 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [308 | 1000] LR: 0.035956\n",
      "Train | 16/16 | Loss:1.2008 | MainLoss:0.3915 | Alpha:0.0260 | SPLoss:31.1648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1675 | MainLoss:0.1675 | SPLoss:30.6479 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.5760 | MainLoss:0.5760 | SPLoss:30.6479 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "\n",
      "Epoch: [309 | 1000] LR: 0.035918\n",
      "Train | 16/16 | Loss:1.2431 | MainLoss:0.3953 | Alpha:0.0281 | SPLoss:30.1567 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1757 | MainLoss:0.1757 | SPLoss:29.6150 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5582 | MainLoss:0.5582 | SPLoss:29.6150 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "\n",
      "Epoch: [310 | 1000] LR: 0.035880\n",
      "Train | 16/16 | Loss:1.1636 | MainLoss:0.3910 | Alpha:0.0265 | SPLoss:29.1894 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2531 | MainLoss:0.2531 | SPLoss:28.6965 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.4150 | MainLoss:0.4150 | SPLoss:28.6964 | CLSLoss:0.0000 | AUROC:0.9933\n",
      "\n",
      "Epoch: [311 | 1000] LR: 0.035842\n",
      "Train | 16/16 | Loss:1.3951 | MainLoss:0.6029 | Alpha:0.0272 | SPLoss:29.1639 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5826 | MainLoss:0.5826 | SPLoss:32.6718 | CLSLoss:0.0000 | AUROC:0.9827\n",
      "Test | 31/16 | Loss:0.5399 | MainLoss:0.5399 | SPLoss:32.6718 | CLSLoss:0.0000 | AUROC:0.8637\n",
      "\n",
      "Epoch: [312 | 1000] LR: 0.035803\n",
      "Train | 16/16 | Loss:1.4143 | MainLoss:0.5115 | Alpha:0.0275 | SPLoss:32.7428 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2394 | MainLoss:0.2394 | SPLoss:37.0967 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.5715 | MainLoss:0.5715 | SPLoss:37.0967 | CLSLoss:0.0000 | AUROC:0.9033\n",
      "\n",
      "Epoch: [313 | 1000] LR: 0.035765\n",
      "Train | 16/16 | Loss:1.6015 | MainLoss:0.5588 | Alpha:0.0264 | SPLoss:39.4961 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2239 | MainLoss:0.2239 | SPLoss:40.2265 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.6803 | MainLoss:0.6803 | SPLoss:40.2264 | CLSLoss:0.0000 | AUROC:0.9145\n",
      "\n",
      "Epoch: [314 | 1000] LR: 0.035726\n",
      "Train | 16/16 | Loss:1.4947 | MainLoss:0.4395 | Alpha:0.0266 | SPLoss:39.6424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2016 | MainLoss:0.2016 | SPLoss:38.9238 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5922 | MainLoss:0.5922 | SPLoss:38.9239 | CLSLoss:0.0000 | AUROC:0.9733\n",
      "\n",
      "Epoch: [315 | 1000] LR: 0.035687\n",
      "Train | 16/16 | Loss:1.4954 | MainLoss:0.4339 | Alpha:0.0277 | SPLoss:38.2639 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1779 | MainLoss:0.1779 | SPLoss:37.6430 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.6363 | MainLoss:0.6363 | SPLoss:37.6431 | CLSLoss:0.0000 | AUROC:0.9814\n",
      "\n",
      "Epoch: [316 | 1000] LR: 0.035648\n",
      "Train | 16/16 | Loss:1.4408 | MainLoss:0.4229 | Alpha:0.0275 | SPLoss:37.0070 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1680 | MainLoss:0.1680 | SPLoss:36.3720 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6139 | MainLoss:0.6139 | SPLoss:36.3720 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [317 | 1000] LR: 0.035609\n",
      "Train | 16/16 | Loss:1.3672 | MainLoss:0.4245 | Alpha:0.0263 | SPLoss:35.8180 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1861 | MainLoss:0.1861 | SPLoss:35.2077 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5814 | MainLoss:0.5814 | SPLoss:35.2077 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "\n",
      "Epoch: [318 | 1000] LR: 0.035569\n",
      "Train | 16/16 | Loss:1.3786 | MainLoss:0.4190 | Alpha:0.0277 | SPLoss:34.6431 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1481 | MainLoss:0.1481 | SPLoss:34.0417 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.6630 | MainLoss:0.6630 | SPLoss:34.0416 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "\n",
      "Epoch: [319 | 1000] LR: 0.035530\n",
      "Train | 16/16 | Loss:1.3168 | MainLoss:0.4202 | Alpha:0.0267 | SPLoss:33.5060 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2164 | MainLoss:0.2164 | SPLoss:32.9388 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.4899 | MainLoss:0.4899 | SPLoss:32.9388 | CLSLoss:0.0000 | AUROC:0.9959\n",
      "\n",
      "Epoch: [320 | 1000] LR: 0.035490\n",
      "Train | 16/16 | Loss:1.2804 | MainLoss:0.4084 | Alpha:0.0265 | SPLoss:32.8646 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2649 | MainLoss:0.2649 | SPLoss:32.4795 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.4044 | MainLoss:0.4044 | SPLoss:32.4795 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "\n",
      "Epoch: [321 | 1000] LR: 0.035450\n",
      "Train | 16/16 | Loss:1.2816 | MainLoss:0.4078 | Alpha:0.0273 | SPLoss:31.9669 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1481 | MainLoss:0.1481 | SPLoss:31.4144 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.6216 | MainLoss:0.6216 | SPLoss:31.4144 | CLSLoss:0.0000 | AUROC:0.9958\n",
      "\n",
      "Epoch: [322 | 1000] LR: 0.035410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.2529 | MainLoss:0.4113 | Alpha:0.0272 | SPLoss:30.9444 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2770 | MainLoss:0.2770 | SPLoss:30.3927 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "Test | 31/16 | Loss:0.3878 | MainLoss:0.3878 | SPLoss:30.3927 | CLSLoss:0.0000 | AUROC:0.9963\n",
      "\n",
      "Epoch: [323 | 1000] LR: 0.035370\n",
      "Train | 16/16 | Loss:1.2355 | MainLoss:0.4038 | Alpha:0.0278 | SPLoss:29.9220 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2038 | MainLoss:0.2038 | SPLoss:29.3895 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4893 | MainLoss:0.4893 | SPLoss:29.3895 | CLSLoss:0.0000 | AUROC:0.9964\n",
      "\n",
      "Epoch: [324 | 1000] LR: 0.035330\n",
      "Train | 16/16 | Loss:1.1883 | MainLoss:0.4078 | Alpha:0.0270 | SPLoss:28.9449 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1846 | MainLoss:0.1846 | SPLoss:28.4595 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5209 | MainLoss:0.5209 | SPLoss:28.4595 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [325 | 1000] LR: 0.035289\n",
      "Train | 16/16 | Loss:1.1807 | MainLoss:0.4032 | Alpha:0.0277 | SPLoss:28.0339 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2560 | MainLoss:0.2560 | SPLoss:27.5223 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.3936 | MainLoss:0.3936 | SPLoss:27.5224 | CLSLoss:0.0000 | AUROC:0.9958\n",
      "\n",
      "Epoch: [326 | 1000] LR: 0.035249\n",
      "Train | 16/16 | Loss:1.1208 | MainLoss:0.4112 | Alpha:0.0262 | SPLoss:27.1245 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2045 | MainLoss:0.2045 | SPLoss:26.6831 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4753 | MainLoss:0.4753 | SPLoss:26.6831 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "\n",
      "Epoch: [327 | 1000] LR: 0.035208\n",
      "Train | 16/16 | Loss:1.1885 | MainLoss:0.4066 | Alpha:0.0298 | SPLoss:26.2230 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2026 | MainLoss:0.2026 | SPLoss:25.7395 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4823 | MainLoss:0.4823 | SPLoss:25.7394 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "\n",
      "Epoch: [328 | 1000] LR: 0.035167\n",
      "Train | 16/16 | Loss:1.0789 | MainLoss:0.4064 | Alpha:0.0265 | SPLoss:25.3751 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1582 | MainLoss:0.1582 | SPLoss:24.9523 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "Test | 31/16 | Loss:0.5732 | MainLoss:0.5732 | SPLoss:24.9524 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "\n",
      "Epoch: [329 | 1000] LR: 0.035126\n",
      "Train | 16/16 | Loss:1.0806 | MainLoss:0.4022 | Alpha:0.0276 | SPLoss:24.5586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2031 | MainLoss:0.2031 | SPLoss:24.1495 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.4693 | MainLoss:0.4693 | SPLoss:24.1495 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "\n",
      "Epoch: [330 | 1000] LR: 0.035085\n",
      "Train | 16/16 | Loss:1.0618 | MainLoss:0.3977 | Alpha:0.0279 | SPLoss:23.7899 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1904 | MainLoss:0.1904 | SPLoss:23.3712 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.4939 | MainLoss:0.4939 | SPLoss:23.3712 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "\n",
      "Epoch: [331 | 1000] LR: 0.035044\n",
      "Train | 16/16 | Loss:0.9957 | MainLoss:0.3976 | Alpha:0.0260 | SPLoss:23.0336 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2042 | MainLoss:0.2042 | SPLoss:22.6715 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4715 | MainLoss:0.4715 | SPLoss:22.6715 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "\n",
      "Epoch: [332 | 1000] LR: 0.035002\n",
      "Train | 16/16 | Loss:1.0040 | MainLoss:0.3931 | Alpha:0.0273 | SPLoss:22.3534 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1718 | MainLoss:0.1718 | SPLoss:21.9560 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5301 | MainLoss:0.5301 | SPLoss:21.9560 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "\n",
      "Epoch: [333 | 1000] LR: 0.034961\n",
      "Train | 16/16 | Loss:0.9802 | MainLoss:0.3989 | Alpha:0.0269 | SPLoss:21.6314 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1566 | MainLoss:0.1566 | SPLoss:21.2785 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5784 | MainLoss:0.5784 | SPLoss:21.2785 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [334 | 1000] LR: 0.034919\n",
      "Train | 16/16 | Loss:0.9830 | MainLoss:0.3919 | Alpha:0.0282 | SPLoss:20.9688 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1828 | MainLoss:0.1828 | SPLoss:20.5936 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.5094 | MainLoss:0.5094 | SPLoss:20.5936 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "\n",
      "Epoch: [335 | 1000] LR: 0.034877\n",
      "Train | 16/16 | Loss:0.9536 | MainLoss:0.3962 | Alpha:0.0275 | SPLoss:20.2940 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1903 | MainLoss:0.1903 | SPLoss:19.9642 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.4942 | MainLoss:0.4942 | SPLoss:19.9642 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "\n",
      "Epoch: [336 | 1000] LR: 0.034835\n",
      "Train | 16/16 | Loss:0.8979 | MainLoss:0.3915 | Alpha:0.0257 | SPLoss:19.6862 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1542 | MainLoss:0.1542 | SPLoss:19.3939 | CLSLoss:0.0000 | AUROC:0.9994\n",
      "Test | 31/16 | Loss:0.5640 | MainLoss:0.5640 | SPLoss:19.3939 | CLSLoss:0.0000 | AUROC:0.9935\n",
      "\n",
      "Epoch: [337 | 1000] LR: 0.034793\n",
      "Train | 16/16 | Loss:0.9289 | MainLoss:0.3947 | Alpha:0.0280 | SPLoss:19.1113 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2030 | MainLoss:0.2030 | SPLoss:18.7677 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.4605 | MainLoss:0.4605 | SPLoss:18.7677 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "\n",
      "Epoch: [338 | 1000] LR: 0.034750\n",
      "Train | 16/16 | Loss:0.8945 | MainLoss:0.3953 | Alpha:0.0270 | SPLoss:18.4911 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1924 | MainLoss:0.1924 | SPLoss:18.2052 | CLSLoss:0.0000 | AUROC:0.9993\n",
      "Test | 31/16 | Loss:0.4854 | MainLoss:0.4854 | SPLoss:18.2052 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "\n",
      "Epoch: [339 | 1000] LR: 0.034708\n",
      "Train | 16/16 | Loss:0.8548 | MainLoss:0.3857 | Alpha:0.0261 | SPLoss:17.9492 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1633 | MainLoss:0.1633 | SPLoss:17.6758 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5397 | MainLoss:0.5397 | SPLoss:17.6758 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [340 | 1000] LR: 0.034665\n",
      "Train | 16/16 | Loss:0.8595 | MainLoss:0.3958 | Alpha:0.0266 | SPLoss:17.4274 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1655 | MainLoss:0.1655 | SPLoss:17.1579 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.5163 | MainLoss:0.5163 | SPLoss:17.1579 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "\n",
      "Epoch: [341 | 1000] LR: 0.034622\n",
      "Train | 16/16 | Loss:0.8553 | MainLoss:0.3834 | Alpha:0.0279 | SPLoss:16.9152 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1754 | MainLoss:0.1754 | SPLoss:16.6250 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.5115 | MainLoss:0.5115 | SPLoss:16.6250 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "\n",
      "Epoch: [342 | 1000] LR: 0.034579\n",
      "Train | 16/16 | Loss:0.8124 | MainLoss:0.3757 | Alpha:0.0266 | SPLoss:16.4045 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2377 | MainLoss:0.2377 | SPLoss:16.1535 | CLSLoss:0.0000 | AUROC:0.9991\n",
      "Test | 31/16 | Loss:0.4146 | MainLoss:0.4146 | SPLoss:16.1534 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "\n",
      "Epoch: [343 | 1000] LR: 0.034536\n",
      "Train | 16/16 | Loss:0.8355 | MainLoss:0.3935 | Alpha:0.0278 | SPLoss:15.9226 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1496 | MainLoss:0.1496 | SPLoss:15.6729 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.5802 | MainLoss:0.5802 | SPLoss:15.6729 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "\n",
      "Epoch: [344 | 1000] LR: 0.034493\n",
      "Train | 16/16 | Loss:0.7935 | MainLoss:0.3796 | Alpha:0.0268 | SPLoss:15.4749 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1704 | MainLoss:0.1704 | SPLoss:15.2195 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.5499 | MainLoss:0.5499 | SPLoss:15.2195 | CLSLoss:0.0000 | AUROC:0.9923\n",
      "\n",
      "Epoch: [345 | 1000] LR: 0.034450\n",
      "Train | 16/16 | Loss:0.7949 | MainLoss:0.3802 | Alpha:0.0276 | SPLoss:14.9863 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1937 | MainLoss:0.1937 | SPLoss:14.7484 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.4778 | MainLoss:0.4778 | SPLoss:14.7484 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "\n",
      "Epoch: [346 | 1000] LR: 0.034406\n",
      "Train | 16/16 | Loss:0.7770 | MainLoss:0.3842 | Alpha:0.0270 | SPLoss:14.5376 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1332 | MainLoss:0.1332 | SPLoss:14.3185 | CLSLoss:0.0000 | AUROC:0.9989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 31/16 | Loss:0.6330 | MainLoss:0.6330 | SPLoss:14.3184 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "\n",
      "Epoch: [347 | 1000] LR: 0.034363\n",
      "Train | 16/16 | Loss:0.7736 | MainLoss:0.3816 | Alpha:0.0278 | SPLoss:14.1164 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1630 | MainLoss:0.1630 | SPLoss:13.8890 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.5288 | MainLoss:0.5288 | SPLoss:13.8890 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "\n",
      "Epoch: [348 | 1000] LR: 0.034319\n",
      "Train | 16/16 | Loss:0.7622 | MainLoss:0.3887 | Alpha:0.0273 | SPLoss:13.7085 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1648 | MainLoss:0.1648 | SPLoss:13.5040 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.5413 | MainLoss:0.5413 | SPLoss:13.5040 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "\n",
      "Epoch: [349 | 1000] LR: 0.034275\n",
      "Train | 16/16 | Loss:0.7443 | MainLoss:0.3839 | Alpha:0.0271 | SPLoss:13.3250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1660 | MainLoss:0.1660 | SPLoss:13.1220 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.5334 | MainLoss:0.5334 | SPLoss:13.1220 | CLSLoss:0.0000 | AUROC:0.9922\n",
      "\n",
      "Epoch: [350 | 1000] LR: 0.034231\n",
      "Train | 16/16 | Loss:0.7277 | MainLoss:0.3803 | Alpha:0.0268 | SPLoss:12.9332 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1487 | MainLoss:0.1487 | SPLoss:12.7617 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.5714 | MainLoss:0.5714 | SPLoss:12.7617 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "\n",
      "Epoch: [351 | 1000] LR: 0.034186\n",
      "Train | 16/16 | Loss:0.7310 | MainLoss:0.3874 | Alpha:0.0273 | SPLoss:12.5951 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1377 | MainLoss:0.1377 | SPLoss:12.4057 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.6166 | MainLoss:0.6166 | SPLoss:12.4057 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "\n",
      "Epoch: [352 | 1000] LR: 0.034142\n",
      "Train | 16/16 | Loss:0.7194 | MainLoss:0.3849 | Alpha:0.0273 | SPLoss:12.2403 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1825 | MainLoss:0.1825 | SPLoss:12.0541 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.4899 | MainLoss:0.4899 | SPLoss:12.0541 | CLSLoss:0.0000 | AUROC:0.9935\n",
      "\n",
      "Epoch: [353 | 1000] LR: 0.034098\n",
      "Train | 16/16 | Loss:0.6977 | MainLoss:0.3838 | Alpha:0.0264 | SPLoss:11.9062 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2548 | MainLoss:0.2548 | SPLoss:11.7233 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.3733 | MainLoss:0.3733 | SPLoss:11.7233 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "\n",
      "Epoch: [354 | 1000] LR: 0.034053\n",
      "Train | 16/16 | Loss:0.6960 | MainLoss:0.3839 | Alpha:0.0270 | SPLoss:11.5905 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2136 | MainLoss:0.2136 | SPLoss:11.4108 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4333 | MainLoss:0.4333 | SPLoss:11.4108 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "\n",
      "Epoch: [355 | 1000] LR: 0.034008\n",
      "Train | 16/16 | Loss:0.6964 | MainLoss:0.3866 | Alpha:0.0275 | SPLoss:11.2454 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1733 | MainLoss:0.1733 | SPLoss:11.0846 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.5160 | MainLoss:0.5160 | SPLoss:11.0846 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [356 | 1000] LR: 0.033963\n",
      "Train | 16/16 | Loss:0.6595 | MainLoss:0.3769 | Alpha:0.0258 | SPLoss:10.9613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1755 | MainLoss:0.1755 | SPLoss:10.8155 | CLSLoss:0.0000 | AUROC:0.9990\n",
      "Test | 31/16 | Loss:0.5050 | MainLoss:0.5050 | SPLoss:10.8155 | CLSLoss:0.0000 | AUROC:0.9907\n",
      "\n",
      "Epoch: [357 | 1000] LR: 0.033918\n",
      "Train | 16/16 | Loss:0.6496 | MainLoss:0.3711 | Alpha:0.0260 | SPLoss:10.6937 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2297 | MainLoss:0.2297 | SPLoss:10.5360 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.4096 | MainLoss:0.4096 | SPLoss:10.5360 | CLSLoss:0.0000 | AUROC:0.9918\n",
      "\n",
      "Epoch: [358 | 1000] LR: 0.033873\n",
      "Train | 16/16 | Loss:0.6557 | MainLoss:0.3759 | Alpha:0.0269 | SPLoss:10.4050 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1444 | MainLoss:0.1444 | SPLoss:10.2660 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.5858 | MainLoss:0.5858 | SPLoss:10.2660 | CLSLoss:0.0000 | AUROC:0.9917\n",
      "\n",
      "Epoch: [359 | 1000] LR: 0.033828\n",
      "Train | 16/16 | Loss:0.6484 | MainLoss:0.3704 | Alpha:0.0274 | SPLoss:10.1427 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2016 | MainLoss:0.2016 | SPLoss:9.9990 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.4614 | MainLoss:0.4614 | SPLoss:9.9990 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [360 | 1000] LR: 0.033782\n",
      "Train | 16/16 | Loss:0.6485 | MainLoss:0.3718 | Alpha:0.0281 | SPLoss:9.8581 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1673 | MainLoss:0.1673 | SPLoss:9.7139 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "Test | 31/16 | Loss:0.5128 | MainLoss:0.5128 | SPLoss:9.7139 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "\n",
      "Epoch: [361 | 1000] LR: 0.033737\n",
      "Train | 16/16 | Loss:0.6157 | MainLoss:0.3661 | Alpha:0.0260 | SPLoss:9.5923 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1445 | MainLoss:0.1445 | SPLoss:9.4792 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.5736 | MainLoss:0.5736 | SPLoss:9.4792 | CLSLoss:0.0000 | AUROC:0.9930\n",
      "\n",
      "Epoch: [362 | 1000] LR: 0.033691\n",
      "Train | 16/16 | Loss:2.1210 | MainLoss:0.3748 | Alpha:0.0261 | SPLoss:71.4169 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1856 | MainLoss:0.1856 | SPLoss:85.8860 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.4628 | MainLoss:0.4628 | SPLoss:85.8860 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "\n",
      "Epoch: [363 | 1000] LR: 0.033645\n",
      "Train | 16/16 | Loss:2.5274 | MainLoss:0.3611 | Alpha:0.0256 | SPLoss:84.7383 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1658 | MainLoss:0.1658 | SPLoss:83.3629 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5066 | MainLoss:0.5066 | SPLoss:83.3628 | CLSLoss:0.0000 | AUROC:0.9914\n",
      "\n",
      "Epoch: [364 | 1000] LR: 0.033599\n",
      "Train | 16/16 | Loss:2.5796 | MainLoss:0.3707 | Alpha:0.0269 | SPLoss:82.1513 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1781 | MainLoss:0.1781 | SPLoss:80.7870 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.4786 | MainLoss:0.4786 | SPLoss:80.7870 | CLSLoss:0.0000 | AUROC:0.9916\n",
      "\n",
      "Epoch: [365 | 1000] LR: 0.033553\n",
      "Train | 16/16 | Loss:2.4094 | MainLoss:0.3720 | Alpha:0.0256 | SPLoss:79.6797 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2405 | MainLoss:0.2405 | SPLoss:78.4239 | CLSLoss:0.0000 | AUROC:0.9980\n",
      "Test | 31/16 | Loss:0.3976 | MainLoss:0.3976 | SPLoss:78.4239 | CLSLoss:0.0000 | AUROC:0.9918\n",
      "\n",
      "Epoch: [366 | 1000] LR: 0.033507\n",
      "Train | 16/16 | Loss:2.3816 | MainLoss:0.3798 | Alpha:0.0259 | SPLoss:77.3451 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2667 | MainLoss:0.2667 | SPLoss:76.0990 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.3541 | MainLoss:0.3541 | SPLoss:76.0989 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [367 | 1000] LR: 0.033460\n",
      "Train | 16/16 | Loss:2.3404 | MainLoss:0.3640 | Alpha:0.0263 | SPLoss:75.0269 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2297 | MainLoss:0.2297 | SPLoss:73.8140 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.4046 | MainLoss:0.4046 | SPLoss:73.8139 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [368 | 1000] LR: 0.033414\n",
      "Train | 16/16 | Loss:2.3185 | MainLoss:0.3693 | Alpha:0.0268 | SPLoss:72.7487 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1576 | MainLoss:0.1576 | SPLoss:71.5660 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.5523 | MainLoss:0.5523 | SPLoss:71.5661 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [369 | 1000] LR: 0.033367\n",
      "Train | 16/16 | Loss:2.2646 | MainLoss:0.3785 | Alpha:0.0267 | SPLoss:70.6069 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1815 | MainLoss:0.1815 | SPLoss:69.3969 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.4755 | MainLoss:0.4755 | SPLoss:69.3969 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [370 | 1000] LR: 0.033320\n",
      "Train | 16/16 | Loss:2.0987 | MainLoss:0.3666 | Alpha:0.0253 | SPLoss:68.4051 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1581 | MainLoss:0.1581 | SPLoss:67.3950 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.5223 | MainLoss:0.5223 | SPLoss:67.3950 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "\n",
      "Epoch: [371 | 1000] LR: 0.033273\n",
      "Train | 16/16 | Loss:2.1750 | MainLoss:0.3572 | Alpha:0.0274 | SPLoss:66.4036 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 128/16 | Loss:0.1646 | MainLoss:0.1646 | SPLoss:65.3269 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.5498 | MainLoss:0.5498 | SPLoss:65.3268 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [372 | 1000] LR: 0.033226\n",
      "Train | 16/16 | Loss:2.1405 | MainLoss:0.3696 | Alpha:0.0275 | SPLoss:64.3959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1905 | MainLoss:0.1905 | SPLoss:63.3066 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.4721 | MainLoss:0.4721 | SPLoss:63.3066 | CLSLoss:0.0000 | AUROC:0.9875\n",
      "\n",
      "Epoch: [373 | 1000] LR: 0.033179\n",
      "Train | 16/16 | Loss:2.0918 | MainLoss:0.3674 | Alpha:0.0276 | SPLoss:62.4171 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1943 | MainLoss:0.1943 | SPLoss:61.3305 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.4649 | MainLoss:0.4649 | SPLoss:61.3306 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [374 | 1000] LR: 0.033132\n",
      "Train | 16/16 | Loss:2.0562 | MainLoss:0.3679 | Alpha:0.0279 | SPLoss:60.4196 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1166 | MainLoss:0.1166 | SPLoss:59.4289 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.7121 | MainLoss:0.7121 | SPLoss:59.4288 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [375 | 1000] LR: 0.033084\n",
      "Train | 16/16 | Loss:1.9861 | MainLoss:0.3773 | Alpha:0.0275 | SPLoss:58.5750 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1588 | MainLoss:0.1588 | SPLoss:57.6010 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.5414 | MainLoss:0.5414 | SPLoss:57.6010 | CLSLoss:0.0000 | AUROC:0.9869\n",
      "\n",
      "Epoch: [376 | 1000] LR: 0.033037\n",
      "Train | 16/16 | Loss:1.9122 | MainLoss:0.3629 | Alpha:0.0273 | SPLoss:56.7415 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1829 | MainLoss:0.1829 | SPLoss:55.8419 | CLSLoss:0.0000 | AUROC:0.9988\n",
      "Test | 31/16 | Loss:0.4883 | MainLoss:0.4883 | SPLoss:55.8418 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [377 | 1000] LR: 0.032989\n",
      "Train | 16/16 | Loss:1.8463 | MainLoss:0.3614 | Alpha:0.0270 | SPLoss:54.9852 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1693 | MainLoss:0.1693 | SPLoss:54.1564 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5156 | MainLoss:0.5156 | SPLoss:54.1563 | CLSLoss:0.0000 | AUROC:0.9871\n",
      "\n",
      "Epoch: [378 | 1000] LR: 0.032941\n",
      "Train | 16/16 | Loss:1.8195 | MainLoss:0.3555 | Alpha:0.0274 | SPLoss:53.4096 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1929 | MainLoss:0.1929 | SPLoss:52.5050 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4634 | MainLoss:0.4634 | SPLoss:52.5051 | CLSLoss:0.0000 | AUROC:0.9860\n",
      "\n",
      "Epoch: [379 | 1000] LR: 0.032893\n",
      "Train | 16/16 | Loss:1.7486 | MainLoss:0.3580 | Alpha:0.0269 | SPLoss:51.7956 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1544 | MainLoss:0.1544 | SPLoss:50.9563 | CLSLoss:0.0000 | AUROC:0.9977\n",
      "Test | 31/16 | Loss:0.5706 | MainLoss:0.5706 | SPLoss:50.9563 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [380 | 1000] LR: 0.032845\n",
      "Train | 16/16 | Loss:1.7256 | MainLoss:0.3654 | Alpha:0.0271 | SPLoss:50.2138 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1874 | MainLoss:0.1874 | SPLoss:49.4291 | CLSLoss:0.0000 | AUROC:0.9978\n",
      "Test | 31/16 | Loss:0.4864 | MainLoss:0.4864 | SPLoss:49.4291 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [381 | 1000] LR: 0.032797\n",
      "Train | 16/16 | Loss:1.8044 | MainLoss:0.4592 | Alpha:0.0275 | SPLoss:48.9597 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1964 | MainLoss:0.1964 | SPLoss:48.3932 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.6228 | MainLoss:0.6228 | SPLoss:48.3932 | CLSLoss:0.0000 | AUROC:0.8797\n",
      "\n",
      "Epoch: [382 | 1000] LR: 0.032748\n",
      "Train | 16/16 | Loss:1.6756 | MainLoss:0.3952 | Alpha:0.0268 | SPLoss:47.7017 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1923 | MainLoss:0.1923 | SPLoss:46.9052 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5421 | MainLoss:0.5421 | SPLoss:46.9052 | CLSLoss:0.0000 | AUROC:0.9558\n",
      "\n",
      "Epoch: [383 | 1000] LR: 0.032700\n",
      "Train | 16/16 | Loss:1.6232 | MainLoss:0.3752 | Alpha:0.0270 | SPLoss:46.2035 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2130 | MainLoss:0.2130 | SPLoss:45.4693 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.4783 | MainLoss:0.4783 | SPLoss:45.4693 | CLSLoss:0.0000 | AUROC:0.9764\n",
      "\n",
      "Epoch: [384 | 1000] LR: 0.032651\n",
      "Train | 16/16 | Loss:1.5695 | MainLoss:0.3809 | Alpha:0.0265 | SPLoss:44.8488 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2086 | MainLoss:0.2086 | SPLoss:44.1432 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.4803 | MainLoss:0.4803 | SPLoss:44.1432 | CLSLoss:0.0000 | AUROC:0.9782\n",
      "\n",
      "Epoch: [385 | 1000] LR: 0.032603\n",
      "Train | 16/16 | Loss:1.5882 | MainLoss:0.3827 | Alpha:0.0277 | SPLoss:43.5360 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1985 | MainLoss:0.1985 | SPLoss:42.8392 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5093 | MainLoss:0.5093 | SPLoss:42.8392 | CLSLoss:0.0000 | AUROC:0.9797\n",
      "\n",
      "Epoch: [386 | 1000] LR: 0.032554\n",
      "Train | 16/16 | Loss:1.5328 | MainLoss:0.3814 | Alpha:0.0273 | SPLoss:42.2660 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1949 | MainLoss:0.1949 | SPLoss:41.5604 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.4968 | MainLoss:0.4968 | SPLoss:41.5604 | CLSLoss:0.0000 | AUROC:0.9843\n",
      "\n",
      "Epoch: [387 | 1000] LR: 0.032505\n",
      "Train | 16/16 | Loss:1.5292 | MainLoss:0.3604 | Alpha:0.0285 | SPLoss:40.9309 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2012 | MainLoss:0.2012 | SPLoss:40.2664 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4889 | MainLoss:0.4889 | SPLoss:40.2664 | CLSLoss:0.0000 | AUROC:0.9831\n",
      "\n",
      "Epoch: [388 | 1000] LR: 0.032456\n",
      "Train | 16/16 | Loss:1.4238 | MainLoss:0.3633 | Alpha:0.0267 | SPLoss:39.7245 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1976 | MainLoss:0.1976 | SPLoss:39.1075 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.4899 | MainLoss:0.4899 | SPLoss:39.1076 | CLSLoss:0.0000 | AUROC:0.9770\n",
      "\n",
      "Epoch: [389 | 1000] LR: 0.032407\n",
      "Train | 16/16 | Loss:1.4404 | MainLoss:0.3770 | Alpha:0.0276 | SPLoss:38.5464 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1873 | MainLoss:0.1873 | SPLoss:37.9515 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5066 | MainLoss:0.5066 | SPLoss:37.9516 | CLSLoss:0.0000 | AUROC:0.9815\n",
      "\n",
      "Epoch: [390 | 1000] LR: 0.032357\n",
      "Train | 16/16 | Loss:1.3612 | MainLoss:0.3574 | Alpha:0.0268 | SPLoss:37.4549 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2774 | MainLoss:0.2774 | SPLoss:36.8440 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.3580 | MainLoss:0.3580 | SPLoss:36.8439 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [391 | 1000] LR: 0.032308\n",
      "Train | 16/16 | Loss:1.3498 | MainLoss:0.3768 | Alpha:0.0268 | SPLoss:36.3205 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.0994 | MainLoss:0.0994 | SPLoss:35.8070 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.7678 | MainLoss:0.7678 | SPLoss:35.8071 | CLSLoss:0.0000 | AUROC:0.9810\n",
      "\n",
      "Epoch: [392 | 1000] LR: 0.032258\n",
      "Train | 16/16 | Loss:1.3466 | MainLoss:0.3728 | Alpha:0.0276 | SPLoss:35.3369 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1862 | MainLoss:0.1862 | SPLoss:34.7443 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4959 | MainLoss:0.4959 | SPLoss:34.7443 | CLSLoss:0.0000 | AUROC:0.9839\n",
      "\n",
      "Epoch: [393 | 1000] LR: 0.032208\n",
      "Train | 16/16 | Loss:1.3072 | MainLoss:0.3667 | Alpha:0.0274 | SPLoss:34.2277 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2095 | MainLoss:0.2095 | SPLoss:33.7358 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.4492 | MainLoss:0.4492 | SPLoss:33.7359 | CLSLoss:0.0000 | AUROC:0.9863\n",
      "\n",
      "Epoch: [394 | 1000] LR: 0.032159\n",
      "Train | 16/16 | Loss:1.3241 | MainLoss:0.3713 | Alpha:0.0272 | SPLoss:35.0375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1403 | MainLoss:0.1403 | SPLoss:48.4310 | CLSLoss:0.0000 | AUROC:0.9977\n",
      "Test | 31/16 | Loss:0.6447 | MainLoss:0.6447 | SPLoss:48.4310 | CLSLoss:0.0000 | AUROC:0.9790\n",
      "\n",
      "Epoch: [395 | 1000] LR: 0.032109\n",
      "Train | 16/16 | Loss:1.6480 | MainLoss:0.3665 | Alpha:0.0268 | SPLoss:47.8103 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1820 | MainLoss:0.1820 | SPLoss:47.0614 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.5008 | MainLoss:0.5008 | SPLoss:47.0614 | CLSLoss:0.0000 | AUROC:0.9769\n",
      "\n",
      "Epoch: [396 | 1000] LR: 0.032059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.5914 | MainLoss:0.3528 | Alpha:0.0267 | SPLoss:46.4058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2323 | MainLoss:0.2323 | SPLoss:45.6948 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4053 | MainLoss:0.4053 | SPLoss:45.6948 | CLSLoss:0.0000 | AUROC:0.9825\n",
      "\n",
      "Epoch: [397 | 1000] LR: 0.032008\n",
      "Train | 16/16 | Loss:1.5995 | MainLoss:0.3572 | Alpha:0.0276 | SPLoss:45.0233 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2250 | MainLoss:0.2250 | SPLoss:44.3520 | CLSLoss:0.0000 | AUROC:0.9986\n",
      "Test | 31/16 | Loss:0.4277 | MainLoss:0.4277 | SPLoss:44.3520 | CLSLoss:0.0000 | AUROC:0.9790\n",
      "\n",
      "Epoch: [398 | 1000] LR: 0.031958\n",
      "Train | 16/16 | Loss:1.5441 | MainLoss:0.3561 | Alpha:0.0271 | SPLoss:43.7882 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1688 | MainLoss:0.1688 | SPLoss:43.0748 | CLSLoss:0.0000 | AUROC:0.9987\n",
      "Test | 31/16 | Loss:0.5023 | MainLoss:0.5023 | SPLoss:43.0748 | CLSLoss:0.0000 | AUROC:0.9781\n",
      "\n",
      "Epoch: [399 | 1000] LR: 0.031908\n",
      "Train | 16/16 | Loss:1.4668 | MainLoss:0.3572 | Alpha:0.0261 | SPLoss:42.5301 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2079 | MainLoss:0.2079 | SPLoss:41.8785 | CLSLoss:0.0000 | AUROC:0.9981\n",
      "Test | 31/16 | Loss:0.4436 | MainLoss:0.4436 | SPLoss:41.8784 | CLSLoss:0.0000 | AUROC:0.9809\n",
      "\n",
      "Epoch: [400 | 1000] LR: 0.031857\n",
      "Train | 16/16 | Loss:1.4547 | MainLoss:0.3638 | Alpha:0.0264 | SPLoss:41.3487 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1939 | MainLoss:0.1939 | SPLoss:40.7308 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4872 | MainLoss:0.4872 | SPLoss:40.7308 | CLSLoss:0.0000 | AUROC:0.9799\n",
      "\n",
      "Epoch: [401 | 1000] LR: 0.003181\n",
      "Train | 16/16 | Loss:1.4443 | MainLoss:0.3574 | Alpha:0.0267 | SPLoss:40.6780 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1910 | MainLoss:0.1910 | SPLoss:40.6121 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4814 | MainLoss:0.4814 | SPLoss:40.6121 | CLSLoss:0.0000 | AUROC:0.9788\n",
      "\n",
      "Epoch: [402 | 1000] LR: 0.003176\n",
      "Train | 16/16 | Loss:1.4523 | MainLoss:0.3520 | Alpha:0.0271 | SPLoss:40.5553 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1854 | MainLoss:0.1854 | SPLoss:40.4923 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4844 | MainLoss:0.4844 | SPLoss:40.4924 | CLSLoss:0.0000 | AUROC:0.9785\n",
      "\n",
      "Epoch: [403 | 1000] LR: 0.003170\n",
      "Train | 16/16 | Loss:1.4421 | MainLoss:0.3448 | Alpha:0.0271 | SPLoss:40.4353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1777 | MainLoss:0.1777 | SPLoss:40.3747 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4973 | MainLoss:0.4973 | SPLoss:40.3747 | CLSLoss:0.0000 | AUROC:0.9777\n",
      "\n",
      "Epoch: [404 | 1000] LR: 0.003165\n",
      "Train | 16/16 | Loss:1.4729 | MainLoss:0.3439 | Alpha:0.0280 | SPLoss:40.3111 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1719 | MainLoss:0.1719 | SPLoss:40.2530 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5074 | MainLoss:0.5074 | SPLoss:40.2529 | CLSLoss:0.0000 | AUROC:0.9766\n",
      "\n",
      "Epoch: [405 | 1000] LR: 0.003160\n",
      "Train | 16/16 | Loss:1.4608 | MainLoss:0.3530 | Alpha:0.0276 | SPLoss:40.2021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1837 | MainLoss:0.1837 | SPLoss:40.1338 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4850 | MainLoss:0.4850 | SPLoss:40.1339 | CLSLoss:0.0000 | AUROC:0.9774\n",
      "\n",
      "Epoch: [406 | 1000] LR: 0.003155\n",
      "Train | 16/16 | Loss:1.4707 | MainLoss:0.3615 | Alpha:0.0277 | SPLoss:40.0786 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1811 | MainLoss:0.1811 | SPLoss:40.0145 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4904 | MainLoss:0.4904 | SPLoss:40.0146 | CLSLoss:0.0000 | AUROC:0.9772\n",
      "\n",
      "Epoch: [407 | 1000] LR: 0.003150\n",
      "Train | 16/16 | Loss:1.4238 | MainLoss:0.3437 | Alpha:0.0270 | SPLoss:39.9583 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1753 | MainLoss:0.1753 | SPLoss:39.8998 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5025 | MainLoss:0.5025 | SPLoss:39.8998 | CLSLoss:0.0000 | AUROC:0.9773\n",
      "\n",
      "Epoch: [408 | 1000] LR: 0.003145\n",
      "Train | 16/16 | Loss:1.3679 | MainLoss:0.3374 | Alpha:0.0259 | SPLoss:39.8491 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1826 | MainLoss:0.1826 | SPLoss:39.7895 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4895 | MainLoss:0.4895 | SPLoss:39.7895 | CLSLoss:0.0000 | AUROC:0.9778\n",
      "\n",
      "Epoch: [409 | 1000] LR: 0.003140\n",
      "Train | 16/16 | Loss:1.3849 | MainLoss:0.3417 | Alpha:0.0263 | SPLoss:39.7391 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1778 | MainLoss:0.1778 | SPLoss:39.6807 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5011 | MainLoss:0.5011 | SPLoss:39.6807 | CLSLoss:0.0000 | AUROC:0.9765\n",
      "\n",
      "Epoch: [410 | 1000] LR: 0.003135\n",
      "Train | 16/16 | Loss:1.3575 | MainLoss:0.3369 | Alpha:0.0258 | SPLoss:39.6295 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1854 | MainLoss:0.1854 | SPLoss:39.5708 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4835 | MainLoss:0.4835 | SPLoss:39.5708 | CLSLoss:0.0000 | AUROC:0.9765\n",
      "\n",
      "Epoch: [411 | 1000] LR: 0.003129\n",
      "Train | 16/16 | Loss:1.3616 | MainLoss:0.3423 | Alpha:0.0258 | SPLoss:39.5224 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1817 | MainLoss:0.1817 | SPLoss:39.4637 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4898 | MainLoss:0.4898 | SPLoss:39.4636 | CLSLoss:0.0000 | AUROC:0.9760\n",
      "\n",
      "Epoch: [412 | 1000] LR: 0.003124\n",
      "Train | 16/16 | Loss:1.3936 | MainLoss:0.3487 | Alpha:0.0265 | SPLoss:39.4108 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1807 | MainLoss:0.1807 | SPLoss:39.3543 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4937 | MainLoss:0.4937 | SPLoss:39.3543 | CLSLoss:0.0000 | AUROC:0.9759\n",
      "\n",
      "Epoch: [413 | 1000] LR: 0.003119\n",
      "Train | 16/16 | Loss:1.4413 | MainLoss:0.3409 | Alpha:0.0280 | SPLoss:39.3003 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1865 | MainLoss:0.1865 | SPLoss:39.2379 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4827 | MainLoss:0.4827 | SPLoss:39.2379 | CLSLoss:0.0000 | AUROC:0.9762\n",
      "\n",
      "Epoch: [414 | 1000] LR: 0.003114\n",
      "Train | 16/16 | Loss:1.3820 | MainLoss:0.3434 | Alpha:0.0265 | SPLoss:39.1876 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1851 | MainLoss:0.1851 | SPLoss:39.1287 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4840 | MainLoss:0.4840 | SPLoss:39.1286 | CLSLoss:0.0000 | AUROC:0.9762\n",
      "\n",
      "Epoch: [415 | 1000] LR: 0.003109\n",
      "Train | 16/16 | Loss:1.4240 | MainLoss:0.3432 | Alpha:0.0277 | SPLoss:39.0745 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1768 | MainLoss:0.1768 | SPLoss:39.0155 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5030 | MainLoss:0.5030 | SPLoss:39.0156 | CLSLoss:0.0000 | AUROC:0.9766\n",
      "\n",
      "Epoch: [416 | 1000] LR: 0.003103\n",
      "Train | 16/16 | Loss:1.3720 | MainLoss:0.3394 | Alpha:0.0265 | SPLoss:38.9614 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1939 | MainLoss:0.1939 | SPLoss:38.9058 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4674 | MainLoss:0.4674 | SPLoss:38.9058 | CLSLoss:0.0000 | AUROC:0.9781\n",
      "\n",
      "Epoch: [417 | 1000] LR: 0.003098\n",
      "Train | 16/16 | Loss:1.4011 | MainLoss:0.3513 | Alpha:0.0270 | SPLoss:38.8546 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1901 | MainLoss:0.1901 | SPLoss:38.7973 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4753 | MainLoss:0.4753 | SPLoss:38.7974 | CLSLoss:0.0000 | AUROC:0.9777\n",
      "\n",
      "Epoch: [418 | 1000] LR: 0.003093\n",
      "Train | 16/16 | Loss:1.4064 | MainLoss:0.3413 | Alpha:0.0275 | SPLoss:38.7461 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1826 | MainLoss:0.1826 | SPLoss:38.6870 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4908 | MainLoss:0.4908 | SPLoss:38.6869 | CLSLoss:0.0000 | AUROC:0.9780\n",
      "\n",
      "Epoch: [419 | 1000] LR: 0.003088\n",
      "Train | 16/16 | Loss:1.3602 | MainLoss:0.3469 | Alpha:0.0262 | SPLoss:38.6395 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1834 | MainLoss:0.1834 | SPLoss:38.5818 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4899 | MainLoss:0.4899 | SPLoss:38.5818 | CLSLoss:0.0000 | AUROC:0.9775\n",
      "\n",
      "Epoch: [420 | 1000] LR: 0.003082\n",
      "Train | 16/16 | Loss:1.3922 | MainLoss:0.3395 | Alpha:0.0273 | SPLoss:38.5286 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1837 | MainLoss:0.1837 | SPLoss:38.4722 | CLSLoss:0.0000 | AUROC:0.9984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 31/16 | Loss:0.4887 | MainLoss:0.4887 | SPLoss:38.4722 | CLSLoss:0.0000 | AUROC:0.9775\n",
      "\n",
      "Epoch: [421 | 1000] LR: 0.003077\n",
      "Train | 16/16 | Loss:1.3665 | MainLoss:0.3377 | Alpha:0.0268 | SPLoss:38.4178 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1811 | MainLoss:0.1811 | SPLoss:38.3647 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4919 | MainLoss:0.4919 | SPLoss:38.3647 | CLSLoss:0.0000 | AUROC:0.9770\n",
      "\n",
      "Epoch: [422 | 1000] LR: 0.003072\n",
      "Train | 16/16 | Loss:1.3319 | MainLoss:0.3429 | Alpha:0.0258 | SPLoss:38.3179 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1830 | MainLoss:0.1830 | SPLoss:38.2618 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4877 | MainLoss:0.4877 | SPLoss:38.2618 | CLSLoss:0.0000 | AUROC:0.9771\n",
      "\n",
      "Epoch: [423 | 1000] LR: 0.003066\n",
      "Train | 16/16 | Loss:1.3083 | MainLoss:0.3503 | Alpha:0.0251 | SPLoss:38.2151 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1825 | MainLoss:0.1825 | SPLoss:38.1647 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4905 | MainLoss:0.4905 | SPLoss:38.1646 | CLSLoss:0.0000 | AUROC:0.9771\n",
      "\n",
      "Epoch: [424 | 1000] LR: 0.003061\n",
      "Train | 16/16 | Loss:1.4154 | MainLoss:0.3412 | Alpha:0.0282 | SPLoss:38.1105 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1694 | MainLoss:0.1694 | SPLoss:38.0529 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5164 | MainLoss:0.5164 | SPLoss:38.0529 | CLSLoss:0.0000 | AUROC:0.9777\n",
      "\n",
      "Epoch: [425 | 1000] LR: 0.003056\n",
      "Train | 16/16 | Loss:1.3392 | MainLoss:0.3377 | Alpha:0.0264 | SPLoss:38.0026 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1745 | MainLoss:0.1745 | SPLoss:37.9478 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5024 | MainLoss:0.5024 | SPLoss:37.9478 | CLSLoss:0.0000 | AUROC:0.9774\n",
      "\n",
      "Epoch: [426 | 1000] LR: 0.003050\n",
      "Train | 16/16 | Loss:1.3891 | MainLoss:0.3419 | Alpha:0.0276 | SPLoss:37.8977 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1799 | MainLoss:0.1799 | SPLoss:37.8400 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4918 | MainLoss:0.4918 | SPLoss:37.8401 | CLSLoss:0.0000 | AUROC:0.9781\n",
      "\n",
      "Epoch: [427 | 1000] LR: 0.003045\n",
      "Train | 16/16 | Loss:1.3373 | MainLoss:0.3366 | Alpha:0.0265 | SPLoss:37.7933 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1934 | MainLoss:0.1934 | SPLoss:37.7373 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4662 | MainLoss:0.4662 | SPLoss:37.7373 | CLSLoss:0.0000 | AUROC:0.9778\n",
      "\n",
      "Epoch: [428 | 1000] LR: 0.003040\n",
      "Train | 16/16 | Loss:1.3253 | MainLoss:0.3395 | Alpha:0.0262 | SPLoss:37.6925 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1842 | MainLoss:0.1842 | SPLoss:37.6377 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4829 | MainLoss:0.4829 | SPLoss:37.6376 | CLSLoss:0.0000 | AUROC:0.9776\n",
      "\n",
      "Epoch: [429 | 1000] LR: 0.003034\n",
      "Train | 16/16 | Loss:1.3055 | MainLoss:0.3413 | Alpha:0.0256 | SPLoss:37.5914 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1819 | MainLoss:0.1819 | SPLoss:37.5395 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4888 | MainLoss:0.4888 | SPLoss:37.5394 | CLSLoss:0.0000 | AUROC:0.9780\n",
      "\n",
      "Epoch: [430 | 1000] LR: 0.003029\n",
      "Train | 16/16 | Loss:1.3990 | MainLoss:0.3430 | Alpha:0.0282 | SPLoss:37.4889 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1920 | MainLoss:0.1920 | SPLoss:37.4308 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4704 | MainLoss:0.4704 | SPLoss:37.4308 | CLSLoss:0.0000 | AUROC:0.9789\n",
      "\n",
      "Epoch: [431 | 1000] LR: 0.003023\n",
      "Train | 16/16 | Loss:1.3542 | MainLoss:0.3384 | Alpha:0.0272 | SPLoss:37.3830 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1759 | MainLoss:0.1759 | SPLoss:37.3293 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4996 | MainLoss:0.4996 | SPLoss:37.3293 | CLSLoss:0.0000 | AUROC:0.9786\n",
      "\n",
      "Epoch: [432 | 1000] LR: 0.003018\n",
      "Train | 16/16 | Loss:1.3341 | MainLoss:0.3415 | Alpha:0.0266 | SPLoss:37.2827 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1844 | MainLoss:0.1844 | SPLoss:37.2282 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4812 | MainLoss:0.4812 | SPLoss:37.2282 | CLSLoss:0.0000 | AUROC:0.9785\n",
      "\n",
      "Epoch: [433 | 1000] LR: 0.003013\n",
      "Train | 16/16 | Loss:1.3548 | MainLoss:0.3406 | Alpha:0.0273 | SPLoss:37.1829 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1906 | MainLoss:0.1906 | SPLoss:37.1250 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4704 | MainLoss:0.4704 | SPLoss:37.1249 | CLSLoss:0.0000 | AUROC:0.9782\n",
      "\n",
      "Epoch: [434 | 1000] LR: 0.003007\n",
      "Train | 16/16 | Loss:1.2752 | MainLoss:0.3428 | Alpha:0.0251 | SPLoss:37.0812 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1765 | MainLoss:0.1765 | SPLoss:37.0328 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4976 | MainLoss:0.4976 | SPLoss:37.0328 | CLSLoss:0.0000 | AUROC:0.9773\n",
      "\n",
      "Epoch: [435 | 1000] LR: 0.003002\n",
      "Train | 16/16 | Loss:1.3604 | MainLoss:0.3419 | Alpha:0.0275 | SPLoss:36.9856 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1750 | MainLoss:0.1750 | SPLoss:36.9312 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5014 | MainLoss:0.5014 | SPLoss:36.9312 | CLSLoss:0.0000 | AUROC:0.9768\n",
      "\n",
      "Epoch: [436 | 1000] LR: 0.002996\n",
      "Train | 16/16 | Loss:1.3514 | MainLoss:0.3373 | Alpha:0.0275 | SPLoss:36.8800 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1785 | MainLoss:0.1785 | SPLoss:36.8277 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4962 | MainLoss:0.4962 | SPLoss:36.8277 | CLSLoss:0.0000 | AUROC:0.9770\n",
      "\n",
      "Epoch: [437 | 1000] LR: 0.002991\n",
      "Train | 16/16 | Loss:1.3151 | MainLoss:0.3362 | Alpha:0.0266 | SPLoss:36.7799 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1796 | MainLoss:0.1796 | SPLoss:36.7293 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4942 | MainLoss:0.4942 | SPLoss:36.7293 | CLSLoss:0.0000 | AUROC:0.9776\n",
      "\n",
      "Epoch: [438 | 1000] LR: 0.002985\n",
      "Train | 16/16 | Loss:1.3152 | MainLoss:0.3319 | Alpha:0.0268 | SPLoss:36.6839 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1884 | MainLoss:0.1884 | SPLoss:36.6297 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4772 | MainLoss:0.4772 | SPLoss:36.6297 | CLSLoss:0.0000 | AUROC:0.9780\n",
      "\n",
      "Epoch: [439 | 1000] LR: 0.002980\n",
      "Train | 16/16 | Loss:1.3143 | MainLoss:0.3450 | Alpha:0.0265 | SPLoss:36.5821 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1805 | MainLoss:0.1805 | SPLoss:36.5327 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4922 | MainLoss:0.4922 | SPLoss:36.5327 | CLSLoss:0.0000 | AUROC:0.9792\n",
      "\n",
      "Epoch: [440 | 1000] LR: 0.002975\n",
      "Train | 16/16 | Loss:1.3415 | MainLoss:0.3553 | Alpha:0.0270 | SPLoss:36.4891 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1890 | MainLoss:0.1890 | SPLoss:36.4348 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4750 | MainLoss:0.4750 | SPLoss:36.4348 | CLSLoss:0.0000 | AUROC:0.9799\n",
      "\n",
      "Epoch: [441 | 1000] LR: 0.002969\n",
      "Train | 16/16 | Loss:1.2922 | MainLoss:0.3394 | Alpha:0.0262 | SPLoss:36.3902 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1719 | MainLoss:0.1719 | SPLoss:36.3436 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5080 | MainLoss:0.5080 | SPLoss:36.3436 | CLSLoss:0.0000 | AUROC:0.9793\n",
      "\n",
      "Epoch: [442 | 1000] LR: 0.002964\n",
      "Train | 16/16 | Loss:1.3712 | MainLoss:0.3507 | Alpha:0.0281 | SPLoss:36.2940 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1851 | MainLoss:0.1851 | SPLoss:36.2412 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4801 | MainLoss:0.4801 | SPLoss:36.2412 | CLSLoss:0.0000 | AUROC:0.9794\n",
      "\n",
      "Epoch: [443 | 1000] LR: 0.002958\n",
      "Train | 16/16 | Loss:1.2981 | MainLoss:0.3381 | Alpha:0.0265 | SPLoss:36.1953 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1836 | MainLoss:0.1836 | SPLoss:36.1468 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4842 | MainLoss:0.4842 | SPLoss:36.1468 | CLSLoss:0.0000 | AUROC:0.9788\n",
      "\n",
      "Epoch: [444 | 1000] LR: 0.002952\n",
      "Train | 16/16 | Loss:1.3211 | MainLoss:0.3374 | Alpha:0.0272 | SPLoss:36.0993 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1795 | MainLoss:0.1795 | SPLoss:36.0495 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4932 | MainLoss:0.4932 | SPLoss:36.0496 | CLSLoss:0.0000 | AUROC:0.9789\n",
      "\n",
      "Epoch: [445 | 1000] LR: 0.002947\n",
      "Train | 16/16 | Loss:1.2844 | MainLoss:0.3384 | Alpha:0.0263 | SPLoss:36.0042 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 128/16 | Loss:0.1820 | MainLoss:0.1820 | SPLoss:35.9563 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4878 | MainLoss:0.4878 | SPLoss:35.9564 | CLSLoss:0.0000 | AUROC:0.9787\n",
      "\n",
      "Epoch: [446 | 1000] LR: 0.002941\n",
      "Train | 16/16 | Loss:1.3023 | MainLoss:0.3341 | Alpha:0.0270 | SPLoss:35.9129 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1908 | MainLoss:0.1908 | SPLoss:35.8603 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4738 | MainLoss:0.4738 | SPLoss:35.8603 | CLSLoss:0.0000 | AUROC:0.9786\n",
      "\n",
      "Epoch: [447 | 1000] LR: 0.002936\n",
      "Train | 16/16 | Loss:1.2852 | MainLoss:0.3310 | Alpha:0.0266 | SPLoss:35.8168 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1793 | MainLoss:0.1793 | SPLoss:35.7673 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4979 | MainLoss:0.4979 | SPLoss:35.7673 | CLSLoss:0.0000 | AUROC:0.9780\n",
      "\n",
      "Epoch: [448 | 1000] LR: 0.002930\n",
      "Train | 16/16 | Loss:1.3035 | MainLoss:0.3343 | Alpha:0.0271 | SPLoss:35.7184 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1822 | MainLoss:0.1822 | SPLoss:35.6714 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4912 | MainLoss:0.4912 | SPLoss:35.6714 | CLSLoss:0.0000 | AUROC:0.9783\n",
      "\n",
      "Epoch: [449 | 1000] LR: 0.002925\n",
      "Train | 16/16 | Loss:1.2894 | MainLoss:0.3364 | Alpha:0.0267 | SPLoss:35.6291 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1894 | MainLoss:0.1894 | SPLoss:35.5780 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4763 | MainLoss:0.4763 | SPLoss:35.5780 | CLSLoss:0.0000 | AUROC:0.9783\n",
      "\n",
      "Epoch: [450 | 1000] LR: 0.002919\n",
      "Train | 16/16 | Loss:1.2798 | MainLoss:0.3387 | Alpha:0.0265 | SPLoss:35.5353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1651 | MainLoss:0.1651 | SPLoss:35.4893 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5250 | MainLoss:0.5250 | SPLoss:35.4893 | CLSLoss:0.0000 | AUROC:0.9777\n",
      "\n",
      "Epoch: [451 | 1000] LR: 0.002914\n",
      "Train | 16/16 | Loss:1.3010 | MainLoss:0.3414 | Alpha:0.0271 | SPLoss:35.4493 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1859 | MainLoss:0.1859 | SPLoss:35.3947 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4798 | MainLoss:0.4798 | SPLoss:35.3947 | CLSLoss:0.0000 | AUROC:0.9786\n",
      "\n",
      "Epoch: [452 | 1000] LR: 0.002908\n",
      "Train | 16/16 | Loss:1.3621 | MainLoss:0.3405 | Alpha:0.0289 | SPLoss:35.3513 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1740 | MainLoss:0.1740 | SPLoss:35.2958 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5036 | MainLoss:0.5036 | SPLoss:35.2958 | CLSLoss:0.0000 | AUROC:0.9784\n",
      "\n",
      "Epoch: [453 | 1000] LR: 0.002902\n",
      "Train | 16/16 | Loss:1.2585 | MainLoss:0.3340 | Alpha:0.0262 | SPLoss:35.2537 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1749 | MainLoss:0.1749 | SPLoss:35.2055 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5002 | MainLoss:0.5002 | SPLoss:35.2054 | CLSLoss:0.0000 | AUROC:0.9784\n",
      "\n",
      "Epoch: [454 | 1000] LR: 0.002897\n",
      "Train | 16/16 | Loss:1.3104 | MainLoss:0.3347 | Alpha:0.0278 | SPLoss:35.1617 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1772 | MainLoss:0.1772 | SPLoss:35.1105 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4974 | MainLoss:0.4974 | SPLoss:35.1105 | CLSLoss:0.0000 | AUROC:0.9792\n",
      "\n",
      "Epoch: [455 | 1000] LR: 0.002891\n",
      "Train | 16/16 | Loss:1.2800 | MainLoss:0.3384 | Alpha:0.0269 | SPLoss:35.0683 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1853 | MainLoss:0.1853 | SPLoss:35.0207 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4814 | MainLoss:0.4814 | SPLoss:35.0206 | CLSLoss:0.0000 | AUROC:0.9786\n",
      "\n",
      "Epoch: [456 | 1000] LR: 0.002886\n",
      "Train | 16/16 | Loss:1.2791 | MainLoss:0.3311 | Alpha:0.0271 | SPLoss:34.9787 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1767 | MainLoss:0.1767 | SPLoss:34.9302 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4983 | MainLoss:0.4983 | SPLoss:34.9301 | CLSLoss:0.0000 | AUROC:0.9773\n",
      "\n",
      "Epoch: [457 | 1000] LR: 0.002880\n",
      "Train | 16/16 | Loss:1.2563 | MainLoss:0.3331 | Alpha:0.0265 | SPLoss:34.8858 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1800 | MainLoss:0.1800 | SPLoss:34.8398 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4907 | MainLoss:0.4907 | SPLoss:34.8399 | CLSLoss:0.0000 | AUROC:0.9783\n",
      "\n",
      "Epoch: [458 | 1000] LR: 0.002874\n",
      "Train | 16/16 | Loss:1.2542 | MainLoss:0.3351 | Alpha:0.0264 | SPLoss:34.7994 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1858 | MainLoss:0.1858 | SPLoss:34.7516 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4796 | MainLoss:0.4796 | SPLoss:34.7516 | CLSLoss:0.0000 | AUROC:0.9783\n",
      "\n",
      "Epoch: [459 | 1000] LR: 0.002869\n",
      "Train | 16/16 | Loss:1.2869 | MainLoss:0.3271 | Alpha:0.0277 | SPLoss:34.7060 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1649 | MainLoss:0.1649 | SPLoss:34.6615 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5272 | MainLoss:0.5272 | SPLoss:34.6615 | CLSLoss:0.0000 | AUROC:0.9775\n",
      "\n",
      "Epoch: [460 | 1000] LR: 0.002863\n",
      "Train | 16/16 | Loss:1.2891 | MainLoss:0.3472 | Alpha:0.0272 | SPLoss:34.6189 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1844 | MainLoss:0.1844 | SPLoss:34.5698 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4839 | MainLoss:0.4839 | SPLoss:34.5699 | CLSLoss:0.0000 | AUROC:0.9785\n",
      "\n",
      "Epoch: [461 | 1000] LR: 0.002857\n",
      "Train | 16/16 | Loss:1.2940 | MainLoss:0.3391 | Alpha:0.0277 | SPLoss:34.5311 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1926 | MainLoss:0.1926 | SPLoss:34.4790 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4690 | MainLoss:0.4690 | SPLoss:34.4791 | CLSLoss:0.0000 | AUROC:0.9784\n",
      "\n",
      "Epoch: [462 | 1000] LR: 0.002852\n",
      "Train | 16/16 | Loss:1.2985 | MainLoss:0.3458 | Alpha:0.0277 | SPLoss:34.4371 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1776 | MainLoss:0.1776 | SPLoss:34.3888 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4965 | MainLoss:0.4965 | SPLoss:34.3889 | CLSLoss:0.0000 | AUROC:0.9780\n",
      "\n",
      "Epoch: [463 | 1000] LR: 0.002846\n",
      "Train | 16/16 | Loss:1.3067 | MainLoss:0.3290 | Alpha:0.0285 | SPLoss:34.3461 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1718 | MainLoss:0.1718 | SPLoss:34.2948 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5087 | MainLoss:0.5087 | SPLoss:34.2948 | CLSLoss:0.0000 | AUROC:0.9788\n",
      "\n",
      "Epoch: [464 | 1000] LR: 0.002840\n",
      "Train | 16/16 | Loss:1.2396 | MainLoss:0.3273 | Alpha:0.0266 | SPLoss:34.2545 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1821 | MainLoss:0.1821 | SPLoss:34.2085 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4880 | MainLoss:0.4880 | SPLoss:34.2085 | CLSLoss:0.0000 | AUROC:0.9789\n",
      "\n",
      "Epoch: [465 | 1000] LR: 0.002834\n",
      "Train | 16/16 | Loss:1.2267 | MainLoss:0.3383 | Alpha:0.0260 | SPLoss:34.1687 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1822 | MainLoss:0.1822 | SPLoss:34.1257 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4863 | MainLoss:0.4863 | SPLoss:34.1257 | CLSLoss:0.0000 | AUROC:0.9791\n",
      "\n",
      "Epoch: [466 | 1000] LR: 0.002829\n",
      "Train | 16/16 | Loss:1.2783 | MainLoss:0.3440 | Alpha:0.0274 | SPLoss:34.0851 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1931 | MainLoss:0.1931 | SPLoss:34.0373 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4673 | MainLoss:0.4673 | SPLoss:34.0373 | CLSLoss:0.0000 | AUROC:0.9789\n",
      "\n",
      "Epoch: [467 | 1000] LR: 0.002823\n",
      "Train | 16/16 | Loss:1.2656 | MainLoss:0.3291 | Alpha:0.0275 | SPLoss:33.9988 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1839 | MainLoss:0.1839 | SPLoss:33.9507 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4870 | MainLoss:0.4870 | SPLoss:33.9507 | CLSLoss:0.0000 | AUROC:0.9780\n",
      "\n",
      "Epoch: [468 | 1000] LR: 0.002817\n",
      "Train | 16/16 | Loss:1.2336 | MainLoss:0.3384 | Alpha:0.0264 | SPLoss:33.9103 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1749 | MainLoss:0.1749 | SPLoss:33.8682 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5074 | MainLoss:0.5074 | SPLoss:33.8682 | CLSLoss:0.0000 | AUROC:0.9772\n",
      "\n",
      "Epoch: [469 | 1000] LR: 0.002812\n",
      "Train | 16/16 | Loss:1.2786 | MainLoss:0.3393 | Alpha:0.0278 | SPLoss:33.8246 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1883 | MainLoss:0.1883 | SPLoss:33.7800 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4805 | MainLoss:0.4805 | SPLoss:33.7801 | CLSLoss:0.0000 | AUROC:0.9775\n",
      "\n",
      "Epoch: [470 | 1000] LR: 0.002806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.2520 | MainLoss:0.3428 | Alpha:0.0269 | SPLoss:33.7392 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1833 | MainLoss:0.1833 | SPLoss:33.6952 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4879 | MainLoss:0.4879 | SPLoss:33.6951 | CLSLoss:0.0000 | AUROC:0.9774\n",
      "\n",
      "Epoch: [471 | 1000] LR: 0.002800\n",
      "Train | 16/16 | Loss:1.2756 | MainLoss:0.3417 | Alpha:0.0277 | SPLoss:33.6579 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1841 | MainLoss:0.1841 | SPLoss:33.6092 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4910 | MainLoss:0.4910 | SPLoss:33.6091 | CLSLoss:0.0000 | AUROC:0.9779\n",
      "\n",
      "Epoch: [472 | 1000] LR: 0.002794\n",
      "Train | 16/16 | Loss:1.2221 | MainLoss:0.3294 | Alpha:0.0266 | SPLoss:33.5712 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1871 | MainLoss:0.1871 | SPLoss:33.5249 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4823 | MainLoss:0.4823 | SPLoss:33.5249 | CLSLoss:0.0000 | AUROC:0.9779\n",
      "\n",
      "Epoch: [473 | 1000] LR: 0.002789\n",
      "Train | 16/16 | Loss:1.2365 | MainLoss:0.3365 | Alpha:0.0269 | SPLoss:33.4846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1974 | MainLoss:0.1974 | SPLoss:33.4423 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4624 | MainLoss:0.4624 | SPLoss:33.4423 | CLSLoss:0.0000 | AUROC:0.9779\n",
      "\n",
      "Epoch: [474 | 1000] LR: 0.002783\n",
      "Train | 16/16 | Loss:1.1895 | MainLoss:0.3390 | Alpha:0.0255 | SPLoss:33.4066 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1750 | MainLoss:0.1750 | SPLoss:33.3663 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5070 | MainLoss:0.5070 | SPLoss:33.3663 | CLSLoss:0.0000 | AUROC:0.9769\n",
      "\n",
      "Epoch: [475 | 1000] LR: 0.002777\n",
      "Train | 16/16 | Loss:1.2711 | MainLoss:0.3346 | Alpha:0.0281 | SPLoss:33.3271 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1942 | MainLoss:0.1942 | SPLoss:33.2787 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4667 | MainLoss:0.4667 | SPLoss:33.2786 | CLSLoss:0.0000 | AUROC:0.9766\n",
      "\n",
      "Epoch: [476 | 1000] LR: 0.002771\n",
      "Train | 16/16 | Loss:1.2211 | MainLoss:0.3361 | Alpha:0.0266 | SPLoss:33.2388 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1775 | MainLoss:0.1775 | SPLoss:33.1975 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4985 | MainLoss:0.4985 | SPLoss:33.1974 | CLSLoss:0.0000 | AUROC:0.9770\n",
      "\n",
      "Epoch: [477 | 1000] LR: 0.002765\n",
      "Train | 16/16 | Loss:1.1846 | MainLoss:0.3289 | Alpha:0.0258 | SPLoss:33.1623 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1858 | MainLoss:0.1858 | SPLoss:33.1188 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4824 | MainLoss:0.4824 | SPLoss:33.1188 | CLSLoss:0.0000 | AUROC:0.9773\n",
      "\n",
      "Epoch: [478 | 1000] LR: 0.002760\n",
      "Train | 16/16 | Loss:1.2042 | MainLoss:0.3371 | Alpha:0.0262 | SPLoss:33.0827 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1719 | MainLoss:0.1719 | SPLoss:33.0401 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5087 | MainLoss:0.5087 | SPLoss:33.0401 | CLSLoss:0.0000 | AUROC:0.9768\n",
      "\n",
      "Epoch: [479 | 1000] LR: 0.002754\n",
      "Train | 16/16 | Loss:1.2068 | MainLoss:0.3330 | Alpha:0.0265 | SPLoss:33.0015 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1696 | MainLoss:0.1696 | SPLoss:32.9612 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5130 | MainLoss:0.5130 | SPLoss:32.9612 | CLSLoss:0.0000 | AUROC:0.9771\n",
      "\n",
      "Epoch: [480 | 1000] LR: 0.002748\n",
      "Train | 16/16 | Loss:1.2266 | MainLoss:0.3353 | Alpha:0.0271 | SPLoss:32.9235 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1767 | MainLoss:0.1767 | SPLoss:32.8798 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4982 | MainLoss:0.4982 | SPLoss:32.8798 | CLSLoss:0.0000 | AUROC:0.9776\n",
      "\n",
      "Epoch: [481 | 1000] LR: 0.002742\n",
      "Train | 16/16 | Loss:1.2450 | MainLoss:0.3385 | Alpha:0.0276 | SPLoss:32.8415 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1713 | MainLoss:0.1713 | SPLoss:32.7983 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.5106 | MainLoss:0.5106 | SPLoss:32.7983 | CLSLoss:0.0000 | AUROC:0.9769\n",
      "\n",
      "Epoch: [482 | 1000] LR: 0.002736\n",
      "Train | 16/16 | Loss:1.2450 | MainLoss:0.3293 | Alpha:0.0280 | SPLoss:32.7603 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1753 | MainLoss:0.1753 | SPLoss:32.7141 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5051 | MainLoss:0.5051 | SPLoss:32.7141 | CLSLoss:0.0000 | AUROC:0.9772\n",
      "\n",
      "Epoch: [483 | 1000] LR: 0.002730\n",
      "Train | 16/16 | Loss:1.2159 | MainLoss:0.3245 | Alpha:0.0273 | SPLoss:32.6744 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1870 | MainLoss:0.1870 | SPLoss:32.6324 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4851 | MainLoss:0.4851 | SPLoss:32.6324 | CLSLoss:0.0000 | AUROC:0.9770\n",
      "\n",
      "Epoch: [484 | 1000] LR: 0.002725\n",
      "Train | 16/16 | Loss:1.2275 | MainLoss:0.3283 | Alpha:0.0276 | SPLoss:32.5916 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1774 | MainLoss:0.1774 | SPLoss:32.5516 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5018 | MainLoss:0.5018 | SPLoss:32.5516 | CLSLoss:0.0000 | AUROC:0.9769\n",
      "\n",
      "Epoch: [485 | 1000] LR: 0.002719\n",
      "Train | 16/16 | Loss:1.1791 | MainLoss:0.3203 | Alpha:0.0264 | SPLoss:32.5144 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1862 | MainLoss:0.1862 | SPLoss:32.4742 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4850 | MainLoss:0.4850 | SPLoss:32.4742 | CLSLoss:0.0000 | AUROC:0.9764\n",
      "\n",
      "Epoch: [486 | 1000] LR: 0.002713\n",
      "Train | 16/16 | Loss:1.2302 | MainLoss:0.3247 | Alpha:0.0279 | SPLoss:32.4356 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1785 | MainLoss:0.1785 | SPLoss:32.3939 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.5023 | MainLoss:0.5023 | SPLoss:32.3939 | CLSLoss:0.0000 | AUROC:0.9761\n",
      "\n",
      "Epoch: [487 | 1000] LR: 0.002707\n",
      "Train | 16/16 | Loss:1.2041 | MainLoss:0.3307 | Alpha:0.0270 | SPLoss:32.3566 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1813 | MainLoss:0.1813 | SPLoss:32.3165 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4958 | MainLoss:0.4958 | SPLoss:32.3165 | CLSLoss:0.0000 | AUROC:0.9754\n",
      "\n",
      "Epoch: [488 | 1000] LR: 0.002701\n",
      "Train | 16/16 | Loss:1.1926 | MainLoss:0.3339 | Alpha:0.0266 | SPLoss:32.2794 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1824 | MainLoss:0.1824 | SPLoss:32.2397 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4957 | MainLoss:0.4957 | SPLoss:32.2397 | CLSLoss:0.0000 | AUROC:0.9760\n",
      "\n",
      "Epoch: [489 | 1000] LR: 0.002695\n",
      "Train | 16/16 | Loss:1.1543 | MainLoss:0.3249 | Alpha:0.0258 | SPLoss:32.2057 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1828 | MainLoss:0.1828 | SPLoss:32.1663 | CLSLoss:0.0000 | AUROC:0.9983\n",
      "Test | 31/16 | Loss:0.4941 | MainLoss:0.4941 | SPLoss:32.1663 | CLSLoss:0.0000 | AUROC:0.9753\n",
      "\n",
      "Epoch: [490 | 1000] LR: 0.002689\n",
      "Train | 16/16 | Loss:1.2001 | MainLoss:0.3352 | Alpha:0.0269 | SPLoss:32.1294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1703 | MainLoss:0.1703 | SPLoss:32.0923 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5175 | MainLoss:0.5175 | SPLoss:32.0923 | CLSLoss:0.0000 | AUROC:0.9747\n",
      "\n",
      "Epoch: [491 | 1000] LR: 0.002683\n",
      "Train | 16/16 | Loss:1.2264 | MainLoss:0.3331 | Alpha:0.0279 | SPLoss:32.0549 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1899 | MainLoss:0.1899 | SPLoss:32.0117 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4764 | MainLoss:0.4764 | SPLoss:32.0117 | CLSLoss:0.0000 | AUROC:0.9756\n",
      "\n",
      "Epoch: [492 | 1000] LR: 0.002677\n",
      "Train | 16/16 | Loss:1.1536 | MainLoss:0.3359 | Alpha:0.0256 | SPLoss:31.9790 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1857 | MainLoss:0.1857 | SPLoss:31.9400 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4883 | MainLoss:0.4883 | SPLoss:31.9400 | CLSLoss:0.0000 | AUROC:0.9753\n",
      "\n",
      "Epoch: [493 | 1000] LR: 0.002672\n",
      "Train | 16/16 | Loss:1.1595 | MainLoss:0.3333 | Alpha:0.0259 | SPLoss:31.9068 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1814 | MainLoss:0.1814 | SPLoss:31.8679 | CLSLoss:0.0000 | AUROC:0.9985\n",
      "Test | 31/16 | Loss:0.4929 | MainLoss:0.4929 | SPLoss:31.8679 | CLSLoss:0.0000 | AUROC:0.9751\n",
      "\n",
      "Epoch: [494 | 1000] LR: 0.002666\n",
      "Train | 16/16 | Loss:1.2163 | MainLoss:0.3468 | Alpha:0.0273 | SPLoss:31.8321 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1765 | MainLoss:0.1765 | SPLoss:31.7914 | CLSLoss:0.0000 | AUROC:0.9985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 31/16 | Loss:0.4994 | MainLoss:0.4994 | SPLoss:31.7914 | CLSLoss:0.0000 | AUROC:0.9746\n",
      "\n",
      "Epoch: [495 | 1000] LR: 0.002660\n",
      "Train | 16/16 | Loss:1.2028 | MainLoss:0.3390 | Alpha:0.0272 | SPLoss:31.7537 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:31.7150 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5130 | MainLoss:0.5130 | SPLoss:31.7151 | CLSLoss:0.0000 | AUROC:0.9760\n",
      "\n",
      "Epoch: [496 | 1000] LR: 0.002654\n",
      "Train | 16/16 | Loss:1.2185 | MainLoss:0.3286 | Alpha:0.0281 | SPLoss:31.6791 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1821 | MainLoss:0.1821 | SPLoss:31.6360 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4908 | MainLoss:0.4908 | SPLoss:31.6360 | CLSLoss:0.0000 | AUROC:0.9763\n",
      "\n",
      "Epoch: [497 | 1000] LR: 0.002648\n",
      "Train | 16/16 | Loss:1.1938 | MainLoss:0.3292 | Alpha:0.0274 | SPLoss:31.6027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1831 | MainLoss:0.1831 | SPLoss:31.5613 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4888 | MainLoss:0.4888 | SPLoss:31.5613 | CLSLoss:0.0000 | AUROC:0.9758\n",
      "\n",
      "Epoch: [498 | 1000] LR: 0.002642\n",
      "Train | 16/16 | Loss:1.1858 | MainLoss:0.3297 | Alpha:0.0272 | SPLoss:31.5270 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1820 | MainLoss:0.1820 | SPLoss:31.4853 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4890 | MainLoss:0.4890 | SPLoss:31.4853 | CLSLoss:0.0000 | AUROC:0.9765\n",
      "\n",
      "Epoch: [499 | 1000] LR: 0.002636\n",
      "Train | 16/16 | Loss:1.1815 | MainLoss:0.3327 | Alpha:0.0270 | SPLoss:31.4516 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1925 | MainLoss:0.1925 | SPLoss:31.4105 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4693 | MainLoss:0.4693 | SPLoss:31.4105 | CLSLoss:0.0000 | AUROC:0.9763\n",
      "\n",
      "Epoch: [500 | 1000] LR: 0.002630\n",
      "Train | 16/16 | Loss:1.1950 | MainLoss:0.3257 | Alpha:0.0277 | SPLoss:31.3757 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1785 | MainLoss:0.1785 | SPLoss:31.3350 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4967 | MainLoss:0.4967 | SPLoss:31.3351 | CLSLoss:0.0000 | AUROC:0.9764\n",
      "\n",
      "Epoch: [501 | 1000] LR: 0.002624\n",
      "Train | 16/16 | Loss:1.2035 | MainLoss:0.3333 | Alpha:0.0278 | SPLoss:31.2980 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1844 | MainLoss:0.1844 | SPLoss:31.2587 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.4834 | MainLoss:0.4834 | SPLoss:31.2587 | CLSLoss:0.0000 | AUROC:0.9767\n",
      "\n",
      "Epoch: [502 | 1000] LR: 0.002618\n",
      "Train | 16/16 | Loss:1.2220 | MainLoss:0.3430 | Alpha:0.0282 | SPLoss:31.2234 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1747 | MainLoss:0.1747 | SPLoss:31.1839 | CLSLoss:0.0000 | AUROC:0.9984\n",
      "Test | 31/16 | Loss:0.5028 | MainLoss:0.5028 | SPLoss:31.1839 | CLSLoss:0.0000 | AUROC:0.9761\n",
      "\n",
      "Epoch: [503 | 1000] LR: 0.002612\n",
      "Train | 16/16 | Loss:1.1898 | MainLoss:0.3399 | Alpha:0.0273 | SPLoss:31.1484 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1777 | MainLoss:0.1777 | SPLoss:31.1105 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "Test | 31/16 | Loss:0.4982 | MainLoss:0.4982 | SPLoss:31.1105 | CLSLoss:0.0000 | AUROC:0.9763\n",
      "\n",
      "Epoch: [504 | 1000] LR: 0.002606\n",
      "Train | 16/16 | Loss:1.2049 | MainLoss:0.3287 | Alpha:0.0282 | SPLoss:31.0769 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch+1)%900 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
