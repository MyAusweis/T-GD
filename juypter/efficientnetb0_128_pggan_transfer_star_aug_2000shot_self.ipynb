{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained =  './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.01\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_star/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'star/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.010000\n",
      "Train | 20/20 | Loss:0.9727 | MainLoss:0.8802 | Alpha:0.0485 | SPLoss:0.0464 | CLSLoss:1.8591 | top1:56.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6773 | MainLoss:0.6773 | SPLoss:0.0682 | CLSLoss:1.8251 | top1:57.9522 | AUROC:0.6135\n",
      "Test | 161/20 | Loss:0.0421 | MainLoss:0.0421 | SPLoss:0.0682 | CLSLoss:1.8251 | top1:99.9065 | AUROC:1.0000\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.013000\n",
      "Train | 20/20 | Loss:1.2398 | MainLoss:0.6822 | Alpha:0.3346 | SPLoss:0.0008 | CLSLoss:1.6659 | top1:57.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6655 | MainLoss:0.6655 | SPLoss:0.0018 | CLSLoss:1.5008 | top1:59.4430 | AUROC:0.6331\n",
      "Test | 161/20 | Loss:0.0743 | MainLoss:0.0743 | SPLoss:0.0018 | CLSLoss:1.5008 | top1:99.8910 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.016000\n",
      "Train | 20/20 | Loss:1.1224 | MainLoss:0.6677 | Alpha:0.3386 | SPLoss:0.0006 | CLSLoss:1.3419 | top1:58.5500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6577 | MainLoss:0.6577 | SPLoss:0.0017 | CLSLoss:1.1800 | top1:60.8683 | AUROC:0.6544\n",
      "Test | 161/20 | Loss:0.0822 | MainLoss:0.0822 | SPLoss:0.0017 | CLSLoss:1.1800 | top1:99.8723 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.019000\n",
      "Train | 20/20 | Loss:1.0147 | MainLoss:0.6632 | Alpha:0.3395 | SPLoss:0.0009 | CLSLoss:1.0343 | top1:59.6000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6466 | MainLoss:0.6466 | SPLoss:0.0026 | CLSLoss:0.8878 | top1:62.6999 | AUROC:0.6774\n",
      "Test | 161/20 | Loss:0.0815 | MainLoss:0.0815 | SPLoss:0.0026 | CLSLoss:0.8878 | top1:99.8318 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.022000\n",
      "Train | 20/20 | Loss:0.9084 | MainLoss:0.6463 | Alpha:0.3424 | SPLoss:0.0020 | CLSLoss:0.7635 | top1:62.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6303 | MainLoss:0.6303 | SPLoss:0.0048 | CLSLoss:0.6395 | top1:64.2857 | AUROC:0.7056\n",
      "Test | 161/20 | Loss:0.0794 | MainLoss:0.0794 | SPLoss:0.0048 | CLSLoss:0.6395 | top1:99.6916 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.025000\n",
      "Train | 20/20 | Loss:0.8181 | MainLoss:0.6304 | Alpha:0.3457 | SPLoss:0.0029 | CLSLoss:0.5398 | top1:64.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.6030 | MainLoss:0.6030 | SPLoss:0.0074 | CLSLoss:0.4415 | top1:66.8644 | AUROC:0.7379\n",
      "Test | 161/20 | Loss:0.0734 | MainLoss:0.0734 | SPLoss:0.0074 | CLSLoss:0.4415 | top1:99.5763 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.028000\n",
      "Train | 20/20 | Loss:0.7307 | MainLoss:0.6000 | Alpha:0.3526 | SPLoss:0.0042 | CLSLoss:0.3665 | top1:67.4000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5731 | MainLoss:0.5731 | SPLoss:0.0107 | CLSLoss:0.2938 | top1:69.6461 | AUROC:0.7692\n",
      "Test | 161/20 | Loss:0.0749 | MainLoss:0.0749 | SPLoss:0.0107 | CLSLoss:0.2938 | top1:99.3707 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.031000\n",
      "Train | 20/20 | Loss:0.6675 | MainLoss:0.5796 | Alpha:0.3587 | SPLoss:0.0055 | CLSLoss:0.2396 | top1:69.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5616 | MainLoss:0.5616 | SPLoss:0.0132 | CLSLoss:0.1830 | top1:71.0321 | AUROC:0.7857\n",
      "Test | 161/20 | Loss:0.1147 | MainLoss:0.1147 | SPLoss:0.0132 | CLSLoss:0.1830 | top1:98.7321 | AUROC:1.0000\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.034000\n",
      "Train | 20/20 | Loss:0.6388 | MainLoss:0.5834 | Alpha:0.3582 | SPLoss:0.0058 | CLSLoss:0.1488 | top1:69.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5398 | MainLoss:0.5398 | SPLoss:0.0103 | CLSLoss:0.1121 | top1:73.1291 | AUROC:0.8103\n",
      "Test | 161/20 | Loss:0.1478 | MainLoss:0.1478 | SPLoss:0.0103 | CLSLoss:0.1121 | top1:98.5140 | AUROC:0.9999\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.037000\n",
      "Train | 20/20 | Loss:0.5852 | MainLoss:0.5469 | Alpha:0.3653 | SPLoss:0.0067 | CLSLoss:0.0979 | top1:72.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5111 | MainLoss:0.5111 | SPLoss:0.0140 | CLSLoss:0.0824 | top1:74.9967 | AUROC:0.8319\n",
      "Test | 161/20 | Loss:0.1592 | MainLoss:0.1592 | SPLoss:0.0140 | CLSLoss:0.0824 | top1:98.2025 | AUROC:0.9999\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5626 | MainLoss:0.5335 | Alpha:0.3711 | SPLoss:0.0061 | CLSLoss:0.0722 | top1:73.6000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4950 | MainLoss:0.4950 | SPLoss:0.0133 | CLSLoss:0.0643 | top1:76.3041 | AUROC:0.8472\n",
      "Test | 161/20 | Loss:0.1900 | MainLoss:0.1900 | SPLoss:0.0133 | CLSLoss:0.0643 | top1:97.1215 | AUROC:0.9997\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5457 | MainLoss:0.5211 | Alpha:0.3753 | SPLoss:0.0057 | CLSLoss:0.0600 | top1:74.3500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4865 | MainLoss:0.4865 | SPLoss:0.0115 | CLSLoss:0.0596 | top1:76.9823 | AUROC:0.8553\n",
      "Test | 161/20 | Loss:0.2015 | MainLoss:0.2015 | SPLoss:0.0115 | CLSLoss:0.0596 | top1:96.1558 | AUROC:0.9997\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5227 | MainLoss:0.4989 | Alpha:0.3806 | SPLoss:0.0060 | CLSLoss:0.0567 | top1:76.5750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.5168 | MainLoss:0.5168 | SPLoss:0.0127 | CLSLoss:0.0611 | top1:75.2556 | AUROC:0.8497\n",
      "Test | 161/20 | Loss:0.1666 | MainLoss:0.1666 | SPLoss:0.0127 | CLSLoss:0.0611 | top1:97.7882 | AUROC:0.9995\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.040000\n",
      "Train | 20/20 | Loss:0.5301 | MainLoss:0.5064 | Alpha:0.3740 | SPLoss:0.0084 | CLSLoss:0.0550 | top1:76.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4688 | MainLoss:0.4688 | SPLoss:0.0139 | CLSLoss:0.0481 | top1:78.6894 | AUROC:0.8724\n",
      "Test | 161/20 | Loss:0.2438 | MainLoss:0.2438 | SPLoss:0.0139 | CLSLoss:0.0481 | top1:94.0841 | AUROC:0.9994\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.039999\n",
      "Train | 20/20 | Loss:0.5161 | MainLoss:0.4815 | Alpha:0.3835 | SPLoss:0.0363 | CLSLoss:0.0540 | top1:77.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4753 | MainLoss:0.4753 | SPLoss:0.0439 | CLSLoss:0.0511 | top1:77.8571 | AUROC:0.8701\n",
      "Test | 161/20 | Loss:0.2552 | MainLoss:0.2552 | SPLoss:0.0439 | CLSLoss:0.0511 | top1:92.4611 | AUROC:0.9993\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.039998\n",
      "Train | 20/20 | Loss:0.4991 | MainLoss:0.4751 | Alpha:0.3819 | SPLoss:0.0070 | CLSLoss:0.0558 | top1:77.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4464 | MainLoss:0.4464 | SPLoss:0.0119 | CLSLoss:0.0527 | top1:79.7936 | AUROC:0.8826\n",
      "Test | 161/20 | Loss:0.2276 | MainLoss:0.2276 | SPLoss:0.0119 | CLSLoss:0.0527 | top1:94.2150 | AUROC:0.9991\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.039998\n",
      "Train | 20/20 | Loss:0.4919 | MainLoss:0.4676 | Alpha:0.3890 | SPLoss:0.0063 | CLSLoss:0.0559 | top1:78.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4431 | MainLoss:0.4431 | SPLoss:0.0117 | CLSLoss:0.0573 | top1:79.8198 | AUROC:0.8873\n",
      "Test | 161/20 | Loss:0.1961 | MainLoss:0.1961 | SPLoss:0.0117 | CLSLoss:0.0573 | top1:95.5483 | AUROC:0.9989\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.039996\n",
      "Train | 20/20 | Loss:0.4626 | MainLoss:0.4357 | Alpha:0.3950 | SPLoss:0.0072 | CLSLoss:0.0609 | top1:80.6000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4334 | MainLoss:0.4334 | SPLoss:0.0134 | CLSLoss:0.0607 | top1:80.1769 | AUROC:0.8899\n",
      "Test | 161/20 | Loss:0.2324 | MainLoss:0.2324 | SPLoss:0.0134 | CLSLoss:0.0607 | top1:92.2804 | AUROC:0.9987\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.039995\n",
      "Train | 20/20 | Loss:0.4767 | MainLoss:0.4508 | Alpha:0.3948 | SPLoss:0.0077 | CLSLoss:0.0578 | top1:79.7500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4429 | MainLoss:0.4429 | SPLoss:0.0140 | CLSLoss:0.0554 | top1:79.6658 | AUROC:0.8917\n",
      "Test | 161/20 | Loss:0.2953 | MainLoss:0.2953 | SPLoss:0.0140 | CLSLoss:0.0554 | top1:88.2866 | AUROC:0.9977\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.039994\n",
      "Train | 20/20 | Loss:0.4573 | MainLoss:0.4314 | Alpha:0.3931 | SPLoss:0.0073 | CLSLoss:0.0585 | top1:80.8000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4170 | MainLoss:0.4170 | SPLoss:0.0133 | CLSLoss:0.0610 | top1:81.3172 | AUROC:0.8976\n",
      "Test | 161/20 | Loss:0.2660 | MainLoss:0.2660 | SPLoss:0.0133 | CLSLoss:0.0610 | top1:89.7259 | AUROC:0.9968\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.039992\n",
      "Train | 20/20 | Loss:0.4734 | MainLoss:0.4478 | Alpha:0.3990 | SPLoss:0.0096 | CLSLoss:0.0544 | top1:79.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4137 | MainLoss:0.4137 | SPLoss:0.0140 | CLSLoss:0.0555 | top1:81.5629 | AUROC:0.8990\n",
      "Test | 161/20 | Loss:0.2514 | MainLoss:0.2514 | SPLoss:0.0140 | CLSLoss:0.0555 | top1:91.0997 | AUROC:0.9976\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.039990\n",
      "Train | 20/20 | Loss:0.4629 | MainLoss:0.4368 | Alpha:0.4003 | SPLoss:0.0083 | CLSLoss:0.0569 | top1:80.2250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4342 | MainLoss:0.4342 | SPLoss:0.0153 | CLSLoss:0.0496 | top1:80.2163 | AUROC:0.9003\n",
      "Test | 161/20 | Loss:0.3200 | MainLoss:0.3200 | SPLoss:0.0153 | CLSLoss:0.0496 | top1:86.8785 | AUROC:0.9969\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.039988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.4357 | MainLoss:0.4097 | Alpha:0.3972 | SPLoss:0.0084 | CLSLoss:0.0571 | top1:82.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4122 | MainLoss:0.4122 | SPLoss:0.0143 | CLSLoss:0.0575 | top1:81.5596 | AUROC:0.9024\n",
      "Test | 161/20 | Loss:0.2674 | MainLoss:0.2674 | SPLoss:0.0143 | CLSLoss:0.0575 | top1:89.6480 | AUROC:0.9974\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.039986\n",
      "Train | 20/20 | Loss:0.4187 | MainLoss:0.3901 | Alpha:0.4028 | SPLoss:0.0085 | CLSLoss:0.0626 | top1:83.3500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4511 | MainLoss:0.4511 | SPLoss:0.0153 | CLSLoss:0.0640 | top1:79.4921 | AUROC:0.9042\n",
      "Test | 161/20 | Loss:0.3316 | MainLoss:0.3316 | SPLoss:0.0153 | CLSLoss:0.0640 | top1:85.4455 | AUROC:0.9964\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.039983\n",
      "Train | 20/20 | Loss:0.4257 | MainLoss:0.3980 | Alpha:0.3970 | SPLoss:0.0092 | CLSLoss:0.0606 | top1:83.3750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4638 | MainLoss:0.4638 | SPLoss:0.0205 | CLSLoss:0.0593 | top1:79.2824 | AUROC:0.9088\n",
      "Test | 161/20 | Loss:0.1776 | MainLoss:0.1776 | SPLoss:0.0205 | CLSLoss:0.0593 | top1:94.9003 | AUROC:0.9963\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.039981\n",
      "Train | 20/20 | Loss:0.4560 | MainLoss:0.4317 | Alpha:0.3875 | SPLoss:0.0108 | CLSLoss:0.0517 | top1:80.5750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3868 | MainLoss:0.3868 | SPLoss:0.0154 | CLSLoss:0.0507 | top1:83.1914 | AUROC:0.9131\n",
      "Test | 161/20 | Loss:0.2654 | MainLoss:0.2654 | SPLoss:0.0154 | CLSLoss:0.0507 | top1:89.7788 | AUROC:0.9961\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.039978\n",
      "Train | 20/20 | Loss:0.4214 | MainLoss:0.3949 | Alpha:0.4112 | SPLoss:0.0085 | CLSLoss:0.0558 | top1:82.7500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4440 | MainLoss:0.4440 | SPLoss:0.0181 | CLSLoss:0.0491 | top1:79.4364 | AUROC:0.9088\n",
      "Test | 161/20 | Loss:0.3832 | MainLoss:0.3832 | SPLoss:0.0181 | CLSLoss:0.0491 | top1:81.9159 | AUROC:0.9950\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.039975\n",
      "Train | 20/20 | Loss:0.4255 | MainLoss:0.4006 | Alpha:0.3960 | SPLoss:0.0094 | CLSLoss:0.0536 | top1:82.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.4124 | MainLoss:0.4124 | SPLoss:0.0134 | CLSLoss:0.0547 | top1:81.4122 | AUROC:0.9123\n",
      "Test | 161/20 | Loss:0.3642 | MainLoss:0.3642 | SPLoss:0.0134 | CLSLoss:0.0547 | top1:83.2555 | AUROC:0.9940\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.039971\n",
      "Train | 20/20 | Loss:0.4102 | MainLoss:0.3835 | Alpha:0.4044 | SPLoss:0.0094 | CLSLoss:0.0566 | top1:83.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3758 | MainLoss:0.3758 | SPLoss:0.0143 | CLSLoss:0.0586 | top1:83.6173 | AUROC:0.9172\n",
      "Test | 161/20 | Loss:0.3010 | MainLoss:0.3010 | SPLoss:0.0143 | CLSLoss:0.0586 | top1:86.8816 | AUROC:0.9944\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.039968\n",
      "Train | 20/20 | Loss:0.8927 | MainLoss:0.3935 | Alpha:0.4140 | SPLoss:1.1513 | CLSLoss:0.0574 | top1:82.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3793 | MainLoss:0.3793 | SPLoss:1.6537 | CLSLoss:0.0507 | top1:83.6435 | AUROC:0.9165\n",
      "Test | 161/20 | Loss:0.3061 | MainLoss:0.3061 | SPLoss:1.6537 | CLSLoss:0.0507 | top1:87.3053 | AUROC:0.9855\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.039964\n",
      "Train | 20/20 | Loss:0.3963 | MainLoss:0.3634 | Alpha:0.4124 | SPLoss:0.0214 | CLSLoss:0.0583 | top1:84.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3751 | MainLoss:0.3751 | SPLoss:0.0277 | CLSLoss:0.0621 | top1:83.8237 | AUROC:0.9170\n",
      "Test | 161/20 | Loss:0.2971 | MainLoss:0.2971 | SPLoss:0.0277 | CLSLoss:0.0621 | top1:87.5919 | AUROC:0.9848\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.039961\n",
      "Train | 20/20 | Loss:0.3963 | MainLoss:0.3676 | Alpha:0.4158 | SPLoss:0.0108 | CLSLoss:0.0582 | top1:84.2250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3814 | MainLoss:0.3814 | SPLoss:0.0183 | CLSLoss:0.0553 | top1:83.3912 | AUROC:0.9197\n",
      "Test | 161/20 | Loss:0.2831 | MainLoss:0.2831 | SPLoss:0.0183 | CLSLoss:0.0553 | top1:88.5576 | AUROC:0.9801\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.039956\n",
      "Train | 20/20 | Loss:0.4012 | MainLoss:0.3739 | Alpha:0.4178 | SPLoss:0.0114 | CLSLoss:0.0539 | top1:84.1000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3661 | MainLoss:0.3661 | SPLoss:0.0165 | CLSLoss:0.0548 | top1:84.0957 | AUROC:0.9213\n",
      "Test | 161/20 | Loss:0.2986 | MainLoss:0.2986 | SPLoss:0.0165 | CLSLoss:0.0548 | top1:87.5888 | AUROC:0.9819\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.039952\n",
      "Train | 20/20 | Loss:0.3850 | MainLoss:0.3567 | Alpha:0.4208 | SPLoss:0.0104 | CLSLoss:0.0568 | top1:84.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3715 | MainLoss:0.3715 | SPLoss:0.0172 | CLSLoss:0.0529 | top1:83.9941 | AUROC:0.9212\n",
      "Test | 161/20 | Loss:0.2982 | MainLoss:0.2982 | SPLoss:0.0172 | CLSLoss:0.0529 | top1:87.6791 | AUROC:0.9781\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.039948\n",
      "Train | 20/20 | Loss:0.3853 | MainLoss:0.3586 | Alpha:0.4187 | SPLoss:0.0097 | CLSLoss:0.0541 | top1:84.9750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3680 | MainLoss:0.3680 | SPLoss:0.0157 | CLSLoss:0.0561 | top1:84.0629 | AUROC:0.9218\n",
      "Test | 161/20 | Loss:0.2962 | MainLoss:0.2962 | SPLoss:0.0157 | CLSLoss:0.0561 | top1:87.5639 | AUROC:0.9797\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.039943\n",
      "Train | 20/20 | Loss:0.3749 | MainLoss:0.3467 | Alpha:0.4231 | SPLoss:0.0108 | CLSLoss:0.0561 | top1:85.7500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3670 | MainLoss:0.3670 | SPLoss:0.0184 | CLSLoss:0.0562 | top1:84.2071 | AUROC:0.9215\n",
      "Test | 161/20 | Loss:0.3021 | MainLoss:0.3021 | SPLoss:0.0184 | CLSLoss:0.0562 | top1:87.2710 | AUROC:0.9811\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.039938\n",
      "Train | 20/20 | Loss:0.3807 | MainLoss:0.3525 | Alpha:0.4238 | SPLoss:0.0126 | CLSLoss:0.0542 | top1:85.1000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3631 | MainLoss:0.3631 | SPLoss:0.0195 | CLSLoss:0.0559 | top1:84.5249 | AUROC:0.9257\n",
      "Test | 161/20 | Loss:0.3146 | MainLoss:0.3146 | SPLoss:0.0195 | CLSLoss:0.0559 | top1:86.4579 | AUROC:0.9812\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.039933\n",
      "Train | 20/20 | Loss:0.3608 | MainLoss:0.3325 | Alpha:0.4264 | SPLoss:0.0120 | CLSLoss:0.0543 | top1:85.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3538 | MainLoss:0.3538 | SPLoss:0.0176 | CLSLoss:0.0564 | top1:84.9378 | AUROC:0.9273\n",
      "Test | 161/20 | Loss:0.3318 | MainLoss:0.3318 | SPLoss:0.0176 | CLSLoss:0.0564 | top1:85.7695 | AUROC:0.9796\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.039928\n",
      "Train | 20/20 | Loss:0.3717 | MainLoss:0.3440 | Alpha:0.4263 | SPLoss:0.0117 | CLSLoss:0.0534 | top1:85.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3600 | MainLoss:0.3600 | SPLoss:0.0223 | CLSLoss:0.0506 | top1:84.4627 | AUROC:0.9261\n",
      "Test | 161/20 | Loss:0.3744 | MainLoss:0.3744 | SPLoss:0.0223 | CLSLoss:0.0506 | top1:83.1963 | AUROC:0.9766\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.039923\n",
      "Train | 20/20 | Loss:0.3838 | MainLoss:0.3565 | Alpha:0.4241 | SPLoss:0.0119 | CLSLoss:0.0526 | top1:84.5750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3627 | MainLoss:0.3627 | SPLoss:0.0214 | CLSLoss:0.0399 | top1:84.7510 | AUROC:0.9263\n",
      "Test | 161/20 | Loss:0.3609 | MainLoss:0.3609 | SPLoss:0.0214 | CLSLoss:0.0399 | top1:83.4611 | AUROC:0.9747\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.039917\n",
      "Train | 20/20 | Loss:0.3371 | MainLoss:0.3099 | Alpha:0.4227 | SPLoss:0.0104 | CLSLoss:0.0540 | top1:87.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3562 | MainLoss:0.3562 | SPLoss:0.0185 | CLSLoss:0.0610 | top1:85.0164 | AUROC:0.9281\n",
      "Test | 161/20 | Loss:0.3699 | MainLoss:0.3699 | SPLoss:0.0185 | CLSLoss:0.0610 | top1:83.9221 | AUROC:0.9688\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.039911\n",
      "Train | 20/20 | Loss:0.3592 | MainLoss:0.3307 | Alpha:0.4300 | SPLoss:0.0121 | CLSLoss:0.0542 | top1:86.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3552 | MainLoss:0.3552 | SPLoss:0.0183 | CLSLoss:0.0550 | top1:85.0393 | AUROC:0.9278\n",
      "Test | 161/20 | Loss:0.3385 | MainLoss:0.3385 | SPLoss:0.0183 | CLSLoss:0.0550 | top1:85.4922 | AUROC:0.9671\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.039905\n",
      "Train | 20/20 | Loss:0.3494 | MainLoss:0.3208 | Alpha:0.4330 | SPLoss:0.0131 | CLSLoss:0.0529 | top1:86.8000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3480 | MainLoss:0.3480 | SPLoss:0.0206 | CLSLoss:0.0519 | top1:85.2589 | AUROC:0.9292\n",
      "Test | 161/20 | Loss:0.3946 | MainLoss:0.3946 | SPLoss:0.0206 | CLSLoss:0.0519 | top1:82.5109 | AUROC:0.9647\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.039899\n",
      "Train | 20/20 | Loss:0.3498 | MainLoss:0.3210 | Alpha:0.4329 | SPLoss:0.0140 | CLSLoss:0.0525 | top1:86.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3533 | MainLoss:0.3533 | SPLoss:0.0204 | CLSLoss:0.0521 | top1:84.8853 | AUROC:0.9286\n",
      "Test | 161/20 | Loss:0.4439 | MainLoss:0.4439 | SPLoss:0.0204 | CLSLoss:0.0521 | top1:79.9190 | AUROC:0.9495\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.039893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.3415 | MainLoss:0.2995 | Alpha:0.4317 | SPLoss:0.0407 | CLSLoss:0.0561 | top1:88.1250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3510 | MainLoss:0.3510 | SPLoss:0.1267 | CLSLoss:0.0466 | top1:84.9541 | AUROC:0.9268\n",
      "Test | 161/20 | Loss:0.3947 | MainLoss:0.3947 | SPLoss:0.1267 | CLSLoss:0.0466 | top1:81.7539 | AUROC:0.9510\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.039886\n",
      "Train | 20/20 | Loss:0.3439 | MainLoss:0.3163 | Alpha:0.4275 | SPLoss:0.0125 | CLSLoss:0.0521 | top1:86.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3781 | MainLoss:0.3781 | SPLoss:0.0233 | CLSLoss:0.0491 | top1:83.4731 | AUROC:0.9291\n",
      "Test | 161/20 | Loss:0.4599 | MainLoss:0.4599 | SPLoss:0.0233 | CLSLoss:0.0491 | top1:78.7570 | AUROC:0.9584\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.039879\n",
      "Train | 20/20 | Loss:0.3356 | MainLoss:0.3078 | Alpha:0.4206 | SPLoss:0.0132 | CLSLoss:0.0528 | top1:87.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3454 | MainLoss:0.3454 | SPLoss:0.0219 | CLSLoss:0.0560 | top1:85.3211 | AUROC:0.9302\n",
      "Test | 161/20 | Loss:0.3920 | MainLoss:0.3920 | SPLoss:0.0219 | CLSLoss:0.0560 | top1:82.8879 | AUROC:0.9498\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.039872\n",
      "Train | 20/20 | Loss:0.3403 | MainLoss:0.3108 | Alpha:0.4329 | SPLoss:0.0132 | CLSLoss:0.0550 | top1:87.1500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3902 | MainLoss:0.3902 | SPLoss:0.0263 | CLSLoss:0.0478 | top1:82.9784 | AUROC:0.9319\n",
      "Test | 161/20 | Loss:0.5304 | MainLoss:0.5304 | SPLoss:0.0263 | CLSLoss:0.0478 | top1:75.3146 | AUROC:0.9574\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.039865\n",
      "Train | 20/20 | Loss:0.3289 | MainLoss:0.3009 | Alpha:0.4197 | SPLoss:0.0136 | CLSLoss:0.0529 | top1:88.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3482 | MainLoss:0.3482 | SPLoss:0.0218 | CLSLoss:0.0562 | top1:85.4227 | AUROC:0.9333\n",
      "Test | 161/20 | Loss:0.4656 | MainLoss:0.4656 | SPLoss:0.0218 | CLSLoss:0.0562 | top1:79.6262 | AUROC:0.9586\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.039858\n",
      "Train | 20/20 | Loss:0.3201 | MainLoss:0.2904 | Alpha:0.4359 | SPLoss:0.0142 | CLSLoss:0.0540 | top1:88.4000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3474 | MainLoss:0.3474 | SPLoss:0.0222 | CLSLoss:0.0529 | top1:85.5079 | AUROC:0.9329\n",
      "Test | 161/20 | Loss:0.4874 | MainLoss:0.4874 | SPLoss:0.0222 | CLSLoss:0.0529 | top1:79.2243 | AUROC:0.9462\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.039850\n",
      "Train | 20/20 | Loss:0.3330 | MainLoss:0.3043 | Alpha:0.4343 | SPLoss:0.0147 | CLSLoss:0.0513 | top1:87.3750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3437 | MainLoss:0.3437 | SPLoss:0.0220 | CLSLoss:0.0506 | top1:85.3899 | AUROC:0.9336\n",
      "Test | 161/20 | Loss:0.4734 | MainLoss:0.4734 | SPLoss:0.0220 | CLSLoss:0.0506 | top1:79.0997 | AUROC:0.9446\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.003984\n",
      "Train | 20/20 | Loss:0.2759 | MainLoss:0.2530 | Alpha:0.4375 | SPLoss:0.0003 | CLSLoss:0.0519 | top1:90.3750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3362 | MainLoss:0.3362 | SPLoss:0.0006 | CLSLoss:0.0534 | top1:85.8650 | AUROC:0.9344\n",
      "Test | 161/20 | Loss:0.4258 | MainLoss:0.4258 | SPLoss:0.0006 | CLSLoss:0.0534 | top1:81.8318 | AUROC:0.9498\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.003983\n",
      "Train | 20/20 | Loss:0.2885 | MainLoss:0.2649 | Alpha:0.4381 | SPLoss:0.0002 | CLSLoss:0.0537 | top1:90.4000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3365 | MainLoss:0.3365 | SPLoss:0.0004 | CLSLoss:0.0540 | top1:86.0256 | AUROC:0.9357\n",
      "Test | 161/20 | Loss:0.4230 | MainLoss:0.4230 | SPLoss:0.0004 | CLSLoss:0.0540 | top1:82.1495 | AUROC:0.9496\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.003983\n",
      "Train | 20/20 | Loss:0.2896 | MainLoss:0.2658 | Alpha:0.4382 | SPLoss:0.0002 | CLSLoss:0.0541 | top1:89.4000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3354 | MainLoss:0.3354 | SPLoss:0.0005 | CLSLoss:0.0539 | top1:85.8650 | AUROC:0.9357\n",
      "Test | 161/20 | Loss:0.4456 | MainLoss:0.4456 | SPLoss:0.0005 | CLSLoss:0.0539 | top1:81.1215 | AUROC:0.9503\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.003982\n",
      "Train | 20/20 | Loss:0.2757 | MainLoss:0.2516 | Alpha:0.4408 | SPLoss:0.0002 | CLSLoss:0.0544 | top1:90.3500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3359 | MainLoss:0.3359 | SPLoss:0.0004 | CLSLoss:0.0553 | top1:86.1009 | AUROC:0.9360\n",
      "Test | 161/20 | Loss:0.4261 | MainLoss:0.4261 | SPLoss:0.0004 | CLSLoss:0.0553 | top1:82.2617 | AUROC:0.9522\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.003981\n",
      "Train | 20/20 | Loss:0.2676 | MainLoss:0.2430 | Alpha:0.4442 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:90.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3385 | MainLoss:0.3385 | SPLoss:0.0005 | CLSLoss:0.0564 | top1:86.0092 | AUROC:0.9363\n",
      "Test | 161/20 | Loss:0.4538 | MainLoss:0.4538 | SPLoss:0.0005 | CLSLoss:0.0564 | top1:81.1184 | AUROC:0.9521\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.003980\n",
      "Train | 20/20 | Loss:0.2612 | MainLoss:0.2359 | Alpha:0.4452 | SPLoss:0.0002 | CLSLoss:0.0567 | top1:91.7000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3395 | MainLoss:0.3395 | SPLoss:0.0004 | CLSLoss:0.0572 | top1:86.0125 | AUROC:0.9357\n",
      "Test | 161/20 | Loss:0.4433 | MainLoss:0.4433 | SPLoss:0.0004 | CLSLoss:0.0572 | top1:81.6885 | AUROC:0.9542\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.003979\n",
      "Train | 20/20 | Loss:0.2483 | MainLoss:0.2224 | Alpha:0.4488 | SPLoss:0.0002 | CLSLoss:0.0575 | top1:91.8250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3409 | MainLoss:0.3409 | SPLoss:0.0004 | CLSLoss:0.0583 | top1:86.0256 | AUROC:0.9361\n",
      "Test | 161/20 | Loss:0.4491 | MainLoss:0.4491 | SPLoss:0.0004 | CLSLoss:0.0583 | top1:81.6885 | AUROC:0.9545\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.003978\n",
      "Train | 20/20 | Loss:0.2667 | MainLoss:0.2408 | Alpha:0.4435 | SPLoss:0.0002 | CLSLoss:0.0582 | top1:90.9500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3390 | MainLoss:0.3390 | SPLoss:0.0005 | CLSLoss:0.0580 | top1:86.1501 | AUROC:0.9365\n",
      "Test | 161/20 | Loss:0.4264 | MainLoss:0.4264 | SPLoss:0.0005 | CLSLoss:0.0580 | top1:82.7165 | AUROC:0.9574\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.003977\n",
      "Train | 20/20 | Loss:0.2491 | MainLoss:0.2229 | Alpha:0.4488 | SPLoss:0.0002 | CLSLoss:0.0583 | top1:91.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3433 | MainLoss:0.3433 | SPLoss:0.0005 | CLSLoss:0.0585 | top1:86.0518 | AUROC:0.9364\n",
      "Test | 161/20 | Loss:0.4067 | MainLoss:0.4067 | SPLoss:0.0005 | CLSLoss:0.0585 | top1:83.5888 | AUROC:0.9575\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.003976\n",
      "Train | 20/20 | Loss:0.2765 | MainLoss:0.2507 | Alpha:0.4415 | SPLoss:0.0003 | CLSLoss:0.0581 | top1:90.2250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3398 | MainLoss:0.3398 | SPLoss:0.0005 | CLSLoss:0.0573 | top1:86.0878 | AUROC:0.9367\n",
      "Test | 161/20 | Loss:0.4159 | MainLoss:0.4159 | SPLoss:0.0005 | CLSLoss:0.0573 | top1:83.0779 | AUROC:0.9562\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.003975\n",
      "Train | 20/20 | Loss:0.2546 | MainLoss:0.2287 | Alpha:0.4461 | SPLoss:0.0003 | CLSLoss:0.0578 | top1:91.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3394 | MainLoss:0.3394 | SPLoss:0.0005 | CLSLoss:0.0577 | top1:86.2025 | AUROC:0.9369\n",
      "Test | 161/20 | Loss:0.4527 | MainLoss:0.4527 | SPLoss:0.0005 | CLSLoss:0.0577 | top1:81.7788 | AUROC:0.9557\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.003974\n",
      "Train | 20/20 | Loss:0.2813 | MainLoss:0.2561 | Alpha:0.4405 | SPLoss:0.0002 | CLSLoss:0.0570 | top1:90.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3366 | MainLoss:0.3366 | SPLoss:0.0004 | CLSLoss:0.0563 | top1:86.1861 | AUROC:0.9369\n",
      "Test | 161/20 | Loss:0.4384 | MainLoss:0.4384 | SPLoss:0.0004 | CLSLoss:0.0563 | top1:82.0374 | AUROC:0.9557\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.003973\n",
      "Train | 20/20 | Loss:0.2718 | MainLoss:0.2469 | Alpha:0.4429 | SPLoss:0.0002 | CLSLoss:0.0560 | top1:90.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3362 | MainLoss:0.3362 | SPLoss:0.0004 | CLSLoss:0.0560 | top1:86.2647 | AUROC:0.9369\n",
      "Test | 161/20 | Loss:0.4431 | MainLoss:0.4431 | SPLoss:0.0004 | CLSLoss:0.0560 | top1:81.9969 | AUROC:0.9534\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.003972\n",
      "Train | 20/20 | Loss:0.2558 | MainLoss:0.2305 | Alpha:0.4479 | SPLoss:0.0002 | CLSLoss:0.0563 | top1:91.2750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3374 | MainLoss:0.3374 | SPLoss:0.0004 | CLSLoss:0.0567 | top1:86.2451 | AUROC:0.9370\n",
      "Test | 161/20 | Loss:0.4426 | MainLoss:0.4426 | SPLoss:0.0004 | CLSLoss:0.0567 | top1:82.0935 | AUROC:0.9532\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.003971\n",
      "Train | 20/20 | Loss:0.2702 | MainLoss:0.2449 | Alpha:0.4439 | SPLoss:0.0002 | CLSLoss:0.0567 | top1:90.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3377 | MainLoss:0.3377 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:86.1763 | AUROC:0.9368\n",
      "Test | 161/20 | Loss:0.4382 | MainLoss:0.4382 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:82.1776 | AUROC:0.9518\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.003970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.2509 | MainLoss:0.2256 | Alpha:0.4484 | SPLoss:0.0002 | CLSLoss:0.0562 | top1:91.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3392 | MainLoss:0.3392 | SPLoss:0.0005 | CLSLoss:0.0567 | top1:86.2549 | AUROC:0.9370\n",
      "Test | 161/20 | Loss:0.4556 | MainLoss:0.4556 | SPLoss:0.0005 | CLSLoss:0.0567 | top1:81.6916 | AUROC:0.9524\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.003969\n",
      "Train | 20/20 | Loss:0.2701 | MainLoss:0.2450 | Alpha:0.4439 | SPLoss:0.0002 | CLSLoss:0.0564 | top1:90.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3373 | MainLoss:0.3373 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:86.3598 | AUROC:0.9373\n",
      "Test | 161/20 | Loss:0.4383 | MainLoss:0.4383 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:82.2181 | AUROC:0.9541\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.003968\n",
      "Train | 20/20 | Loss:0.2674 | MainLoss:0.2427 | Alpha:0.4435 | SPLoss:0.0002 | CLSLoss:0.0554 | top1:90.8250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3374 | MainLoss:0.3374 | SPLoss:0.0005 | CLSLoss:0.0555 | top1:86.3041 | AUROC:0.9370\n",
      "Test | 161/20 | Loss:0.4533 | MainLoss:0.4533 | SPLoss:0.0005 | CLSLoss:0.0555 | top1:81.5639 | AUROC:0.9521\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.003967\n",
      "Train | 20/20 | Loss:0.2555 | MainLoss:0.2305 | Alpha:0.4473 | SPLoss:0.0002 | CLSLoss:0.0557 | top1:90.8000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3371 | MainLoss:0.3371 | SPLoss:0.0004 | CLSLoss:0.0559 | top1:86.2975 | AUROC:0.9373\n",
      "Test | 161/20 | Loss:0.4509 | MainLoss:0.4509 | SPLoss:0.0004 | CLSLoss:0.0559 | top1:81.7259 | AUROC:0.9516\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.003966\n",
      "Train | 20/20 | Loss:0.2545 | MainLoss:0.2294 | Alpha:0.4471 | SPLoss:0.0002 | CLSLoss:0.0560 | top1:91.2750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3366 | MainLoss:0.3366 | SPLoss:0.0004 | CLSLoss:0.0563 | top1:86.3762 | AUROC:0.9374\n",
      "Test | 161/20 | Loss:0.4376 | MainLoss:0.4376 | SPLoss:0.0004 | CLSLoss:0.0563 | top1:82.3302 | AUROC:0.9532\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.003965\n",
      "Train | 20/20 | Loss:0.2571 | MainLoss:0.2319 | Alpha:0.4472 | SPLoss:0.0002 | CLSLoss:0.0562 | top1:91.1250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3364 | MainLoss:0.3364 | SPLoss:0.0004 | CLSLoss:0.0565 | top1:86.3139 | AUROC:0.9373\n",
      "Test | 161/20 | Loss:0.4361 | MainLoss:0.4361 | SPLoss:0.0004 | CLSLoss:0.0565 | top1:82.4081 | AUROC:0.9534\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.003963\n",
      "Train | 20/20 | Loss:0.2565 | MainLoss:0.2310 | Alpha:0.4471 | SPLoss:0.0002 | CLSLoss:0.0568 | top1:91.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3372 | MainLoss:0.3372 | SPLoss:0.0004 | CLSLoss:0.0565 | top1:86.3499 | AUROC:0.9373\n",
      "Test | 161/20 | Loss:0.4624 | MainLoss:0.4624 | SPLoss:0.0004 | CLSLoss:0.0565 | top1:81.3364 | AUROC:0.9521\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.003962\n",
      "Train | 20/20 | Loss:0.2519 | MainLoss:0.2265 | Alpha:0.4491 | SPLoss:0.0002 | CLSLoss:0.0564 | top1:91.5000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3370 | MainLoss:0.3370 | SPLoss:0.0005 | CLSLoss:0.0566 | top1:86.2942 | AUROC:0.9382\n",
      "Test | 161/20 | Loss:0.4247 | MainLoss:0.4247 | SPLoss:0.0005 | CLSLoss:0.0566 | top1:82.8816 | AUROC:0.9520\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.003961\n",
      "Train | 20/20 | Loss:0.2554 | MainLoss:0.2299 | Alpha:0.4466 | SPLoss:0.0002 | CLSLoss:0.0567 | top1:91.5250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3409 | MainLoss:0.3409 | SPLoss:0.0006 | CLSLoss:0.0567 | top1:86.2451 | AUROC:0.9378\n",
      "Test | 161/20 | Loss:0.4957 | MainLoss:0.4957 | SPLoss:0.0006 | CLSLoss:0.0567 | top1:80.1277 | AUROC:0.9491\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.003960\n",
      "Train | 20/20 | Loss:0.2541 | MainLoss:0.2289 | Alpha:0.4462 | SPLoss:0.0003 | CLSLoss:0.0563 | top1:91.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3371 | MainLoss:0.3371 | SPLoss:0.0004 | CLSLoss:0.0567 | top1:86.2779 | AUROC:0.9374\n",
      "Test | 161/20 | Loss:0.4668 | MainLoss:0.4668 | SPLoss:0.0004 | CLSLoss:0.0567 | top1:81.2337 | AUROC:0.9491\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.003958\n",
      "Train | 20/20 | Loss:0.2459 | MainLoss:0.2202 | Alpha:0.4502 | SPLoss:0.0002 | CLSLoss:0.0570 | top1:91.8500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3372 | MainLoss:0.3372 | SPLoss:0.0005 | CLSLoss:0.0568 | top1:86.3106 | AUROC:0.9376\n",
      "Test | 161/20 | Loss:0.4726 | MainLoss:0.4726 | SPLoss:0.0005 | CLSLoss:0.0568 | top1:80.9969 | AUROC:0.9478\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.003957\n",
      "Train | 20/20 | Loss:0.2378 | MainLoss:0.2120 | Alpha:0.4502 | SPLoss:0.0002 | CLSLoss:0.0572 | top1:91.9750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3396 | MainLoss:0.3396 | SPLoss:0.0005 | CLSLoss:0.0576 | top1:86.3041 | AUROC:0.9377\n",
      "Test | 161/20 | Loss:0.4469 | MainLoss:0.4469 | SPLoss:0.0005 | CLSLoss:0.0576 | top1:82.1184 | AUROC:0.9481\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.003956\n",
      "Train | 20/20 | Loss:0.2555 | MainLoss:0.2298 | Alpha:0.4465 | SPLoss:0.0002 | CLSLoss:0.0573 | top1:91.2250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3365 | MainLoss:0.3365 | SPLoss:0.0005 | CLSLoss:0.0567 | top1:86.4548 | AUROC:0.9383\n",
      "Test | 161/20 | Loss:0.4729 | MainLoss:0.4729 | SPLoss:0.0005 | CLSLoss:0.0567 | top1:81.0748 | AUROC:0.9487\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.003955\n",
      "Train | 20/20 | Loss:0.2600 | MainLoss:0.2348 | Alpha:0.4452 | SPLoss:0.0002 | CLSLoss:0.0565 | top1:90.7750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3370 | MainLoss:0.3370 | SPLoss:0.0004 | CLSLoss:0.0564 | top1:86.4581 | AUROC:0.9380\n",
      "Test | 161/20 | Loss:0.4519 | MainLoss:0.4519 | SPLoss:0.0004 | CLSLoss:0.0564 | top1:81.8349 | AUROC:0.9506\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.003953\n",
      "Train | 20/20 | Loss:0.2569 | MainLoss:0.2316 | Alpha:0.4468 | SPLoss:0.0002 | CLSLoss:0.0564 | top1:91.4250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3365 | MainLoss:0.3365 | SPLoss:0.0005 | CLSLoss:0.0560 | top1:86.5170 | AUROC:0.9384\n",
      "Test | 161/20 | Loss:0.4585 | MainLoss:0.4585 | SPLoss:0.0005 | CLSLoss:0.0560 | top1:81.5950 | AUROC:0.9512\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.003952\n",
      "Train | 20/20 | Loss:0.2665 | MainLoss:0.2416 | Alpha:0.4446 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:90.9000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3351 | MainLoss:0.3351 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:86.4220 | AUROC:0.9385\n",
      "Test | 161/20 | Loss:0.4531 | MainLoss:0.4531 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:81.6449 | AUROC:0.9512\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.003950\n",
      "Train | 20/20 | Loss:0.2453 | MainLoss:0.2204 | Alpha:0.4485 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:92.0750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3370 | MainLoss:0.3370 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:86.4712 | AUROC:0.9387\n",
      "Test | 161/20 | Loss:0.4746 | MainLoss:0.4746 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:80.9221 | AUROC:0.9496\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.003949\n",
      "Train | 20/20 | Loss:0.2562 | MainLoss:0.2313 | Alpha:0.4471 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:91.6000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3375 | MainLoss:0.3375 | SPLoss:0.0005 | CLSLoss:0.0555 | top1:86.4319 | AUROC:0.9387\n",
      "Test | 161/20 | Loss:0.5057 | MainLoss:0.5057 | SPLoss:0.0005 | CLSLoss:0.0555 | top1:79.6012 | AUROC:0.9465\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.003948\n",
      "Train | 20/20 | Loss:0.2555 | MainLoss:0.2308 | Alpha:0.4463 | SPLoss:0.0002 | CLSLoss:0.0552 | top1:90.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3347 | MainLoss:0.3347 | SPLoss:0.0004 | CLSLoss:0.0554 | top1:86.6579 | AUROC:0.9389\n",
      "Test | 161/20 | Loss:0.4876 | MainLoss:0.4876 | SPLoss:0.0004 | CLSLoss:0.0554 | top1:80.2804 | AUROC:0.9460\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.003946\n",
      "Train | 20/20 | Loss:0.2544 | MainLoss:0.2297 | Alpha:0.4455 | SPLoss:0.0002 | CLSLoss:0.0552 | top1:90.9750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3337 | MainLoss:0.3337 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:86.6579 | AUROC:0.9394\n",
      "Test | 161/20 | Loss:0.4924 | MainLoss:0.4924 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:80.1246 | AUROC:0.9438\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.003945\n",
      "Train | 20/20 | Loss:0.2447 | MainLoss:0.2198 | Alpha:0.4496 | SPLoss:0.0002 | CLSLoss:0.0551 | top1:92.0000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3357 | MainLoss:0.3357 | SPLoss:0.0005 | CLSLoss:0.0553 | top1:86.5007 | AUROC:0.9391\n",
      "Test | 161/20 | Loss:0.4649 | MainLoss:0.4649 | SPLoss:0.0005 | CLSLoss:0.0553 | top1:81.3551 | AUROC:0.9437\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.003943\n",
      "Train | 20/20 | Loss:0.2568 | MainLoss:0.2322 | Alpha:0.4461 | SPLoss:0.0003 | CLSLoss:0.0549 | top1:91.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3336 | MainLoss:0.3336 | SPLoss:0.0005 | CLSLoss:0.0547 | top1:86.5170 | AUROC:0.9392\n",
      "Test | 161/20 | Loss:0.4561 | MainLoss:0.4561 | SPLoss:0.0005 | CLSLoss:0.0547 | top1:81.7414 | AUROC:0.9466\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.003942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.2563 | MainLoss:0.2317 | Alpha:0.4480 | SPLoss:0.0002 | CLSLoss:0.0547 | top1:91.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3306 | MainLoss:0.3306 | SPLoss:0.0004 | CLSLoss:0.0542 | top1:86.6088 | AUROC:0.9391\n",
      "Test | 161/20 | Loss:0.4680 | MainLoss:0.4680 | SPLoss:0.0004 | CLSLoss:0.0542 | top1:80.9938 | AUROC:0.9452\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.003940\n",
      "Train | 20/20 | Loss:0.2329 | MainLoss:0.2079 | Alpha:0.4530 | SPLoss:0.0002 | CLSLoss:0.0551 | top1:92.0750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3338 | MainLoss:0.3338 | SPLoss:0.0004 | CLSLoss:0.0555 | top1:86.6579 | AUROC:0.9395\n",
      "Test | 161/20 | Loss:0.4978 | MainLoss:0.4978 | SPLoss:0.0004 | CLSLoss:0.0555 | top1:80.1682 | AUROC:0.9435\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.003939\n",
      "Train | 20/20 | Loss:0.2519 | MainLoss:0.2270 | Alpha:0.4474 | SPLoss:0.0002 | CLSLoss:0.0554 | top1:91.1000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3329 | MainLoss:0.3329 | SPLoss:0.0004 | CLSLoss:0.0553 | top1:86.6219 | AUROC:0.9400\n",
      "Test | 161/20 | Loss:0.4792 | MainLoss:0.4792 | SPLoss:0.0004 | CLSLoss:0.0553 | top1:80.8193 | AUROC:0.9456\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.003937\n",
      "Train | 20/20 | Loss:0.2387 | MainLoss:0.2134 | Alpha:0.4504 | SPLoss:0.0002 | CLSLoss:0.0559 | top1:92.1000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3358 | MainLoss:0.3358 | SPLoss:0.0004 | CLSLoss:0.0556 | top1:86.5695 | AUROC:0.9396\n",
      "Test | 161/20 | Loss:0.5030 | MainLoss:0.5030 | SPLoss:0.0004 | CLSLoss:0.0556 | top1:80.0499 | AUROC:0.9452\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.003936\n",
      "Train | 20/20 | Loss:0.2389 | MainLoss:0.2136 | Alpha:0.4513 | SPLoss:0.0003 | CLSLoss:0.0559 | top1:92.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3361 | MainLoss:0.3361 | SPLoss:0.0005 | CLSLoss:0.0558 | top1:86.5334 | AUROC:0.9393\n",
      "Test | 161/20 | Loss:0.4699 | MainLoss:0.4699 | SPLoss:0.0005 | CLSLoss:0.0558 | top1:81.3707 | AUROC:0.9467\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.003934\n",
      "Train | 20/20 | Loss:0.2404 | MainLoss:0.2151 | Alpha:0.4511 | SPLoss:0.0002 | CLSLoss:0.0558 | top1:91.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3361 | MainLoss:0.3361 | SPLoss:0.0004 | CLSLoss:0.0559 | top1:86.6514 | AUROC:0.9395\n",
      "Test | 161/20 | Loss:0.4961 | MainLoss:0.4961 | SPLoss:0.0004 | CLSLoss:0.0559 | top1:80.4237 | AUROC:0.9457\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.003932\n",
      "Train | 20/20 | Loss:0.2502 | MainLoss:0.2250 | Alpha:0.4498 | SPLoss:0.0002 | CLSLoss:0.0560 | top1:91.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3360 | MainLoss:0.3360 | SPLoss:0.0005 | CLSLoss:0.0554 | top1:86.5760 | AUROC:0.9396\n",
      "Test | 161/20 | Loss:0.5108 | MainLoss:0.5108 | SPLoss:0.0005 | CLSLoss:0.0554 | top1:79.9034 | AUROC:0.9425\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.003931\n",
      "Train | 20/20 | Loss:0.2636 | MainLoss:0.2391 | Alpha:0.4452 | SPLoss:0.0003 | CLSLoss:0.0547 | top1:91.1750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3313 | MainLoss:0.3313 | SPLoss:0.0006 | CLSLoss:0.0537 | top1:86.6678 | AUROC:0.9399\n",
      "Test | 161/20 | Loss:0.4786 | MainLoss:0.4786 | SPLoss:0.0006 | CLSLoss:0.0537 | top1:80.8131 | AUROC:0.9432\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.003929\n",
      "Train | 20/20 | Loss:0.2369 | MainLoss:0.2123 | Alpha:0.4510 | SPLoss:0.0002 | CLSLoss:0.0543 | top1:92.4500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3349 | MainLoss:0.3349 | SPLoss:0.0005 | CLSLoss:0.0548 | top1:86.7071 | AUROC:0.9396\n",
      "Test | 161/20 | Loss:0.4958 | MainLoss:0.4958 | SPLoss:0.0005 | CLSLoss:0.0548 | top1:80.3053 | AUROC:0.9445\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.003927\n",
      "Train | 20/20 | Loss:0.2185 | MainLoss:0.1931 | Alpha:0.4551 | SPLoss:0.0002 | CLSLoss:0.0555 | top1:93.0000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3379 | MainLoss:0.3379 | SPLoss:0.0005 | CLSLoss:0.0562 | top1:86.5826 | AUROC:0.9395\n",
      "Test | 161/20 | Loss:0.4932 | MainLoss:0.4932 | SPLoss:0.0005 | CLSLoss:0.0562 | top1:80.7227 | AUROC:0.9427\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.003926\n",
      "Train | 20/20 | Loss:0.2589 | MainLoss:0.2339 | Alpha:0.4463 | SPLoss:0.0003 | CLSLoss:0.0558 | top1:90.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3353 | MainLoss:0.3353 | SPLoss:0.0005 | CLSLoss:0.0547 | top1:86.6186 | AUROC:0.9393\n",
      "Test | 161/20 | Loss:0.5074 | MainLoss:0.5074 | SPLoss:0.0005 | CLSLoss:0.0547 | top1:79.8910 | AUROC:0.9409\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.003924\n",
      "Train | 20/20 | Loss:0.2315 | MainLoss:0.2065 | Alpha:0.4529 | SPLoss:0.0002 | CLSLoss:0.0549 | top1:92.3000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3373 | MainLoss:0.3373 | SPLoss:0.0004 | CLSLoss:0.0552 | top1:86.6317 | AUROC:0.9394\n",
      "Test | 161/20 | Loss:0.4971 | MainLoss:0.4971 | SPLoss:0.0004 | CLSLoss:0.0552 | top1:80.4330 | AUROC:0.9417\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.003922\n",
      "Train | 20/20 | Loss:0.2425 | MainLoss:0.2176 | Alpha:0.4508 | SPLoss:0.0002 | CLSLoss:0.0549 | top1:91.7750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3350 | MainLoss:0.3350 | SPLoss:0.0005 | CLSLoss:0.0548 | top1:86.6612 | AUROC:0.9400\n",
      "Test | 161/20 | Loss:0.4934 | MainLoss:0.4934 | SPLoss:0.0005 | CLSLoss:0.0548 | top1:80.5981 | AUROC:0.9422\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.003921\n",
      "Train | 20/20 | Loss:0.2219 | MainLoss:0.1967 | Alpha:0.4546 | SPLoss:0.0002 | CLSLoss:0.0553 | top1:93.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3357 | MainLoss:0.3357 | SPLoss:0.0004 | CLSLoss:0.0557 | top1:86.7038 | AUROC:0.9398\n",
      "Test | 161/20 | Loss:0.5193 | MainLoss:0.5193 | SPLoss:0.0004 | CLSLoss:0.0557 | top1:79.7913 | AUROC:0.9403\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.003919\n",
      "Train | 20/20 | Loss:0.2293 | MainLoss:0.2040 | Alpha:0.4543 | SPLoss:0.0002 | CLSLoss:0.0555 | top1:92.5250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3368 | MainLoss:0.3368 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:86.6940 | AUROC:0.9400\n",
      "Test | 161/20 | Loss:0.5254 | MainLoss:0.5254 | SPLoss:0.0004 | CLSLoss:0.0558 | top1:79.4642 | AUROC:0.9382\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.003917\n",
      "Train | 20/20 | Loss:0.2573 | MainLoss:0.2326 | Alpha:0.4474 | SPLoss:0.0003 | CLSLoss:0.0550 | top1:91.0500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3340 | MainLoss:0.3340 | SPLoss:0.0006 | CLSLoss:0.0543 | top1:86.5859 | AUROC:0.9397\n",
      "Test | 161/20 | Loss:0.5139 | MainLoss:0.5139 | SPLoss:0.0006 | CLSLoss:0.0543 | top1:79.4579 | AUROC:0.9420\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.003915\n",
      "Train | 20/20 | Loss:0.2458 | MainLoss:0.2213 | Alpha:0.4498 | SPLoss:0.0002 | CLSLoss:0.0542 | top1:91.8500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3322 | MainLoss:0.3322 | SPLoss:0.0004 | CLSLoss:0.0538 | top1:86.6776 | AUROC:0.9399\n",
      "Test | 161/20 | Loss:0.4941 | MainLoss:0.4941 | SPLoss:0.0004 | CLSLoss:0.0538 | top1:80.2617 | AUROC:0.9442\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.003913\n",
      "Train | 20/20 | Loss:0.2177 | MainLoss:0.1926 | Alpha:0.4558 | SPLoss:0.0002 | CLSLoss:0.0550 | top1:92.8250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3358 | MainLoss:0.3358 | SPLoss:0.0004 | CLSLoss:0.0556 | top1:86.6809 | AUROC:0.9402\n",
      "Test | 161/20 | Loss:0.4863 | MainLoss:0.4863 | SPLoss:0.0004 | CLSLoss:0.0556 | top1:80.8660 | AUROC:0.9453\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.003912\n",
      "Train | 20/20 | Loss:0.2379 | MainLoss:0.2129 | Alpha:0.4514 | SPLoss:0.0002 | CLSLoss:0.0554 | top1:92.4750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3348 | MainLoss:0.3348 | SPLoss:0.0004 | CLSLoss:0.0550 | top1:86.7071 | AUROC:0.9404\n",
      "Test | 161/20 | Loss:0.5212 | MainLoss:0.5212 | SPLoss:0.0004 | CLSLoss:0.0550 | top1:79.4206 | AUROC:0.9409\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.003910\n",
      "Train | 20/20 | Loss:0.2386 | MainLoss:0.2136 | Alpha:0.4513 | SPLoss:0.0002 | CLSLoss:0.0550 | top1:91.9000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3335 | MainLoss:0.3335 | SPLoss:0.0004 | CLSLoss:0.0545 | top1:86.6743 | AUROC:0.9405\n",
      "Test | 161/20 | Loss:0.5191 | MainLoss:0.5191 | SPLoss:0.0004 | CLSLoss:0.0545 | top1:79.4704 | AUROC:0.9408\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.003908\n",
      "Train | 20/20 | Loss:0.2384 | MainLoss:0.2137 | Alpha:0.4513 | SPLoss:0.0002 | CLSLoss:0.0544 | top1:91.8750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3329 | MainLoss:0.3329 | SPLoss:0.0005 | CLSLoss:0.0547 | top1:86.7366 | AUROC:0.9404\n",
      "Test | 161/20 | Loss:0.5025 | MainLoss:0.5025 | SPLoss:0.0005 | CLSLoss:0.0547 | top1:80.1589 | AUROC:0.9439\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.003906\n",
      "Train | 20/20 | Loss:0.2380 | MainLoss:0.2135 | Alpha:0.4523 | SPLoss:0.0003 | CLSLoss:0.0540 | top1:92.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3340 | MainLoss:0.3340 | SPLoss:0.0005 | CLSLoss:0.0549 | top1:86.6579 | AUROC:0.9406\n",
      "Test | 161/20 | Loss:0.4826 | MainLoss:0.4826 | SPLoss:0.0005 | CLSLoss:0.0549 | top1:80.9813 | AUROC:0.9442\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.003904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.2311 | MainLoss:0.2061 | Alpha:0.4539 | SPLoss:0.0002 | CLSLoss:0.0548 | top1:92.6750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3346 | MainLoss:0.3346 | SPLoss:0.0004 | CLSLoss:0.0552 | top1:86.7497 | AUROC:0.9406\n",
      "Test | 161/20 | Loss:0.5140 | MainLoss:0.5140 | SPLoss:0.0004 | CLSLoss:0.0552 | top1:79.8879 | AUROC:0.9427\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.003902\n",
      "Train | 20/20 | Loss:0.2344 | MainLoss:0.2093 | Alpha:0.4522 | SPLoss:0.0003 | CLSLoss:0.0551 | top1:92.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3341 | MainLoss:0.3341 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:86.7497 | AUROC:0.9408\n",
      "Test | 161/20 | Loss:0.4930 | MainLoss:0.4930 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:80.7383 | AUROC:0.9431\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.003900\n",
      "Train | 20/20 | Loss:0.2310 | MainLoss:0.2059 | Alpha:0.4522 | SPLoss:0.0003 | CLSLoss:0.0553 | top1:92.2500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3371 | MainLoss:0.3371 | SPLoss:0.0006 | CLSLoss:0.0551 | top1:86.6874 | AUROC:0.9402\n",
      "Test | 161/20 | Loss:0.5255 | MainLoss:0.5255 | SPLoss:0.0006 | CLSLoss:0.0551 | top1:79.4829 | AUROC:0.9427\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.003898\n",
      "Train | 20/20 | Loss:0.2318 | MainLoss:0.2066 | Alpha:0.4520 | SPLoss:0.0002 | CLSLoss:0.0556 | top1:92.2750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3371 | MainLoss:0.3371 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:86.5924 | AUROC:0.9402\n",
      "Test | 161/20 | Loss:0.5039 | MainLoss:0.5039 | SPLoss:0.0005 | CLSLoss:0.0551 | top1:80.4642 | AUROC:0.9452\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.003896\n",
      "Train | 20/20 | Loss:0.2408 | MainLoss:0.2158 | Alpha:0.4510 | SPLoss:0.0003 | CLSLoss:0.0551 | top1:91.5500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3365 | MainLoss:0.3365 | SPLoss:0.0006 | CLSLoss:0.0546 | top1:86.6841 | AUROC:0.9399\n",
      "Test | 161/20 | Loss:0.5152 | MainLoss:0.5152 | SPLoss:0.0006 | CLSLoss:0.0546 | top1:79.9502 | AUROC:0.9399\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.003894\n",
      "Train | 20/20 | Loss:0.2470 | MainLoss:0.2226 | Alpha:0.4504 | SPLoss:0.0003 | CLSLoss:0.0540 | top1:91.6250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3329 | MainLoss:0.3329 | SPLoss:0.0006 | CLSLoss:0.0537 | top1:86.8218 | AUROC:0.9404\n",
      "Test | 161/20 | Loss:0.5179 | MainLoss:0.5179 | SPLoss:0.0006 | CLSLoss:0.0537 | top1:79.6044 | AUROC:0.9429\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.003892\n",
      "Train | 20/20 | Loss:0.2544 | MainLoss:0.2305 | Alpha:0.4469 | SPLoss:0.0002 | CLSLoss:0.0533 | top1:91.8000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3350 | MainLoss:0.3350 | SPLoss:0.0005 | CLSLoss:0.0529 | top1:86.5629 | AUROC:0.9401\n",
      "Test | 161/20 | Loss:0.4712 | MainLoss:0.4712 | SPLoss:0.0005 | CLSLoss:0.0529 | top1:81.3832 | AUROC:0.9437\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.003890\n",
      "Train | 20/20 | Loss:0.2307 | MainLoss:0.2066 | Alpha:0.4531 | SPLoss:0.0002 | CLSLoss:0.0531 | top1:92.5000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3341 | MainLoss:0.3341 | SPLoss:0.0005 | CLSLoss:0.0537 | top1:86.7562 | AUROC:0.9408\n",
      "Test | 161/20 | Loss:0.5248 | MainLoss:0.5248 | SPLoss:0.0005 | CLSLoss:0.0537 | top1:79.1776 | AUROC:0.9429\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.003888\n",
      "Train | 20/20 | Loss:0.2361 | MainLoss:0.2118 | Alpha:0.4512 | SPLoss:0.0003 | CLSLoss:0.0537 | top1:92.1250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3337 | MainLoss:0.3337 | SPLoss:0.0006 | CLSLoss:0.0535 | top1:86.7464 | AUROC:0.9406\n",
      "Test | 161/20 | Loss:0.5259 | MainLoss:0.5259 | SPLoss:0.0006 | CLSLoss:0.0535 | top1:79.1558 | AUROC:0.9423\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.003886\n",
      "Train | 20/20 | Loss:0.2336 | MainLoss:0.2093 | Alpha:0.4520 | SPLoss:0.0003 | CLSLoss:0.0535 | top1:91.8750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3332 | MainLoss:0.3332 | SPLoss:0.0005 | CLSLoss:0.0538 | top1:86.7398 | AUROC:0.9408\n",
      "Test | 161/20 | Loss:0.5363 | MainLoss:0.5363 | SPLoss:0.0005 | CLSLoss:0.0538 | top1:78.7227 | AUROC:0.9435\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.003884\n",
      "Train | 20/20 | Loss:0.2312 | MainLoss:0.2064 | Alpha:0.4529 | SPLoss:0.0003 | CLSLoss:0.0544 | top1:92.3500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3335 | MainLoss:0.3335 | SPLoss:0.0005 | CLSLoss:0.0540 | top1:86.7235 | AUROC:0.9407\n",
      "Test | 161/20 | Loss:0.5112 | MainLoss:0.5112 | SPLoss:0.0005 | CLSLoss:0.0540 | top1:79.6604 | AUROC:0.9434\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.003882\n",
      "Train | 20/20 | Loss:0.2198 | MainLoss:0.1949 | Alpha:0.4570 | SPLoss:0.0002 | CLSLoss:0.0543 | top1:93.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3347 | MainLoss:0.3347 | SPLoss:0.0004 | CLSLoss:0.0548 | top1:86.7955 | AUROC:0.9408\n",
      "Test | 161/20 | Loss:0.5212 | MainLoss:0.5212 | SPLoss:0.0004 | CLSLoss:0.0548 | top1:79.4798 | AUROC:0.9422\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.003880\n",
      "Train | 20/20 | Loss:0.2263 | MainLoss:0.2013 | Alpha:0.4552 | SPLoss:0.0003 | CLSLoss:0.0548 | top1:92.5250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3340 | MainLoss:0.3340 | SPLoss:0.0005 | CLSLoss:0.0548 | top1:86.8218 | AUROC:0.9409\n",
      "Test | 161/20 | Loss:0.5226 | MainLoss:0.5226 | SPLoss:0.0005 | CLSLoss:0.0548 | top1:79.4673 | AUROC:0.9411\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.003877\n",
      "Train | 20/20 | Loss:0.2163 | MainLoss:0.1910 | Alpha:0.4574 | SPLoss:0.0002 | CLSLoss:0.0551 | top1:93.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3363 | MainLoss:0.3363 | SPLoss:0.0004 | CLSLoss:0.0554 | top1:86.7988 | AUROC:0.9408\n",
      "Test | 161/20 | Loss:0.5080 | MainLoss:0.5080 | SPLoss:0.0004 | CLSLoss:0.0554 | top1:80.1994 | AUROC:0.9400\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.003875\n",
      "Train | 20/20 | Loss:0.2154 | MainLoss:0.1898 | Alpha:0.4560 | SPLoss:0.0003 | CLSLoss:0.0558 | top1:93.2000 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3395 | MainLoss:0.3395 | SPLoss:0.0005 | CLSLoss:0.0556 | top1:86.7595 | AUROC:0.9407\n",
      "Test | 161/20 | Loss:0.5245 | MainLoss:0.5245 | SPLoss:0.0005 | CLSLoss:0.0556 | top1:79.8847 | AUROC:0.9386\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.003873\n",
      "Train | 20/20 | Loss:0.2380 | MainLoss:0.2132 | Alpha:0.4520 | SPLoss:0.0003 | CLSLoss:0.0546 | top1:91.9250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3342 | MainLoss:0.3342 | SPLoss:0.0005 | CLSLoss:0.0541 | top1:86.8414 | AUROC:0.9406\n",
      "Test | 161/20 | Loss:0.5146 | MainLoss:0.5146 | SPLoss:0.0005 | CLSLoss:0.0541 | top1:79.8536 | AUROC:0.9379\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.003871\n",
      "Train | 20/20 | Loss:0.2209 | MainLoss:0.1960 | Alpha:0.4557 | SPLoss:0.0003 | CLSLoss:0.0543 | top1:92.8500 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3359 | MainLoss:0.3359 | SPLoss:0.0005 | CLSLoss:0.0546 | top1:86.9135 | AUROC:0.9407\n",
      "Test | 161/20 | Loss:0.5305 | MainLoss:0.5305 | SPLoss:0.0005 | CLSLoss:0.0546 | top1:79.3894 | AUROC:0.9400\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.003869\n",
      "Train | 20/20 | Loss:0.2185 | MainLoss:0.1934 | Alpha:0.4564 | SPLoss:0.0003 | CLSLoss:0.0547 | top1:93.0250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3382 | MainLoss:0.3382 | SPLoss:0.0005 | CLSLoss:0.0550 | top1:86.7955 | AUROC:0.9409\n",
      "Test | 161/20 | Loss:0.5112 | MainLoss:0.5112 | SPLoss:0.0005 | CLSLoss:0.0550 | top1:80.2212 | AUROC:0.9393\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.003866\n",
      "Train | 20/20 | Loss:0.2451 | MainLoss:0.2205 | Alpha:0.4511 | SPLoss:0.0003 | CLSLoss:0.0541 | top1:91.7250 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3355 | MainLoss:0.3355 | SPLoss:0.0005 | CLSLoss:0.0537 | top1:86.6743 | AUROC:0.9405\n",
      "Test | 161/20 | Loss:0.5096 | MainLoss:0.5096 | SPLoss:0.0005 | CLSLoss:0.0537 | top1:80.0343 | AUROC:0.9380\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.003864\n",
      "Train | 20/20 | Loss:0.2370 | MainLoss:0.2126 | Alpha:0.4532 | SPLoss:0.0002 | CLSLoss:0.0535 | top1:92.1750 | AUROC:0.0000\n",
      "Test | 153/20 | Loss:0.3350 | MainLoss:0.3350 | SPLoss:0.0004 | CLSLoss:0.0533 | top1:86.7693 | AUROC:0.9406\n",
      "Test | 161/20 | Loss:0.5267 | MainLoss:0.5267 | SPLoss:0.0004 | CLSLoss:0.0533 | top1:79.3863 | AUROC:0.9378\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.003862\n",
      "Train | 20/20 | Loss:0.2360 | MainLoss:0.2120 | Alpha:0.4514 | SPLoss:0.0002 | CLSLoss:0.0529 | top1:92.1250 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "#     if (epoch+1) % 100 == 0:\n",
    "    teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
