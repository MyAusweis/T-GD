{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/star/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 500\n",
    "test_batch = 500\n",
    "lr = 0.04\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/star/128/b0/to_pggan/2000shot/intracm' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/star/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.4, 'drop_connect_rate':0.4})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.040000\n",
      "Train | 8/8 | Loss:1.2923 | MainLoss:1.0577 | Alpha:0.1115 | SPLoss:0.2302 | CLSLoss:1.8752 | top1:58.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.5736 | MainLoss:0.5736 | SPLoss:0.2824 | CLSLoss:1.7571 | top1:75.0623 | AUROC:0.8642\n",
      "Test | 62/8 | Loss:0.3978 | MainLoss:0.3978 | SPLoss:0.2824 | CLSLoss:1.7571 | top1:87.7654 | AUROC:0.9943\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.052000\n",
      "Train | 8/8 | Loss:0.7881 | MainLoss:0.5608 | Alpha:0.1157 | SPLoss:0.2740 | CLSLoss:1.6920 | top1:74.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.4111 | MainLoss:0.4111 | SPLoss:0.2668 | CLSLoss:1.6110 | top1:85.3364 | AUROC:0.9526\n",
      "Test | 62/8 | Loss:0.3395 | MainLoss:0.3395 | SPLoss:0.2668 | CLSLoss:1.6110 | top1:86.4613 | AUROC:0.9923\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.064000\n",
      "Train | 8/8 | Loss:0.6900 | MainLoss:0.4855 | Alpha:0.1124 | SPLoss:0.2813 | CLSLoss:1.5398 | top1:77.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2682 | MainLoss:0.2682 | SPLoss:0.3055 | CLSLoss:1.4468 | top1:92.7414 | AUROC:0.9876\n",
      "Test | 62/8 | Loss:0.3341 | MainLoss:0.3341 | SPLoss:0.3055 | CLSLoss:1.4468 | top1:83.2307 | AUROC:0.9914\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.076000\n",
      "Train | 8/8 | Loss:0.6154 | MainLoss:0.4263 | Alpha:0.1126 | SPLoss:0.3142 | CLSLoss:1.3632 | top1:81.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:0.3195 | CLSLoss:1.2622 | top1:93.6137 | AUROC:0.9930\n",
      "Test | 62/8 | Loss:0.3011 | MainLoss:0.3011 | SPLoss:0.3195 | CLSLoss:1.2622 | top1:84.9541 | AUROC:0.9920\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.088000\n",
      "Train | 8/8 | Loss:0.5854 | MainLoss:0.4178 | Alpha:0.1114 | SPLoss:0.3257 | CLSLoss:1.1782 | top1:80.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1865 | MainLoss:0.1865 | SPLoss:0.3366 | CLSLoss:1.0754 | top1:95.7165 | AUROC:0.9958\n",
      "Test | 62/8 | Loss:0.3302 | MainLoss:0.3302 | SPLoss:0.3366 | CLSLoss:1.0754 | top1:82.1429 | AUROC:0.9919\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.100000\n",
      "Train | 8/8 | Loss:0.5407 | MainLoss:0.3918 | Alpha:0.1113 | SPLoss:0.3397 | CLSLoss:0.9974 | top1:82.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1631 | MainLoss:0.1631 | SPLoss:0.3556 | CLSLoss:0.8987 | top1:96.3583 | AUROC:0.9967\n",
      "Test | 62/8 | Loss:0.3211 | MainLoss:0.3211 | SPLoss:0.3556 | CLSLoss:0.8987 | top1:82.4214 | AUROC:0.9919\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.112000\n",
      "Train | 8/8 | Loss:0.5062 | MainLoss:0.3751 | Alpha:0.1095 | SPLoss:0.3692 | CLSLoss:0.8271 | top1:83.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2218 | MainLoss:0.2218 | SPLoss:0.3704 | CLSLoss:0.7360 | top1:92.8785 | AUROC:0.9971\n",
      "Test | 62/8 | Loss:0.2344 | MainLoss:0.2344 | SPLoss:0.3704 | CLSLoss:0.7360 | top1:89.8460 | AUROC:0.9928\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.124000\n",
      "Train | 8/8 | Loss:0.5165 | MainLoss:0.3964 | Alpha:0.1139 | SPLoss:0.3906 | CLSLoss:0.6659 | top1:81.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1414 | MainLoss:0.1414 | SPLoss:0.3859 | CLSLoss:0.5891 | top1:97.7134 | AUROC:0.9981\n",
      "Test | 62/8 | Loss:0.3643 | MainLoss:0.3643 | SPLoss:0.3859 | CLSLoss:0.5891 | top1:79.0564 | AUROC:0.9925\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.136000\n",
      "Train | 8/8 | Loss:0.4942 | MainLoss:0.3897 | Alpha:0.1154 | SPLoss:0.3756 | CLSLoss:0.5290 | top1:82.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1615 | MainLoss:0.1615 | SPLoss:0.3799 | CLSLoss:0.4647 | top1:96.7383 | AUROC:0.9979\n",
      "Test | 62/8 | Loss:0.3190 | MainLoss:0.3190 | SPLoss:0.3799 | CLSLoss:0.4647 | top1:82.5360 | AUROC:0.9934\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.148000\n",
      "Train | 8/8 | Loss:0.4760 | MainLoss:0.3815 | Alpha:0.1177 | SPLoss:0.3817 | CLSLoss:0.4218 | top1:82.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2356 | MainLoss:0.2356 | SPLoss:0.3781 | CLSLoss:0.3734 | top1:92.2835 | AUROC:0.9971\n",
      "Test | 62/8 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:0.3781 | CLSLoss:0.3734 | top1:90.8421 | AUROC:0.9934\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.160000\n",
      "Train | 8/8 | Loss:0.4710 | MainLoss:0.3840 | Alpha:0.1191 | SPLoss:0.3899 | CLSLoss:0.3397 | top1:82.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.0958 | MainLoss:0.0958 | SPLoss:0.4332 | CLSLoss:0.3152 | top1:98.4143 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.4934 | MainLoss:0.4934 | SPLoss:0.4332 | CLSLoss:0.3152 | top1:72.2051 | AUROC:0.9913\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.160000\n",
      "Train | 8/8 | Loss:0.4648 | MainLoss:0.3863 | Alpha:0.1098 | SPLoss:0.4295 | CLSLoss:0.2861 | top1:82.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2184 | MainLoss:0.2184 | SPLoss:0.4236 | CLSLoss:0.2677 | top1:93.4517 | AUROC:0.9970\n",
      "Test | 62/8 | Loss:0.2334 | MainLoss:0.2334 | SPLoss:0.4236 | CLSLoss:0.2677 | top1:89.1350 | AUROC:0.9934\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.160000\n",
      "Train | 8/8 | Loss:0.4664 | MainLoss:0.3835 | Alpha:0.1227 | SPLoss:0.4306 | CLSLoss:0.2459 | top1:82.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1235 | MainLoss:0.1235 | SPLoss:0.4415 | CLSLoss:0.2350 | top1:97.9408 | AUROC:0.9978\n",
      "Test | 62/8 | Loss:0.4696 | MainLoss:0.4696 | SPLoss:0.4415 | CLSLoss:0.2350 | top1:71.5760 | AUROC:0.9932\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.159998\n",
      "Train | 8/8 | Loss:0.4481 | MainLoss:0.3696 | Alpha:0.1178 | SPLoss:0.4423 | CLSLoss:0.2237 | top1:83.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1733 | MainLoss:0.1733 | SPLoss:0.4376 | CLSLoss:0.2140 | top1:95.6480 | AUROC:0.9979\n",
      "Test | 62/8 | Loss:0.2764 | MainLoss:0.2764 | SPLoss:0.4376 | CLSLoss:0.2140 | top1:85.7733 | AUROC:0.9945\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.159996\n",
      "Train | 8/8 | Loss:0.4778 | MainLoss:0.4033 | Alpha:0.1147 | SPLoss:0.4562 | CLSLoss:0.1924 | top1:81.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2077 | MainLoss:0.2077 | SPLoss:0.4613 | CLSLoss:0.1783 | top1:95.5016 | AUROC:0.9982\n",
      "Test | 62/8 | Loss:0.2830 | MainLoss:0.2830 | SPLoss:0.4613 | CLSLoss:0.1783 | top1:87.7589 | AUROC:0.9935\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.159994\n",
      "Train | 8/8 | Loss:0.4500 | MainLoss:0.3768 | Alpha:0.1144 | SPLoss:0.4549 | CLSLoss:0.1852 | top1:83.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1536 | MainLoss:0.1536 | SPLoss:0.4539 | CLSLoss:0.1838 | top1:96.9969 | AUROC:0.9977\n",
      "Test | 62/8 | Loss:0.3079 | MainLoss:0.3079 | SPLoss:0.4539 | CLSLoss:0.1838 | top1:83.5387 | AUROC:0.9942\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.159990\n",
      "Train | 8/8 | Loss:0.4805 | MainLoss:0.4072 | Alpha:0.1137 | SPLoss:0.4775 | CLSLoss:0.1665 | top1:82.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1425 | MainLoss:0.1425 | SPLoss:0.4896 | CLSLoss:0.1750 | top1:97.8131 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3451 | MainLoss:0.3451 | SPLoss:0.4896 | CLSLoss:0.1750 | top1:81.4253 | AUROC:0.9920\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.159986\n",
      "Train | 8/8 | Loss:0.4507 | MainLoss:0.3745 | Alpha:0.1163 | SPLoss:0.4795 | CLSLoss:0.1761 | top1:83.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1417 | MainLoss:0.1417 | SPLoss:0.4839 | CLSLoss:0.1636 | top1:98.0717 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3744 | MainLoss:0.3744 | SPLoss:0.4839 | CLSLoss:0.1636 | top1:78.6730 | AUROC:0.9929\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.159981\n",
      "Train | 8/8 | Loss:0.4575 | MainLoss:0.3821 | Alpha:0.1201 | SPLoss:0.4636 | CLSLoss:0.1637 | top1:83.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1813 | MainLoss:0.1813 | SPLoss:0.5154 | CLSLoss:0.1547 | top1:94.6168 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.7370 | MainLoss:0.7370 | SPLoss:0.5154 | CLSLoss:0.1547 | top1:57.0708 | AUROC:0.9897\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.159975\n",
      "Train | 8/8 | Loss:0.4733 | MainLoss:0.3989 | Alpha:0.1149 | SPLoss:0.4987 | CLSLoss:0.1493 | top1:82.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1508 | MainLoss:0.1508 | SPLoss:0.4798 | CLSLoss:0.1617 | top1:96.9533 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3170 | MainLoss:0.3170 | SPLoss:0.4798 | CLSLoss:0.1617 | top1:83.8598 | AUROC:0.9942\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.159968\n",
      "Train | 8/8 | Loss:0.4397 | MainLoss:0.3669 | Alpha:0.1153 | SPLoss:0.4673 | CLSLoss:0.1639 | top1:83.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1187 | MainLoss:0.1187 | SPLoss:0.5026 | CLSLoss:0.1638 | top1:97.5327 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.5611 | MainLoss:0.5611 | SPLoss:0.5026 | CLSLoss:0.1638 | top1:68.9908 | AUROC:0.9922\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.159961\n",
      "Train | 8/8 | Loss:0.4877 | MainLoss:0.4148 | Alpha:0.1118 | SPLoss:0.5100 | CLSLoss:0.1418 | top1:81.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2024 | MainLoss:0.2024 | SPLoss:0.4782 | CLSLoss:0.1498 | top1:94.2991 | AUROC:0.9982\n",
      "Test | 62/8 | Loss:0.2695 | MainLoss:0.2695 | SPLoss:0.4782 | CLSLoss:0.1498 | top1:88.6009 | AUROC:0.9939\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.159952\n",
      "Train | 8/8 | Loss:0.4369 | MainLoss:0.3644 | Alpha:0.1123 | SPLoss:0.4901 | CLSLoss:0.1553 | top1:83.3500 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.2468 | MainLoss:0.2468 | SPLoss:0.4758 | CLSLoss:0.1600 | top1:90.9875 | AUROC:0.9974\n",
      "Test | 62/8 | Loss:0.1995 | MainLoss:0.1995 | SPLoss:0.4758 | CLSLoss:0.1600 | top1:92.4410 | AUROC:0.9942\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.159943\n",
      "Train | 8/8 | Loss:0.4936 | MainLoss:0.4144 | Alpha:0.1152 | SPLoss:0.5456 | CLSLoss:0.1397 | top1:81.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2578 | MainLoss:0.2578 | SPLoss:0.4901 | CLSLoss:0.1503 | top1:89.8785 | AUROC:0.9975\n",
      "Test | 62/8 | Loss:0.2104 | MainLoss:0.2104 | SPLoss:0.4901 | CLSLoss:0.1503 | top1:93.3552 | AUROC:0.9935\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.159933\n",
      "Train | 8/8 | Loss:0.4568 | MainLoss:0.3803 | Alpha:0.1187 | SPLoss:0.4995 | CLSLoss:0.1447 | top1:83.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1163 | MainLoss:0.1163 | SPLoss:0.5224 | CLSLoss:0.1500 | top1:98.4143 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.4899 | MainLoss:0.4899 | SPLoss:0.5224 | CLSLoss:0.1500 | top1:72.0839 | AUROC:0.9926\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.159923\n",
      "Train | 8/8 | Loss:0.4604 | MainLoss:0.3829 | Alpha:0.1205 | SPLoss:0.4988 | CLSLoss:0.1437 | top1:82.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1356 | MainLoss:0.1356 | SPLoss:0.4881 | CLSLoss:0.1492 | top1:97.5763 | AUROC:0.9983\n",
      "Test | 62/8 | Loss:0.3614 | MainLoss:0.3614 | SPLoss:0.4881 | CLSLoss:0.1492 | top1:80.9305 | AUROC:0.9940\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.159911\n",
      "Train | 8/8 | Loss:0.4539 | MainLoss:0.3791 | Alpha:0.1137 | SPLoss:0.5163 | CLSLoss:0.1420 | top1:83.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1169 | MainLoss:0.1169 | SPLoss:0.5281 | CLSLoss:0.1444 | top1:98.4579 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.4215 | MainLoss:0.4215 | SPLoss:0.5281 | CLSLoss:0.1444 | top1:76.7005 | AUROC:0.9921\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.159899\n",
      "Train | 8/8 | Loss:0.4646 | MainLoss:0.3892 | Alpha:0.1182 | SPLoss:0.4953 | CLSLoss:0.1409 | top1:82.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1663 | MainLoss:0.1663 | SPLoss:0.4988 | CLSLoss:0.1335 | top1:97.9284 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3572 | MainLoss:0.3572 | SPLoss:0.4988 | CLSLoss:0.1335 | top1:80.8847 | AUROC:0.9929\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.159886\n",
      "Train | 8/8 | Loss:0.4497 | MainLoss:0.3763 | Alpha:0.1166 | SPLoss:0.4874 | CLSLoss:0.1420 | top1:83.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1934 | MainLoss:0.1934 | SPLoss:0.4812 | CLSLoss:0.1474 | top1:94.4174 | AUROC:0.9981\n",
      "Test | 62/8 | Loss:0.2509 | MainLoss:0.2509 | SPLoss:0.4812 | CLSLoss:0.1474 | top1:88.7418 | AUROC:0.9943\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.159872\n",
      "Train | 8/8 | Loss:0.4484 | MainLoss:0.3782 | Alpha:0.1126 | SPLoss:0.4781 | CLSLoss:0.1458 | top1:82.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1280 | MainLoss:0.1280 | SPLoss:0.4906 | CLSLoss:0.1449 | top1:98.2243 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.4188 | MainLoss:0.4188 | SPLoss:0.4906 | CLSLoss:0.1449 | top1:75.8847 | AUROC:0.9939\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.159858\n",
      "Train | 8/8 | Loss:0.4605 | MainLoss:0.3870 | Alpha:0.1158 | SPLoss:0.4883 | CLSLoss:0.1458 | top1:82.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1895 | MainLoss:0.1895 | SPLoss:0.5106 | CLSLoss:0.1206 | top1:97.8380 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.3747 | MainLoss:0.3747 | SPLoss:0.5106 | CLSLoss:0.1206 | top1:80.5537 | AUROC:0.9927\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.159842\n",
      "Train | 8/8 | Loss:0.4522 | MainLoss:0.3797 | Alpha:0.1137 | SPLoss:0.5022 | CLSLoss:0.1355 | top1:82.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1068 | MainLoss:0.1068 | SPLoss:0.5305 | CLSLoss:0.1467 | top1:98.7196 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.4839 | MainLoss:0.4839 | SPLoss:0.5305 | CLSLoss:0.1467 | top1:71.7759 | AUROC:0.9923\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.159826\n",
      "Train | 8/8 | Loss:0.4523 | MainLoss:0.3783 | Alpha:0.1169 | SPLoss:0.4906 | CLSLoss:0.1410 | top1:83.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1190 | MainLoss:0.1190 | SPLoss:0.5114 | CLSLoss:0.1444 | top1:98.5171 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.4377 | MainLoss:0.4377 | SPLoss:0.5114 | CLSLoss:0.1444 | top1:74.1448 | AUROC:0.9938\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.159809\n",
      "Train | 8/8 | Loss:0.4672 | MainLoss:0.3906 | Alpha:0.1154 | SPLoss:0.5221 | CLSLoss:0.1400 | top1:82.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1383 | MainLoss:0.1383 | SPLoss:0.5351 | CLSLoss:0.1384 | top1:98.5421 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.4136 | MainLoss:0.4136 | SPLoss:0.5351 | CLSLoss:0.1384 | top1:75.7012 | AUROC:0.9919\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.159791\n",
      "Train | 8/8 | Loss:0.4584 | MainLoss:0.3839 | Alpha:0.1132 | SPLoss:0.5191 | CLSLoss:0.1404 | top1:83.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1795 | MainLoss:0.1795 | SPLoss:0.5070 | CLSLoss:0.1541 | top1:94.8598 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.2817 | MainLoss:0.2817 | SPLoss:0.5070 | CLSLoss:0.1541 | top1:86.9004 | AUROC:0.9930\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.159773\n",
      "Train | 8/8 | Loss:0.4307 | MainLoss:0.3538 | Alpha:0.1145 | SPLoss:0.5164 | CLSLoss:0.1554 | top1:84.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2064 | MainLoss:0.2064 | SPLoss:0.4949 | CLSLoss:0.1418 | top1:94.5358 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2905 | MainLoss:0.2905 | SPLoss:0.4949 | CLSLoss:0.1418 | top1:87.4181 | AUROC:0.9936\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.159753\n",
      "Train | 8/8 | Loss:0.4479 | MainLoss:0.3704 | Alpha:0.1182 | SPLoss:0.5091 | CLSLoss:0.1461 | top1:83.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2233 | MainLoss:0.2233 | SPLoss:0.5051 | CLSLoss:0.1502 | top1:92.8629 | AUROC:0.9975\n",
      "Test | 62/8 | Loss:0.2381 | MainLoss:0.2381 | SPLoss:0.5051 | CLSLoss:0.1502 | top1:89.8558 | AUROC:0.9918\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.159733\n",
      "Train | 8/8 | Loss:0.4619 | MainLoss:0.3831 | Alpha:0.1189 | SPLoss:0.5176 | CLSLoss:0.1442 | top1:82.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1248 | MainLoss:0.1248 | SPLoss:0.5184 | CLSLoss:0.1434 | top1:98.6355 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.4578 | MainLoss:0.4578 | SPLoss:0.5184 | CLSLoss:0.1434 | top1:72.2444 | AUROC:0.9926\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.159712\n",
      "Train | 8/8 | Loss:0.4660 | MainLoss:0.3896 | Alpha:0.1161 | SPLoss:0.5160 | CLSLoss:0.1436 | top1:82.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2574 | MainLoss:0.2574 | SPLoss:0.5391 | CLSLoss:0.1372 | top1:91.7819 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2922 | MainLoss:0.2922 | SPLoss:0.5391 | CLSLoss:0.1372 | top1:89.4954 | AUROC:0.9893\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.159691\n",
      "Train | 8/8 | Loss:0.4533 | MainLoss:0.3750 | Alpha:0.1180 | SPLoss:0.5167 | CLSLoss:0.1462 | top1:82.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2617 | MainLoss:0.2617 | SPLoss:0.4696 | CLSLoss:0.1496 | top1:90.0592 | AUROC:0.9972\n",
      "Test | 62/8 | Loss:0.2133 | MainLoss:0.2133 | SPLoss:0.4696 | CLSLoss:0.1496 | top1:92.0413 | AUROC:0.9937\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.159668\n",
      "Train | 8/8 | Loss:0.4578 | MainLoss:0.3841 | Alpha:0.1132 | SPLoss:0.5046 | CLSLoss:0.1459 | top1:83.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1354 | MainLoss:0.1354 | SPLoss:0.5182 | CLSLoss:0.1489 | top1:97.5265 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3496 | MainLoss:0.3496 | SPLoss:0.5182 | CLSLoss:0.1489 | top1:81.9495 | AUROC:0.9930\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.159645\n",
      "Train | 8/8 | Loss:0.4577 | MainLoss:0.3780 | Alpha:0.1209 | SPLoss:0.5124 | CLSLoss:0.1449 | top1:83.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.4591 | MainLoss:0.4591 | SPLoss:0.4669 | CLSLoss:0.1447 | top1:77.7944 | AUROC:0.9949\n",
      "Test | 62/8 | Loss:0.1539 | MainLoss:0.1539 | SPLoss:0.4669 | CLSLoss:0.1447 | top1:96.5400 | AUROC:0.9955\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.159621\n",
      "Train | 8/8 | Loss:0.5178 | MainLoss:0.4324 | Alpha:0.1202 | SPLoss:0.5849 | CLSLoss:0.1254 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2447 | MainLoss:0.2447 | SPLoss:0.5313 | CLSLoss:0.1421 | top1:91.2305 | AUROC:0.9979\n",
      "Test | 62/8 | Loss:0.2500 | MainLoss:0.2500 | SPLoss:0.5313 | CLSLoss:0.1421 | top1:91.5924 | AUROC:0.9906\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.159596\n",
      "Train | 8/8 | Loss:0.4453 | MainLoss:0.3680 | Alpha:0.1148 | SPLoss:0.5263 | CLSLoss:0.1478 | top1:83.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1470 | MainLoss:0.1470 | SPLoss:0.5047 | CLSLoss:0.1575 | top1:96.3053 | AUROC:0.9981\n",
      "Test | 62/8 | Loss:0.3330 | MainLoss:0.3330 | SPLoss:0.5047 | CLSLoss:0.1575 | top1:83.5321 | AUROC:0.9946\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.159570\n",
      "Train | 8/8 | Loss:0.5370 | MainLoss:0.4552 | Alpha:0.1139 | SPLoss:0.5909 | CLSLoss:0.1264 | top1:79.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1631 | MainLoss:0.1631 | SPLoss:0.6718 | CLSLoss:0.1217 | top1:99.0654 | AUROC:0.9997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.5051 | MainLoss:0.5051 | SPLoss:0.6718 | CLSLoss:0.1217 | top1:68.6697 | AUROC:0.9761\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.159544\n",
      "Train | 8/8 | Loss:0.4662 | MainLoss:0.3766 | Alpha:0.1204 | SPLoss:0.6009 | CLSLoss:0.1414 | top1:83.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1431 | MainLoss:0.1431 | SPLoss:0.5487 | CLSLoss:0.1585 | top1:97.2586 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.3584 | MainLoss:0.3584 | SPLoss:0.5487 | CLSLoss:0.1585 | top1:81.9299 | AUROC:0.9917\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.159517\n",
      "Train | 8/8 | Loss:0.4397 | MainLoss:0.3639 | Alpha:0.1087 | SPLoss:0.5359 | CLSLoss:0.1616 | top1:84.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1342 | MainLoss:0.1342 | SPLoss:0.5176 | CLSLoss:0.1648 | top1:97.0499 | AUROC:0.9983\n",
      "Test | 62/8 | Loss:0.3520 | MainLoss:0.3520 | SPLoss:0.5176 | CLSLoss:0.1648 | top1:82.6081 | AUROC:0.9937\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.159489\n",
      "Train | 8/8 | Loss:0.4502 | MainLoss:0.3730 | Alpha:0.1148 | SPLoss:0.5109 | CLSLoss:0.1613 | top1:82.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.3147 | MainLoss:0.3147 | SPLoss:0.4944 | CLSLoss:0.1477 | top1:86.0062 | AUROC:0.9967\n",
      "Test | 62/8 | Loss:0.2186 | MainLoss:0.2186 | SPLoss:0.4944 | CLSLoss:0.1477 | top1:93.4338 | AUROC:0.9914\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.159460\n",
      "Train | 8/8 | Loss:0.4579 | MainLoss:0.3803 | Alpha:0.1162 | SPLoss:0.5199 | CLSLoss:0.1485 | top1:83.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1548 | MainLoss:0.1548 | SPLoss:0.5120 | CLSLoss:0.1480 | top1:96.9470 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3485 | MainLoss:0.3485 | SPLoss:0.5120 | CLSLoss:0.1480 | top1:82.1199 | AUROC:0.9932\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.159431\n",
      "Train | 8/8 | Loss:0.4404 | MainLoss:0.3651 | Alpha:0.1145 | SPLoss:0.5025 | CLSLoss:0.1545 | top1:83.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2272 | MainLoss:0.2272 | SPLoss:0.4822 | CLSLoss:0.1572 | top1:92.1558 | AUROC:0.9975\n",
      "Test | 62/8 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:0.4822 | CLSLoss:0.1572 | top1:91.0452 | AUROC:0.9956\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.159400\n",
      "Train | 8/8 | Loss:0.4395 | MainLoss:0.3872 | Alpha:0.4110 | SPLoss:0.0118 | CLSLoss:0.1153 | top1:82.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1221 | MainLoss:0.1221 | SPLoss:0.0323 | CLSLoss:0.0900 | top1:98.8785 | AUROC:0.9995\n",
      "Test | 62/8 | Loss:0.6183 | MainLoss:0.6183 | SPLoss:0.0323 | CLSLoss:0.0900 | top1:64.9771 | AUROC:0.9830\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.015937\n",
      "Train | 8/8 | Loss:0.4417 | MainLoss:0.3955 | Alpha:0.4112 | SPLoss:0.0266 | CLSLoss:0.0857 | top1:82.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1501 | MainLoss:0.1501 | SPLoss:0.0206 | CLSLoss:0.0817 | top1:98.6355 | AUROC:0.9994\n",
      "Test | 62/8 | Loss:0.4163 | MainLoss:0.4163 | SPLoss:0.0206 | CLSLoss:0.0817 | top1:78.0341 | AUROC:0.9898\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.015934\n",
      "Train | 8/8 | Loss:0.4136 | MainLoss:0.3725 | Alpha:0.4109 | SPLoss:0.0187 | CLSLoss:0.0814 | top1:83.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1717 | MainLoss:0.1717 | SPLoss:0.0171 | CLSLoss:0.0819 | top1:97.4393 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.3475 | MainLoss:0.3475 | SPLoss:0.0171 | CLSLoss:0.0819 | top1:84.0629 | AUROC:0.9907\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.015930\n",
      "Train | 8/8 | Loss:0.4064 | MainLoss:0.3656 | Alpha:0.4112 | SPLoss:0.0164 | CLSLoss:0.0828 | top1:84.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1839 | MainLoss:0.1839 | SPLoss:0.0157 | CLSLoss:0.0833 | top1:96.3956 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3125 | MainLoss:0.3125 | SPLoss:0.0157 | CLSLoss:0.0833 | top1:86.9332 | AUROC:0.9913\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.015927\n",
      "Train | 8/8 | Loss:0.4070 | MainLoss:0.3665 | Alpha:0.4102 | SPLoss:0.0157 | CLSLoss:0.0831 | top1:84.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1778 | MainLoss:0.1778 | SPLoss:0.0157 | CLSLoss:0.0840 | top1:96.5389 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3148 | MainLoss:0.3148 | SPLoss:0.0157 | CLSLoss:0.0840 | top1:86.5531 | AUROC:0.9915\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.015924\n",
      "Train | 8/8 | Loss:0.4032 | MainLoss:0.3622 | Alpha:0.4114 | SPLoss:0.0156 | CLSLoss:0.0841 | top1:84.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1775 | MainLoss:0.1775 | SPLoss:0.0158 | CLSLoss:0.0845 | top1:96.3302 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:0.0158 | CLSLoss:0.0845 | top1:87.0315 | AUROC:0.9919\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.015920\n",
      "Train | 8/8 | Loss:0.4028 | MainLoss:0.3616 | Alpha:0.4105 | SPLoss:0.0159 | CLSLoss:0.0845 | top1:84.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1762 | MainLoss:0.1762 | SPLoss:0.0157 | CLSLoss:0.0844 | top1:96.3146 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3066 | MainLoss:0.3066 | SPLoss:0.0157 | CLSLoss:0.0844 | top1:87.1887 | AUROC:0.9922\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.015917\n",
      "Train | 8/8 | Loss:0.4013 | MainLoss:0.3604 | Alpha:0.4107 | SPLoss:0.0156 | CLSLoss:0.0839 | top1:84.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1704 | MainLoss:0.1704 | SPLoss:0.0159 | CLSLoss:0.0842 | top1:96.6262 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3137 | MainLoss:0.3137 | SPLoss:0.0159 | CLSLoss:0.0842 | top1:86.5236 | AUROC:0.9926\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.015913\n",
      "Train | 8/8 | Loss:0.3912 | MainLoss:0.3499 | Alpha:0.4127 | SPLoss:0.0158 | CLSLoss:0.0844 | top1:84.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1732 | MainLoss:0.1732 | SPLoss:0.0162 | CLSLoss:0.0849 | top1:96.2741 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3023 | MainLoss:0.3023 | SPLoss:0.0162 | CLSLoss:0.0849 | top1:87.3820 | AUROC:0.9927\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.015909\n",
      "Train | 8/8 | Loss:0.3974 | MainLoss:0.3558 | Alpha:0.4116 | SPLoss:0.0162 | CLSLoss:0.0848 | top1:84.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1777 | MainLoss:0.1777 | SPLoss:0.0162 | CLSLoss:0.0846 | top1:95.9065 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2937 | MainLoss:0.2937 | SPLoss:0.0162 | CLSLoss:0.0846 | top1:87.9849 | AUROC:0.9929\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.015905\n",
      "Train | 8/8 | Loss:0.3970 | MainLoss:0.3554 | Alpha:0.4119 | SPLoss:0.0164 | CLSLoss:0.0847 | top1:85.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1766 | MainLoss:0.1766 | SPLoss:0.0165 | CLSLoss:0.0839 | top1:96.0062 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2963 | MainLoss:0.2963 | SPLoss:0.0165 | CLSLoss:0.0839 | top1:87.7687 | AUROC:0.9930\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.015902\n",
      "Train | 8/8 | Loss:0.3914 | MainLoss:0.3497 | Alpha:0.4129 | SPLoss:0.0168 | CLSLoss:0.0841 | top1:85.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1699 | MainLoss:0.1699 | SPLoss:0.0178 | CLSLoss:0.0846 | top1:96.2368 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2997 | MainLoss:0.2997 | SPLoss:0.0178 | CLSLoss:0.0846 | top1:87.4640 | AUROC:0.9930\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.015898\n",
      "Train | 8/8 | Loss:0.3941 | MainLoss:0.3521 | Alpha:0.4125 | SPLoss:0.0177 | CLSLoss:0.0842 | top1:84.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1722 | MainLoss:0.1722 | SPLoss:0.0181 | CLSLoss:0.0841 | top1:96.0062 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2956 | MainLoss:0.2956 | SPLoss:0.0181 | CLSLoss:0.0841 | top1:87.7490 | AUROC:0.9932\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.015893\n",
      "Train | 8/8 | Loss:0.4025 | MainLoss:0.3609 | Alpha:0.4104 | SPLoss:0.0181 | CLSLoss:0.0835 | top1:84.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1861 | MainLoss:0.1861 | SPLoss:0.0179 | CLSLoss:0.0825 | top1:95.2741 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2797 | MainLoss:0.2797 | SPLoss:0.0179 | CLSLoss:0.0825 | top1:88.9876 | AUROC:0.9933\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.015889\n",
      "Train | 8/8 | Loss:0.3970 | MainLoss:0.3556 | Alpha:0.4103 | SPLoss:0.0184 | CLSLoss:0.0825 | top1:84.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1771 | MainLoss:0.1771 | SPLoss:0.0183 | CLSLoss:0.0817 | top1:95.8879 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2949 | MainLoss:0.2949 | SPLoss:0.0183 | CLSLoss:0.0817 | top1:87.8571 | AUROC:0.9935\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.015885\n",
      "Train | 8/8 | Loss:0.3977 | MainLoss:0.3567 | Alpha:0.4110 | SPLoss:0.0183 | CLSLoss:0.0815 | top1:84.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1810 | MainLoss:0.1810 | SPLoss:0.0186 | CLSLoss:0.0815 | top1:95.5732 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2880 | MainLoss:0.2880 | SPLoss:0.0186 | CLSLoss:0.0815 | top1:88.3781 | AUROC:0.9934\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.015881\n",
      "Train | 8/8 | Loss:0.4042 | MainLoss:0.3634 | Alpha:0.4081 | SPLoss:0.0189 | CLSLoss:0.0811 | top1:84.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1703 | MainLoss:0.1703 | SPLoss:0.0192 | CLSLoss:0.0805 | top1:96.3053 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3097 | MainLoss:0.3097 | SPLoss:0.0192 | CLSLoss:0.0805 | top1:86.7923 | AUROC:0.9934\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.015877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.4006 | MainLoss:0.3601 | Alpha:0.4073 | SPLoss:0.0194 | CLSLoss:0.0799 | top1:84.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1656 | MainLoss:0.1656 | SPLoss:0.0196 | CLSLoss:0.0797 | top1:96.6511 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3180 | MainLoss:0.3180 | SPLoss:0.0196 | CLSLoss:0.0797 | top1:86.2713 | AUROC:0.9933\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.015872\n",
      "Train | 8/8 | Loss:0.4013 | MainLoss:0.3610 | Alpha:0.4095 | SPLoss:0.0193 | CLSLoss:0.0790 | top1:84.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1697 | MainLoss:0.1697 | SPLoss:0.0195 | CLSLoss:0.0793 | top1:96.4673 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3104 | MainLoss:0.3104 | SPLoss:0.0195 | CLSLoss:0.0793 | top1:86.8250 | AUROC:0.9935\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.015868\n",
      "Train | 8/8 | Loss:0.3979 | MainLoss:0.3572 | Alpha:0.4100 | SPLoss:0.0197 | CLSLoss:0.0795 | top1:84.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1717 | MainLoss:0.1717 | SPLoss:0.0199 | CLSLoss:0.0792 | top1:96.2866 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3047 | MainLoss:0.3047 | SPLoss:0.0199 | CLSLoss:0.0792 | top1:87.2477 | AUROC:0.9934\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.015863\n",
      "Train | 8/8 | Loss:0.4018 | MainLoss:0.3612 | Alpha:0.4097 | SPLoss:0.0200 | CLSLoss:0.0789 | top1:84.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1828 | MainLoss:0.1828 | SPLoss:0.0198 | CLSLoss:0.0786 | top1:95.7321 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2890 | MainLoss:0.2890 | SPLoss:0.0198 | CLSLoss:0.0786 | top1:88.3912 | AUROC:0.9936\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.015858\n",
      "Train | 8/8 | Loss:0.4009 | MainLoss:0.3604 | Alpha:0.4095 | SPLoss:0.0202 | CLSLoss:0.0786 | top1:84.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1739 | MainLoss:0.1739 | SPLoss:0.0204 | CLSLoss:0.0788 | top1:96.1402 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3010 | MainLoss:0.3010 | SPLoss:0.0204 | CLSLoss:0.0788 | top1:87.4803 | AUROC:0.9936\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.015854\n",
      "Train | 8/8 | Loss:0.3951 | MainLoss:0.3541 | Alpha:0.4114 | SPLoss:0.0206 | CLSLoss:0.0789 | top1:85.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1797 | MainLoss:0.1797 | SPLoss:0.0208 | CLSLoss:0.0792 | top1:95.6916 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2877 | MainLoss:0.2877 | SPLoss:0.0208 | CLSLoss:0.0792 | top1:88.4994 | AUROC:0.9937\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.015849\n",
      "Train | 8/8 | Loss:0.3974 | MainLoss:0.3564 | Alpha:0.4111 | SPLoss:0.0209 | CLSLoss:0.0789 | top1:84.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1734 | MainLoss:0.1734 | SPLoss:0.0211 | CLSLoss:0.0790 | top1:96.0685 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2989 | MainLoss:0.2989 | SPLoss:0.0211 | CLSLoss:0.0790 | top1:87.6343 | AUROC:0.9936\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.015844\n",
      "Train | 8/8 | Loss:0.3930 | MainLoss:0.3520 | Alpha:0.4113 | SPLoss:0.0211 | CLSLoss:0.0788 | top1:84.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:0.0213 | CLSLoss:0.0789 | top1:96.2960 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3037 | MainLoss:0.3037 | SPLoss:0.0213 | CLSLoss:0.0789 | top1:87.2018 | AUROC:0.9937\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.015839\n",
      "Train | 8/8 | Loss:0.3988 | MainLoss:0.3577 | Alpha:0.4104 | SPLoss:0.0212 | CLSLoss:0.0788 | top1:84.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1773 | MainLoss:0.1773 | SPLoss:0.0215 | CLSLoss:0.0789 | top1:95.8816 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2929 | MainLoss:0.2929 | SPLoss:0.0215 | CLSLoss:0.0789 | top1:87.9686 | AUROC:0.9935\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.015834\n",
      "Train | 8/8 | Loss:0.3974 | MainLoss:0.3562 | Alpha:0.4101 | SPLoss:0.0216 | CLSLoss:0.0788 | top1:84.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1813 | MainLoss:0.1813 | SPLoss:0.0215 | CLSLoss:0.0786 | top1:95.6293 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2874 | MainLoss:0.2874 | SPLoss:0.0215 | CLSLoss:0.0786 | top1:88.3650 | AUROC:0.9936\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.015829\n",
      "Train | 8/8 | Loss:0.3937 | MainLoss:0.3521 | Alpha:0.4127 | SPLoss:0.0218 | CLSLoss:0.0790 | top1:85.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1700 | MainLoss:0.1700 | SPLoss:0.0223 | CLSLoss:0.0788 | top1:96.1745 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3028 | MainLoss:0.3028 | SPLoss:0.0223 | CLSLoss:0.0788 | top1:87.3132 | AUROC:0.9936\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.015823\n",
      "Train | 8/8 | Loss:0.3999 | MainLoss:0.3584 | Alpha:0.4104 | SPLoss:0.0221 | CLSLoss:0.0788 | top1:84.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1721 | MainLoss:0.1721 | SPLoss:0.0219 | CLSLoss:0.0776 | top1:96.1713 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3068 | MainLoss:0.3068 | SPLoss:0.0219 | CLSLoss:0.0776 | top1:87.1035 | AUROC:0.9937\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.015818\n",
      "Train | 8/8 | Loss:0.3965 | MainLoss:0.3555 | Alpha:0.4106 | SPLoss:0.0221 | CLSLoss:0.0779 | top1:84.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1813 | MainLoss:0.1813 | SPLoss:0.0219 | CLSLoss:0.0777 | top1:95.6480 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2917 | MainLoss:0.2917 | SPLoss:0.0219 | CLSLoss:0.0777 | top1:88.1586 | AUROC:0.9936\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.015813\n",
      "Train | 8/8 | Loss:0.3983 | MainLoss:0.3573 | Alpha:0.4106 | SPLoss:0.0220 | CLSLoss:0.0779 | top1:84.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1751 | MainLoss:0.1751 | SPLoss:0.0224 | CLSLoss:0.0782 | top1:95.9190 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2975 | MainLoss:0.2975 | SPLoss:0.0224 | CLSLoss:0.0782 | top1:87.7883 | AUROC:0.9938\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.015807\n",
      "Train | 8/8 | Loss:0.3981 | MainLoss:0.3569 | Alpha:0.4105 | SPLoss:0.0224 | CLSLoss:0.0780 | top1:84.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1725 | MainLoss:0.1725 | SPLoss:0.0228 | CLSLoss:0.0785 | top1:95.9688 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2998 | MainLoss:0.2998 | SPLoss:0.0228 | CLSLoss:0.0785 | top1:87.6409 | AUROC:0.9936\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.015802\n",
      "Train | 8/8 | Loss:0.3956 | MainLoss:0.3541 | Alpha:0.4111 | SPLoss:0.0226 | CLSLoss:0.0782 | top1:84.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1742 | MainLoss:0.1742 | SPLoss:0.0229 | CLSLoss:0.0787 | top1:95.8754 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2964 | MainLoss:0.2964 | SPLoss:0.0229 | CLSLoss:0.0787 | top1:87.8408 | AUROC:0.9938\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.015796\n",
      "Train | 8/8 | Loss:0.3791 | MainLoss:0.3365 | Alpha:0.4141 | SPLoss:0.0234 | CLSLoss:0.0794 | top1:86.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1752 | MainLoss:0.1752 | SPLoss:0.0238 | CLSLoss:0.0802 | top1:95.5576 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2892 | MainLoss:0.2892 | SPLoss:0.0238 | CLSLoss:0.0802 | top1:88.2864 | AUROC:0.9937\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.015791\n",
      "Train | 8/8 | Loss:0.3967 | MainLoss:0.3543 | Alpha:0.4103 | SPLoss:0.0237 | CLSLoss:0.0796 | top1:84.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:0.0238 | CLSLoss:0.0789 | top1:96.1215 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3060 | MainLoss:0.3060 | SPLoss:0.0238 | CLSLoss:0.0789 | top1:87.1265 | AUROC:0.9939\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.015785\n",
      "Train | 8/8 | Loss:0.3921 | MainLoss:0.3500 | Alpha:0.4119 | SPLoss:0.0237 | CLSLoss:0.0785 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1667 | MainLoss:0.1667 | SPLoss:0.0242 | CLSLoss:0.0790 | top1:96.2150 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3071 | MainLoss:0.3071 | SPLoss:0.0242 | CLSLoss:0.0790 | top1:87.0216 | AUROC:0.9938\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.015779\n",
      "Train | 8/8 | Loss:0.3997 | MainLoss:0.3579 | Alpha:0.4104 | SPLoss:0.0236 | CLSLoss:0.0782 | top1:84.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1722 | MainLoss:0.1722 | SPLoss:0.0237 | CLSLoss:0.0782 | top1:96.0374 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2996 | MainLoss:0.2996 | SPLoss:0.0237 | CLSLoss:0.0782 | top1:87.5426 | AUROC:0.9938\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.015773\n",
      "Train | 8/8 | Loss:0.3999 | MainLoss:0.3584 | Alpha:0.4104 | SPLoss:0.0235 | CLSLoss:0.0779 | top1:84.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1703 | MainLoss:0.1703 | SPLoss:0.0236 | CLSLoss:0.0778 | top1:96.2523 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3076 | MainLoss:0.3076 | SPLoss:0.0236 | CLSLoss:0.0778 | top1:86.9921 | AUROC:0.9937\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.015767\n",
      "Train | 8/8 | Loss:0.3914 | MainLoss:0.3492 | Alpha:0.4115 | SPLoss:0.0242 | CLSLoss:0.0783 | top1:85.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1692 | MainLoss:0.1692 | SPLoss:0.0241 | CLSLoss:0.0783 | top1:96.2523 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3052 | MainLoss:0.3052 | SPLoss:0.0241 | CLSLoss:0.0783 | top1:87.0544 | AUROC:0.9938\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.015761\n",
      "Train | 8/8 | Loss:0.3945 | MainLoss:0.3524 | Alpha:0.4104 | SPLoss:0.0242 | CLSLoss:0.0783 | top1:84.9500 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.1730 | MainLoss:0.1730 | SPLoss:0.0242 | CLSLoss:0.0779 | top1:96.1308 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3025 | MainLoss:0.3025 | SPLoss:0.0242 | CLSLoss:0.0779 | top1:87.1691 | AUROC:0.9940\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.015755\n",
      "Train | 8/8 | Loss:0.3875 | MainLoss:0.3451 | Alpha:0.4116 | SPLoss:0.0246 | CLSLoss:0.0784 | top1:85.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1767 | MainLoss:0.1767 | SPLoss:0.0245 | CLSLoss:0.0782 | top1:95.9065 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2949 | MainLoss:0.2949 | SPLoss:0.0245 | CLSLoss:0.0782 | top1:87.7228 | AUROC:0.9938\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.015749\n",
      "Train | 8/8 | Loss:0.3895 | MainLoss:0.3469 | Alpha:0.4132 | SPLoss:0.0247 | CLSLoss:0.0782 | top1:85.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1814 | MainLoss:0.1814 | SPLoss:0.0249 | CLSLoss:0.0789 | top1:95.4922 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2832 | MainLoss:0.2832 | SPLoss:0.0249 | CLSLoss:0.0789 | top1:88.5190 | AUROC:0.9937\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.015742\n",
      "Train | 8/8 | Loss:0.3898 | MainLoss:0.3470 | Alpha:0.4123 | SPLoss:0.0251 | CLSLoss:0.0788 | top1:85.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1709 | MainLoss:0.1709 | SPLoss:0.0254 | CLSLoss:0.0789 | top1:96.0031 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2979 | MainLoss:0.2979 | SPLoss:0.0254 | CLSLoss:0.0789 | top1:87.4967 | AUROC:0.9937\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.015736\n",
      "Train | 8/8 | Loss:0.3943 | MainLoss:0.3515 | Alpha:0.4104 | SPLoss:0.0257 | CLSLoss:0.0786 | top1:85.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1743 | MainLoss:0.1743 | SPLoss:0.0255 | CLSLoss:0.0779 | top1:95.9907 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2933 | MainLoss:0.2933 | SPLoss:0.0255 | CLSLoss:0.0779 | top1:87.7392 | AUROC:0.9937\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.015730\n",
      "Train | 8/8 | Loss:0.3939 | MainLoss:0.3515 | Alpha:0.4115 | SPLoss:0.0254 | CLSLoss:0.0776 | top1:84.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1714 | MainLoss:0.1714 | SPLoss:0.0255 | CLSLoss:0.0778 | top1:96.1994 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3005 | MainLoss:0.3005 | SPLoss:0.0255 | CLSLoss:0.0778 | top1:87.2739 | AUROC:0.9939\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.015723\n",
      "Train | 8/8 | Loss:0.4017 | MainLoss:0.3596 | Alpha:0.4091 | SPLoss:0.0253 | CLSLoss:0.0775 | top1:84.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1790 | MainLoss:0.1790 | SPLoss:0.0253 | CLSLoss:0.0771 | top1:95.8006 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2919 | MainLoss:0.2919 | SPLoss:0.0253 | CLSLoss:0.0771 | top1:87.9391 | AUROC:0.9939\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.015716\n",
      "Train | 8/8 | Loss:0.3890 | MainLoss:0.3466 | Alpha:0.4114 | SPLoss:0.0257 | CLSLoss:0.0773 | top1:85.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1774 | MainLoss:0.1774 | SPLoss:0.0253 | CLSLoss:0.0773 | top1:95.8941 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2918 | MainLoss:0.2918 | SPLoss:0.0253 | CLSLoss:0.0773 | top1:87.9784 | AUROC:0.9939\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.015710\n",
      "Train | 8/8 | Loss:0.3925 | MainLoss:0.3501 | Alpha:0.4121 | SPLoss:0.0253 | CLSLoss:0.0774 | top1:85.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1787 | MainLoss:0.1787 | SPLoss:0.0256 | CLSLoss:0.0780 | top1:95.6417 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2869 | MainLoss:0.2869 | SPLoss:0.0256 | CLSLoss:0.0780 | top1:88.3191 | AUROC:0.9938\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.015703\n",
      "Train | 8/8 | Loss:0.3955 | MainLoss:0.3530 | Alpha:0.4098 | SPLoss:0.0257 | CLSLoss:0.0780 | top1:84.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:0.0257 | CLSLoss:0.0778 | top1:96.2835 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3055 | MainLoss:0.3055 | SPLoss:0.0257 | CLSLoss:0.0778 | top1:87.0020 | AUROC:0.9935\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.015696\n",
      "Train | 8/8 | Loss:0.3967 | MainLoss:0.3544 | Alpha:0.4099 | SPLoss:0.0254 | CLSLoss:0.0776 | top1:84.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1611 | MainLoss:0.1611 | SPLoss:0.0255 | CLSLoss:0.0772 | top1:96.8287 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3208 | MainLoss:0.3208 | SPLoss:0.0255 | CLSLoss:0.0772 | top1:85.8978 | AUROC:0.9938\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.015689\n",
      "Train | 8/8 | Loss:0.3842 | MainLoss:0.3523 | Alpha:0.4153 | SPLoss:0.0002 | CLSLoss:0.0767 | top1:85.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1676 | MainLoss:0.1676 | SPLoss:0.0003 | CLSLoss:0.0771 | top1:96.3520 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3044 | MainLoss:0.3044 | SPLoss:0.0003 | CLSLoss:0.0771 | top1:87.0708 | AUROC:0.9937\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.015682\n",
      "Train | 8/8 | Loss:0.3894 | MainLoss:0.3574 | Alpha:0.4138 | SPLoss:0.0004 | CLSLoss:0.0768 | top1:84.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1819 | MainLoss:0.1819 | SPLoss:0.0006 | CLSLoss:0.0759 | top1:95.7664 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2866 | MainLoss:0.2866 | SPLoss:0.0006 | CLSLoss:0.0759 | top1:88.3585 | AUROC:0.9938\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.015675\n",
      "Train | 8/8 | Loss:0.3737 | MainLoss:0.3414 | Alpha:0.4174 | SPLoss:0.0007 | CLSLoss:0.0765 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1735 | MainLoss:0.1735 | SPLoss:0.0010 | CLSLoss:0.0770 | top1:96.0467 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2901 | MainLoss:0.2901 | SPLoss:0.0010 | CLSLoss:0.0770 | top1:87.9882 | AUROC:0.9937\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.015668\n",
      "Train | 8/8 | Loss:0.3776 | MainLoss:0.3450 | Alpha:0.4161 | SPLoss:0.0012 | CLSLoss:0.0772 | top1:85.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1850 | MainLoss:0.1850 | SPLoss:0.0016 | CLSLoss:0.0767 | top1:95.3271 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.2715 | MainLoss:0.2715 | SPLoss:0.0016 | CLSLoss:0.0767 | top1:89.2693 | AUROC:0.9935\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.015661\n",
      "Train | 8/8 | Loss:0.3873 | MainLoss:0.3552 | Alpha:0.4136 | SPLoss:0.0017 | CLSLoss:0.0761 | top1:84.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1842 | MainLoss:0.1842 | SPLoss:0.0020 | CLSLoss:0.0753 | top1:95.4735 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.2755 | MainLoss:0.2755 | SPLoss:0.0020 | CLSLoss:0.0753 | top1:89.1186 | AUROC:0.9935\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.015654\n",
      "Train | 8/8 | Loss:0.3774 | MainLoss:0.3451 | Alpha:0.4153 | SPLoss:0.0022 | CLSLoss:0.0755 | top1:85.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1743 | MainLoss:0.1743 | SPLoss:0.0024 | CLSLoss:0.0757 | top1:95.9408 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.2852 | MainLoss:0.2852 | SPLoss:0.0024 | CLSLoss:0.0757 | top1:88.3060 | AUROC:0.9935\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.015646\n",
      "Train | 8/8 | Loss:0.3757 | MainLoss:0.3430 | Alpha:0.4165 | SPLoss:0.0025 | CLSLoss:0.0760 | top1:85.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1766 | MainLoss:0.1766 | SPLoss:0.0029 | CLSLoss:0.0759 | top1:95.8411 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.2794 | MainLoss:0.2794 | SPLoss:0.0029 | CLSLoss:0.0759 | top1:88.7222 | AUROC:0.9935\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.015639\n",
      "Train | 8/8 | Loss:0.3816 | MainLoss:0.3490 | Alpha:0.4147 | SPLoss:0.0031 | CLSLoss:0.0757 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1706 | MainLoss:0.1706 | SPLoss:0.0034 | CLSLoss:0.0753 | top1:96.0779 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.2851 | MainLoss:0.2851 | SPLoss:0.0034 | CLSLoss:0.0753 | top1:88.4207 | AUROC:0.9932\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.015631\n",
      "Train | 8/8 | Loss:0.3773 | MainLoss:0.3448 | Alpha:0.4155 | SPLoss:0.0036 | CLSLoss:0.0748 | top1:85.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1643 | MainLoss:0.1643 | SPLoss:0.0039 | CLSLoss:0.0751 | top1:96.3427 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2929 | MainLoss:0.2929 | SPLoss:0.0039 | CLSLoss:0.0751 | top1:87.9227 | AUROC:0.9934\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.015624\n",
      "Train | 8/8 | Loss:0.3835 | MainLoss:0.3508 | Alpha:0.4136 | SPLoss:0.0042 | CLSLoss:0.0749 | top1:84.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1609 | MainLoss:0.1609 | SPLoss:0.0043 | CLSLoss:0.0745 | top1:96.6168 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3035 | MainLoss:0.3035 | SPLoss:0.0043 | CLSLoss:0.0745 | top1:87.3067 | AUROC:0.9934\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.015616\n",
      "Train | 8/8 | Loss:0.3798 | MainLoss:0.3468 | Alpha:0.4146 | SPLoss:0.0047 | CLSLoss:0.0747 | top1:85.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1768 | MainLoss:0.1768 | SPLoss:0.0050 | CLSLoss:0.0738 | top1:95.7570 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.2809 | MainLoss:0.2809 | SPLoss:0.0050 | CLSLoss:0.0738 | top1:88.8434 | AUROC:0.9933\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.015608\n",
      "Train | 8/8 | Loss:0.3805 | MainLoss:0.3477 | Alpha:0.4147 | SPLoss:0.0053 | CLSLoss:0.0738 | top1:85.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1729 | MainLoss:0.1729 | SPLoss:0.0056 | CLSLoss:0.0741 | top1:95.8785 | AUROC:0.9988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.2865 | MainLoss:0.2865 | SPLoss:0.0056 | CLSLoss:0.0741 | top1:88.4535 | AUROC:0.9931\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.015601\n",
      "Train | 8/8 | Loss:0.3746 | MainLoss:0.3413 | Alpha:0.4154 | SPLoss:0.0059 | CLSLoss:0.0744 | top1:85.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1682 | MainLoss:0.1682 | SPLoss:0.0060 | CLSLoss:0.0744 | top1:96.0592 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.2901 | MainLoss:0.2901 | SPLoss:0.0060 | CLSLoss:0.0744 | top1:88.1520 | AUROC:0.9933\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.015593\n",
      "Train | 8/8 | Loss:0.3678 | MainLoss:0.3339 | Alpha:0.4173 | SPLoss:0.0065 | CLSLoss:0.0748 | top1:86.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1660 | MainLoss:0.1660 | SPLoss:0.0067 | CLSLoss:0.0748 | top1:95.9969 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.2904 | MainLoss:0.2904 | SPLoss:0.0067 | CLSLoss:0.0748 | top1:88.2176 | AUROC:0.9927\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.015585\n",
      "Train | 8/8 | Loss:0.3783 | MainLoss:0.3445 | Alpha:0.4156 | SPLoss:0.0069 | CLSLoss:0.0743 | top1:85.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1766 | MainLoss:0.1766 | SPLoss:0.0073 | CLSLoss:0.0744 | top1:95.5732 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2783 | MainLoss:0.2783 | SPLoss:0.0073 | CLSLoss:0.0744 | top1:88.9744 | AUROC:0.9932\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.015577\n",
      "Train | 8/8 | Loss:0.3764 | MainLoss:0.3425 | Alpha:0.4156 | SPLoss:0.0074 | CLSLoss:0.0743 | top1:85.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1693 | MainLoss:0.1693 | SPLoss:0.0075 | CLSLoss:0.0741 | top1:96.1028 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2917 | MainLoss:0.2917 | SPLoss:0.0075 | CLSLoss:0.0741 | top1:87.9456 | AUROC:0.9931\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.015569\n",
      "Train | 8/8 | Loss:0.3778 | MainLoss:0.3439 | Alpha:0.4144 | SPLoss:0.0078 | CLSLoss:0.0740 | top1:85.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1645 | MainLoss:0.1645 | SPLoss:0.0081 | CLSLoss:0.0747 | top1:96.1900 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2954 | MainLoss:0.2954 | SPLoss:0.0081 | CLSLoss:0.0747 | top1:87.7589 | AUROC:0.9930\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.015561\n",
      "Train | 8/8 | Loss:0.3743 | MainLoss:0.3398 | Alpha:0.4160 | SPLoss:0.0085 | CLSLoss:0.0745 | top1:85.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1623 | MainLoss:0.1623 | SPLoss:0.0086 | CLSLoss:0.0748 | top1:96.2897 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2978 | MainLoss:0.2978 | SPLoss:0.0086 | CLSLoss:0.0748 | top1:87.5688 | AUROC:0.9928\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.015552\n",
      "Train | 8/8 | Loss:0.3901 | MainLoss:0.3560 | Alpha:0.4121 | SPLoss:0.0087 | CLSLoss:0.0742 | top1:84.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1710 | MainLoss:0.1710 | SPLoss:0.0090 | CLSLoss:0.0728 | top1:96.1869 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2948 | MainLoss:0.2948 | SPLoss:0.0090 | CLSLoss:0.0728 | top1:87.8244 | AUROC:0.9931\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.015544\n",
      "Train | 8/8 | Loss:0.3745 | MainLoss:0.3403 | Alpha:0.4162 | SPLoss:0.0091 | CLSLoss:0.0732 | top1:85.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1623 | MainLoss:0.1623 | SPLoss:0.0093 | CLSLoss:0.0737 | top1:96.5016 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3019 | MainLoss:0.3019 | SPLoss:0.0093 | CLSLoss:0.0737 | top1:87.2706 | AUROC:0.9929\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.015536\n",
      "Train | 8/8 | Loss:0.3751 | MainLoss:0.3403 | Alpha:0.4161 | SPLoss:0.0096 | CLSLoss:0.0741 | top1:85.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.0096 | CLSLoss:0.0739 | top1:96.7913 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3129 | MainLoss:0.3129 | SPLoss:0.0096 | CLSLoss:0.0739 | top1:86.4515 | AUROC:0.9928\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.015527\n",
      "Train | 8/8 | Loss:0.3891 | MainLoss:0.3550 | Alpha:0.4132 | SPLoss:0.0097 | CLSLoss:0.0728 | top1:84.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1741 | MainLoss:0.1741 | SPLoss:0.0099 | CLSLoss:0.0725 | top1:96.0218 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2867 | MainLoss:0.2867 | SPLoss:0.0099 | CLSLoss:0.0725 | top1:88.3159 | AUROC:0.9931\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.015518\n",
      "Train | 8/8 | Loss:0.3738 | MainLoss:0.3391 | Alpha:0.4164 | SPLoss:0.0101 | CLSLoss:0.0733 | top1:86.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1721 | MainLoss:0.1721 | SPLoss:0.0103 | CLSLoss:0.0732 | top1:95.9875 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2832 | MainLoss:0.2832 | SPLoss:0.0103 | CLSLoss:0.0732 | top1:88.5649 | AUROC:0.9932\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.015510\n",
      "Train | 8/8 | Loss:0.3858 | MainLoss:0.3512 | Alpha:0.4130 | SPLoss:0.0105 | CLSLoss:0.0733 | top1:85.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1664 | MainLoss:0.1664 | SPLoss:0.0106 | CLSLoss:0.0726 | top1:96.3676 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2949 | MainLoss:0.2949 | SPLoss:0.0106 | CLSLoss:0.0726 | top1:87.6835 | AUROC:0.9931\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.015501\n",
      "Train | 8/8 | Loss:0.3827 | MainLoss:0.3482 | Alpha:0.4134 | SPLoss:0.0107 | CLSLoss:0.0726 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1730 | MainLoss:0.1730 | SPLoss:0.0109 | CLSLoss:0.0722 | top1:96.0810 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2880 | MainLoss:0.2880 | SPLoss:0.0109 | CLSLoss:0.0722 | top1:88.1586 | AUROC:0.9932\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.015492\n",
      "Train | 8/8 | Loss:0.3764 | MainLoss:0.3416 | Alpha:0.4144 | SPLoss:0.0112 | CLSLoss:0.0727 | top1:85.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1723 | MainLoss:0.1723 | SPLoss:0.0113 | CLSLoss:0.0732 | top1:95.9159 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2840 | MainLoss:0.2840 | SPLoss:0.0113 | CLSLoss:0.0732 | top1:88.4109 | AUROC:0.9933\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.015484\n",
      "Train | 8/8 | Loss:0.3815 | MainLoss:0.3467 | Alpha:0.4141 | SPLoss:0.0114 | CLSLoss:0.0727 | top1:85.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1625 | MainLoss:0.1625 | SPLoss:0.0116 | CLSLoss:0.0734 | top1:96.4299 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2972 | MainLoss:0.2972 | SPLoss:0.0116 | CLSLoss:0.0734 | top1:87.5229 | AUROC:0.9932\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.015475\n",
      "Train | 8/8 | Loss:0.3885 | MainLoss:0.3536 | Alpha:0.4138 | SPLoss:0.0116 | CLSLoss:0.0727 | top1:84.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1792 | MainLoss:0.1792 | SPLoss:0.0118 | CLSLoss:0.0722 | top1:95.7632 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2770 | MainLoss:0.2770 | SPLoss:0.0118 | CLSLoss:0.0722 | top1:88.9515 | AUROC:0.9932\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.015466\n",
      "Train | 8/8 | Loss:0.3770 | MainLoss:0.3417 | Alpha:0.4150 | SPLoss:0.0121 | CLSLoss:0.0728 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1744 | MainLoss:0.1744 | SPLoss:0.0122 | CLSLoss:0.0732 | top1:95.7290 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2759 | MainLoss:0.2759 | SPLoss:0.0122 | CLSLoss:0.0732 | top1:89.0531 | AUROC:0.9932\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.015457\n",
      "Train | 8/8 | Loss:0.3770 | MainLoss:0.3415 | Alpha:0.4152 | SPLoss:0.0123 | CLSLoss:0.0731 | top1:85.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1732 | MainLoss:0.1732 | SPLoss:0.0127 | CLSLoss:0.0734 | top1:95.7259 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2748 | MainLoss:0.2748 | SPLoss:0.0127 | CLSLoss:0.0734 | top1:89.0957 | AUROC:0.9929\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.015447\n",
      "Train | 8/8 | Loss:0.3842 | MainLoss:0.3488 | Alpha:0.4141 | SPLoss:0.0128 | CLSLoss:0.0726 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1739 | MainLoss:0.1739 | SPLoss:0.0130 | CLSLoss:0.0726 | top1:95.7165 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2734 | MainLoss:0.2734 | SPLoss:0.0130 | CLSLoss:0.0726 | top1:89.2398 | AUROC:0.9930\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.015438\n",
      "Train | 8/8 | Loss:0.3786 | MainLoss:0.3429 | Alpha:0.4143 | SPLoss:0.0132 | CLSLoss:0.0728 | top1:85.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1764 | MainLoss:0.1764 | SPLoss:0.0133 | CLSLoss:0.0729 | top1:95.6044 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2696 | MainLoss:0.2696 | SPLoss:0.0133 | CLSLoss:0.0729 | top1:89.4364 | AUROC:0.9929\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.015429\n",
      "Train | 8/8 | Loss:0.3724 | MainLoss:0.3364 | Alpha:0.4166 | SPLoss:0.0133 | CLSLoss:0.0729 | top1:85.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1597 | MainLoss:0.1597 | SPLoss:0.0135 | CLSLoss:0.0733 | top1:96.4081 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2935 | MainLoss:0.2935 | SPLoss:0.0135 | CLSLoss:0.0733 | top1:87.8637 | AUROC:0.9929\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.015420\n",
      "Train | 8/8 | Loss:0.3779 | MainLoss:0.3419 | Alpha:0.4150 | SPLoss:0.0136 | CLSLoss:0.0732 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1637 | MainLoss:0.1637 | SPLoss:0.0138 | CLSLoss:0.0732 | top1:96.1464 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2886 | MainLoss:0.2886 | SPLoss:0.0138 | CLSLoss:0.0732 | top1:88.2798 | AUROC:0.9930\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.015410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.3758 | MainLoss:0.3397 | Alpha:0.4149 | SPLoss:0.0140 | CLSLoss:0.0733 | top1:85.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1506 | MainLoss:0.1506 | SPLoss:0.0139 | CLSLoss:0.0732 | top1:96.8661 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3130 | MainLoss:0.3130 | SPLoss:0.0139 | CLSLoss:0.0732 | top1:86.6186 | AUROC:0.9925\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.015401\n",
      "Train | 8/8 | Loss:0.3860 | MainLoss:0.3501 | Alpha:0.4137 | SPLoss:0.0140 | CLSLoss:0.0728 | top1:85.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1642 | MainLoss:0.1642 | SPLoss:0.0138 | CLSLoss:0.0716 | top1:96.3832 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2977 | MainLoss:0.2977 | SPLoss:0.0138 | CLSLoss:0.0716 | top1:87.7195 | AUROC:0.9929\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.015391\n",
      "Train | 8/8 | Loss:0.3811 | MainLoss:0.3457 | Alpha:0.4139 | SPLoss:0.0141 | CLSLoss:0.0714 | top1:84.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1599 | MainLoss:0.1599 | SPLoss:0.0142 | CLSLoss:0.0724 | top1:96.5763 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3019 | MainLoss:0.3019 | SPLoss:0.0142 | CLSLoss:0.0724 | top1:87.3231 | AUROC:0.9928\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.015381\n",
      "Train | 8/8 | Loss:0.3704 | MainLoss:0.3341 | Alpha:0.4168 | SPLoss:0.0143 | CLSLoss:0.0729 | top1:86.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1587 | MainLoss:0.1587 | SPLoss:0.0144 | CLSLoss:0.0732 | top1:96.4984 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2990 | MainLoss:0.2990 | SPLoss:0.0144 | CLSLoss:0.0732 | top1:87.5459 | AUROC:0.9927\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.015372\n",
      "Train | 8/8 | Loss:0.3861 | MainLoss:0.3501 | Alpha:0.4127 | SPLoss:0.0145 | CLSLoss:0.0726 | top1:85.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1713 | MainLoss:0.1713 | SPLoss:0.0146 | CLSLoss:0.0727 | top1:95.8474 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2794 | MainLoss:0.2794 | SPLoss:0.0146 | CLSLoss:0.0727 | top1:88.8401 | AUROC:0.9927\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.015362\n",
      "Train | 8/8 | Loss:0.3714 | MainLoss:0.3349 | Alpha:0.4171 | SPLoss:0.0147 | CLSLoss:0.0728 | top1:86.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1603 | MainLoss:0.1603 | SPLoss:0.0146 | CLSLoss:0.0724 | top1:96.4455 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2977 | MainLoss:0.2977 | SPLoss:0.0146 | CLSLoss:0.0724 | top1:87.6573 | AUROC:0.9926\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.015352\n",
      "Train | 8/8 | Loss:0.3813 | MainLoss:0.3455 | Alpha:0.4140 | SPLoss:0.0146 | CLSLoss:0.0721 | top1:85.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1685 | MainLoss:0.1685 | SPLoss:0.0147 | CLSLoss:0.0727 | top1:95.9720 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2830 | MainLoss:0.2830 | SPLoss:0.0147 | CLSLoss:0.0727 | top1:88.5878 | AUROC:0.9928\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.015342\n",
      "Train | 8/8 | Loss:0.3759 | MainLoss:0.3395 | Alpha:0.4158 | SPLoss:0.0148 | CLSLoss:0.0728 | top1:85.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1671 | MainLoss:0.1671 | SPLoss:0.0147 | CLSLoss:0.0727 | top1:96.1246 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2846 | MainLoss:0.2846 | SPLoss:0.0147 | CLSLoss:0.0727 | top1:88.4731 | AUROC:0.9930\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.015332\n",
      "Train | 8/8 | Loss:0.3763 | MainLoss:0.3402 | Alpha:0.4150 | SPLoss:0.0148 | CLSLoss:0.0724 | top1:85.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1772 | MainLoss:0.1772 | SPLoss:0.0152 | CLSLoss:0.0731 | top1:95.4673 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2684 | MainLoss:0.2684 | SPLoss:0.0152 | CLSLoss:0.0731 | top1:89.4528 | AUROC:0.9931\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.015322\n",
      "Train | 8/8 | Loss:0.3820 | MainLoss:0.3456 | Alpha:0.4140 | SPLoss:0.0152 | CLSLoss:0.0728 | top1:85.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1720 | MainLoss:0.1720 | SPLoss:0.0153 | CLSLoss:0.0727 | top1:95.8069 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2748 | MainLoss:0.2748 | SPLoss:0.0153 | CLSLoss:0.0727 | top1:89.0433 | AUROC:0.9928\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.015312\n",
      "Train | 8/8 | Loss:0.3746 | MainLoss:0.3380 | Alpha:0.4154 | SPLoss:0.0154 | CLSLoss:0.0728 | top1:86.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1699 | MainLoss:0.1699 | SPLoss:0.0152 | CLSLoss:0.0728 | top1:95.8723 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2778 | MainLoss:0.2778 | SPLoss:0.0152 | CLSLoss:0.0728 | top1:88.8565 | AUROC:0.9928\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.015302\n",
      "Train | 8/8 | Loss:0.3794 | MainLoss:0.3431 | Alpha:0.4143 | SPLoss:0.0152 | CLSLoss:0.0725 | top1:85.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1658 | MainLoss:0.1658 | SPLoss:0.0152 | CLSLoss:0.0722 | top1:96.1558 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2841 | MainLoss:0.2841 | SPLoss:0.0152 | CLSLoss:0.0722 | top1:88.5026 | AUROC:0.9928\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.015291\n",
      "Train | 8/8 | Loss:0.3702 | MainLoss:0.3336 | Alpha:0.4174 | SPLoss:0.0153 | CLSLoss:0.0724 | top1:86.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1703 | MainLoss:0.1703 | SPLoss:0.0154 | CLSLoss:0.0724 | top1:95.9408 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2781 | MainLoss:0.2781 | SPLoss:0.0154 | CLSLoss:0.0724 | top1:88.7844 | AUROC:0.9930\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.015281\n",
      "Train | 8/8 | Loss:0.3808 | MainLoss:0.3445 | Alpha:0.4144 | SPLoss:0.0154 | CLSLoss:0.0721 | top1:85.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1692 | MainLoss:0.1692 | SPLoss:0.0154 | CLSLoss:0.0726 | top1:96.0872 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2787 | MainLoss:0.2787 | SPLoss:0.0154 | CLSLoss:0.0726 | top1:88.6796 | AUROC:0.9932\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.015270\n",
      "Train | 8/8 | Loss:0.3748 | MainLoss:0.3381 | Alpha:0.4160 | SPLoss:0.0153 | CLSLoss:0.0729 | top1:85.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1476 | MainLoss:0.1476 | SPLoss:0.0154 | CLSLoss:0.0729 | top1:97.2025 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3159 | MainLoss:0.3159 | SPLoss:0.0154 | CLSLoss:0.0729 | top1:86.1501 | AUROC:0.9928\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.015260\n",
      "Train | 8/8 | Loss:0.3873 | MainLoss:0.3507 | Alpha:0.4128 | SPLoss:0.0156 | CLSLoss:0.0730 | top1:84.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1771 | MainLoss:0.1771 | SPLoss:0.0157 | CLSLoss:0.0719 | top1:95.6978 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2682 | MainLoss:0.2682 | SPLoss:0.0157 | CLSLoss:0.0719 | top1:89.3873 | AUROC:0.9933\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.015249\n",
      "Train | 8/8 | Loss:0.3817 | MainLoss:0.3520 | Alpha:0.4151 | SPLoss:0.0001 | CLSLoss:0.0715 | top1:84.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1689 | MainLoss:0.1689 | SPLoss:0.0002 | CLSLoss:0.0712 | top1:96.1838 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2798 | MainLoss:0.2798 | SPLoss:0.0002 | CLSLoss:0.0712 | top1:88.7025 | AUROC:0.9932\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.015239\n",
      "Train | 8/8 | Loss:0.3784 | MainLoss:0.3488 | Alpha:0.4159 | SPLoss:0.0003 | CLSLoss:0.0711 | top1:84.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1758 | MainLoss:0.1758 | SPLoss:0.0004 | CLSLoss:0.0712 | top1:95.7726 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2702 | MainLoss:0.2702 | SPLoss:0.0004 | CLSLoss:0.0712 | top1:89.3021 | AUROC:0.9930\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.015228\n",
      "Train | 8/8 | Loss:0.3668 | MainLoss:0.3366 | Alpha:0.4183 | SPLoss:0.0006 | CLSLoss:0.0716 | top1:85.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1623 | MainLoss:0.1623 | SPLoss:0.0007 | CLSLoss:0.0720 | top1:96.3645 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2854 | MainLoss:0.2854 | SPLoss:0.0007 | CLSLoss:0.0720 | top1:88.3617 | AUROC:0.9929\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.015217\n",
      "Train | 8/8 | Loss:0.3678 | MainLoss:0.3372 | Alpha:0.4181 | SPLoss:0.0009 | CLSLoss:0.0722 | top1:86.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1732 | MainLoss:0.1732 | SPLoss:0.0011 | CLSLoss:0.0721 | top1:95.6916 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2672 | MainLoss:0.2672 | SPLoss:0.0011 | CLSLoss:0.0721 | top1:89.4561 | AUROC:0.9931\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.015206\n",
      "Train | 8/8 | Loss:0.3648 | MainLoss:0.3343 | Alpha:0.4183 | SPLoss:0.0013 | CLSLoss:0.0719 | top1:86.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1653 | MainLoss:0.1653 | SPLoss:0.0015 | CLSLoss:0.0728 | top1:95.9969 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2740 | MainLoss:0.2740 | SPLoss:0.0015 | CLSLoss:0.0728 | top1:89.0105 | AUROC:0.9928\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.015195\n",
      "Train | 8/8 | Loss:0.3719 | MainLoss:0.3408 | Alpha:0.4170 | SPLoss:0.0017 | CLSLoss:0.0729 | top1:85.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1669 | MainLoss:0.1669 | SPLoss:0.0019 | CLSLoss:0.0720 | top1:96.0062 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2738 | MainLoss:0.2738 | SPLoss:0.0019 | CLSLoss:0.0720 | top1:89.0269 | AUROC:0.9928\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.015184\n",
      "Train | 8/8 | Loss:0.3719 | MainLoss:0.3410 | Alpha:0.4177 | SPLoss:0.0022 | CLSLoss:0.0718 | top1:85.5750 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.1727 | MainLoss:0.1727 | SPLoss:0.0023 | CLSLoss:0.0712 | top1:95.8037 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2677 | MainLoss:0.2677 | SPLoss:0.0023 | CLSLoss:0.0712 | top1:89.4233 | AUROC:0.9929\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.015173\n",
      "Train | 8/8 | Loss:0.3642 | MainLoss:0.3334 | Alpha:0.4184 | SPLoss:0.0025 | CLSLoss:0.0712 | top1:85.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1617 | MainLoss:0.1617 | SPLoss:0.0028 | CLSLoss:0.0720 | top1:96.2991 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2790 | MainLoss:0.2790 | SPLoss:0.0028 | CLSLoss:0.0720 | top1:88.6697 | AUROC:0.9926\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.015162\n",
      "Train | 8/8 | Loss:0.3780 | MainLoss:0.3471 | Alpha:0.4156 | SPLoss:0.0030 | CLSLoss:0.0714 | top1:85.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1603 | MainLoss:0.1603 | SPLoss:0.0033 | CLSLoss:0.0717 | top1:96.3863 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2819 | MainLoss:0.2819 | SPLoss:0.0033 | CLSLoss:0.0717 | top1:88.4699 | AUROC:0.9924\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.015151\n",
      "Train | 8/8 | Loss:0.3586 | MainLoss:0.3269 | Alpha:0.4196 | SPLoss:0.0035 | CLSLoss:0.0721 | top1:86.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1761 | MainLoss:0.1761 | SPLoss:0.0037 | CLSLoss:0.0729 | top1:95.4206 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2541 | MainLoss:0.2541 | SPLoss:0.0037 | CLSLoss:0.0729 | top1:90.2163 | AUROC:0.9927\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.015139\n",
      "Train | 8/8 | Loss:0.3714 | MainLoss:0.3397 | Alpha:0.4168 | SPLoss:0.0039 | CLSLoss:0.0723 | top1:85.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1625 | MainLoss:0.1625 | SPLoss:0.0041 | CLSLoss:0.0722 | top1:96.1776 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2745 | MainLoss:0.2745 | SPLoss:0.0041 | CLSLoss:0.0722 | top1:88.8663 | AUROC:0.9926\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.015128\n",
      "Train | 8/8 | Loss:0.3705 | MainLoss:0.3389 | Alpha:0.4169 | SPLoss:0.0043 | CLSLoss:0.0717 | top1:85.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1888 | MainLoss:0.1888 | SPLoss:0.0046 | CLSLoss:0.0715 | top1:94.8349 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.2456 | MainLoss:0.2456 | SPLoss:0.0046 | CLSLoss:0.0715 | top1:91.0321 | AUROC:0.9927\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.015117\n",
      "Train | 8/8 | Loss:0.3734 | MainLoss:0.3415 | Alpha:0.4170 | SPLoss:0.0048 | CLSLoss:0.0716 | top1:85.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1629 | MainLoss:0.1629 | SPLoss:0.0049 | CLSLoss:0.0718 | top1:96.1122 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2764 | MainLoss:0.2764 | SPLoss:0.0049 | CLSLoss:0.0718 | top1:88.8204 | AUROC:0.9925\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.015105\n",
      "Train | 8/8 | Loss:0.3710 | MainLoss:0.3390 | Alpha:0.4168 | SPLoss:0.0052 | CLSLoss:0.0715 | top1:85.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1597 | MainLoss:0.1597 | SPLoss:0.0055 | CLSLoss:0.0715 | top1:96.2897 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2795 | MainLoss:0.2795 | SPLoss:0.0055 | CLSLoss:0.0715 | top1:88.6402 | AUROC:0.9921\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.015094\n",
      "Train | 8/8 | Loss:0.3779 | MainLoss:0.3461 | Alpha:0.4158 | SPLoss:0.0056 | CLSLoss:0.0709 | top1:84.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1657 | MainLoss:0.1657 | SPLoss:0.0059 | CLSLoss:0.0705 | top1:96.1589 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2761 | MainLoss:0.2761 | SPLoss:0.0059 | CLSLoss:0.0705 | top1:88.8303 | AUROC:0.9924\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.015082\n",
      "Train | 8/8 | Loss:0.3650 | MainLoss:0.3326 | Alpha:0.4184 | SPLoss:0.0061 | CLSLoss:0.0714 | top1:86.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1577 | MainLoss:0.1577 | SPLoss:0.0063 | CLSLoss:0.0716 | top1:96.4704 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2855 | MainLoss:0.2855 | SPLoss:0.0063 | CLSLoss:0.0716 | top1:88.1914 | AUROC:0.9922\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.015070\n",
      "Train | 8/8 | Loss:0.3628 | MainLoss:0.3299 | Alpha:0.4184 | SPLoss:0.0065 | CLSLoss:0.0720 | top1:85.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.0066 | CLSLoss:0.0714 | top1:96.6573 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2901 | MainLoss:0.2901 | SPLoss:0.0066 | CLSLoss:0.0714 | top1:87.8834 | AUROC:0.9925\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.015058\n",
      "Train | 8/8 | Loss:0.3710 | MainLoss:0.3382 | Alpha:0.4168 | SPLoss:0.0068 | CLSLoss:0.0717 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1602 | MainLoss:0.1602 | SPLoss:0.0070 | CLSLoss:0.0714 | top1:96.3832 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2806 | MainLoss:0.2806 | SPLoss:0.0070 | CLSLoss:0.0714 | top1:88.4961 | AUROC:0.9923\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.015046\n",
      "Train | 8/8 | Loss:0.3707 | MainLoss:0.3379 | Alpha:0.4171 | SPLoss:0.0072 | CLSLoss:0.0714 | top1:86.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1476 | MainLoss:0.1476 | SPLoss:0.0075 | CLSLoss:0.0714 | top1:96.9751 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.3012 | MainLoss:0.3012 | SPLoss:0.0075 | CLSLoss:0.0714 | top1:87.1265 | AUROC:0.9918\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.015035\n",
      "Train | 8/8 | Loss:0.3729 | MainLoss:0.3402 | Alpha:0.4167 | SPLoss:0.0076 | CLSLoss:0.0709 | top1:85.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1544 | MainLoss:0.1544 | SPLoss:0.0078 | CLSLoss:0.0711 | top1:96.6355 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2894 | MainLoss:0.2894 | SPLoss:0.0078 | CLSLoss:0.0711 | top1:87.9882 | AUROC:0.9920\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.015023\n",
      "Train | 8/8 | Loss:0.3669 | MainLoss:0.3338 | Alpha:0.4178 | SPLoss:0.0079 | CLSLoss:0.0713 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1663 | MainLoss:0.1663 | SPLoss:0.0080 | CLSLoss:0.0714 | top1:95.9346 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2714 | MainLoss:0.2714 | SPLoss:0.0080 | CLSLoss:0.0714 | top1:89.1776 | AUROC:0.9922\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.015010\n",
      "Train | 8/8 | Loss:0.3695 | MainLoss:0.3362 | Alpha:0.4170 | SPLoss:0.0082 | CLSLoss:0.0717 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1624 | MainLoss:0.1624 | SPLoss:0.0083 | CLSLoss:0.0712 | top1:96.0966 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2783 | MainLoss:0.2783 | SPLoss:0.0083 | CLSLoss:0.0712 | top1:88.7582 | AUROC:0.9921\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.014998\n",
      "Train | 8/8 | Loss:0.3629 | MainLoss:0.3293 | Alpha:0.4192 | SPLoss:0.0085 | CLSLoss:0.0716 | top1:86.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1623 | MainLoss:0.1623 | SPLoss:0.0086 | CLSLoss:0.0713 | top1:96.0966 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2751 | MainLoss:0.2751 | SPLoss:0.0086 | CLSLoss:0.0713 | top1:88.9319 | AUROC:0.9920\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.014986\n",
      "Train | 8/8 | Loss:0.3600 | MainLoss:0.3264 | Alpha:0.4185 | SPLoss:0.0087 | CLSLoss:0.0714 | top1:86.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1541 | MainLoss:0.1541 | SPLoss:0.0089 | CLSLoss:0.0715 | top1:96.4922 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2867 | MainLoss:0.2867 | SPLoss:0.0089 | CLSLoss:0.0715 | top1:88.1291 | AUROC:0.9920\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.014974\n",
      "Train | 8/8 | Loss:0.3655 | MainLoss:0.3320 | Alpha:0.4177 | SPLoss:0.0090 | CLSLoss:0.0712 | top1:86.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1687 | MainLoss:0.1687 | SPLoss:0.0092 | CLSLoss:0.0713 | top1:95.7165 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2659 | MainLoss:0.2659 | SPLoss:0.0092 | CLSLoss:0.0713 | top1:89.4594 | AUROC:0.9919\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.014961\n",
      "Train | 8/8 | Loss:0.3633 | MainLoss:0.3297 | Alpha:0.4182 | SPLoss:0.0093 | CLSLoss:0.0712 | top1:85.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1612 | MainLoss:0.1612 | SPLoss:0.0094 | CLSLoss:0.0715 | top1:96.1745 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2758 | MainLoss:0.2758 | SPLoss:0.0094 | CLSLoss:0.0715 | top1:88.8565 | AUROC:0.9920\n",
      "\n",
      "Epoch: [177 | 1000] LR: 0.014949\n",
      "Train | 8/8 | Loss:0.3639 | MainLoss:0.3300 | Alpha:0.4184 | SPLoss:0.0095 | CLSLoss:0.0714 | top1:86.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1566 | MainLoss:0.1566 | SPLoss:0.0097 | CLSLoss:0.0717 | top1:96.3801 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2794 | MainLoss:0.2794 | SPLoss:0.0097 | CLSLoss:0.0717 | top1:88.5780 | AUROC:0.9920\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.014937\n",
      "Train | 8/8 | Loss:0.3643 | MainLoss:0.3306 | Alpha:0.4181 | SPLoss:0.0097 | CLSLoss:0.0710 | top1:86.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1633 | MainLoss:0.1633 | SPLoss:0.0097 | CLSLoss:0.0713 | top1:96.0779 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2696 | MainLoss:0.2696 | SPLoss:0.0097 | CLSLoss:0.0713 | top1:89.1973 | AUROC:0.9921\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.014924\n",
      "Train | 8/8 | Loss:0.3675 | MainLoss:0.3336 | Alpha:0.4180 | SPLoss:0.0099 | CLSLoss:0.0711 | top1:85.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1599 | MainLoss:0.1599 | SPLoss:0.0101 | CLSLoss:0.0711 | top1:96.1682 | AUROC:0.9992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.2772 | MainLoss:0.2772 | SPLoss:0.0101 | CLSLoss:0.0711 | top1:88.8860 | AUROC:0.9921\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.014911\n",
      "Train | 8/8 | Loss:0.3602 | MainLoss:0.3259 | Alpha:0.4192 | SPLoss:0.0103 | CLSLoss:0.0715 | top1:86.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1590 | MainLoss:0.1590 | SPLoss:0.0104 | CLSLoss:0.0720 | top1:96.1215 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2751 | MainLoss:0.2751 | SPLoss:0.0104 | CLSLoss:0.0720 | top1:88.9253 | AUROC:0.9918\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.014899\n",
      "Train | 8/8 | Loss:0.3627 | MainLoss:0.3282 | Alpha:0.4190 | SPLoss:0.0106 | CLSLoss:0.0718 | top1:86.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1696 | MainLoss:0.1696 | SPLoss:0.0107 | CLSLoss:0.0715 | top1:95.6044 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2640 | MainLoss:0.2640 | SPLoss:0.0107 | CLSLoss:0.0715 | top1:89.6461 | AUROC:0.9917\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.014886\n",
      "Train | 8/8 | Loss:0.3692 | MainLoss:0.3349 | Alpha:0.4174 | SPLoss:0.0108 | CLSLoss:0.0712 | top1:86.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1698 | MainLoss:0.1698 | SPLoss:0.0109 | CLSLoss:0.0705 | top1:95.7695 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2686 | MainLoss:0.2686 | SPLoss:0.0109 | CLSLoss:0.0705 | top1:89.3054 | AUROC:0.9916\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.014873\n",
      "Train | 8/8 | Loss:0.3612 | MainLoss:0.3270 | Alpha:0.4191 | SPLoss:0.0109 | CLSLoss:0.0708 | top1:86.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1686 | MainLoss:0.1686 | SPLoss:0.0110 | CLSLoss:0.0709 | top1:95.7819 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2675 | MainLoss:0.2675 | SPLoss:0.0110 | CLSLoss:0.0709 | top1:89.3021 | AUROC:0.9921\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.014860\n",
      "Train | 8/8 | Loss:0.3761 | MainLoss:0.3423 | Alpha:0.4151 | SPLoss:0.0110 | CLSLoss:0.0704 | top1:85.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1707 | MainLoss:0.1707 | SPLoss:0.0111 | CLSLoss:0.0701 | top1:95.7009 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2646 | MainLoss:0.2646 | SPLoss:0.0111 | CLSLoss:0.0701 | top1:89.5675 | AUROC:0.9920\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.014847\n",
      "Train | 8/8 | Loss:0.3681 | MainLoss:0.3341 | Alpha:0.4166 | SPLoss:0.0112 | CLSLoss:0.0705 | top1:85.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1665 | MainLoss:0.1665 | SPLoss:0.0113 | CLSLoss:0.0699 | top1:96.0561 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2692 | MainLoss:0.2692 | SPLoss:0.0113 | CLSLoss:0.0699 | top1:89.2431 | AUROC:0.9922\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.014834\n",
      "Train | 8/8 | Loss:0.3735 | MainLoss:0.3397 | Alpha:0.4173 | SPLoss:0.0114 | CLSLoss:0.0697 | top1:85.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1709 | MainLoss:0.1709 | SPLoss:0.0115 | CLSLoss:0.0698 | top1:95.7570 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2643 | MainLoss:0.2643 | SPLoss:0.0115 | CLSLoss:0.0698 | top1:89.5773 | AUROC:0.9923\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.014821\n",
      "Train | 8/8 | Loss:0.3694 | MainLoss:0.3355 | Alpha:0.4169 | SPLoss:0.0116 | CLSLoss:0.0697 | top1:85.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1641 | MainLoss:0.1641 | SPLoss:0.0117 | CLSLoss:0.0702 | top1:95.9688 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2696 | MainLoss:0.2696 | SPLoss:0.0117 | CLSLoss:0.0702 | top1:89.2988 | AUROC:0.9920\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.014808\n",
      "Train | 8/8 | Loss:0.3716 | MainLoss:0.3375 | Alpha:0.4164 | SPLoss:0.0117 | CLSLoss:0.0702 | top1:85.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1626 | MainLoss:0.1626 | SPLoss:0.0117 | CLSLoss:0.0699 | top1:96.1371 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2762 | MainLoss:0.2762 | SPLoss:0.0117 | CLSLoss:0.0699 | top1:88.8630 | AUROC:0.9922\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.014795\n",
      "Train | 8/8 | Loss:0.3741 | MainLoss:0.3401 | Alpha:0.4155 | SPLoss:0.0119 | CLSLoss:0.0699 | top1:84.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1614 | MainLoss:0.1614 | SPLoss:0.0119 | CLSLoss:0.0696 | top1:96.2523 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2804 | MainLoss:0.2804 | SPLoss:0.0119 | CLSLoss:0.0696 | top1:88.6861 | AUROC:0.9924\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.014781\n",
      "Train | 8/8 | Loss:0.3653 | MainLoss:0.3309 | Alpha:0.4173 | SPLoss:0.0120 | CLSLoss:0.0705 | top1:86.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1513 | MainLoss:0.1513 | SPLoss:0.0121 | CLSLoss:0.0705 | top1:96.7539 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2940 | MainLoss:0.2940 | SPLoss:0.0121 | CLSLoss:0.0705 | top1:87.6016 | AUROC:0.9921\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.014768\n",
      "Train | 8/8 | Loss:0.3709 | MainLoss:0.3367 | Alpha:0.4165 | SPLoss:0.0121 | CLSLoss:0.0700 | top1:86.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1769 | MainLoss:0.1769 | SPLoss:0.0122 | CLSLoss:0.0705 | top1:95.2866 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2567 | MainLoss:0.2567 | SPLoss:0.0122 | CLSLoss:0.0705 | top1:90.1671 | AUROC:0.9922\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.014755\n",
      "Train | 8/8 | Loss:0.3586 | MainLoss:0.3237 | Alpha:0.4198 | SPLoss:0.0123 | CLSLoss:0.0709 | top1:86.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1618 | MainLoss:0.1618 | SPLoss:0.0124 | CLSLoss:0.0712 | top1:96.0561 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2733 | MainLoss:0.2733 | SPLoss:0.0124 | CLSLoss:0.0712 | top1:88.9122 | AUROC:0.9917\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.014741\n",
      "Train | 8/8 | Loss:0.3643 | MainLoss:0.3292 | Alpha:0.4184 | SPLoss:0.0125 | CLSLoss:0.0713 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1643 | MainLoss:0.1643 | SPLoss:0.0126 | CLSLoss:0.0712 | top1:95.8723 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2701 | MainLoss:0.2701 | SPLoss:0.0126 | CLSLoss:0.0712 | top1:89.2005 | AUROC:0.9918\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.014728\n",
      "Train | 8/8 | Loss:0.3683 | MainLoss:0.3334 | Alpha:0.4169 | SPLoss:0.0127 | CLSLoss:0.0710 | top1:86.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1567 | MainLoss:0.1567 | SPLoss:0.0128 | CLSLoss:0.0707 | top1:96.3146 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2828 | MainLoss:0.2828 | SPLoss:0.0128 | CLSLoss:0.0707 | top1:88.4142 | AUROC:0.9917\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.014714\n",
      "Train | 8/8 | Loss:0.3626 | MainLoss:0.3277 | Alpha:0.4185 | SPLoss:0.0127 | CLSLoss:0.0706 | top1:86.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1577 | MainLoss:0.1577 | SPLoss:0.0128 | CLSLoss:0.0705 | top1:96.3022 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2810 | MainLoss:0.2810 | SPLoss:0.0128 | CLSLoss:0.0705 | top1:88.5190 | AUROC:0.9917\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.014700\n",
      "Train | 8/8 | Loss:0.3771 | MainLoss:0.3426 | Alpha:0.4151 | SPLoss:0.0129 | CLSLoss:0.0702 | top1:85.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1639 | MainLoss:0.1639 | SPLoss:0.0129 | CLSLoss:0.0693 | top1:96.0654 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2763 | MainLoss:0.2763 | SPLoss:0.0129 | CLSLoss:0.0693 | top1:88.9056 | AUROC:0.9917\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.014686\n",
      "Train | 8/8 | Loss:0.3654 | MainLoss:0.3309 | Alpha:0.4174 | SPLoss:0.0129 | CLSLoss:0.0697 | top1:86.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1573 | MainLoss:0.1573 | SPLoss:0.0130 | CLSLoss:0.0699 | top1:96.3458 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2843 | MainLoss:0.2843 | SPLoss:0.0130 | CLSLoss:0.0699 | top1:88.3716 | AUROC:0.9915\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.014673\n",
      "Train | 8/8 | Loss:0.3629 | MainLoss:0.3282 | Alpha:0.4183 | SPLoss:0.0131 | CLSLoss:0.0699 | top1:86.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1707 | MainLoss:0.1707 | SPLoss:0.0131 | CLSLoss:0.0703 | top1:95.6324 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2640 | MainLoss:0.2640 | SPLoss:0.0131 | CLSLoss:0.0703 | top1:89.6461 | AUROC:0.9917\n",
      "\n",
      "Epoch: [199 | 1000] LR: 0.014659\n",
      "Train | 8/8 | Loss:0.3654 | MainLoss:0.3302 | Alpha:0.4190 | SPLoss:0.0132 | CLSLoss:0.0706 | top1:86.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1718 | MainLoss:0.1718 | SPLoss:0.0132 | CLSLoss:0.0704 | top1:95.5545 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2633 | MainLoss:0.2633 | SPLoss:0.0132 | CLSLoss:0.0704 | top1:89.6592 | AUROC:0.9918\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.014645\n",
      "Train | 8/8 | Loss:0.3676 | MainLoss:0.3326 | Alpha:0.4180 | SPLoss:0.0133 | CLSLoss:0.0705 | top1:86.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1567 | MainLoss:0.1567 | SPLoss:0.0134 | CLSLoss:0.0700 | top1:96.3801 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2876 | MainLoss:0.2876 | SPLoss:0.0134 | CLSLoss:0.0700 | top1:88.0537 | AUROC:0.9919\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.014631\n",
      "Train | 8/8 | Loss:0.3692 | MainLoss:0.3399 | Alpha:0.4188 | SPLoss:0.0001 | CLSLoss:0.0699 | top1:85.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1678 | MainLoss:0.1678 | SPLoss:0.0002 | CLSLoss:0.0694 | top1:95.9034 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2740 | MainLoss:0.2740 | SPLoss:0.0002 | CLSLoss:0.0694 | top1:88.8794 | AUROC:0.9916\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.014617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.3686 | MainLoss:0.3394 | Alpha:0.4181 | SPLoss:0.0003 | CLSLoss:0.0694 | top1:85.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1680 | MainLoss:0.1680 | SPLoss:0.0004 | CLSLoss:0.0689 | top1:95.9782 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2757 | MainLoss:0.2757 | SPLoss:0.0004 | CLSLoss:0.0689 | top1:88.7353 | AUROC:0.9918\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.014602\n",
      "Train | 8/8 | Loss:0.3508 | MainLoss:0.3212 | Alpha:0.4218 | SPLoss:0.0006 | CLSLoss:0.0695 | top1:86.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1704 | MainLoss:0.1704 | SPLoss:0.0008 | CLSLoss:0.0709 | top1:95.6137 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2649 | MainLoss:0.2649 | SPLoss:0.0008 | CLSLoss:0.0709 | top1:89.3807 | AUROC:0.9914\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.014588\n",
      "Train | 8/8 | Loss:0.3565 | MainLoss:0.3262 | Alpha:0.4206 | SPLoss:0.0009 | CLSLoss:0.0711 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1674 | MainLoss:0.1674 | SPLoss:0.0011 | CLSLoss:0.0708 | top1:95.6916 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2696 | MainLoss:0.2696 | SPLoss:0.0011 | CLSLoss:0.0708 | top1:89.0400 | AUROC:0.9916\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.014574\n",
      "Train | 8/8 | Loss:0.3610 | MainLoss:0.3308 | Alpha:0.4201 | SPLoss:0.0012 | CLSLoss:0.0707 | top1:85.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1638 | MainLoss:0.1638 | SPLoss:0.0014 | CLSLoss:0.0702 | top1:95.9688 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2758 | MainLoss:0.2758 | SPLoss:0.0014 | CLSLoss:0.0702 | top1:88.7123 | AUROC:0.9918\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.014560\n",
      "Train | 8/8 | Loss:0.3521 | MainLoss:0.3214 | Alpha:0.4224 | SPLoss:0.0015 | CLSLoss:0.0710 | top1:87.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1520 | MainLoss:0.1520 | SPLoss:0.0018 | CLSLoss:0.0708 | top1:96.5576 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2921 | MainLoss:0.2921 | SPLoss:0.0018 | CLSLoss:0.0708 | top1:87.5295 | AUROC:0.9914\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.014545\n",
      "Train | 8/8 | Loss:0.3580 | MainLoss:0.3275 | Alpha:0.4212 | SPLoss:0.0019 | CLSLoss:0.0704 | top1:86.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1548 | MainLoss:0.1548 | SPLoss:0.0021 | CLSLoss:0.0705 | top1:96.3489 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2859 | MainLoss:0.2859 | SPLoss:0.0021 | CLSLoss:0.0705 | top1:88.0111 | AUROC:0.9914\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.014531\n",
      "Train | 8/8 | Loss:0.3622 | MainLoss:0.3317 | Alpha:0.4201 | SPLoss:0.0022 | CLSLoss:0.0704 | top1:86.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.0024 | CLSLoss:0.0701 | top1:96.4517 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2862 | MainLoss:0.2862 | SPLoss:0.0024 | CLSLoss:0.0701 | top1:88.0374 | AUROC:0.9913\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.014516\n",
      "Train | 8/8 | Loss:0.3506 | MainLoss:0.3200 | Alpha:0.4214 | SPLoss:0.0026 | CLSLoss:0.0702 | top1:86.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1632 | MainLoss:0.1632 | SPLoss:0.0028 | CLSLoss:0.0707 | top1:95.8754 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2700 | MainLoss:0.2700 | SPLoss:0.0028 | CLSLoss:0.0707 | top1:89.0695 | AUROC:0.9915\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.014502\n",
      "Train | 8/8 | Loss:0.3649 | MainLoss:0.3342 | Alpha:0.4192 | SPLoss:0.0030 | CLSLoss:0.0703 | top1:85.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1632 | MainLoss:0.1632 | SPLoss:0.0032 | CLSLoss:0.0701 | top1:95.9128 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2700 | MainLoss:0.2700 | SPLoss:0.0032 | CLSLoss:0.0701 | top1:89.0662 | AUROC:0.9916\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.014487\n",
      "Train | 8/8 | Loss:0.3572 | MainLoss:0.3262 | Alpha:0.4203 | SPLoss:0.0034 | CLSLoss:0.0702 | top1:86.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1504 | MainLoss:0.1504 | SPLoss:0.0037 | CLSLoss:0.0702 | top1:96.6075 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2884 | MainLoss:0.2884 | SPLoss:0.0037 | CLSLoss:0.0702 | top1:87.8309 | AUROC:0.9912\n",
      "\n",
      "Epoch: [212 | 1000] LR: 0.014472\n",
      "Train | 8/8 | Loss:0.3528 | MainLoss:0.3217 | Alpha:0.4213 | SPLoss:0.0038 | CLSLoss:0.0700 | top1:86.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1577 | MainLoss:0.1577 | SPLoss:0.0040 | CLSLoss:0.0703 | top1:96.2150 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2749 | MainLoss:0.2749 | SPLoss:0.0040 | CLSLoss:0.0703 | top1:88.7320 | AUROC:0.9911\n",
      "\n",
      "Epoch: [213 | 1000] LR: 0.014457\n",
      "Train | 8/8 | Loss:0.3628 | MainLoss:0.3317 | Alpha:0.4186 | SPLoss:0.0042 | CLSLoss:0.0701 | top1:86.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1456 | MainLoss:0.1456 | SPLoss:0.0044 | CLSLoss:0.0699 | top1:96.8474 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2969 | MainLoss:0.2969 | SPLoss:0.0044 | CLSLoss:0.0699 | top1:87.3558 | AUROC:0.9909\n",
      "\n",
      "Epoch: [214 | 1000] LR: 0.014442\n",
      "Train | 8/8 | Loss:0.3594 | MainLoss:0.3280 | Alpha:0.4199 | SPLoss:0.0046 | CLSLoss:0.0700 | top1:86.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1656 | MainLoss:0.1656 | SPLoss:0.0049 | CLSLoss:0.0702 | top1:95.7290 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2655 | MainLoss:0.2655 | SPLoss:0.0049 | CLSLoss:0.0702 | top1:89.4856 | AUROC:0.9911\n",
      "\n",
      "Epoch: [215 | 1000] LR: 0.014428\n",
      "Train | 8/8 | Loss:0.3564 | MainLoss:0.3248 | Alpha:0.4204 | SPLoss:0.0050 | CLSLoss:0.0700 | top1:86.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1547 | MainLoss:0.1547 | SPLoss:0.0051 | CLSLoss:0.0702 | top1:96.2087 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2792 | MainLoss:0.2792 | SPLoss:0.0051 | CLSLoss:0.0702 | top1:88.6075 | AUROC:0.9909\n",
      "\n",
      "Epoch: [216 | 1000] LR: 0.014413\n",
      "Train | 8/8 | Loss:0.3587 | MainLoss:0.3272 | Alpha:0.4204 | SPLoss:0.0053 | CLSLoss:0.0699 | top1:86.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1601 | MainLoss:0.1601 | SPLoss:0.0054 | CLSLoss:0.0701 | top1:95.9284 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2724 | MainLoss:0.2724 | SPLoss:0.0054 | CLSLoss:0.0701 | top1:88.9843 | AUROC:0.9907\n",
      "\n",
      "Epoch: [217 | 1000] LR: 0.014397\n",
      "Train | 8/8 | Loss:0.3593 | MainLoss:0.3275 | Alpha:0.4195 | SPLoss:0.0057 | CLSLoss:0.0700 | top1:86.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1718 | MainLoss:0.1718 | SPLoss:0.0060 | CLSLoss:0.0701 | top1:95.3832 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.2564 | MainLoss:0.2564 | SPLoss:0.0060 | CLSLoss:0.0701 | top1:90.0492 | AUROC:0.9905\n",
      "\n",
      "Epoch: [218 | 1000] LR: 0.014382\n",
      "Train | 8/8 | Loss:0.3538 | MainLoss:0.3217 | Alpha:0.4214 | SPLoss:0.0061 | CLSLoss:0.0700 | top1:86.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1452 | MainLoss:0.1452 | SPLoss:0.0062 | CLSLoss:0.0704 | top1:96.7072 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2946 | MainLoss:0.2946 | SPLoss:0.0062 | CLSLoss:0.0704 | top1:87.5098 | AUROC:0.9909\n",
      "\n",
      "Epoch: [219 | 1000] LR: 0.014367\n",
      "Train | 8/8 | Loss:0.3615 | MainLoss:0.3294 | Alpha:0.4198 | SPLoss:0.0064 | CLSLoss:0.0701 | top1:85.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1493 | MainLoss:0.1493 | SPLoss:0.0065 | CLSLoss:0.0697 | top1:96.5421 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2868 | MainLoss:0.2868 | SPLoss:0.0065 | CLSLoss:0.0697 | top1:87.9751 | AUROC:0.9911\n",
      "\n",
      "Epoch: [220 | 1000] LR: 0.014352\n",
      "Train | 8/8 | Loss:0.3597 | MainLoss:0.3278 | Alpha:0.4197 | SPLoss:0.0067 | CLSLoss:0.0693 | top1:86.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1670 | MainLoss:0.1670 | SPLoss:0.0071 | CLSLoss:0.0695 | top1:95.6231 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2618 | MainLoss:0.2618 | SPLoss:0.0071 | CLSLoss:0.0695 | top1:89.7346 | AUROC:0.9911\n",
      "\n",
      "Epoch: [221 | 1000] LR: 0.014337\n",
      "Train | 8/8 | Loss:0.3511 | MainLoss:0.3187 | Alpha:0.4222 | SPLoss:0.0072 | CLSLoss:0.0695 | top1:87.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1483 | MainLoss:0.1483 | SPLoss:0.0072 | CLSLoss:0.0698 | top1:96.6667 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2890 | MainLoss:0.2890 | SPLoss:0.0072 | CLSLoss:0.0698 | top1:87.7359 | AUROC:0.9910\n",
      "\n",
      "Epoch: [222 | 1000] LR: 0.014321\n",
      "Train | 8/8 | Loss:0.3645 | MainLoss:0.3323 | Alpha:0.4181 | SPLoss:0.0074 | CLSLoss:0.0695 | top1:85.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1613 | MainLoss:0.1613 | SPLoss:0.0076 | CLSLoss:0.0691 | top1:96.1153 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2713 | MainLoss:0.2713 | SPLoss:0.0076 | CLSLoss:0.0691 | top1:88.9450 | AUROC:0.9912\n",
      "\n",
      "Epoch: [223 | 1000] LR: 0.014306\n",
      "Train | 8/8 | Loss:0.3476 | MainLoss:0.3150 | Alpha:0.4221 | SPLoss:0.0076 | CLSLoss:0.0696 | top1:86.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1654 | MainLoss:0.1654 | SPLoss:0.0078 | CLSLoss:0.0702 | top1:95.6231 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2575 | MainLoss:0.2575 | SPLoss:0.0078 | CLSLoss:0.0702 | top1:89.9050 | AUROC:0.9913\n",
      "\n",
      "Epoch: [224 | 1000] LR: 0.014290\n",
      "Train | 8/8 | Loss:0.3584 | MainLoss:0.3257 | Alpha:0.4193 | SPLoss:0.0079 | CLSLoss:0.0700 | top1:86.2000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.1745 | MainLoss:0.1745 | SPLoss:0.0082 | CLSLoss:0.0698 | top1:95.2337 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.2502 | MainLoss:0.2502 | SPLoss:0.0082 | CLSLoss:0.0698 | top1:90.3735 | AUROC:0.9911\n",
      "\n",
      "Epoch: [225 | 1000] LR: 0.014275\n",
      "Train | 8/8 | Loss:0.3583 | MainLoss:0.3257 | Alpha:0.4198 | SPLoss:0.0082 | CLSLoss:0.0694 | top1:86.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1526 | MainLoss:0.1526 | SPLoss:0.0083 | CLSLoss:0.0700 | top1:96.2523 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2757 | MainLoss:0.2757 | SPLoss:0.0083 | CLSLoss:0.0700 | top1:88.7320 | AUROC:0.9909\n",
      "\n",
      "Epoch: [226 | 1000] LR: 0.014259\n",
      "Train | 8/8 | Loss:0.3488 | MainLoss:0.3159 | Alpha:0.4209 | SPLoss:0.0084 | CLSLoss:0.0698 | top1:86.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1610 | MainLoss:0.1610 | SPLoss:0.0086 | CLSLoss:0.0701 | top1:95.8318 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2633 | MainLoss:0.2633 | SPLoss:0.0086 | CLSLoss:0.0701 | top1:89.5249 | AUROC:0.9910\n",
      "\n",
      "Epoch: [227 | 1000] LR: 0.014243\n",
      "Train | 8/8 | Loss:0.3472 | MainLoss:0.3140 | Alpha:0.4224 | SPLoss:0.0087 | CLSLoss:0.0700 | top1:87.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1640 | MainLoss:0.1640 | SPLoss:0.0089 | CLSLoss:0.0703 | top1:95.6137 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2600 | MainLoss:0.2600 | SPLoss:0.0089 | CLSLoss:0.0703 | top1:89.8100 | AUROC:0.9908\n",
      "\n",
      "Epoch: [228 | 1000] LR: 0.014228\n",
      "Train | 8/8 | Loss:0.3606 | MainLoss:0.3275 | Alpha:0.4195 | SPLoss:0.0090 | CLSLoss:0.0699 | top1:86.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1586 | MainLoss:0.1586 | SPLoss:0.0090 | CLSLoss:0.0694 | top1:96.0031 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2699 | MainLoss:0.2699 | SPLoss:0.0090 | CLSLoss:0.0694 | top1:89.1579 | AUROC:0.9908\n",
      "\n",
      "Epoch: [229 | 1000] LR: 0.014212\n",
      "Train | 8/8 | Loss:0.3570 | MainLoss:0.3239 | Alpha:0.4205 | SPLoss:0.0092 | CLSLoss:0.0696 | top1:86.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1690 | MainLoss:0.1690 | SPLoss:0.0093 | CLSLoss:0.0692 | top1:95.4237 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2567 | MainLoss:0.2567 | SPLoss:0.0093 | CLSLoss:0.0692 | top1:90.0524 | AUROC:0.9911\n",
      "\n",
      "Epoch: [230 | 1000] LR: 0.014196\n",
      "Train | 8/8 | Loss:0.3624 | MainLoss:0.3295 | Alpha:0.4193 | SPLoss:0.0094 | CLSLoss:0.0689 | top1:86.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1658 | MainLoss:0.1658 | SPLoss:0.0095 | CLSLoss:0.0684 | top1:95.6324 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2604 | MainLoss:0.2604 | SPLoss:0.0095 | CLSLoss:0.0684 | top1:89.9083 | AUROC:0.9913\n",
      "\n",
      "Epoch: [231 | 1000] LR: 0.014180\n",
      "Train | 8/8 | Loss:0.3617 | MainLoss:0.3289 | Alpha:0.4183 | SPLoss:0.0097 | CLSLoss:0.0687 | top1:86.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1521 | MainLoss:0.1521 | SPLoss:0.0097 | CLSLoss:0.0685 | top1:96.3894 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2792 | MainLoss:0.2792 | SPLoss:0.0097 | CLSLoss:0.0685 | top1:88.5911 | AUROC:0.9908\n",
      "\n",
      "Epoch: [232 | 1000] LR: 0.014164\n",
      "Train | 8/8 | Loss:0.3493 | MainLoss:0.3159 | Alpha:0.4221 | SPLoss:0.0098 | CLSLoss:0.0692 | top1:86.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1573 | MainLoss:0.1573 | SPLoss:0.0098 | CLSLoss:0.0692 | top1:96.0561 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2675 | MainLoss:0.2675 | SPLoss:0.0098 | CLSLoss:0.0692 | top1:89.3480 | AUROC:0.9908\n",
      "\n",
      "Epoch: [233 | 1000] LR: 0.014148\n",
      "Train | 8/8 | Loss:0.3674 | MainLoss:0.3346 | Alpha:0.4175 | SPLoss:0.0098 | CLSLoss:0.0687 | top1:85.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1665 | MainLoss:0.1665 | SPLoss:0.0098 | CLSLoss:0.0683 | top1:95.6449 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2585 | MainLoss:0.2585 | SPLoss:0.0098 | CLSLoss:0.0683 | top1:89.9738 | AUROC:0.9910\n",
      "\n",
      "Epoch: [234 | 1000] LR: 0.014132\n",
      "Train | 8/8 | Loss:0.3581 | MainLoss:0.3252 | Alpha:0.4204 | SPLoss:0.0099 | CLSLoss:0.0682 | top1:86.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1579 | MainLoss:0.1579 | SPLoss:0.0100 | CLSLoss:0.0684 | top1:96.1495 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2700 | MainLoss:0.2700 | SPLoss:0.0100 | CLSLoss:0.0684 | top1:89.1284 | AUROC:0.9911\n",
      "\n",
      "Epoch: [235 | 1000] LR: 0.014116\n",
      "Train | 8/8 | Loss:0.3521 | MainLoss:0.3190 | Alpha:0.4214 | SPLoss:0.0101 | CLSLoss:0.0685 | top1:86.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1573 | MainLoss:0.1573 | SPLoss:0.0103 | CLSLoss:0.0694 | top1:95.9782 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2676 | MainLoss:0.2676 | SPLoss:0.0103 | CLSLoss:0.0694 | top1:89.2824 | AUROC:0.9912\n",
      "\n",
      "Epoch: [236 | 1000] LR: 0.014100\n",
      "Train | 8/8 | Loss:0.3725 | MainLoss:0.3395 | Alpha:0.4164 | SPLoss:0.0103 | CLSLoss:0.0690 | top1:85.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1821 | MainLoss:0.1821 | SPLoss:0.0106 | CLSLoss:0.0677 | top1:95.0748 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2473 | MainLoss:0.2473 | SPLoss:0.0106 | CLSLoss:0.0677 | top1:90.8290 | AUROC:0.9913\n",
      "\n",
      "Epoch: [237 | 1000] LR: 0.014083\n",
      "Train | 8/8 | Loss:0.3601 | MainLoss:0.3270 | Alpha:0.4195 | SPLoss:0.0106 | CLSLoss:0.0684 | top1:85.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1674 | MainLoss:0.1674 | SPLoss:0.0106 | CLSLoss:0.0682 | top1:95.7321 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2580 | MainLoss:0.2580 | SPLoss:0.0106 | CLSLoss:0.0682 | top1:89.9509 | AUROC:0.9909\n",
      "\n",
      "Epoch: [238 | 1000] LR: 0.014067\n",
      "Train | 8/8 | Loss:0.3585 | MainLoss:0.3253 | Alpha:0.4205 | SPLoss:0.0105 | CLSLoss:0.0684 | top1:86.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1616 | MainLoss:0.1616 | SPLoss:0.0106 | CLSLoss:0.0687 | top1:95.9159 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2626 | MainLoss:0.2626 | SPLoss:0.0106 | CLSLoss:0.0687 | top1:89.5446 | AUROC:0.9908\n",
      "\n",
      "Epoch: [239 | 1000] LR: 0.014050\n",
      "Train | 8/8 | Loss:0.3480 | MainLoss:0.3144 | Alpha:0.4218 | SPLoss:0.0107 | CLSLoss:0.0690 | top1:86.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1549 | MainLoss:0.1549 | SPLoss:0.0107 | CLSLoss:0.0697 | top1:96.1246 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2679 | MainLoss:0.2679 | SPLoss:0.0107 | CLSLoss:0.0697 | top1:89.2005 | AUROC:0.9910\n",
      "\n",
      "Epoch: [240 | 1000] LR: 0.014034\n",
      "Train | 8/8 | Loss:0.3575 | MainLoss:0.3236 | Alpha:0.4206 | SPLoss:0.0107 | CLSLoss:0.0698 | top1:86.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1621 | MainLoss:0.1621 | SPLoss:0.0108 | CLSLoss:0.0692 | top1:95.8785 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2627 | MainLoss:0.2627 | SPLoss:0.0108 | CLSLoss:0.0692 | top1:89.4725 | AUROC:0.9908\n",
      "\n",
      "Epoch: [241 | 1000] LR: 0.014017\n",
      "Train | 8/8 | Loss:0.3613 | MainLoss:0.3278 | Alpha:0.4194 | SPLoss:0.0108 | CLSLoss:0.0689 | top1:86.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1601 | MainLoss:0.1601 | SPLoss:0.0109 | CLSLoss:0.0692 | top1:95.9813 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2642 | MainLoss:0.2642 | SPLoss:0.0109 | CLSLoss:0.0692 | top1:89.3447 | AUROC:0.9909\n",
      "\n",
      "Epoch: [242 | 1000] LR: 0.014001\n",
      "Train | 8/8 | Loss:0.3504 | MainLoss:0.3163 | Alpha:0.4221 | SPLoss:0.0110 | CLSLoss:0.0698 | top1:87.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1626 | MainLoss:0.1626 | SPLoss:0.0112 | CLSLoss:0.0693 | top1:95.7041 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2581 | MainLoss:0.2581 | SPLoss:0.0112 | CLSLoss:0.0693 | top1:89.8395 | AUROC:0.9910\n",
      "\n",
      "Epoch: [243 | 1000] LR: 0.013984\n",
      "Train | 8/8 | Loss:0.3588 | MainLoss:0.3250 | Alpha:0.4193 | SPLoss:0.0113 | CLSLoss:0.0693 | top1:86.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.0113 | CLSLoss:0.0686 | top1:96.2523 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2700 | MainLoss:0.2700 | SPLoss:0.0113 | CLSLoss:0.0686 | top1:89.0990 | AUROC:0.9907\n",
      "\n",
      "Epoch: [244 | 1000] LR: 0.013968\n",
      "Train | 8/8 | Loss:0.3573 | MainLoss:0.3237 | Alpha:0.4200 | SPLoss:0.0114 | CLSLoss:0.0685 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1606 | MainLoss:0.1606 | SPLoss:0.0115 | CLSLoss:0.0687 | top1:95.8910 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2607 | MainLoss:0.2607 | SPLoss:0.0115 | CLSLoss:0.0687 | top1:89.7412 | AUROC:0.9904\n",
      "\n",
      "Epoch: [245 | 1000] LR: 0.013951\n",
      "Train | 8/8 | Loss:0.3576 | MainLoss:0.3239 | Alpha:0.4195 | SPLoss:0.0116 | CLSLoss:0.0688 | top1:85.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1610 | MainLoss:0.1610 | SPLoss:0.0116 | CLSLoss:0.0688 | top1:95.9190 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2620 | MainLoss:0.2620 | SPLoss:0.0116 | CLSLoss:0.0688 | top1:89.5970 | AUROC:0.9903\n",
      "\n",
      "Epoch: [246 | 1000] LR: 0.013934\n",
      "Train | 8/8 | Loss:0.3686 | MainLoss:0.3350 | Alpha:0.4172 | SPLoss:0.0117 | CLSLoss:0.0687 | top1:85.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1748 | MainLoss:0.1748 | SPLoss:0.0119 | CLSLoss:0.0679 | top1:95.3333 | AUROC:0.9992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.2509 | MainLoss:0.2509 | SPLoss:0.0119 | CLSLoss:0.0679 | top1:90.4686 | AUROC:0.9905\n",
      "\n",
      "Epoch: [247 | 1000] LR: 0.013917\n",
      "Train | 8/8 | Loss:0.3585 | MainLoss:0.3249 | Alpha:0.4201 | SPLoss:0.0118 | CLSLoss:0.0680 | top1:86.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1598 | MainLoss:0.1598 | SPLoss:0.0119 | CLSLoss:0.0689 | top1:95.8816 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2641 | MainLoss:0.2641 | SPLoss:0.0119 | CLSLoss:0.0689 | top1:89.4856 | AUROC:0.9905\n",
      "\n",
      "Epoch: [248 | 1000] LR: 0.013900\n",
      "Train | 8/8 | Loss:0.3588 | MainLoss:0.3249 | Alpha:0.4190 | SPLoss:0.0120 | CLSLoss:0.0688 | top1:86.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1611 | MainLoss:0.1611 | SPLoss:0.0121 | CLSLoss:0.0690 | top1:95.8193 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2626 | MainLoss:0.2626 | SPLoss:0.0121 | CLSLoss:0.0690 | top1:89.5347 | AUROC:0.9908\n",
      "\n",
      "Epoch: [249 | 1000] LR: 0.013883\n",
      "Train | 8/8 | Loss:0.3663 | MainLoss:0.3326 | Alpha:0.4172 | SPLoss:0.0120 | CLSLoss:0.0689 | top1:85.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1519 | MainLoss:0.1519 | SPLoss:0.0119 | CLSLoss:0.0682 | top1:96.5109 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2784 | MainLoss:0.2784 | SPLoss:0.0119 | CLSLoss:0.0682 | top1:88.4436 | AUROC:0.9909\n",
      "\n",
      "Epoch: [250 | 1000] LR: 0.013866\n",
      "Train | 8/8 | Loss:0.3527 | MainLoss:0.3190 | Alpha:0.4218 | SPLoss:0.0120 | CLSLoss:0.0681 | top1:86.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1491 | MainLoss:0.1491 | SPLoss:0.0119 | CLSLoss:0.0691 | top1:96.4704 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2805 | MainLoss:0.2805 | SPLoss:0.0119 | CLSLoss:0.0691 | top1:88.3552 | AUROC:0.9907\n",
      "\n",
      "Epoch: [251 | 1000] LR: 0.013849\n",
      "Train | 8/8 | Loss:0.3547 | MainLoss:0.3257 | Alpha:0.4213 | SPLoss:0.0001 | CLSLoss:0.0688 | top1:86.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1604 | MainLoss:0.1604 | SPLoss:0.0002 | CLSLoss:0.0692 | top1:95.7601 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2618 | MainLoss:0.2618 | SPLoss:0.0002 | CLSLoss:0.0692 | top1:89.6199 | AUROC:0.9905\n",
      "\n",
      "Epoch: [252 | 1000] LR: 0.001383\n",
      "Train | 8/8 | Loss:0.3547 | MainLoss:0.3255 | Alpha:0.4219 | SPLoss:0.0002 | CLSLoss:0.0692 | top1:86.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1607 | MainLoss:0.1607 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:95.7508 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2616 | MainLoss:0.2616 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:89.6330 | AUROC:0.9906\n",
      "\n",
      "Epoch: [253 | 1000] LR: 0.001381\n",
      "Train | 8/8 | Loss:0.3480 | MainLoss:0.3186 | Alpha:0.4236 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:86.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1588 | MainLoss:0.1588 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:95.8692 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2641 | MainLoss:0.2641 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:89.4430 | AUROC:0.9906\n",
      "\n",
      "Epoch: [254 | 1000] LR: 0.001380\n",
      "Train | 8/8 | Loss:0.3567 | MainLoss:0.3276 | Alpha:0.4211 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:85.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1580 | MainLoss:0.1580 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:95.9252 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2653 | MainLoss:0.2653 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:89.3349 | AUROC:0.9907\n",
      "\n",
      "Epoch: [255 | 1000] LR: 0.001378\n",
      "Train | 8/8 | Loss:0.3532 | MainLoss:0.3240 | Alpha:0.4216 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:86.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1574 | MainLoss:0.1574 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:95.9470 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2661 | MainLoss:0.2661 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:89.3054 | AUROC:0.9905\n",
      "\n",
      "Epoch: [256 | 1000] LR: 0.001376\n",
      "Train | 8/8 | Loss:0.3464 | MainLoss:0.3170 | Alpha:0.4247 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:86.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1543 | MainLoss:0.1543 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:96.1308 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2704 | MainLoss:0.2704 | SPLoss:0.0002 | CLSLoss:0.0690 | top1:89.0662 | AUROC:0.9906\n",
      "\n",
      "Epoch: [257 | 1000] LR: 0.001375\n",
      "Train | 8/8 | Loss:0.3385 | MainLoss:0.3090 | Alpha:0.4252 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:87.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:96.0499 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2685 | MainLoss:0.2685 | SPLoss:0.0002 | CLSLoss:0.0691 | top1:89.1645 | AUROC:0.9907\n",
      "\n",
      "Epoch: [258 | 1000] LR: 0.001373\n",
      "Train | 8/8 | Loss:0.3374 | MainLoss:0.3079 | Alpha:0.4254 | SPLoss:0.0002 | CLSLoss:0.0692 | top1:87.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1544 | MainLoss:0.1544 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:96.0966 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2693 | MainLoss:0.2693 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:89.1121 | AUROC:0.9906\n",
      "\n",
      "Epoch: [259 | 1000] LR: 0.001371\n",
      "Train | 8/8 | Loss:0.3603 | MainLoss:0.3311 | Alpha:0.4199 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:85.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1559 | MainLoss:0.1559 | SPLoss:0.0003 | CLSLoss:0.0691 | top1:96.0312 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2676 | MainLoss:0.2676 | SPLoss:0.0003 | CLSLoss:0.0691 | top1:89.2169 | AUROC:0.9906\n",
      "\n",
      "Epoch: [260 | 1000] LR: 0.001369\n",
      "Train | 8/8 | Loss:0.3437 | MainLoss:0.3142 | Alpha:0.4239 | SPLoss:0.0003 | CLSLoss:0.0691 | top1:86.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1550 | MainLoss:0.1550 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:96.0748 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2687 | MainLoss:0.2687 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:89.1743 | AUROC:0.9906\n",
      "\n",
      "Epoch: [261 | 1000] LR: 0.001367\n",
      "Train | 8/8 | Loss:0.3480 | MainLoss:0.3187 | Alpha:0.4222 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:86.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1566 | MainLoss:0.1566 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:96.0031 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2664 | MainLoss:0.2664 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:89.2693 | AUROC:0.9907\n",
      "\n",
      "Epoch: [262 | 1000] LR: 0.001366\n",
      "Train | 8/8 | Loss:0.3471 | MainLoss:0.3177 | Alpha:0.4230 | SPLoss:0.0003 | CLSLoss:0.0691 | top1:86.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1570 | MainLoss:0.1570 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:95.9813 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2655 | MainLoss:0.2655 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:89.3316 | AUROC:0.9906\n",
      "\n",
      "Epoch: [263 | 1000] LR: 0.001364\n",
      "Train | 8/8 | Loss:0.3453 | MainLoss:0.3158 | Alpha:0.4237 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:86.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1577 | MainLoss:0.1577 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:95.9408 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2644 | MainLoss:0.2644 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:89.3611 | AUROC:0.9907\n",
      "\n",
      "Epoch: [264 | 1000] LR: 0.001362\n",
      "Train | 8/8 | Loss:0.3438 | MainLoss:0.3143 | Alpha:0.4237 | SPLoss:0.0003 | CLSLoss:0.0692 | top1:87.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1592 | MainLoss:0.1592 | SPLoss:0.0003 | CLSLoss:0.0693 | top1:95.8629 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2622 | MainLoss:0.2622 | SPLoss:0.0003 | CLSLoss:0.0693 | top1:89.5151 | AUROC:0.9906\n",
      "\n",
      "Epoch: [265 | 1000] LR: 0.001360\n",
      "Train | 8/8 | Loss:0.3489 | MainLoss:0.3195 | Alpha:0.4231 | SPLoss:0.0003 | CLSLoss:0.0693 | top1:86.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1606 | MainLoss:0.1606 | SPLoss:0.0004 | CLSLoss:0.0693 | top1:95.7446 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2602 | MainLoss:0.2602 | SPLoss:0.0004 | CLSLoss:0.0693 | top1:89.6560 | AUROC:0.9906\n",
      "\n",
      "Epoch: [266 | 1000] LR: 0.001359\n",
      "Train | 8/8 | Loss:0.3590 | MainLoss:0.3297 | Alpha:0.4205 | SPLoss:0.0004 | CLSLoss:0.0692 | top1:86.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1590 | MainLoss:0.1590 | SPLoss:0.0004 | CLSLoss:0.0692 | top1:95.8879 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2624 | MainLoss:0.2624 | SPLoss:0.0004 | CLSLoss:0.0692 | top1:89.4889 | AUROC:0.9906\n",
      "\n",
      "Epoch: [267 | 1000] LR: 0.001357\n",
      "Train | 8/8 | Loss:0.3552 | MainLoss:0.3260 | Alpha:0.4202 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:85.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1599 | MainLoss:0.1599 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:95.8349 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2615 | MainLoss:0.2615 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:89.5446 | AUROC:0.9907\n",
      "\n",
      "Epoch: [268 | 1000] LR: 0.001355\n",
      "Train | 8/8 | Loss:0.3552 | MainLoss:0.3259 | Alpha:0.4225 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:86.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1573 | MainLoss:0.1573 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:95.9844 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2648 | MainLoss:0.2648 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:89.3381 | AUROC:0.9906\n",
      "\n",
      "Epoch: [269 | 1000] LR: 0.001353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.3461 | MainLoss:0.3167 | Alpha:0.4236 | SPLoss:0.0004 | CLSLoss:0.0690 | top1:86.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1569 | MainLoss:0.1569 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:96.0000 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2652 | MainLoss:0.2652 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:89.3218 | AUROC:0.9906\n",
      "\n",
      "Epoch: [270 | 1000] LR: 0.001351\n",
      "Train | 8/8 | Loss:0.3443 | MainLoss:0.3148 | Alpha:0.4237 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:86.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1566 | MainLoss:0.1566 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:96.0062 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2657 | MainLoss:0.2657 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:89.2955 | AUROC:0.9907\n",
      "\n",
      "Epoch: [271 | 1000] LR: 0.001349\n",
      "Train | 8/8 | Loss:0.3517 | MainLoss:0.3224 | Alpha:0.4220 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:86.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1542 | MainLoss:0.1542 | SPLoss:0.0004 | CLSLoss:0.0690 | top1:96.1371 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2690 | MainLoss:0.2690 | SPLoss:0.0004 | CLSLoss:0.0690 | top1:89.1153 | AUROC:0.9907\n",
      "\n",
      "Epoch: [272 | 1000] LR: 0.001348\n",
      "Train | 8/8 | Loss:0.3428 | MainLoss:0.3133 | Alpha:0.4248 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:87.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1561 | MainLoss:0.1561 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:96.0249 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2663 | MainLoss:0.2663 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:89.2693 | AUROC:0.9907\n",
      "\n",
      "Epoch: [273 | 1000] LR: 0.001346\n",
      "Train | 8/8 | Loss:0.3469 | MainLoss:0.3175 | Alpha:0.4224 | SPLoss:0.0004 | CLSLoss:0.0691 | top1:86.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1570 | MainLoss:0.1570 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:95.9969 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2650 | MainLoss:0.2650 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:89.3185 | AUROC:0.9906\n",
      "\n",
      "Epoch: [274 | 1000] LR: 0.001344\n",
      "Train | 8/8 | Loss:0.3485 | MainLoss:0.3191 | Alpha:0.4232 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:86.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1557 | MainLoss:0.1557 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:96.0654 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2670 | MainLoss:0.2670 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:89.2398 | AUROC:0.9906\n",
      "\n",
      "Epoch: [275 | 1000] LR: 0.001342\n",
      "Train | 8/8 | Loss:0.3502 | MainLoss:0.3208 | Alpha:0.4225 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:86.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1574 | MainLoss:0.1574 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:95.9844 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2649 | MainLoss:0.2649 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:89.3119 | AUROC:0.9906\n",
      "\n",
      "Epoch: [276 | 1000] LR: 0.001340\n",
      "Train | 8/8 | Loss:0.3478 | MainLoss:0.3183 | Alpha:0.4235 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:87.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1538 | MainLoss:0.1538 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:96.1651 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2697 | MainLoss:0.2697 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:89.0433 | AUROC:0.9906\n",
      "\n",
      "Epoch: [277 | 1000] LR: 0.001338\n",
      "Train | 8/8 | Loss:0.3405 | MainLoss:0.3110 | Alpha:0.4241 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:87.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1546 | MainLoss:0.1546 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:96.1122 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2683 | MainLoss:0.2683 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:89.1252 | AUROC:0.9906\n",
      "\n",
      "Epoch: [278 | 1000] LR: 0.001337\n",
      "Train | 8/8 | Loss:0.3458 | MainLoss:0.3163 | Alpha:0.4228 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:86.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1547 | MainLoss:0.1547 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:96.1090 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2680 | MainLoss:0.2680 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:89.1448 | AUROC:0.9905\n",
      "\n",
      "Epoch: [279 | 1000] LR: 0.001335\n",
      "Train | 8/8 | Loss:0.3499 | MainLoss:0.3204 | Alpha:0.4227 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:86.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1531 | MainLoss:0.1531 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:96.2056 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2703 | MainLoss:0.2703 | SPLoss:0.0005 | CLSLoss:0.0691 | top1:88.9843 | AUROC:0.9905\n",
      "\n",
      "Epoch: [280 | 1000] LR: 0.001333\n",
      "Train | 8/8 | Loss:0.3400 | MainLoss:0.3104 | Alpha:0.4244 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:87.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1536 | MainLoss:0.1536 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:96.1682 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2694 | MainLoss:0.2694 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:89.0465 | AUROC:0.9906\n",
      "\n",
      "Epoch: [281 | 1000] LR: 0.001331\n",
      "Train | 8/8 | Loss:0.3525 | MainLoss:0.3231 | Alpha:0.4216 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:86.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1559 | MainLoss:0.1559 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:96.0499 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2663 | MainLoss:0.2663 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:89.2267 | AUROC:0.9906\n",
      "\n",
      "Epoch: [282 | 1000] LR: 0.001329\n",
      "Train | 8/8 | Loss:0.3522 | MainLoss:0.3229 | Alpha:0.4208 | SPLoss:0.0006 | CLSLoss:0.0691 | top1:86.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1563 | MainLoss:0.1563 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:96.0093 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2656 | MainLoss:0.2656 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:89.2726 | AUROC:0.9906\n",
      "\n",
      "Epoch: [283 | 1000] LR: 0.001327\n",
      "Train | 8/8 | Loss:0.3476 | MainLoss:0.3182 | Alpha:0.4219 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:86.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1569 | MainLoss:0.1569 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:96.0031 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2646 | MainLoss:0.2646 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:89.3283 | AUROC:0.9906\n",
      "\n",
      "Epoch: [284 | 1000] LR: 0.001325\n",
      "Train | 8/8 | Loss:0.3452 | MainLoss:0.3157 | Alpha:0.4239 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:86.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1570 | MainLoss:0.1570 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:95.9938 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2644 | MainLoss:0.2644 | SPLoss:0.0006 | CLSLoss:0.0692 | top1:89.3414 | AUROC:0.9906\n",
      "\n",
      "Epoch: [285 | 1000] LR: 0.001323\n",
      "Train | 8/8 | Loss:0.3523 | MainLoss:0.3229 | Alpha:0.4218 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1580 | MainLoss:0.1580 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:95.9626 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2634 | MainLoss:0.2634 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:89.3807 | AUROC:0.9907\n",
      "\n",
      "Epoch: [286 | 1000] LR: 0.001321\n",
      "Train | 8/8 | Loss:0.3457 | MainLoss:0.3162 | Alpha:0.4231 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:86.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1551 | MainLoss:0.1551 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:96.0872 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2674 | MainLoss:0.2674 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:89.1579 | AUROC:0.9907\n",
      "\n",
      "Epoch: [287 | 1000] LR: 0.001320\n",
      "Train | 8/8 | Loss:0.3489 | MainLoss:0.3194 | Alpha:0.4226 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:86.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:96.0872 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2672 | MainLoss:0.2672 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:89.1514 | AUROC:0.9906\n",
      "\n",
      "Epoch: [288 | 1000] LR: 0.001318\n",
      "Train | 8/8 | Loss:0.3527 | MainLoss:0.3232 | Alpha:0.4223 | SPLoss:0.0007 | CLSLoss:0.0692 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1541 | MainLoss:0.1541 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:96.1807 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2689 | MainLoss:0.2689 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:89.0727 | AUROC:0.9906\n",
      "\n",
      "Epoch: [289 | 1000] LR: 0.001316\n",
      "Train | 8/8 | Loss:0.3556 | MainLoss:0.3262 | Alpha:0.4217 | SPLoss:0.0007 | CLSLoss:0.0690 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1556 | MainLoss:0.1556 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:96.0810 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2667 | MainLoss:0.2667 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:89.1874 | AUROC:0.9906\n",
      "\n",
      "Epoch: [290 | 1000] LR: 0.001314\n",
      "Train | 8/8 | Loss:0.3467 | MainLoss:0.3172 | Alpha:0.4228 | SPLoss:0.0007 | CLSLoss:0.0691 | top1:87.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1571 | MainLoss:0.1571 | SPLoss:0.0008 | CLSLoss:0.0691 | top1:95.9938 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2646 | MainLoss:0.2646 | SPLoss:0.0008 | CLSLoss:0.0691 | top1:89.3283 | AUROC:0.9906\n",
      "\n",
      "Epoch: [291 | 1000] LR: 0.001312\n",
      "Train | 8/8 | Loss:0.3380 | MainLoss:0.3082 | Alpha:0.4259 | SPLoss:0.0008 | CLSLoss:0.0691 | top1:87.3000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.1549 | MainLoss:0.1549 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:96.0903 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2671 | MainLoss:0.2671 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:89.1612 | AUROC:0.9906\n",
      "\n",
      "Epoch: [292 | 1000] LR: 0.001310\n",
      "Train | 8/8 | Loss:0.3444 | MainLoss:0.3148 | Alpha:0.4237 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:87.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1547 | MainLoss:0.1547 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:96.0841 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2671 | MainLoss:0.2671 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:89.1579 | AUROC:0.9906\n",
      "\n",
      "Epoch: [293 | 1000] LR: 0.001308\n",
      "Train | 8/8 | Loss:0.3477 | MainLoss:0.3181 | Alpha:0.4228 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:86.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1560 | MainLoss:0.1560 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:96.0374 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2654 | MainLoss:0.2654 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:89.2464 | AUROC:0.9906\n",
      "\n",
      "Epoch: [294 | 1000] LR: 0.001306\n",
      "Train | 8/8 | Loss:0.3482 | MainLoss:0.3186 | Alpha:0.4228 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:86.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1559 | MainLoss:0.1559 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:96.0499 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2657 | MainLoss:0.2657 | SPLoss:0.0008 | CLSLoss:0.0692 | top1:89.2333 | AUROC:0.9906\n",
      "\n",
      "Epoch: [295 | 1000] LR: 0.001304\n",
      "Train | 8/8 | Loss:0.3473 | MainLoss:0.3177 | Alpha:0.4225 | SPLoss:0.0009 | CLSLoss:0.0692 | top1:86.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1542 | MainLoss:0.1542 | SPLoss:0.0009 | CLSLoss:0.0692 | top1:96.1277 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2679 | MainLoss:0.2679 | SPLoss:0.0009 | CLSLoss:0.0692 | top1:89.1022 | AUROC:0.9905\n",
      "\n",
      "Epoch: [296 | 1000] LR: 0.001302\n",
      "Train | 8/8 | Loss:0.3423 | MainLoss:0.3127 | Alpha:0.4235 | SPLoss:0.0009 | CLSLoss:0.0692 | top1:86.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1554 | MainLoss:0.1554 | SPLoss:0.0009 | CLSLoss:0.0693 | top1:96.0623 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2661 | MainLoss:0.2661 | SPLoss:0.0009 | CLSLoss:0.0693 | top1:89.2071 | AUROC:0.9905\n",
      "\n",
      "Epoch: [297 | 1000] LR: 0.001300\n",
      "Train | 8/8 | Loss:0.3459 | MainLoss:0.3162 | Alpha:0.4226 | SPLoss:0.0009 | CLSLoss:0.0692 | top1:87.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1555 | MainLoss:0.1555 | SPLoss:0.0009 | CLSLoss:0.0693 | top1:96.0561 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2657 | MainLoss:0.2657 | SPLoss:0.0009 | CLSLoss:0.0693 | top1:89.2333 | AUROC:0.9905\n",
      "\n",
      "Epoch: [298 | 1000] LR: 0.001298\n",
      "Train | 8/8 | Loss:0.3424 | MainLoss:0.3126 | Alpha:0.4248 | SPLoss:0.0009 | CLSLoss:0.0692 | top1:87.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1559 | MainLoss:0.1559 | SPLoss:0.0009 | CLSLoss:0.0693 | top1:96.0249 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2650 | MainLoss:0.2650 | SPLoss:0.0009 | CLSLoss:0.0693 | top1:89.2759 | AUROC:0.9905\n",
      "\n",
      "Epoch: [299 | 1000] LR: 0.001296\n",
      "Train | 8/8 | Loss:0.3384 | MainLoss:0.3085 | Alpha:0.4250 | SPLoss:0.0009 | CLSLoss:0.0694 | top1:87.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1536 | MainLoss:0.1536 | SPLoss:0.0010 | CLSLoss:0.0694 | top1:96.1371 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2679 | MainLoss:0.2679 | SPLoss:0.0010 | CLSLoss:0.0694 | top1:89.0957 | AUROC:0.9905\n",
      "\n",
      "Epoch: [300 | 1000] LR: 0.001294\n",
      "Train | 8/8 | Loss:0.3430 | MainLoss:0.3132 | Alpha:0.4238 | SPLoss:0.0010 | CLSLoss:0.0694 | top1:86.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.0010 | CLSLoss:0.0694 | top1:96.0436 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2653 | MainLoss:0.2653 | SPLoss:0.0010 | CLSLoss:0.0694 | top1:89.2366 | AUROC:0.9905\n",
      "\n",
      "Epoch: [301 | 1000] LR: 0.001292\n",
      "Train | 8/8 | Loss:0.3461 | MainLoss:0.3167 | Alpha:0.4236 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:86.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1534 | MainLoss:0.1534 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:96.1526 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2677 | MainLoss:0.2677 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:89.1055 | AUROC:0.9905\n",
      "\n",
      "Epoch: [302 | 1000] LR: 0.001290\n",
      "Train | 8/8 | Loss:0.3451 | MainLoss:0.3156 | Alpha:0.4241 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:87.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1526 | MainLoss:0.1526 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:96.1900 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2688 | MainLoss:0.2688 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:89.0236 | AUROC:0.9905\n",
      "\n",
      "Epoch: [303 | 1000] LR: 0.001288\n",
      "Train | 8/8 | Loss:0.3401 | MainLoss:0.3106 | Alpha:0.4250 | SPLoss:0.0000 | CLSLoss:0.0695 | top1:87.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1548 | MainLoss:0.1548 | SPLoss:0.0000 | CLSLoss:0.0695 | top1:96.0623 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2654 | MainLoss:0.2654 | SPLoss:0.0000 | CLSLoss:0.0695 | top1:89.2333 | AUROC:0.9905\n",
      "\n",
      "Epoch: [304 | 1000] LR: 0.001286\n",
      "Train | 8/8 | Loss:0.3443 | MainLoss:0.3149 | Alpha:0.4239 | SPLoss:0.0000 | CLSLoss:0.0695 | top1:87.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1538 | MainLoss:0.1538 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:96.1028 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2667 | MainLoss:0.2667 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:89.1579 | AUROC:0.9905\n",
      "\n",
      "Epoch: [305 | 1000] LR: 0.001284\n",
      "Train | 8/8 | Loss:0.3473 | MainLoss:0.3178 | Alpha:0.4238 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:86.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1553 | MainLoss:0.1553 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:96.0343 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2647 | MainLoss:0.2647 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:89.2628 | AUROC:0.9905\n",
      "\n",
      "Epoch: [306 | 1000] LR: 0.001282\n",
      "Train | 8/8 | Loss:0.3469 | MainLoss:0.3175 | Alpha:0.4234 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:86.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1549 | MainLoss:0.1549 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:96.0561 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2654 | MainLoss:0.2654 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:89.2366 | AUROC:0.9905\n",
      "\n",
      "Epoch: [307 | 1000] LR: 0.001280\n",
      "Train | 8/8 | Loss:0.3423 | MainLoss:0.3128 | Alpha:0.4253 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:87.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1546 | MainLoss:0.1546 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:96.0623 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2659 | MainLoss:0.2659 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:89.2071 | AUROC:0.9906\n",
      "\n",
      "Epoch: [308 | 1000] LR: 0.001278\n",
      "Train | 8/8 | Loss:0.3563 | MainLoss:0.3271 | Alpha:0.4210 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:86.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1531 | MainLoss:0.1531 | SPLoss:0.0000 | CLSLoss:0.0692 | top1:96.1776 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2685 | MainLoss:0.2685 | SPLoss:0.0000 | CLSLoss:0.0692 | top1:89.0629 | AUROC:0.9905\n",
      "\n",
      "Epoch: [309 | 1000] LR: 0.001276\n",
      "Train | 8/8 | Loss:0.3433 | MainLoss:0.3139 | Alpha:0.4245 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:86.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1526 | MainLoss:0.1526 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:96.1932 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2691 | MainLoss:0.2691 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:89.0302 | AUROC:0.9905\n",
      "\n",
      "Epoch: [310 | 1000] LR: 0.001274\n",
      "Train | 8/8 | Loss:0.3442 | MainLoss:0.3149 | Alpha:0.4235 | SPLoss:0.0000 | CLSLoss:0.0692 | top1:86.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1557 | MainLoss:0.1557 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:96.0062 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2642 | MainLoss:0.2642 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:89.3250 | AUROC:0.9905\n",
      "\n",
      "Epoch: [311 | 1000] LR: 0.001272\n",
      "Train | 8/8 | Loss:0.3382 | MainLoss:0.3086 | Alpha:0.4263 | SPLoss:0.0000 | CLSLoss:0.0693 | top1:87.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1540 | MainLoss:0.1540 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:96.0935 | AUROC:0.9992\n",
      "Test | 62/8 | Loss:0.2661 | MainLoss:0.2661 | SPLoss:0.0000 | CLSLoss:0.0694 | top1:89.2005 | AUROC:0.9906\n",
      "\n",
      "Epoch: [312 | 1000] LR: 0.001270\n",
      "Train | 8/8 | Loss:0.3299 | MainLoss:0.3002 | Alpha:0.4280 | SPLoss:0.0001 | CLSLoss:0.0694 | top1:87.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1544 | MainLoss:0.1544 | SPLoss:0.0001 | CLSLoss:0.0695 | top1:96.0405 | AUROC:0.9993\n",
      "Test | 62/8 | Loss:0.2652 | MainLoss:0.2652 | SPLoss:0.0001 | CLSLoss:0.0695 | top1:89.2661 | AUROC:0.9906\n",
      "\n",
      "Epoch: [313 | 1000] LR: 0.001268\n",
      "Train | 8/8 | Loss:0.3479 | MainLoss:0.3185 | Alpha:0.4236 | SPLoss:0.0001 | CLSLoss:0.0695 | top1:87.5250 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "    loss_list.append([train_loss, test_loss, source_loss])\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
