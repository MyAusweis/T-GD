{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 250\n",
    "test_batch = 250\n",
    "lr = 0.1\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0/to_pggan/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 200, gamma=0.1, last_epoch=-1)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=100, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, ac=arc.avg))\n",
    "    return (losses.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, ac=arc.avg))\n",
    "    return (losses.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 16/16 | Loss:1.2608 | MainLoss:1.2401 | Alpha:0.0283 | SPLoss:0.7336 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.7650 | MainLoss:0.7650 | SPLoss:1.1205 | CLSLoss:0.0000 | AUROC:0.5235\n",
      "Test | 30/16 | Loss:0.2728 | MainLoss:0.2728 | SPLoss:1.1205 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.103000\n",
      "Train | 16/16 | Loss:0.7696 | MainLoss:0.7337 | Alpha:0.0295 | SPLoss:1.2158 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.7217 | MainLoss:0.7217 | SPLoss:1.2663 | CLSLoss:0.0000 | AUROC:0.5346\n",
      "Test | 30/16 | Loss:0.2847 | MainLoss:0.2847 | SPLoss:1.2663 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.106000\n",
      "Train | 16/16 | Loss:0.7449 | MainLoss:0.7057 | Alpha:0.0289 | SPLoss:1.3606 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6996 | MainLoss:0.6996 | SPLoss:1.4854 | CLSLoss:0.0000 | AUROC:0.5413\n",
      "Test | 30/16 | Loss:0.3282 | MainLoss:0.3282 | SPLoss:1.4854 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.109000\n",
      "Train | 16/16 | Loss:0.7390 | MainLoss:0.6946 | Alpha:0.0301 | SPLoss:1.4726 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6951 | MainLoss:0.6951 | SPLoss:1.5055 | CLSLoss:0.0000 | AUROC:0.5456\n",
      "Test | 30/16 | Loss:0.3298 | MainLoss:0.3298 | SPLoss:1.5054 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.112000\n",
      "Train | 16/16 | Loss:0.7323 | MainLoss:0.6876 | Alpha:0.0300 | SPLoss:1.4896 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6969 | MainLoss:0.6969 | SPLoss:1.4709 | CLSLoss:0.0000 | AUROC:0.5497\n",
      "Test | 30/16 | Loss:0.3075 | MainLoss:0.3075 | SPLoss:1.4709 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.115000\n",
      "Train | 16/16 | Loss:0.7282 | MainLoss:0.6852 | Alpha:0.0289 | SPLoss:1.4845 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6950 | MainLoss:0.6950 | SPLoss:1.4541 | CLSLoss:0.0000 | AUROC:0.5550\n",
      "Test | 30/16 | Loss:0.3092 | MainLoss:0.3092 | SPLoss:1.4541 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.118000\n",
      "Train | 16/16 | Loss:0.7285 | MainLoss:0.6862 | Alpha:0.0288 | SPLoss:1.4678 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6923 | MainLoss:0.6923 | SPLoss:1.4500 | CLSLoss:0.0000 | AUROC:0.5576\n",
      "Test | 30/16 | Loss:0.3180 | MainLoss:0.3180 | SPLoss:1.4500 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.121000\n",
      "Train | 16/16 | Loss:0.7259 | MainLoss:0.6826 | Alpha:0.0300 | SPLoss:1.4436 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6894 | MainLoss:0.6894 | SPLoss:1.4499 | CLSLoss:0.0000 | AUROC:0.5612\n",
      "Test | 30/16 | Loss:0.3310 | MainLoss:0.3310 | SPLoss:1.4499 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.124000\n",
      "Train | 16/16 | Loss:0.7279 | MainLoss:0.6864 | Alpha:0.0288 | SPLoss:1.4377 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6889 | MainLoss:0.6889 | SPLoss:1.4404 | CLSLoss:0.0000 | AUROC:0.5641\n",
      "Test | 30/16 | Loss:0.3288 | MainLoss:0.3288 | SPLoss:1.4404 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.127000\n",
      "Train | 16/16 | Loss:0.7218 | MainLoss:0.6803 | Alpha:0.0291 | SPLoss:1.4236 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6893 | MainLoss:0.6893 | SPLoss:1.4037 | CLSLoss:0.0000 | AUROC:0.5687\n",
      "Test | 30/16 | Loss:0.3195 | MainLoss:0.3195 | SPLoss:1.4037 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.130000\n",
      "Train | 16/16 | Loss:0.7246 | MainLoss:0.6837 | Alpha:0.0293 | SPLoss:1.3946 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6862 | MainLoss:0.6862 | SPLoss:1.4539 | CLSLoss:0.0000 | AUROC:0.5718\n",
      "Test | 30/16 | Loss:0.3711 | MainLoss:0.3711 | SPLoss:1.4539 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.133000\n",
      "Train | 16/16 | Loss:0.7234 | MainLoss:0.6830 | Alpha:0.0288 | SPLoss:1.3963 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6922 | MainLoss:0.6922 | SPLoss:1.3507 | CLSLoss:0.0000 | AUROC:0.5738\n",
      "Test | 30/16 | Loss:0.3004 | MainLoss:0.3004 | SPLoss:1.3507 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.136000\n",
      "Train | 16/16 | Loss:0.7213 | MainLoss:0.6809 | Alpha:0.0296 | SPLoss:1.3638 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6843 | MainLoss:0.6843 | SPLoss:1.4012 | CLSLoss:0.0000 | AUROC:0.5777\n",
      "Test | 30/16 | Loss:0.3583 | MainLoss:0.3583 | SPLoss:1.4012 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.139000\n",
      "Train | 16/16 | Loss:0.7211 | MainLoss:0.6814 | Alpha:0.0289 | SPLoss:1.3759 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6948 | MainLoss:0.6948 | SPLoss:1.3152 | CLSLoss:0.0000 | AUROC:0.5784\n",
      "Test | 30/16 | Loss:0.2892 | MainLoss:0.2892 | SPLoss:1.3152 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.142000\n",
      "Train | 16/16 | Loss:0.7194 | MainLoss:0.6803 | Alpha:0.0290 | SPLoss:1.3498 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6846 | MainLoss:0.6846 | SPLoss:1.3516 | CLSLoss:0.0000 | AUROC:0.5827\n",
      "Test | 30/16 | Loss:0.3328 | MainLoss:0.3328 | SPLoss:1.3516 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.145000\n",
      "Train | 16/16 | Loss:0.7166 | MainLoss:0.6782 | Alpha:0.0289 | SPLoss:1.3271 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6830 | MainLoss:0.6830 | SPLoss:1.3457 | CLSLoss:0.0000 | AUROC:0.5866\n",
      "Test | 30/16 | Loss:0.3474 | MainLoss:0.3474 | SPLoss:1.3457 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.148000\n",
      "Train | 16/16 | Loss:0.7215 | MainLoss:0.6830 | Alpha:0.0288 | SPLoss:1.3313 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6842 | MainLoss:0.6842 | SPLoss:1.3174 | CLSLoss:0.0000 | AUROC:0.5889\n",
      "Test | 30/16 | Loss:0.3275 | MainLoss:0.3275 | SPLoss:1.3174 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.151000\n",
      "Train | 16/16 | Loss:0.7204 | MainLoss:0.6831 | Alpha:0.0282 | SPLoss:1.3222 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6827 | MainLoss:0.6827 | SPLoss:1.3224 | CLSLoss:0.0000 | AUROC:0.5934\n",
      "Test | 30/16 | Loss:0.3322 | MainLoss:0.3322 | SPLoss:1.3224 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.154000\n",
      "Train | 16/16 | Loss:0.7174 | MainLoss:0.6790 | Alpha:0.0292 | SPLoss:1.3161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6847 | MainLoss:0.6847 | SPLoss:1.2814 | CLSLoss:0.0000 | AUROC:0.5960\n",
      "Test | 30/16 | Loss:0.3137 | MainLoss:0.3137 | SPLoss:1.2814 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.157000\n",
      "Train | 16/16 | Loss:0.7180 | MainLoss:0.6806 | Alpha:0.0290 | SPLoss:1.2902 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6807 | MainLoss:0.6807 | SPLoss:1.3301 | CLSLoss:0.0000 | AUROC:0.5986\n",
      "Test | 30/16 | Loss:0.3650 | MainLoss:0.3650 | SPLoss:1.3301 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.160000\n",
      "Train | 16/16 | Loss:0.7200 | MainLoss:0.6821 | Alpha:0.0295 | SPLoss:1.2848 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6851 | MainLoss:0.6851 | SPLoss:1.3693 | CLSLoss:0.0000 | AUROC:0.5985\n",
      "Test | 30/16 | Loss:0.4055 | MainLoss:0.4055 | SPLoss:1.3693 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.163000\n",
      "Train | 16/16 | Loss:0.7186 | MainLoss:0.6807 | Alpha:0.0293 | SPLoss:1.2899 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6879 | MainLoss:0.6879 | SPLoss:1.2474 | CLSLoss:0.0000 | AUROC:0.6023\n",
      "Test | 30/16 | Loss:0.2955 | MainLoss:0.2955 | SPLoss:1.2474 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.166000\n",
      "Train | 16/16 | Loss:0.7107 | MainLoss:0.6727 | Alpha:0.0299 | SPLoss:1.2689 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6787 | MainLoss:0.6787 | SPLoss:1.2806 | CLSLoss:0.0000 | AUROC:0.6045\n",
      "Test | 30/16 | Loss:0.3491 | MainLoss:0.3491 | SPLoss:1.2806 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.169000\n",
      "Train | 16/16 | Loss:0.7122 | MainLoss:0.6763 | Alpha:0.0283 | SPLoss:1.2678 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6763 | MainLoss:0.6763 | SPLoss:1.2898 | CLSLoss:0.0000 | AUROC:0.6115\n",
      "Test | 30/16 | Loss:0.3634 | MainLoss:0.3634 | SPLoss:1.2898 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.172000\n",
      "Train | 16/16 | Loss:0.7152 | MainLoss:0.6778 | Alpha:0.0297 | SPLoss:1.2590 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6793 | MainLoss:0.6793 | SPLoss:1.2601 | CLSLoss:0.0000 | AUROC:0.6136\n",
      "Test | 30/16 | Loss:0.3199 | MainLoss:0.3199 | SPLoss:1.2601 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.175000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.7157 | MainLoss:0.6787 | Alpha:0.0293 | SPLoss:1.2672 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6798 | MainLoss:0.6798 | SPLoss:1.3309 | CLSLoss:0.0000 | AUROC:0.6089\n",
      "Test | 30/16 | Loss:0.3834 | MainLoss:0.3834 | SPLoss:1.3309 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.178000\n",
      "Train | 16/16 | Loss:0.7168 | MainLoss:0.6795 | Alpha:0.0293 | SPLoss:1.2719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6842 | MainLoss:0.6842 | SPLoss:1.2400 | CLSLoss:0.0000 | AUROC:0.6114\n",
      "Test | 30/16 | Loss:0.3026 | MainLoss:0.3026 | SPLoss:1.2400 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.181000\n",
      "Train | 16/16 | Loss:0.7143 | MainLoss:0.6777 | Alpha:0.0291 | SPLoss:1.2562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6786 | MainLoss:0.6786 | SPLoss:1.2614 | CLSLoss:0.0000 | AUROC:0.6122\n",
      "Test | 30/16 | Loss:0.3348 | MainLoss:0.3348 | SPLoss:1.2614 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.184000\n",
      "Train | 16/16 | Loss:0.7122 | MainLoss:0.6768 | Alpha:0.0282 | SPLoss:1.2532 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6860 | MainLoss:0.6860 | SPLoss:1.2091 | CLSLoss:0.0000 | AUROC:0.6229\n",
      "Test | 30/16 | Loss:0.2845 | MainLoss:0.2845 | SPLoss:1.2091 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.187000\n",
      "Train | 16/16 | Loss:0.7173 | MainLoss:0.6803 | Alpha:0.0292 | SPLoss:1.2673 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6771 | MainLoss:0.6771 | SPLoss:1.2633 | CLSLoss:0.0000 | AUROC:0.6204\n",
      "Test | 30/16 | Loss:0.3262 | MainLoss:0.3262 | SPLoss:1.2633 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.190000\n",
      "Train | 16/16 | Loss:0.7161 | MainLoss:0.6802 | Alpha:0.0283 | SPLoss:1.2675 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6768 | MainLoss:0.6768 | SPLoss:1.2802 | CLSLoss:0.0000 | AUROC:0.6253\n",
      "Test | 30/16 | Loss:0.3215 | MainLoss:0.3215 | SPLoss:1.2802 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.193000\n",
      "Train | 16/16 | Loss:0.7140 | MainLoss:0.6781 | Alpha:0.0280 | SPLoss:1.2838 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6824 | MainLoss:0.6824 | SPLoss:1.3793 | CLSLoss:0.0000 | AUROC:0.6272\n",
      "Test | 30/16 | Loss:0.4251 | MainLoss:0.4251 | SPLoss:1.3793 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.196000\n",
      "Train | 16/16 | Loss:0.7143 | MainLoss:0.6777 | Alpha:0.0283 | SPLoss:1.2909 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6762 | MainLoss:0.6762 | SPLoss:1.2746 | CLSLoss:0.0000 | AUROC:0.6297\n",
      "Test | 30/16 | Loss:0.3181 | MainLoss:0.3181 | SPLoss:1.2746 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.199000\n",
      "Train | 16/16 | Loss:0.7145 | MainLoss:0.6771 | Alpha:0.0291 | SPLoss:1.2800 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6723 | MainLoss:0.6723 | SPLoss:1.3076 | CLSLoss:0.0000 | AUROC:0.6307\n",
      "Test | 30/16 | Loss:0.3613 | MainLoss:0.3613 | SPLoss:1.3076 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.202000\n",
      "Train | 16/16 | Loss:0.7239 | MainLoss:0.6857 | Alpha:0.0296 | SPLoss:1.2884 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6944 | MainLoss:0.6944 | SPLoss:1.2377 | CLSLoss:0.0000 | AUROC:0.6277\n",
      "Test | 30/16 | Loss:0.2657 | MainLoss:0.2657 | SPLoss:1.2377 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.205000\n",
      "Train | 16/16 | Loss:0.7207 | MainLoss:0.6834 | Alpha:0.0288 | SPLoss:1.2967 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6820 | MainLoss:0.6820 | SPLoss:1.2760 | CLSLoss:0.0000 | AUROC:0.6302\n",
      "Test | 30/16 | Loss:0.2969 | MainLoss:0.2969 | SPLoss:1.2760 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.208000\n",
      "Train | 16/16 | Loss:0.7181 | MainLoss:0.6802 | Alpha:0.0292 | SPLoss:1.2934 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6717 | MainLoss:0.6717 | SPLoss:1.3213 | CLSLoss:0.0000 | AUROC:0.6337\n",
      "Test | 30/16 | Loss:0.3633 | MainLoss:0.3633 | SPLoss:1.3213 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.211000\n",
      "Train | 16/16 | Loss:0.7176 | MainLoss:0.6800 | Alpha:0.0289 | SPLoss:1.3007 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6738 | MainLoss:0.6738 | SPLoss:1.3363 | CLSLoss:0.0000 | AUROC:0.6312\n",
      "Test | 30/16 | Loss:0.3828 | MainLoss:0.3828 | SPLoss:1.3363 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.214000\n",
      "Train | 16/16 | Loss:0.7152 | MainLoss:0.6778 | Alpha:0.0293 | SPLoss:1.2810 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6718 | MainLoss:0.6718 | SPLoss:1.3303 | CLSLoss:0.0000 | AUROC:0.6359\n",
      "Test | 30/16 | Loss:0.3695 | MainLoss:0.3695 | SPLoss:1.3303 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.217000\n",
      "Train | 16/16 | Loss:0.7180 | MainLoss:0.6809 | Alpha:0.0286 | SPLoss:1.2939 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6765 | MainLoss:0.6765 | SPLoss:1.3752 | CLSLoss:0.0000 | AUROC:0.6349\n",
      "Test | 30/16 | Loss:0.4029 | MainLoss:0.4029 | SPLoss:1.3751 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.220000\n",
      "Train | 16/16 | Loss:0.7237 | MainLoss:0.6861 | Alpha:0.0285 | SPLoss:1.3160 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6795 | MainLoss:0.6795 | SPLoss:1.4066 | CLSLoss:0.0000 | AUROC:0.6367\n",
      "Test | 30/16 | Loss:0.4221 | MainLoss:0.4221 | SPLoss:1.4066 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.223000\n",
      "Train | 16/16 | Loss:0.7056 | MainLoss:0.6674 | Alpha:0.0293 | SPLoss:1.3049 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6667 | MainLoss:0.6667 | SPLoss:1.3221 | CLSLoss:0.0000 | AUROC:0.6432\n",
      "Test | 30/16 | Loss:0.3677 | MainLoss:0.3677 | SPLoss:1.3221 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.226000\n",
      "Train | 16/16 | Loss:0.7160 | MainLoss:0.6781 | Alpha:0.0289 | SPLoss:1.3088 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6667 | MainLoss:0.6667 | SPLoss:1.3209 | CLSLoss:0.0000 | AUROC:0.6457\n",
      "Test | 30/16 | Loss:0.3495 | MainLoss:0.3495 | SPLoss:1.3209 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.229000\n",
      "Train | 16/16 | Loss:0.7164 | MainLoss:0.6774 | Alpha:0.0298 | SPLoss:1.3095 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6666 | MainLoss:0.6666 | SPLoss:1.3521 | CLSLoss:0.0000 | AUROC:0.6478\n",
      "Test | 30/16 | Loss:0.3600 | MainLoss:0.3600 | SPLoss:1.3521 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.232000\n",
      "Train | 16/16 | Loss:0.7179 | MainLoss:0.6782 | Alpha:0.0299 | SPLoss:1.3208 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6687 | MainLoss:0.6687 | SPLoss:1.4015 | CLSLoss:0.0000 | AUROC:0.6510\n",
      "Test | 30/16 | Loss:0.3950 | MainLoss:0.3950 | SPLoss:1.4015 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.235000\n",
      "Train | 16/16 | Loss:0.7191 | MainLoss:0.6803 | Alpha:0.0284 | SPLoss:1.3622 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6652 | MainLoss:0.6652 | SPLoss:1.3564 | CLSLoss:0.0000 | AUROC:0.6616\n",
      "Test | 30/16 | Loss:0.3130 | MainLoss:0.3130 | SPLoss:1.3564 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.238000\n",
      "Train | 16/16 | Loss:0.7204 | MainLoss:0.6805 | Alpha:0.0286 | SPLoss:1.3829 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6625 | MainLoss:0.6625 | SPLoss:1.4623 | CLSLoss:0.0000 | AUROC:0.6645\n",
      "Test | 30/16 | Loss:0.3998 | MainLoss:0.3998 | SPLoss:1.4623 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.241000\n",
      "Train | 16/16 | Loss:0.7131 | MainLoss:0.6713 | Alpha:0.0295 | SPLoss:1.4171 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6944 | MainLoss:0.6944 | SPLoss:1.3161 | CLSLoss:0.0000 | AUROC:0.6655\n",
      "Test | 30/16 | Loss:0.2471 | MainLoss:0.2471 | SPLoss:1.3161 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.244000\n",
      "Train | 16/16 | Loss:0.7132 | MainLoss:0.6727 | Alpha:0.0289 | SPLoss:1.4002 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6554 | MainLoss:0.6554 | SPLoss:1.4461 | CLSLoss:0.0000 | AUROC:0.6738\n",
      "Test | 30/16 | Loss:0.3693 | MainLoss:0.3693 | SPLoss:1.4461 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.247000\n",
      "Train | 16/16 | Loss:0.7204 | MainLoss:0.6792 | Alpha:0.0288 | SPLoss:1.4280 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6584 | MainLoss:0.6584 | SPLoss:1.4890 | CLSLoss:0.0000 | AUROC:0.6771\n",
      "Test | 30/16 | Loss:0.3836 | MainLoss:0.3836 | SPLoss:1.4890 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.7154 | MainLoss:0.6730 | Alpha:0.0292 | SPLoss:1.4454 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6620 | MainLoss:0.6620 | SPLoss:1.4079 | CLSLoss:0.0000 | AUROC:0.6840\n",
      "Test | 30/16 | Loss:0.2936 | MainLoss:0.2936 | SPLoss:1.4079 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.253000\n",
      "Train | 16/16 | Loss:0.7161 | MainLoss:0.6726 | Alpha:0.0301 | SPLoss:1.4450 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6534 | MainLoss:0.6534 | SPLoss:1.5188 | CLSLoss:0.0000 | AUROC:0.6871\n",
      "Test | 30/16 | Loss:0.3734 | MainLoss:0.3734 | SPLoss:1.5188 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.256000\n",
      "Train | 16/16 | Loss:0.7237 | MainLoss:0.6805 | Alpha:0.0286 | SPLoss:1.5058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6611 | MainLoss:0.6611 | SPLoss:1.5129 | CLSLoss:0.0000 | AUROC:0.6963\n",
      "Test | 30/16 | Loss:0.2973 | MainLoss:0.2973 | SPLoss:1.5129 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.259000\n",
      "Train | 16/16 | Loss:0.7158 | MainLoss:0.6706 | Alpha:0.0290 | SPLoss:1.5612 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6642 | MainLoss:0.6642 | SPLoss:1.5189 | CLSLoss:0.0000 | AUROC:0.7051\n",
      "Test | 30/16 | Loss:0.2754 | MainLoss:0.2754 | SPLoss:1.5189 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.262000\n",
      "Train | 16/16 | Loss:0.7129 | MainLoss:0.6683 | Alpha:0.0283 | SPLoss:1.5770 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6633 | MainLoss:0.6633 | SPLoss:1.7409 | CLSLoss:0.0000 | AUROC:0.7166\n",
      "Test | 30/16 | Loss:0.4693 | MainLoss:0.4693 | SPLoss:1.7409 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.265000\n",
      "Train | 16/16 | Loss:0.7194 | MainLoss:0.6728 | Alpha:0.0287 | SPLoss:1.6233 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6443 | MainLoss:0.6443 | SPLoss:1.7167 | CLSLoss:0.0000 | AUROC:0.7245\n",
      "Test | 30/16 | Loss:0.4366 | MainLoss:0.4366 | SPLoss:1.7167 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.268000\n",
      "Train | 16/16 | Loss:0.7202 | MainLoss:0.6719 | Alpha:0.0293 | SPLoss:1.6479 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6400 | MainLoss:0.6400 | SPLoss:1.6291 | CLSLoss:0.0000 | AUROC:0.7289\n",
      "Test | 30/16 | Loss:0.3006 | MainLoss:0.3006 | SPLoss:1.6291 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.271000\n",
      "Train | 16/16 | Loss:0.7151 | MainLoss:0.6651 | Alpha:0.0295 | SPLoss:1.6977 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6220 | MainLoss:0.6220 | SPLoss:1.8502 | CLSLoss:0.0000 | AUROC:0.7529\n",
      "Test | 30/16 | Loss:0.4171 | MainLoss:0.4171 | SPLoss:1.8502 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.274000\n",
      "Train | 16/16 | Loss:0.7203 | MainLoss:0.6677 | Alpha:0.0288 | SPLoss:1.8229 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6116 | MainLoss:0.6116 | SPLoss:1.8868 | CLSLoss:0.0000 | AUROC:0.7700\n",
      "Test | 30/16 | Loss:0.3685 | MainLoss:0.3685 | SPLoss:1.8868 | CLSLoss:0.0000 | AUROC:0.9996\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.277000\n",
      "Train | 16/16 | Loss:0.7085 | MainLoss:0.6526 | Alpha:0.0296 | SPLoss:1.8929 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5929 | MainLoss:0.5929 | SPLoss:1.9544 | CLSLoss:0.0000 | AUROC:0.7968\n",
      "Test | 30/16 | Loss:0.3375 | MainLoss:0.3375 | SPLoss:1.9544 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.280000\n",
      "Train | 16/16 | Loss:0.7068 | MainLoss:0.6461 | Alpha:0.0299 | SPLoss:2.0249 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5457 | MainLoss:0.5457 | SPLoss:2.1390 | CLSLoss:0.0000 | AUROC:0.8326\n",
      "Test | 30/16 | Loss:0.4115 | MainLoss:0.4115 | SPLoss:2.1390 | CLSLoss:0.0000 | AUROC:0.9995\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.283000\n",
      "Train | 16/16 | Loss:0.6986 | MainLoss:0.6352 | Alpha:0.0287 | SPLoss:2.2194 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.5285 | MainLoss:0.5285 | SPLoss:2.4755 | CLSLoss:0.0000 | AUROC:0.8743\n",
      "Test | 30/16 | Loss:0.4650 | MainLoss:0.4650 | SPLoss:2.4755 | CLSLoss:0.0000 | AUROC:0.9992\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.286000\n",
      "Train | 16/16 | Loss:0.7065 | MainLoss:0.6355 | Alpha:0.0284 | SPLoss:2.4986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4640 | MainLoss:0.4640 | SPLoss:2.6322 | CLSLoss:0.0000 | AUROC:0.9140\n",
      "Test | 30/16 | Loss:0.3700 | MainLoss:0.3700 | SPLoss:2.6322 | CLSLoss:0.0000 | AUROC:0.9989\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.289000\n",
      "Train | 16/16 | Loss:0.6880 | MainLoss:0.6088 | Alpha:0.0277 | SPLoss:2.8382 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4398 | MainLoss:0.4398 | SPLoss:2.9393 | CLSLoss:0.0000 | AUROC:0.9429\n",
      "Test | 30/16 | Loss:0.3497 | MainLoss:0.3497 | SPLoss:2.9393 | CLSLoss:0.0000 | AUROC:0.9982\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.292000\n",
      "Train | 16/16 | Loss:0.6538 | MainLoss:0.5655 | Alpha:0.0285 | SPLoss:3.0946 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3267 | MainLoss:0.3267 | SPLoss:3.2712 | CLSLoss:0.0000 | AUROC:0.9667\n",
      "Test | 30/16 | Loss:0.4713 | MainLoss:0.4713 | SPLoss:3.2712 | CLSLoss:0.0000 | AUROC:0.9973\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.295000\n",
      "Train | 16/16 | Loss:0.6818 | MainLoss:0.5584 | Alpha:0.0281 | SPLoss:4.3725 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3206 | MainLoss:0.3206 | SPLoss:4.5700 | CLSLoss:0.0000 | AUROC:0.9794\n",
      "Test | 30/16 | Loss:0.4227 | MainLoss:0.4227 | SPLoss:4.5700 | CLSLoss:0.0000 | AUROC:0.9966\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.298000\n",
      "Train | 16/16 | Loss:0.6628 | MainLoss:0.5307 | Alpha:0.0289 | SPLoss:4.5717 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2602 | MainLoss:0.2602 | SPLoss:4.5669 | CLSLoss:0.0000 | AUROC:0.9852\n",
      "Test | 30/16 | Loss:0.5419 | MainLoss:0.5419 | SPLoss:4.5669 | CLSLoss:0.0000 | AUROC:0.9959\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.301000\n",
      "Train | 16/16 | Loss:0.6669 | MainLoss:0.5382 | Alpha:0.0291 | SPLoss:4.4094 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2376 | MainLoss:0.2376 | SPLoss:4.4383 | CLSLoss:0.0000 | AUROC:0.9875\n",
      "Test | 30/16 | Loss:0.5801 | MainLoss:0.5801 | SPLoss:4.4383 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.304000\n",
      "Train | 16/16 | Loss:0.6484 | MainLoss:0.5263 | Alpha:0.0283 | SPLoss:4.3033 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3039 | MainLoss:0.3039 | SPLoss:4.2486 | CLSLoss:0.0000 | AUROC:0.9884\n",
      "Test | 30/16 | Loss:0.4198 | MainLoss:0.4198 | SPLoss:4.2486 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.307000\n",
      "Train | 16/16 | Loss:0.6333 | MainLoss:0.5077 | Alpha:0.0291 | SPLoss:4.3053 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2620 | MainLoss:0.2620 | SPLoss:4.2852 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "Test | 30/16 | Loss:0.5235 | MainLoss:0.5235 | SPLoss:4.2852 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.310000\n",
      "Train | 16/16 | Loss:0.6317 | MainLoss:0.5111 | Alpha:0.0286 | SPLoss:4.2143 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3344 | MainLoss:0.3344 | SPLoss:4.1784 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "Test | 30/16 | Loss:0.3808 | MainLoss:0.3808 | SPLoss:4.1784 | CLSLoss:0.0000 | AUROC:0.9958\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.313000\n",
      "Train | 16/16 | Loss:0.6326 | MainLoss:0.5060 | Alpha:0.0299 | SPLoss:4.2386 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:4.1754 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "Test | 30/16 | Loss:0.6103 | MainLoss:0.6103 | SPLoss:4.1754 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.316000\n",
      "Train | 16/16 | Loss:0.6301 | MainLoss:0.5126 | Alpha:0.0284 | SPLoss:4.1391 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2534 | MainLoss:0.2534 | SPLoss:4.2127 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "Test | 30/16 | Loss:0.5575 | MainLoss:0.5575 | SPLoss:4.2127 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.319000\n",
      "Train | 16/16 | Loss:0.6200 | MainLoss:0.4977 | Alpha:0.0295 | SPLoss:4.1361 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1981 | MainLoss:0.1981 | SPLoss:4.2220 | CLSLoss:0.0000 | AUROC:0.9916\n",
      "Test | 30/16 | Loss:0.6965 | MainLoss:0.6965 | SPLoss:4.2220 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.322000\n",
      "Train | 16/16 | Loss:0.6267 | MainLoss:0.5058 | Alpha:0.0294 | SPLoss:4.1049 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2402 | MainLoss:0.2402 | SPLoss:4.1817 | CLSLoss:0.0000 | AUROC:0.9916\n",
      "Test | 30/16 | Loss:0.5409 | MainLoss:0.5409 | SPLoss:4.1817 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.325000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6193 | MainLoss:0.4997 | Alpha:0.0288 | SPLoss:4.1562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4032 | MainLoss:0.4032 | SPLoss:4.0521 | CLSLoss:0.0000 | AUROC:0.9914\n",
      "Test | 30/16 | Loss:0.3110 | MainLoss:0.3110 | SPLoss:4.0521 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.328000\n",
      "Train | 16/16 | Loss:0.6188 | MainLoss:0.4984 | Alpha:0.0293 | SPLoss:4.1076 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3417 | MainLoss:0.3417 | SPLoss:4.0370 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "Test | 30/16 | Loss:0.3669 | MainLoss:0.3669 | SPLoss:4.0370 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.331000\n",
      "Train | 16/16 | Loss:0.6091 | MainLoss:0.4907 | Alpha:0.0287 | SPLoss:4.1177 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2568 | MainLoss:0.2568 | SPLoss:3.9817 | CLSLoss:0.0000 | AUROC:0.9916\n",
      "Test | 30/16 | Loss:0.4874 | MainLoss:0.4874 | SPLoss:3.9817 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.334000\n",
      "Train | 16/16 | Loss:0.6130 | MainLoss:0.4973 | Alpha:0.0286 | SPLoss:4.0424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2361 | MainLoss:0.2361 | SPLoss:4.1999 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "Test | 30/16 | Loss:0.5632 | MainLoss:0.5632 | SPLoss:4.1999 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.337000\n",
      "Train | 16/16 | Loss:0.6104 | MainLoss:0.4937 | Alpha:0.0285 | SPLoss:4.1005 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2357 | MainLoss:0.2357 | SPLoss:4.1495 | CLSLoss:0.0000 | AUROC:0.9922\n",
      "Test | 30/16 | Loss:0.5553 | MainLoss:0.5553 | SPLoss:4.1495 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.340000\n",
      "Train | 16/16 | Loss:0.6078 | MainLoss:0.4891 | Alpha:0.0290 | SPLoss:4.0887 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2321 | MainLoss:0.2321 | SPLoss:4.0504 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "Test | 30/16 | Loss:0.5498 | MainLoss:0.5498 | SPLoss:4.0504 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.343000\n",
      "Train | 16/16 | Loss:0.6186 | MainLoss:0.5045 | Alpha:0.0286 | SPLoss:3.9867 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2253 | MainLoss:0.2253 | SPLoss:3.9370 | CLSLoss:0.0000 | AUROC:0.9917\n",
      "Test | 30/16 | Loss:0.5786 | MainLoss:0.5786 | SPLoss:3.9370 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.346000\n",
      "Train | 16/16 | Loss:0.6088 | MainLoss:0.4945 | Alpha:0.0293 | SPLoss:3.9058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2168 | MainLoss:0.2168 | SPLoss:4.0062 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "Test | 30/16 | Loss:0.5954 | MainLoss:0.5954 | SPLoss:4.0062 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.349000\n",
      "Train | 16/16 | Loss:0.6189 | MainLoss:0.4831 | Alpha:0.0292 | SPLoss:4.6748 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3112 | MainLoss:0.3112 | SPLoss:6.8879 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "Test | 30/16 | Loss:0.4316 | MainLoss:0.4316 | SPLoss:6.8879 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.352000\n",
      "Train | 16/16 | Loss:0.6600 | MainLoss:0.4732 | Alpha:0.0286 | SPLoss:6.5408 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3378 | MainLoss:0.3378 | SPLoss:5.9268 | CLSLoss:0.0000 | AUROC:0.9916\n",
      "Test | 30/16 | Loss:0.3710 | MainLoss:0.3710 | SPLoss:5.9268 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.355000\n",
      "Train | 16/16 | Loss:0.6465 | MainLoss:0.4823 | Alpha:0.0286 | SPLoss:5.7355 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2426 | MainLoss:0.2426 | SPLoss:5.3837 | CLSLoss:0.0000 | AUROC:0.9932\n",
      "Test | 30/16 | Loss:0.5200 | MainLoss:0.5200 | SPLoss:5.3837 | CLSLoss:0.0000 | AUROC:0.9928\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.358000\n",
      "Train | 16/16 | Loss:0.6329 | MainLoss:0.4825 | Alpha:0.0294 | SPLoss:5.1104 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3843 | MainLoss:0.3843 | SPLoss:4.6744 | CLSLoss:0.0000 | AUROC:0.9913\n",
      "Test | 30/16 | Loss:0.3272 | MainLoss:0.3272 | SPLoss:4.6744 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.361000\n",
      "Train | 16/16 | Loss:0.6650 | MainLoss:0.5208 | Alpha:0.0296 | SPLoss:4.8575 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2791 | MainLoss:0.2791 | SPLoss:4.7872 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "Test | 30/16 | Loss:0.4777 | MainLoss:0.4777 | SPLoss:4.7872 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.364000\n",
      "Train | 16/16 | Loss:0.6247 | MainLoss:0.4906 | Alpha:0.0292 | SPLoss:4.5880 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2888 | MainLoss:0.2888 | SPLoss:4.4233 | CLSLoss:0.0000 | AUROC:0.9918\n",
      "Test | 30/16 | Loss:0.4602 | MainLoss:0.4602 | SPLoss:4.4233 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.367000\n",
      "Train | 16/16 | Loss:0.6145 | MainLoss:0.4881 | Alpha:0.0292 | SPLoss:4.3185 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3491 | MainLoss:0.3491 | SPLoss:4.1320 | CLSLoss:0.0000 | AUROC:0.9918\n",
      "Test | 30/16 | Loss:0.3691 | MainLoss:0.3691 | SPLoss:4.1320 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.370000\n",
      "Train | 16/16 | Loss:0.6199 | MainLoss:0.4933 | Alpha:0.0296 | SPLoss:4.2536 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2208 | MainLoss:0.2208 | SPLoss:4.1499 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "Test | 30/16 | Loss:0.5710 | MainLoss:0.5710 | SPLoss:4.1499 | CLSLoss:0.0000 | AUROC:0.9934\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.373000\n",
      "Train | 16/16 | Loss:0.6055 | MainLoss:0.4876 | Alpha:0.0289 | SPLoss:4.0855 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1906 | MainLoss:0.1906 | SPLoss:4.2244 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "Test | 30/16 | Loss:0.7391 | MainLoss:0.7391 | SPLoss:4.2244 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.376000\n",
      "Train | 16/16 | Loss:0.6183 | MainLoss:0.4983 | Alpha:0.0293 | SPLoss:4.0977 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2696 | MainLoss:0.2696 | SPLoss:4.1979 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "Test | 30/16 | Loss:0.4700 | MainLoss:0.4700 | SPLoss:4.1979 | CLSLoss:0.0000 | AUROC:0.9924\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.379000\n",
      "Train | 16/16 | Loss:0.6106 | MainLoss:0.4918 | Alpha:0.0283 | SPLoss:4.2006 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2500 | MainLoss:0.2500 | SPLoss:4.2593 | CLSLoss:0.0000 | AUROC:0.9912\n",
      "Test | 30/16 | Loss:0.5738 | MainLoss:0.5738 | SPLoss:4.2593 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.382000\n",
      "Train | 16/16 | Loss:0.6157 | MainLoss:0.4951 | Alpha:0.0289 | SPLoss:4.1796 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2557 | MainLoss:0.2557 | SPLoss:4.1170 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "Test | 30/16 | Loss:0.4964 | MainLoss:0.4964 | SPLoss:4.1170 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.385000\n",
      "Train | 16/16 | Loss:0.6130 | MainLoss:0.4936 | Alpha:0.0291 | SPLoss:4.1020 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1930 | MainLoss:0.1930 | SPLoss:4.2463 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "Test | 30/16 | Loss:0.7115 | MainLoss:0.7115 | SPLoss:4.2463 | CLSLoss:0.0000 | AUROC:0.9918\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.388000\n",
      "Train | 16/16 | Loss:0.5976 | MainLoss:0.4798 | Alpha:0.0290 | SPLoss:4.0572 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2575 | MainLoss:0.2575 | SPLoss:4.1091 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "Test | 30/16 | Loss:0.5040 | MainLoss:0.5040 | SPLoss:4.1091 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.391000\n",
      "Train | 16/16 | Loss:0.6094 | MainLoss:0.4890 | Alpha:0.0299 | SPLoss:4.0350 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2688 | MainLoss:0.2688 | SPLoss:3.8760 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "Test | 30/16 | Loss:0.4879 | MainLoss:0.4879 | SPLoss:3.8760 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.394000\n",
      "Train | 16/16 | Loss:0.6114 | MainLoss:0.4943 | Alpha:0.0295 | SPLoss:3.9715 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.4483 | MainLoss:0.4483 | SPLoss:3.6409 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "Test | 30/16 | Loss:0.2822 | MainLoss:0.2822 | SPLoss:3.6409 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.397000\n",
      "Train | 16/16 | Loss:0.6031 | MainLoss:0.4889 | Alpha:0.0287 | SPLoss:3.9858 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3467 | MainLoss:0.3467 | SPLoss:3.8637 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "Test | 30/16 | Loss:0.3671 | MainLoss:0.3671 | SPLoss:3.8637 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.400000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.9413 | MainLoss:0.4964 | Alpha:0.0296 | SPLoss:15.4738 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2909 | MainLoss:0.2909 | SPLoss:33.8702 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "Test | 30/16 | Loss:0.4603 | MainLoss:0.4603 | SPLoss:33.8702 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.400000\n",
      "Train | 16/16 | Loss:1.3361 | MainLoss:0.5051 | Alpha:0.0288 | SPLoss:28.9600 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2930 | MainLoss:0.2930 | SPLoss:23.6892 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "Test | 30/16 | Loss:0.4408 | MainLoss:0.4408 | SPLoss:23.6892 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.399999\n",
      "Train | 16/16 | Loss:1.0850 | MainLoss:0.5002 | Alpha:0.0288 | SPLoss:20.2959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2245 | MainLoss:0.2245 | SPLoss:17.1282 | CLSLoss:0.0000 | AUROC:0.9924\n",
      "Test | 30/16 | Loss:0.5910 | MainLoss:0.5910 | SPLoss:17.1282 | CLSLoss:0.0000 | AUROC:0.9910\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.399996\n",
      "Train | 16/16 | Loss:0.9012 | MainLoss:0.4769 | Alpha:0.0284 | SPLoss:15.0740 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3522 | MainLoss:0.3522 | SPLoss:12.5387 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "Test | 30/16 | Loss:0.3663 | MainLoss:0.3663 | SPLoss:12.5387 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.399991\n",
      "Train | 16/16 | Loss:0.8070 | MainLoss:0.4809 | Alpha:0.0293 | SPLoss:11.0725 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2446 | MainLoss:0.2446 | SPLoss:9.5476 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.5268 | MainLoss:0.5268 | SPLoss:9.5476 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.399984\n",
      "Train | 16/16 | Loss:0.7454 | MainLoss:0.4956 | Alpha:0.0291 | SPLoss:8.5776 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2624 | MainLoss:0.2624 | SPLoss:7.5562 | CLSLoss:0.0000 | AUROC:0.9923\n",
      "Test | 30/16 | Loss:0.4896 | MainLoss:0.4896 | SPLoss:7.5562 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.399975\n",
      "Train | 16/16 | Loss:0.6940 | MainLoss:0.4896 | Alpha:0.0291 | SPLoss:7.0156 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1997 | MainLoss:0.1997 | SPLoss:6.3177 | CLSLoss:0.0000 | AUROC:0.9932\n",
      "Test | 30/16 | Loss:0.6228 | MainLoss:0.6228 | SPLoss:6.3177 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.399964\n",
      "Train | 16/16 | Loss:0.6523 | MainLoss:0.4828 | Alpha:0.0282 | SPLoss:6.0034 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2263 | MainLoss:0.2263 | SPLoss:5.7674 | CLSLoss:0.0000 | AUROC:0.9921\n",
      "Test | 30/16 | Loss:0.6144 | MainLoss:0.6144 | SPLoss:5.7674 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.399952\n",
      "Train | 16/16 | Loss:0.6377 | MainLoss:0.4808 | Alpha:0.0292 | SPLoss:5.3546 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2496 | MainLoss:0.2496 | SPLoss:4.9561 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "Test | 30/16 | Loss:0.5371 | MainLoss:0.5371 | SPLoss:4.9561 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.399937\n",
      "Train | 16/16 | Loss:0.6311 | MainLoss:0.4894 | Alpha:0.0292 | SPLoss:4.8682 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2334 | MainLoss:0.2334 | SPLoss:4.5932 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "Test | 30/16 | Loss:0.5378 | MainLoss:0.5378 | SPLoss:4.5932 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.399920\n",
      "Train | 16/16 | Loss:0.6488 | MainLoss:0.5093 | Alpha:0.0294 | SPLoss:4.7288 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2565 | MainLoss:0.2565 | SPLoss:5.3266 | CLSLoss:0.0000 | AUROC:0.9928\n",
      "Test | 30/16 | Loss:0.4955 | MainLoss:0.4955 | SPLoss:5.3266 | CLSLoss:0.0000 | AUROC:0.9934\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.399901\n",
      "Train | 16/16 | Loss:0.6323 | MainLoss:0.4841 | Alpha:0.0292 | SPLoss:5.0613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3177 | MainLoss:0.3177 | SPLoss:4.6409 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "Test | 30/16 | Loss:0.3888 | MainLoss:0.3888 | SPLoss:4.6409 | CLSLoss:0.0000 | AUROC:0.9934\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.399881\n",
      "Train | 16/16 | Loss:0.6052 | MainLoss:0.4740 | Alpha:0.0294 | SPLoss:4.4356 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2145 | MainLoss:0.2145 | SPLoss:4.3172 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "Test | 30/16 | Loss:0.6071 | MainLoss:0.6071 | SPLoss:4.3172 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.399858\n",
      "Train | 16/16 | Loss:0.8667 | MainLoss:0.4967 | Alpha:0.0291 | SPLoss:13.8927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1938 | MainLoss:0.1938 | SPLoss:25.9778 | CLSLoss:0.0000 | AUROC:0.9932\n",
      "Test | 30/16 | Loss:0.6990 | MainLoss:0.6990 | SPLoss:25.9778 | CLSLoss:0.0000 | AUROC:0.9910\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.399833\n",
      "Train | 16/16 | Loss:1.1203 | MainLoss:0.4742 | Alpha:0.0294 | SPLoss:21.9871 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2188 | MainLoss:0.2188 | SPLoss:18.2054 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "Test | 30/16 | Loss:0.6323 | MainLoss:0.6323 | SPLoss:18.2054 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.399807\n",
      "Train | 16/16 | Loss:0.9870 | MainLoss:0.5103 | Alpha:0.0302 | SPLoss:15.6913 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2274 | MainLoss:0.2274 | SPLoss:13.3009 | CLSLoss:0.0000 | AUROC:0.9932\n",
      "Test | 30/16 | Loss:0.5661 | MainLoss:0.5661 | SPLoss:13.3009 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.399778\n",
      "Train | 16/16 | Loss:0.8046 | MainLoss:0.4737 | Alpha:0.0286 | SPLoss:11.6125 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2523 | MainLoss:0.2523 | SPLoss:9.9463 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.5075 | MainLoss:0.5075 | SPLoss:9.9463 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.399747\n",
      "Train | 16/16 | Loss:0.7368 | MainLoss:0.4764 | Alpha:0.0293 | SPLoss:8.8814 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2410 | MainLoss:0.2410 | SPLoss:7.8046 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "Test | 30/16 | Loss:0.5213 | MainLoss:0.5213 | SPLoss:7.8046 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.399715\n",
      "Train | 16/16 | Loss:0.7869 | MainLoss:0.4901 | Alpha:0.0285 | SPLoss:10.3109 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1579 | MainLoss:0.1579 | SPLoss:9.8596 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 30/16 | Loss:0.8096 | MainLoss:0.8096 | SPLoss:9.8596 | CLSLoss:0.0000 | AUROC:0.9866\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.399680\n",
      "Train | 16/16 | Loss:0.7834 | MainLoss:0.5248 | Alpha:0.0289 | SPLoss:8.8847 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3080 | MainLoss:0.3080 | SPLoss:7.8493 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "Test | 30/16 | Loss:0.4212 | MainLoss:0.4212 | SPLoss:7.8493 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.399644\n",
      "Train | 16/16 | Loss:0.6878 | MainLoss:0.4805 | Alpha:0.0289 | SPLoss:7.2419 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2944 | MainLoss:0.2944 | SPLoss:6.2606 | CLSLoss:0.0000 | AUROC:0.9933\n",
      "Test | 30/16 | Loss:0.4430 | MainLoss:0.4430 | SPLoss:6.2606 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.399605\n",
      "Train | 16/16 | Loss:0.6510 | MainLoss:0.4793 | Alpha:0.0292 | SPLoss:5.8679 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2154 | MainLoss:0.2154 | SPLoss:5.5329 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "Test | 30/16 | Loss:0.6442 | MainLoss:0.6442 | SPLoss:5.5329 | CLSLoss:0.0000 | AUROC:0.9879\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.399565\n",
      "Train | 16/16 | Loss:0.6355 | MainLoss:0.4898 | Alpha:0.0284 | SPLoss:5.1204 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2653 | MainLoss:0.2653 | SPLoss:4.8070 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "Test | 30/16 | Loss:0.5076 | MainLoss:0.5076 | SPLoss:4.8070 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.399523\n",
      "Train | 16/16 | Loss:0.6612 | MainLoss:0.5111 | Alpha:0.0286 | SPLoss:5.2534 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1977 | MainLoss:0.1977 | SPLoss:5.3322 | CLSLoss:0.0000 | AUROC:0.9933\n",
      "Test | 30/16 | Loss:0.6609 | MainLoss:0.6609 | SPLoss:5.3322 | CLSLoss:0.0000 | AUROC:0.9932\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.399478\n",
      "Train | 16/16 | Loss:0.7820 | MainLoss:0.4787 | Alpha:0.0293 | SPLoss:10.5499 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2469 | MainLoss:0.2469 | SPLoss:10.4552 | CLSLoss:0.0000 | AUROC:0.9932\n",
      "Test | 30/16 | Loss:0.5193 | MainLoss:0.5193 | SPLoss:10.4552 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.399432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.7538 | MainLoss:0.4852 | Alpha:0.0289 | SPLoss:9.2949 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2236 | MainLoss:0.2236 | SPLoss:8.3035 | CLSLoss:0.0000 | AUROC:0.9934\n",
      "Test | 30/16 | Loss:0.6637 | MainLoss:0.6637 | SPLoss:8.3035 | CLSLoss:0.0000 | AUROC:0.9923\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.399383\n",
      "Train | 16/16 | Loss:0.7030 | MainLoss:0.4884 | Alpha:0.0292 | SPLoss:7.2800 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2295 | MainLoss:0.2295 | SPLoss:6.6378 | CLSLoss:0.0000 | AUROC:0.9930\n",
      "Test | 30/16 | Loss:0.5520 | MainLoss:0.5520 | SPLoss:6.6378 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.399333\n",
      "Train | 16/16 | Loss:0.6620 | MainLoss:0.4893 | Alpha:0.0281 | SPLoss:6.1680 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2676 | MainLoss:0.2676 | SPLoss:5.7256 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.4777 | MainLoss:0.4777 | SPLoss:5.7256 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.399281\n",
      "Train | 16/16 | Loss:0.6224 | MainLoss:0.4719 | Alpha:0.0286 | SPLoss:5.2675 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2237 | MainLoss:0.2237 | SPLoss:4.9026 | CLSLoss:0.0000 | AUROC:0.9924\n",
      "Test | 30/16 | Loss:0.5996 | MainLoss:0.5996 | SPLoss:4.9026 | CLSLoss:0.0000 | AUROC:0.9877\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.399227\n",
      "Train | 16/16 | Loss:0.6172 | MainLoss:0.4823 | Alpha:0.0286 | SPLoss:4.7115 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1948 | MainLoss:0.1948 | SPLoss:4.7095 | CLSLoss:0.0000 | AUROC:0.9912\n",
      "Test | 30/16 | Loss:0.7372 | MainLoss:0.7372 | SPLoss:4.7095 | CLSLoss:0.0000 | AUROC:0.9866\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.399171\n",
      "Train | 16/16 | Loss:1.5561 | MainLoss:0.4963 | Alpha:0.0288 | SPLoss:36.7830 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3211 | MainLoss:0.3211 | SPLoss:41.9407 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "Test | 30/16 | Loss:0.4085 | MainLoss:0.4085 | SPLoss:41.9406 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.399112\n",
      "Train | 16/16 | Loss:1.5796 | MainLoss:0.4941 | Alpha:0.0309 | SPLoss:35.9377 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2718 | MainLoss:0.2718 | SPLoss:28.0226 | CLSLoss:0.0000 | AUROC:0.9924\n",
      "Test | 30/16 | Loss:0.4861 | MainLoss:0.4861 | SPLoss:28.0226 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.399052\n",
      "Train | 16/16 | Loss:1.2375 | MainLoss:0.5181 | Alpha:0.0298 | SPLoss:24.0287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2309 | MainLoss:0.2309 | SPLoss:19.9850 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "Test | 30/16 | Loss:0.5601 | MainLoss:0.5601 | SPLoss:19.9850 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.398990\n",
      "Train | 16/16 | Loss:1.0014 | MainLoss:0.4786 | Alpha:0.0292 | SPLoss:17.9178 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2163 | MainLoss:0.2163 | SPLoss:20.7065 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "Test | 30/16 | Loss:0.5890 | MainLoss:0.5890 | SPLoss:20.7065 | CLSLoss:0.0000 | AUROC:0.9927\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.398926\n",
      "Train | 16/16 | Loss:1.0003 | MainLoss:0.4774 | Alpha:0.0296 | SPLoss:17.6930 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1878 | MainLoss:0.1878 | SPLoss:14.7138 | CLSLoss:0.0000 | AUROC:0.9934\n",
      "Test | 30/16 | Loss:0.6749 | MainLoss:0.6749 | SPLoss:14.7138 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.398860\n",
      "Train | 16/16 | Loss:0.8531 | MainLoss:0.4815 | Alpha:0.0288 | SPLoss:12.8218 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2734 | MainLoss:0.2734 | SPLoss:11.0403 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "Test | 30/16 | Loss:0.4741 | MainLoss:0.4741 | SPLoss:11.0403 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.398792\n",
      "Train | 16/16 | Loss:0.7703 | MainLoss:0.4861 | Alpha:0.0291 | SPLoss:9.8616 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3545 | MainLoss:0.3545 | SPLoss:8.5393 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.3751 | MainLoss:0.3751 | SPLoss:8.5393 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.398722\n",
      "Train | 16/16 | Loss:0.7103 | MainLoss:0.4793 | Alpha:0.0298 | SPLoss:7.7614 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2729 | MainLoss:0.2729 | SPLoss:6.8709 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.4942 | MainLoss:0.4942 | SPLoss:6.8709 | CLSLoss:0.0000 | AUROC:0.9868\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.398650\n",
      "Train | 16/16 | Loss:0.6694 | MainLoss:0.4841 | Alpha:0.0285 | SPLoss:6.5051 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2909 | MainLoss:0.2909 | SPLoss:6.0536 | CLSLoss:0.0000 | AUROC:0.9941\n",
      "Test | 30/16 | Loss:0.4521 | MainLoss:0.4521 | SPLoss:6.0536 | CLSLoss:0.0000 | AUROC:0.9922\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.398577\n",
      "Train | 16/16 | Loss:0.6396 | MainLoss:0.4774 | Alpha:0.0290 | SPLoss:5.5917 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2115 | MainLoss:0.2115 | SPLoss:5.1812 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "Test | 30/16 | Loss:0.6595 | MainLoss:0.6595 | SPLoss:5.1812 | CLSLoss:0.0000 | AUROC:0.9935\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.398501\n",
      "Train | 16/16 | Loss:0.7292 | MainLoss:0.4996 | Alpha:0.0290 | SPLoss:7.9490 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2194 | MainLoss:0.2194 | SPLoss:7.7900 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5695 | MainLoss:0.5695 | SPLoss:7.7900 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.398423\n",
      "Train | 16/16 | Loss:0.6742 | MainLoss:0.4764 | Alpha:0.0282 | SPLoss:7.0063 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2085 | MainLoss:0.2085 | SPLoss:6.4076 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "Test | 30/16 | Loss:0.6622 | MainLoss:0.6622 | SPLoss:6.4076 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.398343\n",
      "Train | 16/16 | Loss:0.6640 | MainLoss:0.4857 | Alpha:0.0302 | SPLoss:5.8936 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2518 | MainLoss:0.2518 | SPLoss:5.5159 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 30/16 | Loss:0.5126 | MainLoss:0.5126 | SPLoss:5.5159 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.398262\n",
      "Train | 16/16 | Loss:0.6611 | MainLoss:0.4946 | Alpha:0.0289 | SPLoss:5.7007 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3287 | MainLoss:0.3287 | SPLoss:7.0622 | CLSLoss:0.0000 | AUROC:0.9933\n",
      "Test | 30/16 | Loss:0.4129 | MainLoss:0.4129 | SPLoss:7.0622 | CLSLoss:0.0000 | AUROC:0.9923\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.398178\n",
      "Train | 16/16 | Loss:0.7009 | MainLoss:0.4869 | Alpha:0.0296 | SPLoss:7.2085 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1832 | MainLoss:0.1832 | SPLoss:7.2771 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "Test | 30/16 | Loss:0.7924 | MainLoss:0.7924 | SPLoss:7.2772 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.398092\n",
      "Train | 16/16 | Loss:0.6981 | MainLoss:0.5081 | Alpha:0.0279 | SPLoss:6.8266 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1981 | MainLoss:0.1981 | SPLoss:6.3593 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.6368 | MainLoss:0.6368 | SPLoss:6.3593 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.398005\n",
      "Train | 16/16 | Loss:0.6559 | MainLoss:0.4846 | Alpha:0.0292 | SPLoss:5.8996 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2370 | MainLoss:0.2370 | SPLoss:5.2336 | CLSLoss:0.0000 | AUROC:0.9942\n",
      "Test | 30/16 | Loss:0.5382 | MainLoss:0.5382 | SPLoss:5.2336 | CLSLoss:0.0000 | AUROC:0.9910\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.397915\n",
      "Train | 16/16 | Loss:0.6203 | MainLoss:0.4747 | Alpha:0.0295 | SPLoss:4.9592 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3214 | MainLoss:0.3214 | SPLoss:4.5406 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "Test | 30/16 | Loss:0.3975 | MainLoss:0.3975 | SPLoss:4.5406 | CLSLoss:0.0000 | AUROC:0.9910\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.397824\n",
      "Train | 16/16 | Loss:0.6301 | MainLoss:0.4978 | Alpha:0.0293 | SPLoss:4.5161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2082 | MainLoss:0.2082 | SPLoss:4.5224 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.6010 | MainLoss:0.6010 | SPLoss:4.5224 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.397730\n",
      "Train | 16/16 | Loss:0.6017 | MainLoss:0.4771 | Alpha:0.0288 | SPLoss:4.3185 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2630 | MainLoss:0.2630 | SPLoss:4.0803 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.4744 | MainLoss:0.4744 | SPLoss:4.0803 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.397635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6168 | MainLoss:0.4957 | Alpha:0.0288 | SPLoss:4.1824 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2180 | MainLoss:0.2180 | SPLoss:4.1285 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5755 | MainLoss:0.5755 | SPLoss:4.1285 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.397538\n",
      "Train | 16/16 | Loss:0.8475 | MainLoss:0.4793 | Alpha:0.0297 | SPLoss:12.2603 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2342 | MainLoss:0.2342 | SPLoss:13.4577 | CLSLoss:0.0000 | AUROC:0.9931\n",
      "Test | 30/16 | Loss:0.5455 | MainLoss:0.5455 | SPLoss:13.4577 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.397438\n",
      "Train | 16/16 | Loss:0.9524 | MainLoss:0.5863 | Alpha:0.0293 | SPLoss:12.5031 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.6058 | MainLoss:0.6058 | SPLoss:12.7898 | CLSLoss:0.0000 | AUROC:0.9872\n",
      "Test | 30/16 | Loss:0.5125 | MainLoss:0.5125 | SPLoss:12.7898 | CLSLoss:0.0000 | AUROC:0.9142\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.397337\n",
      "Train | 16/16 | Loss:0.8460 | MainLoss:0.5202 | Alpha:0.0282 | SPLoss:11.5172 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2118 | MainLoss:0.2118 | SPLoss:9.8929 | CLSLoss:0.0000 | AUROC:0.9925\n",
      "Test | 30/16 | Loss:0.6734 | MainLoss:0.6734 | SPLoss:9.8929 | CLSLoss:0.0000 | AUROC:0.9760\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.397234\n",
      "Train | 16/16 | Loss:0.7426 | MainLoss:0.4845 | Alpha:0.0296 | SPLoss:8.7446 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3187 | MainLoss:0.3187 | SPLoss:7.5463 | CLSLoss:0.0000 | AUROC:0.9933\n",
      "Test | 30/16 | Loss:0.4446 | MainLoss:0.4446 | SPLoss:7.5463 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.397129\n",
      "Train | 16/16 | Loss:0.6869 | MainLoss:0.4823 | Alpha:0.0296 | SPLoss:6.8983 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1700 | MainLoss:0.1700 | SPLoss:6.2882 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "Test | 30/16 | Loss:0.7862 | MainLoss:0.7862 | SPLoss:6.2882 | CLSLoss:0.0000 | AUROC:0.9802\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.397022\n",
      "Train | 16/16 | Loss:0.6590 | MainLoss:0.4893 | Alpha:0.0293 | SPLoss:5.7986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3368 | MainLoss:0.3368 | SPLoss:5.3154 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "Test | 30/16 | Loss:0.4094 | MainLoss:0.4094 | SPLoss:5.3154 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.396913\n",
      "Train | 16/16 | Loss:0.6406 | MainLoss:0.4935 | Alpha:0.0281 | SPLoss:5.2337 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2185 | MainLoss:0.2185 | SPLoss:5.1334 | CLSLoss:0.0000 | AUROC:0.9941\n",
      "Test | 30/16 | Loss:0.6229 | MainLoss:0.6229 | SPLoss:5.1334 | CLSLoss:0.0000 | AUROC:0.9849\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.396802\n",
      "Train | 16/16 | Loss:0.6224 | MainLoss:0.4853 | Alpha:0.0284 | SPLoss:4.8200 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1989 | MainLoss:0.1989 | SPLoss:4.7134 | CLSLoss:0.0000 | AUROC:0.9924\n",
      "Test | 30/16 | Loss:0.6731 | MainLoss:0.6731 | SPLoss:4.7134 | CLSLoss:0.0000 | AUROC:0.9786\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.396689\n",
      "Train | 16/16 | Loss:0.6114 | MainLoss:0.4771 | Alpha:0.0295 | SPLoss:4.5524 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2197 | MainLoss:0.2197 | SPLoss:4.2382 | CLSLoss:0.0000 | AUROC:0.9935\n",
      "Test | 30/16 | Loss:0.5860 | MainLoss:0.5860 | SPLoss:4.2382 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.396574\n",
      "Train | 16/16 | Loss:0.6099 | MainLoss:0.4885 | Alpha:0.0290 | SPLoss:4.1859 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2311 | MainLoss:0.2311 | SPLoss:4.2095 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 30/16 | Loss:0.5756 | MainLoss:0.5756 | SPLoss:4.2095 | CLSLoss:0.0000 | AUROC:0.9854\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.396457\n",
      "Train | 16/16 | Loss:0.6071 | MainLoss:0.4842 | Alpha:0.0295 | SPLoss:4.1512 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2070 | MainLoss:0.2070 | SPLoss:4.1014 | CLSLoss:0.0000 | AUROC:0.9937\n",
      "Test | 30/16 | Loss:0.6244 | MainLoss:0.6244 | SPLoss:4.1014 | CLSLoss:0.0000 | AUROC:0.9809\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.396339\n",
      "Train | 16/16 | Loss:0.6054 | MainLoss:0.4888 | Alpha:0.0280 | SPLoss:4.1592 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2512 | MainLoss:0.2512 | SPLoss:3.9686 | CLSLoss:0.0000 | AUROC:0.9912\n",
      "Test | 30/16 | Loss:0.5297 | MainLoss:0.5297 | SPLoss:3.9686 | CLSLoss:0.0000 | AUROC:0.9788\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.396218\n",
      "Train | 16/16 | Loss:0.6007 | MainLoss:0.4843 | Alpha:0.0291 | SPLoss:3.9906 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2295 | MainLoss:0.2295 | SPLoss:3.9135 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.5717 | MainLoss:0.5717 | SPLoss:3.9135 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.396096\n",
      "Train | 16/16 | Loss:0.6098 | MainLoss:0.4968 | Alpha:0.0284 | SPLoss:3.9619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3220 | MainLoss:0.3220 | SPLoss:4.2771 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "Test | 30/16 | Loss:0.4538 | MainLoss:0.4538 | SPLoss:4.2771 | CLSLoss:0.0000 | AUROC:0.9767\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.395971\n",
      "Train | 16/16 | Loss:0.5891 | MainLoss:0.4741 | Alpha:0.0288 | SPLoss:3.9927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2401 | MainLoss:0.2401 | SPLoss:3.7599 | CLSLoss:0.0000 | AUROC:0.9933\n",
      "Test | 30/16 | Loss:0.5279 | MainLoss:0.5279 | SPLoss:3.7599 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.395845\n",
      "Train | 16/16 | Loss:0.5856 | MainLoss:0.4775 | Alpha:0.0286 | SPLoss:3.7735 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1942 | MainLoss:0.1942 | SPLoss:3.9004 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.7012 | MainLoss:0.7012 | SPLoss:3.9004 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.395716\n",
      "Train | 16/16 | Loss:0.5820 | MainLoss:0.4707 | Alpha:0.0291 | SPLoss:3.8259 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2104 | MainLoss:0.2104 | SPLoss:3.9600 | CLSLoss:0.0000 | AUROC:0.9928\n",
      "Test | 30/16 | Loss:0.6086 | MainLoss:0.6086 | SPLoss:3.9600 | CLSLoss:0.0000 | AUROC:0.9863\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.395586\n",
      "Train | 16/16 | Loss:0.6103 | MainLoss:0.4797 | Alpha:0.0285 | SPLoss:4.6165 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2434 | MainLoss:0.2434 | SPLoss:4.7551 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "Test | 30/16 | Loss:0.5346 | MainLoss:0.5346 | SPLoss:4.7551 | CLSLoss:0.0000 | AUROC:0.9846\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.395454\n",
      "Train | 16/16 | Loss:0.5963 | MainLoss:0.4649 | Alpha:0.0294 | SPLoss:4.4753 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2419 | MainLoss:0.2419 | SPLoss:4.1696 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.5237 | MainLoss:0.5237 | SPLoss:4.1696 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.395319\n",
      "Train | 16/16 | Loss:0.5932 | MainLoss:0.4730 | Alpha:0.0293 | SPLoss:4.1059 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2351 | MainLoss:0.2351 | SPLoss:4.0499 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "Test | 30/16 | Loss:0.6187 | MainLoss:0.6187 | SPLoss:4.0499 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.395183\n",
      "Train | 16/16 | Loss:0.6167 | MainLoss:0.5003 | Alpha:0.0288 | SPLoss:4.0595 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2061 | MainLoss:0.2061 | SPLoss:4.3327 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "Test | 30/16 | Loss:0.6705 | MainLoss:0.6705 | SPLoss:4.3327 | CLSLoss:0.0000 | AUROC:0.9849\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.395045\n",
      "Train | 16/16 | Loss:0.5865 | MainLoss:0.4703 | Alpha:0.0286 | SPLoss:4.0716 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3552 | MainLoss:0.3552 | SPLoss:3.7877 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 30/16 | Loss:0.3790 | MainLoss:0.3790 | SPLoss:3.7877 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.394905\n",
      "Train | 16/16 | Loss:0.5911 | MainLoss:0.4808 | Alpha:0.0275 | SPLoss:4.0160 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2518 | MainLoss:0.2518 | SPLoss:4.1946 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5027 | MainLoss:0.5027 | SPLoss:4.1946 | CLSLoss:0.0000 | AUROC:0.9877\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.394763\n",
      "Train | 16/16 | Loss:0.5962 | MainLoss:0.4790 | Alpha:0.0290 | SPLoss:4.0389 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2300 | MainLoss:0.2300 | SPLoss:3.8810 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "Test | 30/16 | Loss:0.6294 | MainLoss:0.6294 | SPLoss:3.8810 | CLSLoss:0.0000 | AUROC:0.9856\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.394620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5994 | MainLoss:0.4871 | Alpha:0.0286 | SPLoss:3.9090 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2335 | MainLoss:0.2335 | SPLoss:3.7747 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5294 | MainLoss:0.5294 | SPLoss:3.7747 | CLSLoss:0.0000 | AUROC:0.9843\n",
      "\n",
      "Epoch: [177 | 1000] LR: 0.394474\n",
      "Train | 16/16 | Loss:0.5777 | MainLoss:0.4731 | Alpha:0.0282 | SPLoss:3.7147 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2152 | MainLoss:0.2152 | SPLoss:3.6596 | CLSLoss:0.0000 | AUROC:0.9939\n",
      "Test | 30/16 | Loss:0.5989 | MainLoss:0.5989 | SPLoss:3.6596 | CLSLoss:0.0000 | AUROC:0.9849\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.394326\n",
      "Train | 16/16 | Loss:0.5821 | MainLoss:0.4763 | Alpha:0.0288 | SPLoss:3.6848 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2285 | MainLoss:0.2285 | SPLoss:4.2263 | CLSLoss:0.0000 | AUROC:0.9920\n",
      "Test | 30/16 | Loss:0.6701 | MainLoss:0.6701 | SPLoss:4.2263 | CLSLoss:0.0000 | AUROC:0.9831\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.394177\n",
      "Train | 16/16 | Loss:0.6129 | MainLoss:0.4765 | Alpha:0.0288 | SPLoss:4.7244 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1978 | MainLoss:0.1978 | SPLoss:4.5764 | CLSLoss:0.0000 | AUROC:0.9941\n",
      "Test | 30/16 | Loss:0.6373 | MainLoss:0.6373 | SPLoss:4.5764 | CLSLoss:0.0000 | AUROC:0.9863\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.394025\n",
      "Train | 16/16 | Loss:0.6010 | MainLoss:0.4759 | Alpha:0.0287 | SPLoss:4.3502 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2029 | MainLoss:0.2029 | SPLoss:4.2679 | CLSLoss:0.0000 | AUROC:0.9926\n",
      "Test | 30/16 | Loss:0.6917 | MainLoss:0.6917 | SPLoss:4.2679 | CLSLoss:0.0000 | AUROC:0.9768\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.393872\n",
      "Train | 16/16 | Loss:0.5944 | MainLoss:0.4754 | Alpha:0.0292 | SPLoss:4.0753 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2685 | MainLoss:0.2685 | SPLoss:4.0248 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "Test | 30/16 | Loss:0.4834 | MainLoss:0.4834 | SPLoss:4.0248 | CLSLoss:0.0000 | AUROC:0.9814\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.393717\n",
      "Train | 16/16 | Loss:0.5898 | MainLoss:0.4744 | Alpha:0.0292 | SPLoss:3.9468 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2388 | MainLoss:0.2388 | SPLoss:3.9720 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5385 | MainLoss:0.5385 | SPLoss:3.9720 | CLSLoss:0.0000 | AUROC:0.9869\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.393559\n",
      "Train | 16/16 | Loss:0.6690 | MainLoss:0.4807 | Alpha:0.0295 | SPLoss:6.2534 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2112 | MainLoss:0.2112 | SPLoss:8.2406 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5934 | MainLoss:0.5934 | SPLoss:8.2406 | CLSLoss:0.0000 | AUROC:0.9817\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.393400\n",
      "Train | 16/16 | Loss:0.8008 | MainLoss:0.4677 | Alpha:0.0301 | SPLoss:10.7738 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2548 | MainLoss:0.2548 | SPLoss:12.4805 | CLSLoss:0.0000 | AUROC:0.9941\n",
      "Test | 30/16 | Loss:0.5082 | MainLoss:0.5082 | SPLoss:12.4805 | CLSLoss:0.0000 | AUROC:0.9844\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.393239\n",
      "Train | 16/16 | Loss:0.7939 | MainLoss:0.4746 | Alpha:0.0291 | SPLoss:10.9686 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2900 | MainLoss:0.2900 | SPLoss:9.4096 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.4441 | MainLoss:0.4441 | SPLoss:9.4097 | CLSLoss:0.0000 | AUROC:0.9869\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.393076\n",
      "Train | 16/16 | Loss:0.7206 | MainLoss:0.4781 | Alpha:0.0290 | SPLoss:8.3638 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2144 | MainLoss:0.2144 | SPLoss:8.8168 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "Test | 30/16 | Loss:0.6135 | MainLoss:0.6135 | SPLoss:8.8168 | CLSLoss:0.0000 | AUROC:0.9768\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.392911\n",
      "Train | 16/16 | Loss:0.7109 | MainLoss:0.4829 | Alpha:0.0289 | SPLoss:7.8670 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.1977 | MainLoss:0.1977 | SPLoss:7.0026 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.6405 | MainLoss:0.6405 | SPLoss:7.0026 | CLSLoss:0.0000 | AUROC:0.9847\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.392745\n",
      "Train | 16/16 | Loss:0.6462 | MainLoss:0.4637 | Alpha:0.0290 | SPLoss:6.2840 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2132 | MainLoss:0.2132 | SPLoss:5.7171 | CLSLoss:0.0000 | AUROC:0.9929\n",
      "Test | 30/16 | Loss:0.6742 | MainLoss:0.6742 | SPLoss:5.7171 | CLSLoss:0.0000 | AUROC:0.9818\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.392576\n",
      "Train | 16/16 | Loss:0.6395 | MainLoss:0.4710 | Alpha:0.0295 | SPLoss:5.6222 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.3638 | MainLoss:0.3638 | SPLoss:7.1587 | CLSLoss:0.0000 | AUROC:0.9936\n",
      "Test | 30/16 | Loss:0.3942 | MainLoss:0.3942 | SPLoss:7.1587 | CLSLoss:0.0000 | AUROC:0.9805\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.392406\n",
      "Train | 16/16 | Loss:0.6837 | MainLoss:0.4921 | Alpha:0.0278 | SPLoss:6.8779 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2284 | MainLoss:0.2284 | SPLoss:6.1908 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5787 | MainLoss:0.5787 | SPLoss:6.1908 | CLSLoss:0.0000 | AUROC:0.9760\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.392233\n",
      "Train | 16/16 | Loss:0.6238 | MainLoss:0.4636 | Alpha:0.0283 | SPLoss:5.6544 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:5.0449 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5517 | MainLoss:0.5517 | SPLoss:5.0449 | CLSLoss:0.0000 | AUROC:0.9838\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.392059\n",
      "Train | 16/16 | Loss:0.6154 | MainLoss:0.4740 | Alpha:0.0291 | SPLoss:4.8513 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2489 | MainLoss:0.2489 | SPLoss:4.5428 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5278 | MainLoss:0.5278 | SPLoss:4.5428 | CLSLoss:0.0000 | AUROC:0.9820\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.391882\n",
      "Train | 16/16 | Loss:0.6057 | MainLoss:0.4803 | Alpha:0.0282 | SPLoss:4.4439 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2427 | MainLoss:0.2427 | SPLoss:4.3003 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5259 | MainLoss:0.5259 | SPLoss:4.3003 | CLSLoss:0.0000 | AUROC:0.9812\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.391704\n",
      "Train | 16/16 | Loss:0.5932 | MainLoss:0.4741 | Alpha:0.0284 | SPLoss:4.2105 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2510 | MainLoss:0.2510 | SPLoss:3.9234 | CLSLoss:0.0000 | AUROC:0.9941\n",
      "Test | 30/16 | Loss:0.5293 | MainLoss:0.5293 | SPLoss:3.9234 | CLSLoss:0.0000 | AUROC:0.9848\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.391524\n",
      "Train | 16/16 | Loss:0.6151 | MainLoss:0.4702 | Alpha:0.0294 | SPLoss:4.7477 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2267 | MainLoss:0.2267 | SPLoss:6.7143 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "Test | 30/16 | Loss:0.5574 | MainLoss:0.5574 | SPLoss:6.7143 | CLSLoss:0.0000 | AUROC:0.9823\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.391342\n",
      "Train | 16/16 | Loss:0.6464 | MainLoss:0.4724 | Alpha:0.0279 | SPLoss:6.2089 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2160 | MainLoss:0.2160 | SPLoss:5.8663 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "Test | 30/16 | Loss:0.7324 | MainLoss:0.7324 | SPLoss:5.8663 | CLSLoss:0.0000 | AUROC:0.9837\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.391159\n",
      "Train | 16/16 | Loss:0.6336 | MainLoss:0.4764 | Alpha:0.0288 | SPLoss:5.4491 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2208 | MainLoss:0.2208 | SPLoss:5.0058 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5728 | MainLoss:0.5728 | SPLoss:5.0058 | CLSLoss:0.0000 | AUROC:0.9854\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.390973\n",
      "Train | 16/16 | Loss:0.6219 | MainLoss:0.4754 | Alpha:0.0291 | SPLoss:5.0390 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2609 | MainLoss:0.2609 | SPLoss:5.1466 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.4771 | MainLoss:0.4771 | SPLoss:5.1466 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [199 | 1000] LR: 0.390785\n",
      "Train | 16/16 | Loss:0.6201 | MainLoss:0.4775 | Alpha:0.0288 | SPLoss:4.9387 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2377 | MainLoss:0.2377 | SPLoss:4.7120 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5329 | MainLoss:0.5329 | SPLoss:4.7120 | CLSLoss:0.0000 | AUROC:0.9858\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.390596\n",
      "Train | 16/16 | Loss:0.6175 | MainLoss:0.4814 | Alpha:0.0303 | SPLoss:4.4943 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2144 | MainLoss:0.2144 | SPLoss:4.3783 | CLSLoss:0.0000 | AUROC:0.9930\n",
      "Test | 30/16 | Loss:0.6116 | MainLoss:0.6116 | SPLoss:4.3783 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.039040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5966 | MainLoss:0.4733 | Alpha:0.0285 | SPLoss:4.3184 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2242 | MainLoss:0.2242 | SPLoss:4.2677 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "Test | 30/16 | Loss:0.5555 | MainLoss:0.5555 | SPLoss:4.2677 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.039021\n",
      "Train | 16/16 | Loss:0.5844 | MainLoss:0.4622 | Alpha:0.0289 | SPLoss:4.2339 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2266 | MainLoss:0.2266 | SPLoss:4.1994 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5453 | MainLoss:0.5453 | SPLoss:4.1994 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.039002\n",
      "Train | 16/16 | Loss:0.5816 | MainLoss:0.4638 | Alpha:0.0282 | SPLoss:4.1701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2159 | MainLoss:0.2159 | SPLoss:4.1663 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5731 | MainLoss:0.5731 | SPLoss:4.1663 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.038982\n",
      "Train | 16/16 | Loss:0.5736 | MainLoss:0.4561 | Alpha:0.0285 | SPLoss:4.1211 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2255 | MainLoss:0.2255 | SPLoss:4.0920 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5440 | MainLoss:0.5440 | SPLoss:4.0920 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.038962\n",
      "Train | 16/16 | Loss:0.5782 | MainLoss:0.4564 | Alpha:0.0298 | SPLoss:4.0823 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2284 | MainLoss:0.2284 | SPLoss:4.0362 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5363 | MainLoss:0.5363 | SPLoss:4.0362 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.038942\n",
      "Train | 16/16 | Loss:0.5773 | MainLoss:0.4629 | Alpha:0.0285 | SPLoss:4.0142 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2197 | MainLoss:0.2197 | SPLoss:4.0050 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5594 | MainLoss:0.5594 | SPLoss:4.0050 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.038922\n",
      "Train | 16/16 | Loss:0.5743 | MainLoss:0.4583 | Alpha:0.0292 | SPLoss:3.9720 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2235 | MainLoss:0.2235 | SPLoss:3.9558 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5489 | MainLoss:0.5489 | SPLoss:3.9559 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.038901\n",
      "Train | 16/16 | Loss:0.5705 | MainLoss:0.4568 | Alpha:0.0289 | SPLoss:3.9368 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2091 | MainLoss:0.2091 | SPLoss:3.9337 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5866 | MainLoss:0.5866 | SPLoss:3.9337 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.038881\n",
      "Train | 16/16 | Loss:0.5706 | MainLoss:0.4593 | Alpha:0.0285 | SPLoss:3.9026 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2224 | MainLoss:0.2224 | SPLoss:3.8695 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5480 | MainLoss:0.5480 | SPLoss:3.8695 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.038860\n",
      "Train | 16/16 | Loss:0.5779 | MainLoss:0.4643 | Alpha:0.0295 | SPLoss:3.8532 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:3.8271 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5461 | MainLoss:0.5461 | SPLoss:3.8271 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.038839\n",
      "Train | 16/16 | Loss:0.5669 | MainLoss:0.4574 | Alpha:0.0287 | SPLoss:3.8113 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:3.7645 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5467 | MainLoss:0.5467 | SPLoss:3.7645 | CLSLoss:0.0000 | AUROC:0.9872\n",
      "\n",
      "Epoch: [212 | 1000] LR: 0.038818\n",
      "Train | 16/16 | Loss:0.5678 | MainLoss:0.4577 | Alpha:0.0293 | SPLoss:3.7535 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:3.7236 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5481 | MainLoss:0.5481 | SPLoss:3.7236 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [213 | 1000] LR: 0.038796\n",
      "Train | 16/16 | Loss:0.5779 | MainLoss:0.4700 | Alpha:0.0291 | SPLoss:3.7044 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2217 | MainLoss:0.2217 | SPLoss:3.6961 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5525 | MainLoss:0.5525 | SPLoss:3.6961 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [214 | 1000] LR: 0.038775\n",
      "Train | 16/16 | Loss:0.5647 | MainLoss:0.4575 | Alpha:0.0292 | SPLoss:3.6766 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2290 | MainLoss:0.2290 | SPLoss:3.6432 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5391 | MainLoss:0.5391 | SPLoss:3.6432 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [215 | 1000] LR: 0.038753\n",
      "Train | 16/16 | Loss:0.5598 | MainLoss:0.4573 | Alpha:0.0282 | SPLoss:3.6294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2222 | MainLoss:0.2222 | SPLoss:3.6451 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5556 | MainLoss:0.5556 | SPLoss:3.6451 | CLSLoss:0.0000 | AUROC:0.9876\n",
      "\n",
      "Epoch: [216 | 1000] LR: 0.038731\n",
      "Train | 16/16 | Loss:0.5697 | MainLoss:0.4654 | Alpha:0.0288 | SPLoss:3.6232 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2240 | MainLoss:0.2240 | SPLoss:3.6111 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5529 | MainLoss:0.5529 | SPLoss:3.6111 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [217 | 1000] LR: 0.038709\n",
      "Train | 16/16 | Loss:0.5699 | MainLoss:0.4671 | Alpha:0.0286 | SPLoss:3.5970 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2215 | MainLoss:0.2215 | SPLoss:3.5919 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5570 | MainLoss:0.5570 | SPLoss:3.5919 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [218 | 1000] LR: 0.038687\n",
      "Train | 16/16 | Loss:0.5597 | MainLoss:0.4576 | Alpha:0.0287 | SPLoss:3.5555 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2189 | MainLoss:0.2189 | SPLoss:3.5496 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5584 | MainLoss:0.5584 | SPLoss:3.5496 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [219 | 1000] LR: 0.038664\n",
      "Train | 16/16 | Loss:0.5638 | MainLoss:0.4612 | Alpha:0.0289 | SPLoss:3.5457 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2215 | MainLoss:0.2215 | SPLoss:3.5331 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5564 | MainLoss:0.5564 | SPLoss:3.5331 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [220 | 1000] LR: 0.038641\n",
      "Train | 16/16 | Loss:0.5660 | MainLoss:0.4634 | Alpha:0.0292 | SPLoss:3.5122 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2222 | MainLoss:0.2222 | SPLoss:3.5137 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5582 | MainLoss:0.5582 | SPLoss:3.5137 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [221 | 1000] LR: 0.038619\n",
      "Train | 16/16 | Loss:0.5668 | MainLoss:0.4619 | Alpha:0.0301 | SPLoss:3.4846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2277 | MainLoss:0.2277 | SPLoss:3.4554 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5414 | MainLoss:0.5414 | SPLoss:3.4554 | CLSLoss:0.0000 | AUROC:0.9884\n",
      "\n",
      "Epoch: [222 | 1000] LR: 0.038596\n",
      "Train | 16/16 | Loss:0.5573 | MainLoss:0.4554 | Alpha:0.0296 | SPLoss:3.4456 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2235 | MainLoss:0.2235 | SPLoss:3.4504 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5575 | MainLoss:0.5575 | SPLoss:3.4504 | CLSLoss:0.0000 | AUROC:0.9877\n",
      "\n",
      "Epoch: [223 | 1000] LR: 0.038572\n",
      "Train | 16/16 | Loss:0.5580 | MainLoss:0.4600 | Alpha:0.0286 | SPLoss:3.4286 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2272 | MainLoss:0.2272 | SPLoss:3.4103 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5432 | MainLoss:0.5432 | SPLoss:3.4103 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [224 | 1000] LR: 0.038549\n",
      "Train | 16/16 | Loss:0.5675 | MainLoss:0.4655 | Alpha:0.0299 | SPLoss:3.4106 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:3.4142 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5585 | MainLoss:0.5585 | SPLoss:3.4142 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [225 | 1000] LR: 0.038525\n",
      "Train | 16/16 | Loss:0.5521 | MainLoss:0.4562 | Alpha:0.0283 | SPLoss:3.3865 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2169 | MainLoss:0.2169 | SPLoss:3.3985 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5682 | MainLoss:0.5682 | SPLoss:3.3985 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [226 | 1000] LR: 0.038502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5719 | MainLoss:0.4722 | Alpha:0.0295 | SPLoss:3.3875 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2526 | MainLoss:0.2526 | SPLoss:3.3399 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.4863 | MainLoss:0.4863 | SPLoss:3.3399 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [227 | 1000] LR: 0.038478\n",
      "Train | 16/16 | Loss:0.5564 | MainLoss:0.4602 | Alpha:0.0287 | SPLoss:3.3561 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2355 | MainLoss:0.2355 | SPLoss:3.3417 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5262 | MainLoss:0.5262 | SPLoss:3.3417 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [228 | 1000] LR: 0.038453\n",
      "Train | 16/16 | Loss:0.5611 | MainLoss:0.4659 | Alpha:0.0286 | SPLoss:3.3282 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2257 | MainLoss:0.2257 | SPLoss:3.3231 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5537 | MainLoss:0.5537 | SPLoss:3.3231 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [229 | 1000] LR: 0.038429\n",
      "Train | 16/16 | Loss:0.5572 | MainLoss:0.4625 | Alpha:0.0288 | SPLoss:3.2927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2264 | MainLoss:0.2264 | SPLoss:3.2695 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5465 | MainLoss:0.5465 | SPLoss:3.2695 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [230 | 1000] LR: 0.038405\n",
      "Train | 16/16 | Loss:0.5638 | MainLoss:0.4711 | Alpha:0.0283 | SPLoss:3.2728 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2347 | MainLoss:0.2347 | SPLoss:3.2593 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5261 | MainLoss:0.5261 | SPLoss:3.2593 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [231 | 1000] LR: 0.038380\n",
      "Train | 16/16 | Loss:0.5530 | MainLoss:0.4590 | Alpha:0.0288 | SPLoss:3.2589 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2306 | MainLoss:0.2306 | SPLoss:3.2497 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5352 | MainLoss:0.5352 | SPLoss:3.2497 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [232 | 1000] LR: 0.038355\n",
      "Train | 16/16 | Loss:0.5551 | MainLoss:0.4632 | Alpha:0.0284 | SPLoss:3.2403 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2332 | MainLoss:0.2332 | SPLoss:3.2253 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5294 | MainLoss:0.5294 | SPLoss:3.2253 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [233 | 1000] LR: 0.038330\n",
      "Train | 16/16 | Loss:0.5592 | MainLoss:0.4665 | Alpha:0.0288 | SPLoss:3.2193 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2237 | MainLoss:0.2237 | SPLoss:3.2298 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5607 | MainLoss:0.5607 | SPLoss:3.2298 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [234 | 1000] LR: 0.038305\n",
      "Train | 16/16 | Loss:0.5581 | MainLoss:0.4660 | Alpha:0.0287 | SPLoss:3.2079 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2287 | MainLoss:0.2287 | SPLoss:3.2097 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5422 | MainLoss:0.5422 | SPLoss:3.2097 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [235 | 1000] LR: 0.038279\n",
      "Train | 16/16 | Loss:0.5563 | MainLoss:0.4628 | Alpha:0.0292 | SPLoss:3.2018 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:3.1897 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5558 | MainLoss:0.5558 | SPLoss:3.1897 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [236 | 1000] LR: 0.038254\n",
      "Train | 16/16 | Loss:0.5489 | MainLoss:0.4551 | Alpha:0.0296 | SPLoss:3.1696 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:3.1567 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5481 | MainLoss:0.5481 | SPLoss:3.1567 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [237 | 1000] LR: 0.038228\n",
      "Train | 16/16 | Loss:0.5543 | MainLoss:0.4650 | Alpha:0.0283 | SPLoss:3.1547 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2217 | MainLoss:0.2217 | SPLoss:3.1597 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5569 | MainLoss:0.5569 | SPLoss:3.1597 | CLSLoss:0.0000 | AUROC:0.9883\n",
      "\n",
      "Epoch: [238 | 1000] LR: 0.038202\n",
      "Train | 16/16 | Loss:0.5541 | MainLoss:0.4604 | Alpha:0.0297 | SPLoss:3.1537 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2343 | MainLoss:0.2343 | SPLoss:3.1514 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5254 | MainLoss:0.5254 | SPLoss:3.1514 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [239 | 1000] LR: 0.038176\n",
      "Train | 16/16 | Loss:0.5516 | MainLoss:0.4612 | Alpha:0.0287 | SPLoss:3.1426 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:3.1572 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5486 | MainLoss:0.5486 | SPLoss:3.1572 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [240 | 1000] LR: 0.038150\n",
      "Train | 16/16 | Loss:0.5626 | MainLoss:0.4694 | Alpha:0.0296 | SPLoss:3.1486 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2274 | MainLoss:0.2274 | SPLoss:3.1431 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5453 | MainLoss:0.5453 | SPLoss:3.1431 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [241 | 1000] LR: 0.038123\n",
      "Train | 16/16 | Loss:0.5490 | MainLoss:0.4572 | Alpha:0.0293 | SPLoss:3.1289 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2176 | MainLoss:0.2176 | SPLoss:3.1344 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5737 | MainLoss:0.5737 | SPLoss:3.1344 | CLSLoss:0.0000 | AUROC:0.9869\n",
      "\n",
      "Epoch: [242 | 1000] LR: 0.038097\n",
      "Train | 16/16 | Loss:0.5481 | MainLoss:0.4583 | Alpha:0.0286 | SPLoss:3.1344 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2243 | MainLoss:0.2243 | SPLoss:3.1194 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5451 | MainLoss:0.5451 | SPLoss:3.1194 | CLSLoss:0.0000 | AUROC:0.9879\n",
      "\n",
      "Epoch: [243 | 1000] LR: 0.038070\n",
      "Train | 16/16 | Loss:0.5518 | MainLoss:0.4587 | Alpha:0.0298 | SPLoss:3.1242 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2348 | MainLoss:0.2348 | SPLoss:3.1059 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5215 | MainLoss:0.5215 | SPLoss:3.1059 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [244 | 1000] LR: 0.038043\n",
      "Train | 16/16 | Loss:0.5619 | MainLoss:0.4727 | Alpha:0.0287 | SPLoss:3.1017 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2254 | MainLoss:0.2254 | SPLoss:3.1102 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5462 | MainLoss:0.5462 | SPLoss:3.1102 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [245 | 1000] LR: 0.038015\n",
      "Train | 16/16 | Loss:0.5465 | MainLoss:0.4587 | Alpha:0.0284 | SPLoss:3.0944 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2264 | MainLoss:0.2264 | SPLoss:3.0923 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5485 | MainLoss:0.5485 | SPLoss:3.0923 | CLSLoss:0.0000 | AUROC:0.9883\n",
      "\n",
      "Epoch: [246 | 1000] LR: 0.037988\n",
      "Train | 16/16 | Loss:0.5528 | MainLoss:0.4653 | Alpha:0.0283 | SPLoss:3.0907 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2283 | MainLoss:0.2283 | SPLoss:3.1074 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5452 | MainLoss:0.5452 | SPLoss:3.1074 | CLSLoss:0.0000 | AUROC:0.9873\n",
      "\n",
      "Epoch: [247 | 1000] LR: 0.037961\n",
      "Train | 16/16 | Loss:0.5550 | MainLoss:0.4634 | Alpha:0.0296 | SPLoss:3.0980 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2169 | MainLoss:0.2169 | SPLoss:3.1044 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5758 | MainLoss:0.5758 | SPLoss:3.1044 | CLSLoss:0.0000 | AUROC:0.9884\n",
      "\n",
      "Epoch: [248 | 1000] LR: 0.037933\n",
      "Train | 16/16 | Loss:0.5487 | MainLoss:0.4590 | Alpha:0.0291 | SPLoss:3.0844 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:3.0723 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5544 | MainLoss:0.5544 | SPLoss:3.0723 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [249 | 1000] LR: 0.037905\n",
      "Train | 16/16 | Loss:0.5503 | MainLoss:0.4602 | Alpha:0.0294 | SPLoss:3.0671 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2349 | MainLoss:0.2349 | SPLoss:3.0727 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5303 | MainLoss:0.5303 | SPLoss:3.0727 | CLSLoss:0.0000 | AUROC:0.9876\n",
      "\n",
      "Epoch: [250 | 1000] LR: 0.037877\n",
      "Train | 16/16 | Loss:0.5499 | MainLoss:0.4610 | Alpha:0.0290 | SPLoss:3.0690 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2337 | MainLoss:0.2337 | SPLoss:3.0622 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5283 | MainLoss:0.5283 | SPLoss:3.0622 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [251 | 1000] LR: 0.037849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5498 | MainLoss:0.4619 | Alpha:0.0287 | SPLoss:3.0633 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2164 | MainLoss:0.2164 | SPLoss:3.0702 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5672 | MainLoss:0.5672 | SPLoss:3.0702 | CLSLoss:0.0000 | AUROC:0.9876\n",
      "\n",
      "Epoch: [252 | 1000] LR: 0.037820\n",
      "Train | 16/16 | Loss:0.5526 | MainLoss:0.4633 | Alpha:0.0290 | SPLoss:3.0763 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2322 | MainLoss:0.2322 | SPLoss:3.0457 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5221 | MainLoss:0.5221 | SPLoss:3.0457 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [253 | 1000] LR: 0.037792\n",
      "Train | 16/16 | Loss:0.5493 | MainLoss:0.4615 | Alpha:0.0288 | SPLoss:3.0501 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2222 | MainLoss:0.2222 | SPLoss:3.0716 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5659 | MainLoss:0.5659 | SPLoss:3.0716 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [254 | 1000] LR: 0.037763\n",
      "Train | 16/16 | Loss:0.5480 | MainLoss:0.4587 | Alpha:0.0293 | SPLoss:3.0438 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2221 | MainLoss:0.2221 | SPLoss:3.0598 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5584 | MainLoss:0.5584 | SPLoss:3.0598 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [255 | 1000] LR: 0.037734\n",
      "Train | 16/16 | Loss:0.5649 | MainLoss:0.4731 | Alpha:0.0299 | SPLoss:3.0676 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2286 | MainLoss:0.2286 | SPLoss:3.1594 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5246 | MainLoss:0.5246 | SPLoss:3.1594 | CLSLoss:0.0000 | AUROC:0.9922\n",
      "\n",
      "Epoch: [256 | 1000] LR: 0.037705\n",
      "Train | 16/16 | Loss:0.5684 | MainLoss:0.4774 | Alpha:0.0287 | SPLoss:3.1723 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2187 | MainLoss:0.2187 | SPLoss:3.1840 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5487 | MainLoss:0.5487 | SPLoss:3.1840 | CLSLoss:0.0000 | AUROC:0.9905\n",
      "\n",
      "Epoch: [257 | 1000] LR: 0.037675\n",
      "Train | 16/16 | Loss:0.5685 | MainLoss:0.4747 | Alpha:0.0293 | SPLoss:3.2023 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2373 | MainLoss:0.2373 | SPLoss:3.1767 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5096 | MainLoss:0.5096 | SPLoss:3.1767 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [258 | 1000] LR: 0.037646\n",
      "Train | 16/16 | Loss:0.5549 | MainLoss:0.4636 | Alpha:0.0287 | SPLoss:3.1827 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2216 | MainLoss:0.2216 | SPLoss:3.1910 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5556 | MainLoss:0.5556 | SPLoss:3.1910 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [259 | 1000] LR: 0.037616\n",
      "Train | 16/16 | Loss:0.5564 | MainLoss:0.4657 | Alpha:0.0286 | SPLoss:3.1670 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:3.1565 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5473 | MainLoss:0.5473 | SPLoss:3.1565 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [260 | 1000] LR: 0.037586\n",
      "Train | 16/16 | Loss:0.5564 | MainLoss:0.4664 | Alpha:0.0285 | SPLoss:3.1591 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2293 | MainLoss:0.2293 | SPLoss:3.1452 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5315 | MainLoss:0.5315 | SPLoss:3.1452 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [261 | 1000] LR: 0.037556\n",
      "Train | 16/16 | Loss:0.5484 | MainLoss:0.4585 | Alpha:0.0285 | SPLoss:3.1505 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2232 | MainLoss:0.2232 | SPLoss:3.1480 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5470 | MainLoss:0.5470 | SPLoss:3.1480 | CLSLoss:0.0000 | AUROC:0.9867\n",
      "\n",
      "Epoch: [262 | 1000] LR: 0.037526\n",
      "Train | 16/16 | Loss:0.5522 | MainLoss:0.4627 | Alpha:0.0284 | SPLoss:3.1473 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2277 | MainLoss:0.2277 | SPLoss:3.1478 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5412 | MainLoss:0.5412 | SPLoss:3.1478 | CLSLoss:0.0000 | AUROC:0.9866\n",
      "\n",
      "Epoch: [263 | 1000] LR: 0.037496\n",
      "Train | 16/16 | Loss:0.5488 | MainLoss:0.4584 | Alpha:0.0287 | SPLoss:3.1420 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2208 | MainLoss:0.2208 | SPLoss:3.1263 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5492 | MainLoss:0.5492 | SPLoss:3.1263 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [264 | 1000] LR: 0.037465\n",
      "Train | 16/16 | Loss:0.5558 | MainLoss:0.4661 | Alpha:0.0287 | SPLoss:3.1273 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2232 | MainLoss:0.2232 | SPLoss:3.1448 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5542 | MainLoss:0.5542 | SPLoss:3.1448 | CLSLoss:0.0000 | AUROC:0.9869\n",
      "\n",
      "Epoch: [265 | 1000] LR: 0.037435\n",
      "Train | 16/16 | Loss:0.5637 | MainLoss:0.4657 | Alpha:0.0313 | SPLoss:3.1339 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2303 | MainLoss:0.2303 | SPLoss:3.1242 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5346 | MainLoss:0.5346 | SPLoss:3.1242 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [266 | 1000] LR: 0.037404\n",
      "Train | 16/16 | Loss:0.5473 | MainLoss:0.4568 | Alpha:0.0289 | SPLoss:3.1238 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2252 | MainLoss:0.2252 | SPLoss:3.1267 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5506 | MainLoss:0.5506 | SPLoss:3.1267 | CLSLoss:0.0000 | AUROC:0.9860\n",
      "\n",
      "Epoch: [267 | 1000] LR: 0.037373\n",
      "Train | 16/16 | Loss:0.5474 | MainLoss:0.4568 | Alpha:0.0291 | SPLoss:3.1097 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:3.0876 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5268 | MainLoss:0.5268 | SPLoss:3.0876 | CLSLoss:0.0000 | AUROC:0.9868\n",
      "\n",
      "Epoch: [268 | 1000] LR: 0.037341\n",
      "Train | 16/16 | Loss:0.5487 | MainLoss:0.4599 | Alpha:0.0287 | SPLoss:3.0932 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2305 | MainLoss:0.2305 | SPLoss:3.0898 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5357 | MainLoss:0.5357 | SPLoss:3.0898 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [269 | 1000] LR: 0.037310\n",
      "Train | 16/16 | Loss:0.5509 | MainLoss:0.4616 | Alpha:0.0291 | SPLoss:3.0734 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2293 | MainLoss:0.2293 | SPLoss:3.0779 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5319 | MainLoss:0.5319 | SPLoss:3.0779 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [270 | 1000] LR: 0.037278\n",
      "Train | 16/16 | Loss:0.5503 | MainLoss:0.4594 | Alpha:0.0297 | SPLoss:3.0648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2160 | MainLoss:0.2160 | SPLoss:3.0772 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5689 | MainLoss:0.5689 | SPLoss:3.0772 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [271 | 1000] LR: 0.037247\n",
      "Train | 16/16 | Loss:0.5387 | MainLoss:0.4524 | Alpha:0.0283 | SPLoss:3.0550 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2196 | MainLoss:0.2196 | SPLoss:3.0428 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5536 | MainLoss:0.5536 | SPLoss:3.0428 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [272 | 1000] LR: 0.037215\n",
      "Train | 16/16 | Loss:0.5562 | MainLoss:0.4669 | Alpha:0.0294 | SPLoss:3.0398 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2298 | MainLoss:0.2298 | SPLoss:3.0351 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5295 | MainLoss:0.5295 | SPLoss:3.0351 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [273 | 1000] LR: 0.037183\n",
      "Train | 16/16 | Loss:0.5482 | MainLoss:0.4609 | Alpha:0.0289 | SPLoss:3.0242 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2336 | MainLoss:0.2336 | SPLoss:3.0393 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5240 | MainLoss:0.5240 | SPLoss:3.0393 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [274 | 1000] LR: 0.037151\n",
      "Train | 16/16 | Loss:0.5429 | MainLoss:0.4557 | Alpha:0.0287 | SPLoss:3.0305 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:3.0212 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5517 | MainLoss:0.5517 | SPLoss:3.0211 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [275 | 1000] LR: 0.037118\n",
      "Train | 16/16 | Loss:0.5495 | MainLoss:0.4641 | Alpha:0.0284 | SPLoss:3.0082 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:2.9955 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5223 | MainLoss:0.5223 | SPLoss:2.9955 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [276 | 1000] LR: 0.037086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5452 | MainLoss:0.4629 | Alpha:0.0274 | SPLoss:3.0075 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2297 | MainLoss:0.2297 | SPLoss:3.0051 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5248 | MainLoss:0.5248 | SPLoss:3.0051 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [277 | 1000] LR: 0.037053\n",
      "Train | 16/16 | Loss:0.5521 | MainLoss:0.4639 | Alpha:0.0293 | SPLoss:3.0120 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:3.0257 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5420 | MainLoss:0.5420 | SPLoss:3.0257 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [278 | 1000] LR: 0.037020\n",
      "Train | 16/16 | Loss:0.5532 | MainLoss:0.4644 | Alpha:0.0293 | SPLoss:3.0362 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2244 | MainLoss:0.2244 | SPLoss:3.0437 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5487 | MainLoss:0.5487 | SPLoss:3.0437 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [279 | 1000] LR: 0.036987\n",
      "Train | 16/16 | Loss:0.5427 | MainLoss:0.4561 | Alpha:0.0288 | SPLoss:3.0102 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2291 | MainLoss:0.2291 | SPLoss:2.9976 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5308 | MainLoss:0.5308 | SPLoss:2.9976 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [280 | 1000] LR: 0.036954\n",
      "Train | 16/16 | Loss:0.5514 | MainLoss:0.4652 | Alpha:0.0287 | SPLoss:3.0010 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2277 | MainLoss:0.2277 | SPLoss:3.0132 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5454 | MainLoss:0.5454 | SPLoss:3.0132 | CLSLoss:0.0000 | AUROC:0.9905\n",
      "\n",
      "Epoch: [281 | 1000] LR: 0.036920\n",
      "Train | 16/16 | Loss:0.5471 | MainLoss:0.4597 | Alpha:0.0292 | SPLoss:2.9943 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2267 | MainLoss:0.2267 | SPLoss:2.9961 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5394 | MainLoss:0.5394 | SPLoss:2.9961 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [282 | 1000] LR: 0.036887\n",
      "Train | 16/16 | Loss:0.5415 | MainLoss:0.4564 | Alpha:0.0285 | SPLoss:2.9868 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2354 | MainLoss:0.2354 | SPLoss:2.9798 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "Test | 30/16 | Loss:0.5213 | MainLoss:0.5213 | SPLoss:2.9798 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [283 | 1000] LR: 0.036853\n",
      "Train | 16/16 | Loss:0.5360 | MainLoss:0.4527 | Alpha:0.0279 | SPLoss:2.9823 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2169 | MainLoss:0.2169 | SPLoss:3.0016 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5678 | MainLoss:0.5678 | SPLoss:3.0016 | CLSLoss:0.0000 | AUROC:0.9853\n",
      "\n",
      "Epoch: [284 | 1000] LR: 0.036819\n",
      "Train | 16/16 | Loss:0.5558 | MainLoss:0.4669 | Alpha:0.0298 | SPLoss:2.9854 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2347 | MainLoss:0.2347 | SPLoss:2.9760 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5273 | MainLoss:0.5273 | SPLoss:2.9760 | CLSLoss:0.0000 | AUROC:0.9872\n",
      "\n",
      "Epoch: [285 | 1000] LR: 0.036785\n",
      "Train | 16/16 | Loss:0.5494 | MainLoss:0.4631 | Alpha:0.0291 | SPLoss:2.9701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2352 | MainLoss:0.2352 | SPLoss:2.9576 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 30/16 | Loss:0.5312 | MainLoss:0.5312 | SPLoss:2.9576 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [286 | 1000] LR: 0.036751\n",
      "Train | 16/16 | Loss:0.5381 | MainLoss:0.4549 | Alpha:0.0281 | SPLoss:2.9619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2309 | MainLoss:0.2309 | SPLoss:2.9599 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5244 | MainLoss:0.5244 | SPLoss:2.9599 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [287 | 1000] LR: 0.036716\n",
      "Train | 16/16 | Loss:0.5446 | MainLoss:0.4597 | Alpha:0.0287 | SPLoss:2.9574 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2212 | MainLoss:0.2212 | SPLoss:2.9733 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5576 | MainLoss:0.5576 | SPLoss:2.9733 | CLSLoss:0.0000 | AUROC:0.9861\n",
      "\n",
      "Epoch: [288 | 1000] LR: 0.036682\n",
      "Train | 16/16 | Loss:0.5515 | MainLoss:0.4669 | Alpha:0.0284 | SPLoss:2.9727 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2318 | MainLoss:0.2318 | SPLoss:2.9733 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5274 | MainLoss:0.5274 | SPLoss:2.9733 | CLSLoss:0.0000 | AUROC:0.9874\n",
      "\n",
      "Epoch: [289 | 1000] LR: 0.036647\n",
      "Train | 16/16 | Loss:0.5452 | MainLoss:0.4606 | Alpha:0.0284 | SPLoss:2.9762 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2311 | MainLoss:0.2311 | SPLoss:2.9742 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5292 | MainLoss:0.5292 | SPLoss:2.9742 | CLSLoss:0.0000 | AUROC:0.9870\n",
      "\n",
      "Epoch: [290 | 1000] LR: 0.036612\n",
      "Train | 16/16 | Loss:0.5560 | MainLoss:0.4716 | Alpha:0.0282 | SPLoss:2.9894 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2344 | MainLoss:0.2344 | SPLoss:3.0094 | CLSLoss:0.0000 | AUROC:0.9938\n",
      "Test | 30/16 | Loss:0.5420 | MainLoss:0.5420 | SPLoss:3.0094 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [291 | 1000] LR: 0.036577\n",
      "Train | 16/16 | Loss:0.5478 | MainLoss:0.4632 | Alpha:0.0281 | SPLoss:3.0082 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2336 | MainLoss:0.2336 | SPLoss:3.0137 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5185 | MainLoss:0.5185 | SPLoss:3.0137 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [292 | 1000] LR: 0.036542\n",
      "Train | 16/16 | Loss:0.5613 | MainLoss:0.4719 | Alpha:0.0296 | SPLoss:3.0170 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2307 | MainLoss:0.2307 | SPLoss:3.0117 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5261 | MainLoss:0.5261 | SPLoss:3.0117 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [293 | 1000] LR: 0.036506\n",
      "Train | 16/16 | Loss:0.5477 | MainLoss:0.4598 | Alpha:0.0292 | SPLoss:3.0077 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2252 | MainLoss:0.2252 | SPLoss:3.0128 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5371 | MainLoss:0.5371 | SPLoss:3.0128 | CLSLoss:0.0000 | AUROC:0.9876\n",
      "\n",
      "Epoch: [294 | 1000] LR: 0.036471\n",
      "Train | 16/16 | Loss:0.5458 | MainLoss:0.4583 | Alpha:0.0290 | SPLoss:3.0203 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2287 | MainLoss:0.2287 | SPLoss:3.0196 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5333 | MainLoss:0.5333 | SPLoss:3.0196 | CLSLoss:0.0000 | AUROC:0.9869\n",
      "\n",
      "Epoch: [295 | 1000] LR: 0.036435\n",
      "Train | 16/16 | Loss:0.5512 | MainLoss:0.4665 | Alpha:0.0281 | SPLoss:3.0195 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2205 | MainLoss:0.2205 | SPLoss:3.0300 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5606 | MainLoss:0.5606 | SPLoss:3.0300 | CLSLoss:0.0000 | AUROC:0.9870\n",
      "\n",
      "Epoch: [296 | 1000] LR: 0.036399\n",
      "Train | 16/16 | Loss:0.5393 | MainLoss:0.4536 | Alpha:0.0284 | SPLoss:3.0194 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2245 | MainLoss:0.2245 | SPLoss:3.0062 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5400 | MainLoss:0.5400 | SPLoss:3.0062 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [297 | 1000] LR: 0.036363\n",
      "Train | 16/16 | Loss:0.5484 | MainLoss:0.4609 | Alpha:0.0292 | SPLoss:3.0024 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2209 | MainLoss:0.2209 | SPLoss:3.0091 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5527 | MainLoss:0.5527 | SPLoss:3.0091 | CLSLoss:0.0000 | AUROC:0.9873\n",
      "\n",
      "Epoch: [298 | 1000] LR: 0.036327\n",
      "Train | 16/16 | Loss:0.5410 | MainLoss:0.4521 | Alpha:0.0297 | SPLoss:2.9914 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2260 | MainLoss:0.2260 | SPLoss:2.9779 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5381 | MainLoss:0.5381 | SPLoss:2.9779 | CLSLoss:0.0000 | AUROC:0.9871\n",
      "\n",
      "Epoch: [299 | 1000] LR: 0.036290\n",
      "Train | 16/16 | Loss:0.5455 | MainLoss:0.4592 | Alpha:0.0289 | SPLoss:2.9869 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2475 | MainLoss:0.2475 | SPLoss:2.9798 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.4908 | MainLoss:0.4908 | SPLoss:2.9798 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [300 | 1000] LR: 0.036254\n",
      "Train | 16/16 | Loss:0.5540 | MainLoss:0.4659 | Alpha:0.0295 | SPLoss:2.9923 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2344 | MainLoss:0.2344 | SPLoss:2.9814 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5177 | MainLoss:0.5177 | SPLoss:2.9814 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [301 | 1000] LR: 0.036217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5495 | MainLoss:0.4624 | Alpha:0.0292 | SPLoss:2.9819 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2228 | MainLoss:0.2228 | SPLoss:3.0067 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5575 | MainLoss:0.5575 | SPLoss:3.0067 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [302 | 1000] LR: 0.036180\n",
      "Train | 16/16 | Loss:0.5489 | MainLoss:0.4607 | Alpha:0.0295 | SPLoss:2.9931 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2301 | MainLoss:0.2301 | SPLoss:2.9948 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5320 | MainLoss:0.5320 | SPLoss:2.9948 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [303 | 1000] LR: 0.036143\n",
      "Train | 16/16 | Loss:0.5543 | MainLoss:0.4688 | Alpha:0.0286 | SPLoss:2.9908 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2224 | MainLoss:0.2224 | SPLoss:3.0092 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5496 | MainLoss:0.5496 | SPLoss:3.0091 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [304 | 1000] LR: 0.036106\n",
      "Train | 16/16 | Loss:0.5502 | MainLoss:0.4611 | Alpha:0.0296 | SPLoss:3.0083 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2276 | MainLoss:0.2276 | SPLoss:2.9978 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5340 | MainLoss:0.5340 | SPLoss:2.9979 | CLSLoss:0.0000 | AUROC:0.9883\n",
      "\n",
      "Epoch: [305 | 1000] LR: 0.036069\n",
      "Train | 16/16 | Loss:0.5491 | MainLoss:0.4615 | Alpha:0.0294 | SPLoss:2.9785 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2194 | MainLoss:0.2194 | SPLoss:2.9692 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5441 | MainLoss:0.5441 | SPLoss:2.9692 | CLSLoss:0.0000 | AUROC:0.9879\n",
      "\n",
      "Epoch: [306 | 1000] LR: 0.036031\n",
      "Train | 16/16 | Loss:0.5470 | MainLoss:0.4600 | Alpha:0.0292 | SPLoss:2.9807 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2216 | MainLoss:0.2216 | SPLoss:3.0088 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5554 | MainLoss:0.5554 | SPLoss:3.0088 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [307 | 1000] LR: 0.035994\n",
      "Train | 16/16 | Loss:0.5431 | MainLoss:0.4567 | Alpha:0.0290 | SPLoss:2.9852 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2222 | MainLoss:0.2222 | SPLoss:2.9804 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5491 | MainLoss:0.5491 | SPLoss:2.9804 | CLSLoss:0.0000 | AUROC:0.9877\n",
      "\n",
      "Epoch: [308 | 1000] LR: 0.035956\n",
      "Train | 16/16 | Loss:0.5439 | MainLoss:0.4591 | Alpha:0.0286 | SPLoss:2.9664 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2224 | MainLoss:0.2224 | SPLoss:2.9957 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5620 | MainLoss:0.5620 | SPLoss:2.9957 | CLSLoss:0.0000 | AUROC:0.9881\n",
      "\n",
      "Epoch: [309 | 1000] LR: 0.035918\n",
      "Train | 16/16 | Loss:0.5511 | MainLoss:0.4634 | Alpha:0.0294 | SPLoss:2.9868 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2348 | MainLoss:0.2348 | SPLoss:2.9757 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5229 | MainLoss:0.5229 | SPLoss:2.9757 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [310 | 1000] LR: 0.035880\n",
      "Train | 16/16 | Loss:0.5453 | MainLoss:0.4598 | Alpha:0.0288 | SPLoss:2.9729 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2273 | MainLoss:0.2273 | SPLoss:2.9705 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5367 | MainLoss:0.5367 | SPLoss:2.9705 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [311 | 1000] LR: 0.035842\n",
      "Train | 16/16 | Loss:0.5451 | MainLoss:0.4602 | Alpha:0.0287 | SPLoss:2.9595 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:2.9661 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5429 | MainLoss:0.5429 | SPLoss:2.9661 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [312 | 1000] LR: 0.035803\n",
      "Train | 16/16 | Loss:0.5551 | MainLoss:0.4693 | Alpha:0.0289 | SPLoss:2.9622 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2304 | MainLoss:0.2304 | SPLoss:2.9629 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5253 | MainLoss:0.5253 | SPLoss:2.9629 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [313 | 1000] LR: 0.035765\n",
      "Train | 16/16 | Loss:0.5472 | MainLoss:0.4618 | Alpha:0.0289 | SPLoss:2.9569 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2237 | MainLoss:0.2237 | SPLoss:2.9585 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5470 | MainLoss:0.5470 | SPLoss:2.9585 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [314 | 1000] LR: 0.035726\n",
      "Train | 16/16 | Loss:0.5441 | MainLoss:0.4565 | Alpha:0.0298 | SPLoss:2.9424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2143 | MainLoss:0.2143 | SPLoss:2.9964 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5945 | MainLoss:0.5945 | SPLoss:2.9964 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [315 | 1000] LR: 0.035687\n",
      "Train | 16/16 | Loss:0.5469 | MainLoss:0.4620 | Alpha:0.0286 | SPLoss:2.9680 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2261 | MainLoss:0.2261 | SPLoss:2.9610 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5402 | MainLoss:0.5402 | SPLoss:2.9610 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [316 | 1000] LR: 0.035648\n",
      "Train | 16/16 | Loss:0.5462 | MainLoss:0.4589 | Alpha:0.0295 | SPLoss:2.9532 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2161 | MainLoss:0.2161 | SPLoss:2.9533 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5652 | MainLoss:0.5652 | SPLoss:2.9533 | CLSLoss:0.0000 | AUROC:0.9884\n",
      "\n",
      "Epoch: [317 | 1000] LR: 0.035609\n",
      "Train | 16/16 | Loss:0.5527 | MainLoss:0.4667 | Alpha:0.0291 | SPLoss:2.9559 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2286 | MainLoss:0.2286 | SPLoss:2.9691 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.5397 | MainLoss:0.5397 | SPLoss:2.9690 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [318 | 1000] LR: 0.035569\n",
      "Train | 16/16 | Loss:0.5479 | MainLoss:0.4598 | Alpha:0.0298 | SPLoss:2.9576 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2312 | MainLoss:0.2312 | SPLoss:2.9536 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5408 | MainLoss:0.5408 | SPLoss:2.9536 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [319 | 1000] LR: 0.035530\n",
      "Train | 16/16 | Loss:0.5475 | MainLoss:0.4616 | Alpha:0.0292 | SPLoss:2.9441 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2269 | MainLoss:0.2269 | SPLoss:2.9456 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5423 | MainLoss:0.5423 | SPLoss:2.9456 | CLSLoss:0.0000 | AUROC:0.9880\n",
      "\n",
      "Epoch: [320 | 1000] LR: 0.035490\n",
      "Train | 16/16 | Loss:0.5578 | MainLoss:0.4679 | Alpha:0.0304 | SPLoss:2.9530 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2357 | MainLoss:0.2357 | SPLoss:2.9598 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5235 | MainLoss:0.5235 | SPLoss:2.9598 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [321 | 1000] LR: 0.035450\n",
      "Train | 16/16 | Loss:0.5466 | MainLoss:0.4617 | Alpha:0.0286 | SPLoss:2.9624 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2202 | MainLoss:0.2202 | SPLoss:2.9849 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5617 | MainLoss:0.5617 | SPLoss:2.9849 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [322 | 1000] LR: 0.035410\n",
      "Train | 16/16 | Loss:0.5423 | MainLoss:0.4553 | Alpha:0.0293 | SPLoss:2.9686 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2358 | MainLoss:0.2358 | SPLoss:2.9528 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5188 | MainLoss:0.5188 | SPLoss:2.9528 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [323 | 1000] LR: 0.035370\n",
      "Train | 16/16 | Loss:0.5465 | MainLoss:0.4597 | Alpha:0.0294 | SPLoss:2.9515 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2338 | MainLoss:0.2338 | SPLoss:2.9458 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5185 | MainLoss:0.5185 | SPLoss:2.9458 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [324 | 1000] LR: 0.035330\n",
      "Train | 16/16 | Loss:0.5421 | MainLoss:0.4577 | Alpha:0.0285 | SPLoss:2.9578 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2378 | MainLoss:0.2378 | SPLoss:2.9560 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5160 | MainLoss:0.5160 | SPLoss:2.9560 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [325 | 1000] LR: 0.035289\n",
      "Train | 16/16 | Loss:0.5387 | MainLoss:0.4548 | Alpha:0.0286 | SPLoss:2.9386 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2307 | MainLoss:0.2307 | SPLoss:2.9252 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5213 | MainLoss:0.5213 | SPLoss:2.9252 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [326 | 1000] LR: 0.035249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5506 | MainLoss:0.4696 | Alpha:0.0276 | SPLoss:2.9365 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.9429 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5473 | MainLoss:0.5473 | SPLoss:2.9429 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [327 | 1000] LR: 0.035208\n",
      "Train | 16/16 | Loss:0.5485 | MainLoss:0.4629 | Alpha:0.0290 | SPLoss:2.9514 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2278 | MainLoss:0.2278 | SPLoss:2.9466 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5331 | MainLoss:0.5331 | SPLoss:2.9466 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [328 | 1000] LR: 0.035167\n",
      "Train | 16/16 | Loss:0.5388 | MainLoss:0.4541 | Alpha:0.0288 | SPLoss:2.9422 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2482 | MainLoss:0.2482 | SPLoss:2.9184 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.4879 | MainLoss:0.4879 | SPLoss:2.9184 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [329 | 1000] LR: 0.035126\n",
      "Train | 16/16 | Loss:0.5437 | MainLoss:0.4575 | Alpha:0.0295 | SPLoss:2.9252 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2352 | MainLoss:0.2352 | SPLoss:2.9225 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5151 | MainLoss:0.5151 | SPLoss:2.9225 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [330 | 1000] LR: 0.035085\n",
      "Train | 16/16 | Loss:0.5478 | MainLoss:0.4633 | Alpha:0.0288 | SPLoss:2.9397 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2290 | MainLoss:0.2290 | SPLoss:2.9502 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5335 | MainLoss:0.5335 | SPLoss:2.9502 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [331 | 1000] LR: 0.035044\n",
      "Train | 16/16 | Loss:0.5388 | MainLoss:0.4538 | Alpha:0.0290 | SPLoss:2.9308 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2210 | MainLoss:0.2210 | SPLoss:2.9131 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5445 | MainLoss:0.5445 | SPLoss:2.9131 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [332 | 1000] LR: 0.035002\n",
      "Train | 16/16 | Loss:0.5350 | MainLoss:0.4509 | Alpha:0.0290 | SPLoss:2.9030 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2200 | MainLoss:0.2200 | SPLoss:2.9116 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5504 | MainLoss:0.5504 | SPLoss:2.9116 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [333 | 1000] LR: 0.034961\n",
      "Train | 16/16 | Loss:0.5452 | MainLoss:0.4620 | Alpha:0.0286 | SPLoss:2.9127 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2195 | MainLoss:0.2195 | SPLoss:2.9330 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5590 | MainLoss:0.5590 | SPLoss:2.9330 | CLSLoss:0.0000 | AUROC:0.9884\n",
      "\n",
      "Epoch: [334 | 1000] LR: 0.034919\n",
      "Train | 16/16 | Loss:0.5430 | MainLoss:0.4547 | Alpha:0.0303 | SPLoss:2.9161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2275 | MainLoss:0.2275 | SPLoss:2.8919 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5350 | MainLoss:0.5350 | SPLoss:2.8919 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [335 | 1000] LR: 0.034877\n",
      "Train | 16/16 | Loss:0.5362 | MainLoss:0.4558 | Alpha:0.0277 | SPLoss:2.8996 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:2.8997 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5449 | MainLoss:0.5449 | SPLoss:2.8997 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [336 | 1000] LR: 0.034835\n",
      "Train | 16/16 | Loss:0.5412 | MainLoss:0.4567 | Alpha:0.0291 | SPLoss:2.9078 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.9040 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5525 | MainLoss:0.5525 | SPLoss:2.9040 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [337 | 1000] LR: 0.034793\n",
      "Train | 16/16 | Loss:0.5505 | MainLoss:0.4647 | Alpha:0.0297 | SPLoss:2.8951 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2403 | MainLoss:0.2403 | SPLoss:2.8775 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5076 | MainLoss:0.5076 | SPLoss:2.8775 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [338 | 1000] LR: 0.034750\n",
      "Train | 16/16 | Loss:0.5550 | MainLoss:0.4665 | Alpha:0.0306 | SPLoss:2.8957 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2347 | MainLoss:0.2347 | SPLoss:2.9119 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "Test | 30/16 | Loss:0.5316 | MainLoss:0.5316 | SPLoss:2.9119 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [339 | 1000] LR: 0.034708\n",
      "Train | 16/16 | Loss:0.5481 | MainLoss:0.4639 | Alpha:0.0291 | SPLoss:2.8968 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:2.8812 | CLSLoss:0.0000 | AUROC:0.9943\n",
      "Test | 30/16 | Loss:0.5342 | MainLoss:0.5342 | SPLoss:2.8812 | CLSLoss:0.0000 | AUROC:0.9909\n",
      "\n",
      "Epoch: [340 | 1000] LR: 0.034665\n",
      "Train | 16/16 | Loss:0.5539 | MainLoss:0.4691 | Alpha:0.0294 | SPLoss:2.8809 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2369 | MainLoss:0.2369 | SPLoss:2.8810 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5202 | MainLoss:0.5202 | SPLoss:2.8810 | CLSLoss:0.0000 | AUROC:0.9917\n",
      "\n",
      "Epoch: [341 | 1000] LR: 0.034622\n",
      "Train | 16/16 | Loss:0.5425 | MainLoss:0.4591 | Alpha:0.0290 | SPLoss:2.8755 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2188 | MainLoss:0.2188 | SPLoss:2.8937 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5613 | MainLoss:0.5613 | SPLoss:2.8937 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [342 | 1000] LR: 0.034579\n",
      "Train | 16/16 | Loss:0.5450 | MainLoss:0.4615 | Alpha:0.0289 | SPLoss:2.8912 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2382 | MainLoss:0.2382 | SPLoss:2.8893 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "Test | 30/16 | Loss:0.5151 | MainLoss:0.5151 | SPLoss:2.8893 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [343 | 1000] LR: 0.034536\n",
      "Train | 16/16 | Loss:0.5448 | MainLoss:0.4590 | Alpha:0.0297 | SPLoss:2.8901 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2256 | MainLoss:0.2256 | SPLoss:2.8826 | CLSLoss:0.0000 | AUROC:0.9946\n",
      "Test | 30/16 | Loss:0.5404 | MainLoss:0.5404 | SPLoss:2.8826 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [344 | 1000] LR: 0.034493\n",
      "Train | 16/16 | Loss:0.5567 | MainLoss:0.4760 | Alpha:0.0280 | SPLoss:2.8889 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2183 | MainLoss:0.2183 | SPLoss:2.9171 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5685 | MainLoss:0.5685 | SPLoss:2.9171 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [345 | 1000] LR: 0.034450\n",
      "Train | 16/16 | Loss:0.5412 | MainLoss:0.4598 | Alpha:0.0280 | SPLoss:2.9058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2289 | MainLoss:0.2289 | SPLoss:2.9080 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5320 | MainLoss:0.5320 | SPLoss:2.9080 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [346 | 1000] LR: 0.034406\n",
      "Train | 16/16 | Loss:0.5522 | MainLoss:0.4670 | Alpha:0.0293 | SPLoss:2.9042 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2283 | MainLoss:0.2283 | SPLoss:2.9038 | CLSLoss:0.0000 | AUROC:0.9947\n",
      "Test | 30/16 | Loss:0.5418 | MainLoss:0.5418 | SPLoss:2.9038 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [347 | 1000] LR: 0.034363\n",
      "Train | 16/16 | Loss:0.5510 | MainLoss:0.4667 | Alpha:0.0290 | SPLoss:2.9039 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2389 | MainLoss:0.2389 | SPLoss:2.9066 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5117 | MainLoss:0.5117 | SPLoss:2.9065 | CLSLoss:0.0000 | AUROC:0.9913\n",
      "\n",
      "Epoch: [348 | 1000] LR: 0.034319\n",
      "Train | 16/16 | Loss:0.5533 | MainLoss:0.4701 | Alpha:0.0287 | SPLoss:2.9009 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2258 | MainLoss:0.2258 | SPLoss:2.9066 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5359 | MainLoss:0.5359 | SPLoss:2.9066 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [349 | 1000] LR: 0.034275\n",
      "Train | 16/16 | Loss:0.5371 | MainLoss:0.4535 | Alpha:0.0287 | SPLoss:2.9090 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2261 | MainLoss:0.2261 | SPLoss:2.8988 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5341 | MainLoss:0.5341 | SPLoss:2.8988 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [350 | 1000] LR: 0.034231\n",
      "Train | 16/16 | Loss:0.5462 | MainLoss:0.4608 | Alpha:0.0295 | SPLoss:2.8983 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2283 | MainLoss:0.2283 | SPLoss:2.8955 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5336 | MainLoss:0.5336 | SPLoss:2.8955 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [351 | 1000] LR: 0.034186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5481 | MainLoss:0.4639 | Alpha:0.0290 | SPLoss:2.9016 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2557 | MainLoss:0.2557 | SPLoss:2.8843 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.4658 | MainLoss:0.4658 | SPLoss:2.8843 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [352 | 1000] LR: 0.034142\n",
      "Train | 16/16 | Loss:0.5454 | MainLoss:0.4624 | Alpha:0.0286 | SPLoss:2.9028 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:2.9078 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5479 | MainLoss:0.5479 | SPLoss:2.9078 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [353 | 1000] LR: 0.034098\n",
      "Train | 16/16 | Loss:0.5454 | MainLoss:0.4601 | Alpha:0.0293 | SPLoss:2.9080 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2351 | MainLoss:0.2351 | SPLoss:2.8957 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5167 | MainLoss:0.5167 | SPLoss:2.8957 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [354 | 1000] LR: 0.034053\n",
      "Train | 16/16 | Loss:0.5428 | MainLoss:0.4571 | Alpha:0.0296 | SPLoss:2.8907 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2271 | MainLoss:0.2271 | SPLoss:2.8767 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5386 | MainLoss:0.5386 | SPLoss:2.8767 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [355 | 1000] LR: 0.034008\n",
      "Train | 16/16 | Loss:0.5465 | MainLoss:0.4652 | Alpha:0.0282 | SPLoss:2.8871 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2470 | MainLoss:0.2470 | SPLoss:2.8813 | CLSLoss:0.0000 | AUROC:0.9948\n",
      "Test | 30/16 | Loss:0.4957 | MainLoss:0.4957 | SPLoss:2.8813 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [356 | 1000] LR: 0.033963\n",
      "Train | 16/16 | Loss:0.5524 | MainLoss:0.4689 | Alpha:0.0289 | SPLoss:2.8924 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2361 | MainLoss:0.2361 | SPLoss:2.9026 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5215 | MainLoss:0.5215 | SPLoss:2.9026 | CLSLoss:0.0000 | AUROC:0.9908\n",
      "\n",
      "Epoch: [357 | 1000] LR: 0.033918\n",
      "Train | 16/16 | Loss:0.5413 | MainLoss:0.4588 | Alpha:0.0286 | SPLoss:2.8870 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2363 | MainLoss:0.2363 | SPLoss:2.8638 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5091 | MainLoss:0.5091 | SPLoss:2.8638 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [358 | 1000] LR: 0.033873\n",
      "Train | 16/16 | Loss:0.5414 | MainLoss:0.4581 | Alpha:0.0290 | SPLoss:2.8685 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2275 | MainLoss:0.2275 | SPLoss:2.8821 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5341 | MainLoss:0.5341 | SPLoss:2.8821 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [359 | 1000] LR: 0.033828\n",
      "Train | 16/16 | Loss:0.5409 | MainLoss:0.4568 | Alpha:0.0291 | SPLoss:2.8855 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2249 | MainLoss:0.2249 | SPLoss:2.8828 | CLSLoss:0.0000 | AUROC:0.9945\n",
      "Test | 30/16 | Loss:0.5508 | MainLoss:0.5508 | SPLoss:2.8828 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [360 | 1000] LR: 0.033782\n",
      "Train | 16/16 | Loss:0.5350 | MainLoss:0.4537 | Alpha:0.0282 | SPLoss:2.8823 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2240 | MainLoss:0.2240 | SPLoss:2.8806 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5453 | MainLoss:0.5453 | SPLoss:2.8806 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [361 | 1000] LR: 0.033737\n",
      "Train | 16/16 | Loss:0.5418 | MainLoss:0.4599 | Alpha:0.0283 | SPLoss:2.8978 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2311 | MainLoss:0.2311 | SPLoss:2.9028 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5305 | MainLoss:0.5305 | SPLoss:2.9028 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [362 | 1000] LR: 0.033691\n",
      "Train | 16/16 | Loss:0.5342 | MainLoss:0.4520 | Alpha:0.0284 | SPLoss:2.8910 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2221 | MainLoss:0.2221 | SPLoss:2.8879 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5527 | MainLoss:0.5527 | SPLoss:2.8879 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [363 | 1000] LR: 0.033645\n",
      "Train | 16/16 | Loss:0.5429 | MainLoss:0.4599 | Alpha:0.0287 | SPLoss:2.8935 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2361 | MainLoss:0.2361 | SPLoss:2.8953 | CLSLoss:0.0000 | AUROC:0.9944\n",
      "Test | 30/16 | Loss:0.5262 | MainLoss:0.5262 | SPLoss:2.8953 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [364 | 1000] LR: 0.033599\n",
      "Train | 16/16 | Loss:0.5499 | MainLoss:0.4666 | Alpha:0.0287 | SPLoss:2.9021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2274 | MainLoss:0.2274 | SPLoss:2.8932 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5364 | MainLoss:0.5364 | SPLoss:2.8932 | CLSLoss:0.0000 | AUROC:0.9886\n",
      "\n",
      "Epoch: [365 | 1000] LR: 0.033553\n",
      "Train | 16/16 | Loss:0.5406 | MainLoss:0.4587 | Alpha:0.0283 | SPLoss:2.8930 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:2.8843 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5198 | MainLoss:0.5198 | SPLoss:2.8843 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [366 | 1000] LR: 0.033507\n",
      "Train | 16/16 | Loss:0.5423 | MainLoss:0.4560 | Alpha:0.0298 | SPLoss:2.8923 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2305 | MainLoss:0.2305 | SPLoss:2.9008 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5314 | MainLoss:0.5314 | SPLoss:2.9008 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [367 | 1000] LR: 0.033460\n",
      "Train | 16/16 | Loss:0.5453 | MainLoss:0.4597 | Alpha:0.0295 | SPLoss:2.9070 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2386 | MainLoss:0.2386 | SPLoss:2.8841 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5120 | MainLoss:0.5120 | SPLoss:2.8840 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [368 | 1000] LR: 0.033414\n",
      "Train | 16/16 | Loss:0.5370 | MainLoss:0.4531 | Alpha:0.0289 | SPLoss:2.9080 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2315 | MainLoss:0.2315 | SPLoss:2.9055 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5326 | MainLoss:0.5326 | SPLoss:2.9055 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [369 | 1000] LR: 0.033367\n",
      "Train | 16/16 | Loss:0.5467 | MainLoss:0.4627 | Alpha:0.0288 | SPLoss:2.9139 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2324 | MainLoss:0.2324 | SPLoss:2.9030 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5267 | MainLoss:0.5267 | SPLoss:2.9030 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [370 | 1000] LR: 0.033320\n",
      "Train | 16/16 | Loss:0.5461 | MainLoss:0.4623 | Alpha:0.0289 | SPLoss:2.8998 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2323 | MainLoss:0.2323 | SPLoss:2.8893 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5200 | MainLoss:0.5200 | SPLoss:2.8893 | CLSLoss:0.0000 | AUROC:0.9872\n",
      "\n",
      "Epoch: [371 | 1000] LR: 0.033273\n",
      "Train | 16/16 | Loss:0.5447 | MainLoss:0.4609 | Alpha:0.0290 | SPLoss:2.8895 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:2.9017 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5415 | MainLoss:0.5415 | SPLoss:2.9017 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [372 | 1000] LR: 0.033226\n",
      "Train | 16/16 | Loss:0.5396 | MainLoss:0.4532 | Alpha:0.0299 | SPLoss:2.8949 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2096 | MainLoss:0.2096 | SPLoss:2.9084 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5737 | MainLoss:0.5737 | SPLoss:2.9084 | CLSLoss:0.0000 | AUROC:0.9865\n",
      "\n",
      "Epoch: [373 | 1000] LR: 0.033179\n",
      "Train | 16/16 | Loss:0.5376 | MainLoss:0.4547 | Alpha:0.0286 | SPLoss:2.9031 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2365 | MainLoss:0.2365 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5134 | MainLoss:0.5134 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.9868\n",
      "\n",
      "Epoch: [374 | 1000] LR: 0.033132\n",
      "Train | 16/16 | Loss:0.5519 | MainLoss:0.4678 | Alpha:0.0291 | SPLoss:2.8896 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:2.9114 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5465 | MainLoss:0.5465 | SPLoss:2.9114 | CLSLoss:0.0000 | AUROC:0.9879\n",
      "\n",
      "Epoch: [375 | 1000] LR: 0.033084\n",
      "Train | 16/16 | Loss:0.5387 | MainLoss:0.4536 | Alpha:0.0294 | SPLoss:2.8957 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2445 | MainLoss:0.2445 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.4965 | MainLoss:0.4965 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [376 | 1000] LR: 0.033037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5604 | MainLoss:0.4760 | Alpha:0.0292 | SPLoss:2.8927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2291 | MainLoss:0.2291 | SPLoss:2.8973 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5351 | MainLoss:0.5351 | SPLoss:2.8973 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [377 | 1000] LR: 0.032989\n",
      "Train | 16/16 | Loss:0.5450 | MainLoss:0.4622 | Alpha:0.0286 | SPLoss:2.8952 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2325 | MainLoss:0.2325 | SPLoss:2.9006 | CLSLoss:0.0000 | AUROC:0.9940\n",
      "Test | 30/16 | Loss:0.5377 | MainLoss:0.5377 | SPLoss:2.9006 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [378 | 1000] LR: 0.032941\n",
      "Train | 16/16 | Loss:0.5469 | MainLoss:0.4643 | Alpha:0.0286 | SPLoss:2.8915 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2328 | MainLoss:0.2328 | SPLoss:2.8720 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5193 | MainLoss:0.5193 | SPLoss:2.8720 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [379 | 1000] LR: 0.032893\n",
      "Train | 16/16 | Loss:0.5458 | MainLoss:0.4609 | Alpha:0.0295 | SPLoss:2.8818 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2339 | MainLoss:0.2339 | SPLoss:2.8829 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5149 | MainLoss:0.5149 | SPLoss:2.8829 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [380 | 1000] LR: 0.032845\n",
      "Train | 16/16 | Loss:0.5412 | MainLoss:0.4602 | Alpha:0.0280 | SPLoss:2.8884 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2145 | MainLoss:0.2145 | SPLoss:2.9044 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5740 | MainLoss:0.5740 | SPLoss:2.9044 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [381 | 1000] LR: 0.032797\n",
      "Train | 16/16 | Loss:0.5348 | MainLoss:0.4532 | Alpha:0.0282 | SPLoss:2.8959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2290 | MainLoss:0.2290 | SPLoss:2.8998 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5346 | MainLoss:0.5346 | SPLoss:2.8998 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [382 | 1000] LR: 0.032748\n",
      "Train | 16/16 | Loss:0.5478 | MainLoss:0.4626 | Alpha:0.0294 | SPLoss:2.8982 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2329 | MainLoss:0.2329 | SPLoss:2.8939 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5197 | MainLoss:0.5197 | SPLoss:2.8939 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [383 | 1000] LR: 0.032700\n",
      "Train | 16/16 | Loss:0.5419 | MainLoss:0.4622 | Alpha:0.0276 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:2.8888 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5426 | MainLoss:0.5426 | SPLoss:2.8888 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [384 | 1000] LR: 0.032651\n",
      "Train | 16/16 | Loss:0.5427 | MainLoss:0.4575 | Alpha:0.0296 | SPLoss:2.8806 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2230 | MainLoss:0.2230 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5376 | MainLoss:0.5376 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.9883\n",
      "\n",
      "Epoch: [385 | 1000] LR: 0.032603\n",
      "Train | 16/16 | Loss:0.5405 | MainLoss:0.4555 | Alpha:0.0295 | SPLoss:2.8848 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2249 | MainLoss:0.2249 | SPLoss:2.8872 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5494 | MainLoss:0.5494 | SPLoss:2.8872 | CLSLoss:0.0000 | AUROC:0.9882\n",
      "\n",
      "Epoch: [386 | 1000] LR: 0.032554\n",
      "Train | 16/16 | Loss:0.5391 | MainLoss:0.4594 | Alpha:0.0277 | SPLoss:2.8789 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8956 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5463 | MainLoss:0.5463 | SPLoss:2.8956 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [387 | 1000] LR: 0.032505\n",
      "Train | 16/16 | Loss:0.5448 | MainLoss:0.4613 | Alpha:0.0288 | SPLoss:2.8956 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2181 | MainLoss:0.2181 | SPLoss:2.9004 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5583 | MainLoss:0.5583 | SPLoss:2.9004 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [388 | 1000] LR: 0.032456\n",
      "Train | 16/16 | Loss:0.5444 | MainLoss:0.4615 | Alpha:0.0286 | SPLoss:2.8991 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2280 | MainLoss:0.2280 | SPLoss:2.8984 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5320 | MainLoss:0.5320 | SPLoss:2.8984 | CLSLoss:0.0000 | AUROC:0.9887\n",
      "\n",
      "Epoch: [389 | 1000] LR: 0.032407\n",
      "Train | 16/16 | Loss:0.5411 | MainLoss:0.4562 | Alpha:0.0294 | SPLoss:2.8838 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2296 | MainLoss:0.2296 | SPLoss:2.8840 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5278 | MainLoss:0.5278 | SPLoss:2.8840 | CLSLoss:0.0000 | AUROC:0.9888\n",
      "\n",
      "Epoch: [390 | 1000] LR: 0.032357\n",
      "Train | 16/16 | Loss:0.5362 | MainLoss:0.4524 | Alpha:0.0291 | SPLoss:2.8819 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2155 | MainLoss:0.2155 | SPLoss:2.8904 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.5748 | MainLoss:0.5748 | SPLoss:2.8904 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [391 | 1000] LR: 0.032308\n",
      "Train | 16/16 | Loss:0.5387 | MainLoss:0.4542 | Alpha:0.0294 | SPLoss:2.8772 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:2.8588 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5175 | MainLoss:0.5175 | SPLoss:2.8589 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [392 | 1000] LR: 0.032258\n",
      "Train | 16/16 | Loss:0.5375 | MainLoss:0.4581 | Alpha:0.0277 | SPLoss:2.8651 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2167 | MainLoss:0.2167 | SPLoss:2.8742 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5565 | MainLoss:0.5565 | SPLoss:2.8742 | CLSLoss:0.0000 | AUROC:0.9885\n",
      "\n",
      "Epoch: [393 | 1000] LR: 0.032208\n",
      "Train | 16/16 | Loss:0.5426 | MainLoss:0.4564 | Alpha:0.0301 | SPLoss:2.8641 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2217 | MainLoss:0.2217 | SPLoss:2.8638 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5408 | MainLoss:0.5408 | SPLoss:2.8638 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [394 | 1000] LR: 0.032159\n",
      "Train | 16/16 | Loss:0.5479 | MainLoss:0.4645 | Alpha:0.0289 | SPLoss:2.8817 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2164 | MainLoss:0.2164 | SPLoss:2.9024 | CLSLoss:0.0000 | AUROC:0.9950\n",
      "Test | 30/16 | Loss:0.5754 | MainLoss:0.5754 | SPLoss:2.9024 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [395 | 1000] LR: 0.032109\n",
      "Train | 16/16 | Loss:0.5385 | MainLoss:0.4558 | Alpha:0.0287 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:2.8721 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5477 | MainLoss:0.5477 | SPLoss:2.8721 | CLSLoss:0.0000 | AUROC:0.9878\n",
      "\n",
      "Epoch: [396 | 1000] LR: 0.032059\n",
      "Train | 16/16 | Loss:0.5442 | MainLoss:0.4630 | Alpha:0.0281 | SPLoss:2.8852 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2271 | MainLoss:0.2271 | SPLoss:2.8913 | CLSLoss:0.0000 | AUROC:0.9949\n",
      "Test | 30/16 | Loss:0.5456 | MainLoss:0.5456 | SPLoss:2.8913 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [397 | 1000] LR: 0.032008\n",
      "Train | 16/16 | Loss:0.5418 | MainLoss:0.4602 | Alpha:0.0283 | SPLoss:2.8864 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2490 | MainLoss:0.2490 | SPLoss:2.8650 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.4886 | MainLoss:0.4886 | SPLoss:2.8650 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [398 | 1000] LR: 0.031958\n",
      "Train | 16/16 | Loss:0.5409 | MainLoss:0.4595 | Alpha:0.0282 | SPLoss:2.8870 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:2.8883 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5404 | MainLoss:0.5404 | SPLoss:2.8883 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [399 | 1000] LR: 0.031908\n",
      "Train | 16/16 | Loss:0.5565 | MainLoss:0.4722 | Alpha:0.0291 | SPLoss:2.8969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2478 | MainLoss:0.2478 | SPLoss:2.8850 | CLSLoss:0.0000 | AUROC:0.9952\n",
      "Test | 30/16 | Loss:0.4911 | MainLoss:0.4911 | SPLoss:2.8850 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [400 | 1000] LR: 0.031857\n",
      "Train | 16/16 | Loss:0.5559 | MainLoss:0.4705 | Alpha:0.0294 | SPLoss:2.9014 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2315 | MainLoss:0.2315 | SPLoss:2.9043 | CLSLoss:0.0000 | AUROC:0.9951\n",
      "Test | 30/16 | Loss:0.5297 | MainLoss:0.5297 | SPLoss:2.9043 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [401 | 1000] LR: 0.003181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5450 | MainLoss:0.4590 | Alpha:0.0296 | SPLoss:2.9039 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2295 | MainLoss:0.2295 | SPLoss:2.9020 | CLSLoss:0.0000 | AUROC:0.9953\n",
      "Test | 30/16 | Loss:0.5314 | MainLoss:0.5314 | SPLoss:2.9020 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [402 | 1000] LR: 0.003176\n",
      "Train | 16/16 | Loss:0.5453 | MainLoss:0.4614 | Alpha:0.0289 | SPLoss:2.9027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2278 | MainLoss:0.2278 | SPLoss:2.9026 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5350 | MainLoss:0.5350 | SPLoss:2.9026 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [403 | 1000] LR: 0.003170\n",
      "Train | 16/16 | Loss:0.5327 | MainLoss:0.4489 | Alpha:0.0289 | SPLoss:2.8994 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2285 | MainLoss:0.2285 | SPLoss:2.8974 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5304 | MainLoss:0.5304 | SPLoss:2.8974 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [404 | 1000] LR: 0.003165\n",
      "Train | 16/16 | Loss:0.5409 | MainLoss:0.4579 | Alpha:0.0286 | SPLoss:2.8975 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2252 | MainLoss:0.2252 | SPLoss:2.8987 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5385 | MainLoss:0.5385 | SPLoss:2.8987 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [405 | 1000] LR: 0.003160\n",
      "Train | 16/16 | Loss:0.5440 | MainLoss:0.4608 | Alpha:0.0287 | SPLoss:2.8969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2250 | MainLoss:0.2250 | SPLoss:2.8982 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5387 | MainLoss:0.5387 | SPLoss:2.8982 | CLSLoss:0.0000 | AUROC:0.9890\n",
      "\n",
      "Epoch: [406 | 1000] LR: 0.003155\n",
      "Train | 16/16 | Loss:0.5471 | MainLoss:0.4662 | Alpha:0.0279 | SPLoss:2.8987 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2254 | MainLoss:0.2254 | SPLoss:2.8996 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5366 | MainLoss:0.5366 | SPLoss:2.8996 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [407 | 1000] LR: 0.003150\n",
      "Train | 16/16 | Loss:0.5410 | MainLoss:0.4576 | Alpha:0.0288 | SPLoss:2.8972 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2255 | MainLoss:0.2255 | SPLoss:2.8957 | CLSLoss:0.0000 | AUROC:0.9958\n",
      "Test | 30/16 | Loss:0.5358 | MainLoss:0.5358 | SPLoss:2.8957 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [408 | 1000] LR: 0.003145\n",
      "Train | 16/16 | Loss:0.5398 | MainLoss:0.4549 | Alpha:0.0293 | SPLoss:2.8958 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2252 | MainLoss:0.2252 | SPLoss:2.8952 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5363 | MainLoss:0.5363 | SPLoss:2.8952 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [409 | 1000] LR: 0.003140\n",
      "Train | 16/16 | Loss:0.5321 | MainLoss:0.4490 | Alpha:0.0287 | SPLoss:2.8935 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2264 | MainLoss:0.2264 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5321 | MainLoss:0.5321 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [410 | 1000] LR: 0.003135\n",
      "Train | 16/16 | Loss:0.5378 | MainLoss:0.4543 | Alpha:0.0289 | SPLoss:2.8892 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2239 | MainLoss:0.2239 | SPLoss:2.8893 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5378 | MainLoss:0.5378 | SPLoss:2.8893 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [411 | 1000] LR: 0.003129\n",
      "Train | 16/16 | Loss:0.5400 | MainLoss:0.4587 | Alpha:0.0281 | SPLoss:2.8909 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2260 | MainLoss:0.2260 | SPLoss:2.8882 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5328 | MainLoss:0.5328 | SPLoss:2.8882 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [412 | 1000] LR: 0.003124\n",
      "Train | 16/16 | Loss:0.5428 | MainLoss:0.4619 | Alpha:0.0280 | SPLoss:2.8867 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2266 | MainLoss:0.2266 | SPLoss:2.8874 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5311 | MainLoss:0.5311 | SPLoss:2.8874 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [413 | 1000] LR: 0.003119\n",
      "Train | 16/16 | Loss:0.5344 | MainLoss:0.4539 | Alpha:0.0279 | SPLoss:2.8890 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2260 | MainLoss:0.2260 | SPLoss:2.8880 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5340 | MainLoss:0.5340 | SPLoss:2.8880 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [414 | 1000] LR: 0.003114\n",
      "Train | 16/16 | Loss:0.5451 | MainLoss:0.4641 | Alpha:0.0280 | SPLoss:2.8907 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2255 | MainLoss:0.2255 | SPLoss:2.8905 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5357 | MainLoss:0.5357 | SPLoss:2.8905 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [415 | 1000] LR: 0.003109\n",
      "Train | 16/16 | Loss:0.5504 | MainLoss:0.4662 | Alpha:0.0291 | SPLoss:2.8931 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2249 | MainLoss:0.2249 | SPLoss:2.8936 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5382 | MainLoss:0.5382 | SPLoss:2.8936 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [416 | 1000] LR: 0.003103\n",
      "Train | 16/16 | Loss:0.5449 | MainLoss:0.4613 | Alpha:0.0289 | SPLoss:2.8920 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2256 | MainLoss:0.2256 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5364 | MainLoss:0.5364 | SPLoss:2.8917 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [417 | 1000] LR: 0.003098\n",
      "Train | 16/16 | Loss:0.5318 | MainLoss:0.4486 | Alpha:0.0288 | SPLoss:2.8901 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2257 | MainLoss:0.2257 | SPLoss:2.8886 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5349 | MainLoss:0.5349 | SPLoss:2.8886 | CLSLoss:0.0000 | AUROC:0.9889\n",
      "\n",
      "Epoch: [418 | 1000] LR: 0.003093\n",
      "Train | 16/16 | Loss:0.5358 | MainLoss:0.4549 | Alpha:0.0280 | SPLoss:2.8882 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2232 | MainLoss:0.2232 | SPLoss:2.8890 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5415 | MainLoss:0.5415 | SPLoss:2.8890 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [419 | 1000] LR: 0.003088\n",
      "Train | 16/16 | Loss:0.5386 | MainLoss:0.4532 | Alpha:0.0295 | SPLoss:2.8887 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2245 | MainLoss:0.2245 | SPLoss:2.8866 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5366 | MainLoss:0.5366 | SPLoss:2.8866 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [420 | 1000] LR: 0.003082\n",
      "Train | 16/16 | Loss:0.5480 | MainLoss:0.4659 | Alpha:0.0284 | SPLoss:2.8888 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8896 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5430 | MainLoss:0.5430 | SPLoss:2.8896 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [421 | 1000] LR: 0.003077\n",
      "Train | 16/16 | Loss:0.5461 | MainLoss:0.4626 | Alpha:0.0289 | SPLoss:2.8886 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:2.8861 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5376 | MainLoss:0.5376 | SPLoss:2.8861 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [422 | 1000] LR: 0.003072\n",
      "Train | 16/16 | Loss:0.5498 | MainLoss:0.4680 | Alpha:0.0283 | SPLoss:2.8847 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2239 | MainLoss:0.2239 | SPLoss:2.8865 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5397 | MainLoss:0.5397 | SPLoss:2.8865 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [423 | 1000] LR: 0.003066\n",
      "Train | 16/16 | Loss:0.5455 | MainLoss:0.4608 | Alpha:0.0294 | SPLoss:2.8863 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2242 | MainLoss:0.2242 | SPLoss:2.8851 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5400 | MainLoss:0.5400 | SPLoss:2.8851 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [424 | 1000] LR: 0.003061\n",
      "Train | 16/16 | Loss:0.5412 | MainLoss:0.4581 | Alpha:0.0288 | SPLoss:2.8843 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2262 | MainLoss:0.2262 | SPLoss:2.8810 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5329 | MainLoss:0.5329 | SPLoss:2.8810 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [425 | 1000] LR: 0.003056\n",
      "Train | 16/16 | Loss:0.5344 | MainLoss:0.4502 | Alpha:0.0293 | SPLoss:2.8791 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2256 | MainLoss:0.2256 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5339 | MainLoss:0.5339 | SPLoss:2.8790 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [426 | 1000] LR: 0.003050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5425 | MainLoss:0.4599 | Alpha:0.0287 | SPLoss:2.8789 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2256 | MainLoss:0.2256 | SPLoss:2.8777 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5341 | MainLoss:0.5341 | SPLoss:2.8777 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [427 | 1000] LR: 0.003045\n",
      "Train | 16/16 | Loss:0.5361 | MainLoss:0.4529 | Alpha:0.0289 | SPLoss:2.8787 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2244 | MainLoss:0.2244 | SPLoss:2.8778 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5365 | MainLoss:0.5365 | SPLoss:2.8778 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [428 | 1000] LR: 0.003040\n",
      "Train | 16/16 | Loss:0.5431 | MainLoss:0.4632 | Alpha:0.0278 | SPLoss:2.8764 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2259 | MainLoss:0.2259 | SPLoss:2.8749 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5331 | MainLoss:0.5331 | SPLoss:2.8749 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [429 | 1000] LR: 0.003034\n",
      "Train | 16/16 | Loss:0.5398 | MainLoss:0.4575 | Alpha:0.0286 | SPLoss:2.8737 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2259 | MainLoss:0.2259 | SPLoss:2.8730 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5329 | MainLoss:0.5329 | SPLoss:2.8730 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [430 | 1000] LR: 0.003029\n",
      "Train | 16/16 | Loss:0.5384 | MainLoss:0.4531 | Alpha:0.0297 | SPLoss:2.8706 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2253 | MainLoss:0.2253 | SPLoss:2.8709 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5332 | MainLoss:0.5332 | SPLoss:2.8709 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [431 | 1000] LR: 0.003023\n",
      "Train | 16/16 | Loss:0.5428 | MainLoss:0.4595 | Alpha:0.0290 | SPLoss:2.8742 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2235 | MainLoss:0.2235 | SPLoss:2.8748 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5386 | MainLoss:0.5386 | SPLoss:2.8748 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [432 | 1000] LR: 0.003018\n",
      "Train | 16/16 | Loss:0.5377 | MainLoss:0.4563 | Alpha:0.0283 | SPLoss:2.8728 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2225 | MainLoss:0.2225 | SPLoss:2.8735 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5411 | MainLoss:0.5411 | SPLoss:2.8735 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [433 | 1000] LR: 0.003013\n",
      "Train | 16/16 | Loss:0.5433 | MainLoss:0.4595 | Alpha:0.0292 | SPLoss:2.8735 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:2.8742 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5387 | MainLoss:0.5387 | SPLoss:2.8742 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [434 | 1000] LR: 0.003007\n",
      "Train | 16/16 | Loss:0.5431 | MainLoss:0.4586 | Alpha:0.0294 | SPLoss:2.8726 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2250 | MainLoss:0.2250 | SPLoss:2.8722 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5349 | MainLoss:0.5349 | SPLoss:2.8722 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [435 | 1000] LR: 0.003002\n",
      "Train | 16/16 | Loss:0.5396 | MainLoss:0.4586 | Alpha:0.0282 | SPLoss:2.8701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2250 | MainLoss:0.2250 | SPLoss:2.8701 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5342 | MainLoss:0.5342 | SPLoss:2.8700 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [436 | 1000] LR: 0.002996\n",
      "Train | 16/16 | Loss:0.5352 | MainLoss:0.4503 | Alpha:0.0296 | SPLoss:2.8705 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2236 | MainLoss:0.2236 | SPLoss:2.8679 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5370 | MainLoss:0.5370 | SPLoss:2.8679 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [437 | 1000] LR: 0.002991\n",
      "Train | 16/16 | Loss:0.5464 | MainLoss:0.4656 | Alpha:0.0282 | SPLoss:2.8689 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2222 | MainLoss:0.2222 | SPLoss:2.8708 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5413 | MainLoss:0.5413 | SPLoss:2.8708 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [438 | 1000] LR: 0.002985\n",
      "Train | 16/16 | Loss:0.5349 | MainLoss:0.4540 | Alpha:0.0282 | SPLoss:2.8698 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:2.8686 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5387 | MainLoss:0.5387 | SPLoss:2.8686 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [439 | 1000] LR: 0.002980\n",
      "Train | 16/16 | Loss:0.5351 | MainLoss:0.4550 | Alpha:0.0280 | SPLoss:2.8670 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2209 | MainLoss:0.2209 | SPLoss:2.8692 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5448 | MainLoss:0.5448 | SPLoss:2.8692 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [440 | 1000] LR: 0.002975\n",
      "Train | 16/16 | Loss:0.5516 | MainLoss:0.4691 | Alpha:0.0287 | SPLoss:2.8713 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2207 | MainLoss:0.2207 | SPLoss:2.8728 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5484 | MainLoss:0.5484 | SPLoss:2.8728 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [441 | 1000] LR: 0.002969\n",
      "Train | 16/16 | Loss:0.5314 | MainLoss:0.4503 | Alpha:0.0283 | SPLoss:2.8706 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2241 | MainLoss:0.2241 | SPLoss:2.8660 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5374 | MainLoss:0.5374 | SPLoss:2.8660 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [442 | 1000] LR: 0.002964\n",
      "Train | 16/16 | Loss:0.5447 | MainLoss:0.4624 | Alpha:0.0287 | SPLoss:2.8682 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2228 | MainLoss:0.2228 | SPLoss:2.8676 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5412 | MainLoss:0.5412 | SPLoss:2.8676 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [443 | 1000] LR: 0.002958\n",
      "Train | 16/16 | Loss:0.5390 | MainLoss:0.4591 | Alpha:0.0279 | SPLoss:2.8693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2235 | MainLoss:0.2235 | SPLoss:2.8679 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5393 | MainLoss:0.5393 | SPLoss:2.8679 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [444 | 1000] LR: 0.002952\n",
      "Train | 16/16 | Loss:0.5311 | MainLoss:0.4486 | Alpha:0.0288 | SPLoss:2.8645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:2.8650 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5357 | MainLoss:0.5357 | SPLoss:2.8650 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [445 | 1000] LR: 0.002947\n",
      "Train | 16/16 | Loss:0.5443 | MainLoss:0.4604 | Alpha:0.0293 | SPLoss:2.8647 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2265 | MainLoss:0.2265 | SPLoss:2.8639 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5313 | MainLoss:0.5313 | SPLoss:2.8639 | CLSLoss:0.0000 | AUROC:0.9893\n",
      "\n",
      "Epoch: [446 | 1000] LR: 0.002941\n",
      "Train | 16/16 | Loss:0.5427 | MainLoss:0.4579 | Alpha:0.0296 | SPLoss:2.8648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:2.8636 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5383 | MainLoss:0.5383 | SPLoss:2.8636 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [447 | 1000] LR: 0.002936\n",
      "Train | 16/16 | Loss:0.5386 | MainLoss:0.4559 | Alpha:0.0289 | SPLoss:2.8639 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:2.8634 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5395 | MainLoss:0.5395 | SPLoss:2.8634 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [448 | 1000] LR: 0.002930\n",
      "Train | 16/16 | Loss:0.5285 | MainLoss:0.4462 | Alpha:0.0287 | SPLoss:2.8637 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2199 | MainLoss:0.2199 | SPLoss:2.8627 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5466 | MainLoss:0.5466 | SPLoss:2.8627 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [449 | 1000] LR: 0.002925\n",
      "Train | 16/16 | Loss:0.5351 | MainLoss:0.4521 | Alpha:0.0290 | SPLoss:2.8623 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:2.8617 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5413 | MainLoss:0.5413 | SPLoss:2.8617 | CLSLoss:0.0000 | AUROC:0.9891\n",
      "\n",
      "Epoch: [450 | 1000] LR: 0.002919\n",
      "Train | 16/16 | Loss:0.5352 | MainLoss:0.4520 | Alpha:0.0291 | SPLoss:2.8598 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:2.8585 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5355 | MainLoss:0.5355 | SPLoss:2.8585 | CLSLoss:0.0000 | AUROC:0.9892\n",
      "\n",
      "Epoch: [451 | 1000] LR: 0.002914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5348 | MainLoss:0.4516 | Alpha:0.0292 | SPLoss:2.8562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2225 | MainLoss:0.2225 | SPLoss:2.8564 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5386 | MainLoss:0.5386 | SPLoss:2.8564 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [452 | 1000] LR: 0.002908\n",
      "Train | 16/16 | Loss:0.5388 | MainLoss:0.4560 | Alpha:0.0290 | SPLoss:2.8574 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.8565 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5391 | MainLoss:0.5391 | SPLoss:2.8565 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [453 | 1000] LR: 0.002902\n",
      "Train | 16/16 | Loss:0.5320 | MainLoss:0.4492 | Alpha:0.0290 | SPLoss:2.8548 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2217 | MainLoss:0.2217 | SPLoss:2.8572 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5404 | MainLoss:0.5404 | SPLoss:2.8572 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [454 | 1000] LR: 0.002897\n",
      "Train | 16/16 | Loss:0.5421 | MainLoss:0.4601 | Alpha:0.0287 | SPLoss:2.8585 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:2.8570 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5378 | MainLoss:0.5378 | SPLoss:2.8570 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [455 | 1000] LR: 0.002891\n",
      "Train | 16/16 | Loss:0.5402 | MainLoss:0.4569 | Alpha:0.0291 | SPLoss:2.8575 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:2.8559 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5393 | MainLoss:0.5393 | SPLoss:2.8559 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [456 | 1000] LR: 0.002886\n",
      "Train | 16/16 | Loss:0.5422 | MainLoss:0.4590 | Alpha:0.0292 | SPLoss:2.8542 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2225 | MainLoss:0.2225 | SPLoss:2.8538 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5397 | MainLoss:0.5397 | SPLoss:2.8538 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [457 | 1000] LR: 0.002880\n",
      "Train | 16/16 | Loss:0.5384 | MainLoss:0.4544 | Alpha:0.0295 | SPLoss:2.8511 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:2.8507 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5349 | MainLoss:0.5349 | SPLoss:2.8507 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [458 | 1000] LR: 0.002874\n",
      "Train | 16/16 | Loss:0.5421 | MainLoss:0.4566 | Alpha:0.0300 | SPLoss:2.8508 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2233 | MainLoss:0.2233 | SPLoss:2.8503 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5365 | MainLoss:0.5365 | SPLoss:2.8503 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [459 | 1000] LR: 0.002869\n",
      "Train | 16/16 | Loss:0.5358 | MainLoss:0.4560 | Alpha:0.0280 | SPLoss:2.8504 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2208 | MainLoss:0.2208 | SPLoss:2.8503 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5434 | MainLoss:0.5434 | SPLoss:2.8503 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [460 | 1000] LR: 0.002863\n",
      "Train | 16/16 | Loss:0.5416 | MainLoss:0.4570 | Alpha:0.0297 | SPLoss:2.8502 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:2.8503 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5414 | MainLoss:0.5414 | SPLoss:2.8503 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [461 | 1000] LR: 0.002857\n",
      "Train | 16/16 | Loss:0.5336 | MainLoss:0.4513 | Alpha:0.0289 | SPLoss:2.8489 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:2.8452 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5328 | MainLoss:0.5328 | SPLoss:2.8452 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [462 | 1000] LR: 0.002852\n",
      "Train | 16/16 | Loss:0.5393 | MainLoss:0.4547 | Alpha:0.0297 | SPLoss:2.8454 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2270 | MainLoss:0.2270 | SPLoss:2.8448 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5272 | MainLoss:0.5272 | SPLoss:2.8448 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [463 | 1000] LR: 0.002846\n",
      "Train | 16/16 | Loss:0.5410 | MainLoss:0.4613 | Alpha:0.0280 | SPLoss:2.8459 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2239 | MainLoss:0.2239 | SPLoss:2.8473 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5351 | MainLoss:0.5351 | SPLoss:2.8473 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [464 | 1000] LR: 0.002840\n",
      "Train | 16/16 | Loss:0.5356 | MainLoss:0.4535 | Alpha:0.0288 | SPLoss:2.8478 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2218 | MainLoss:0.2218 | SPLoss:2.8480 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5395 | MainLoss:0.5395 | SPLoss:2.8480 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [465 | 1000] LR: 0.002834\n",
      "Train | 16/16 | Loss:0.5377 | MainLoss:0.4552 | Alpha:0.0290 | SPLoss:2.8480 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2224 | MainLoss:0.2224 | SPLoss:2.8462 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5375 | MainLoss:0.5375 | SPLoss:2.8462 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [466 | 1000] LR: 0.002829\n",
      "Train | 16/16 | Loss:0.5411 | MainLoss:0.4605 | Alpha:0.0283 | SPLoss:2.8462 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2229 | MainLoss:0.2229 | SPLoss:2.8471 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5380 | MainLoss:0.5380 | SPLoss:2.8471 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [467 | 1000] LR: 0.002823\n",
      "Train | 16/16 | Loss:0.5357 | MainLoss:0.4548 | Alpha:0.0284 | SPLoss:2.8468 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2239 | MainLoss:0.2239 | SPLoss:2.8461 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5341 | MainLoss:0.5341 | SPLoss:2.8461 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [468 | 1000] LR: 0.002817\n",
      "Train | 16/16 | Loss:0.5388 | MainLoss:0.4568 | Alpha:0.0288 | SPLoss:2.8451 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2237 | MainLoss:0.2237 | SPLoss:2.8468 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5358 | MainLoss:0.5358 | SPLoss:2.8468 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [469 | 1000] LR: 0.002812\n",
      "Train | 16/16 | Loss:0.5321 | MainLoss:0.4507 | Alpha:0.0286 | SPLoss:2.8459 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:2.8448 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5369 | MainLoss:0.5369 | SPLoss:2.8448 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [470 | 1000] LR: 0.002806\n",
      "Train | 16/16 | Loss:0.5333 | MainLoss:0.4527 | Alpha:0.0283 | SPLoss:2.8452 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2213 | MainLoss:0.2213 | SPLoss:2.8441 | CLSLoss:0.0000 | AUROC:0.9957\n",
      "Test | 30/16 | Loss:0.5417 | MainLoss:0.5417 | SPLoss:2.8441 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [471 | 1000] LR: 0.002800\n",
      "Train | 16/16 | Loss:0.5451 | MainLoss:0.4619 | Alpha:0.0292 | SPLoss:2.8429 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.8439 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5400 | MainLoss:0.5400 | SPLoss:2.8439 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [472 | 1000] LR: 0.002794\n",
      "Train | 16/16 | Loss:0.5338 | MainLoss:0.4536 | Alpha:0.0282 | SPLoss:2.8429 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8429 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5383 | MainLoss:0.5383 | SPLoss:2.8429 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [473 | 1000] LR: 0.002789\n",
      "Train | 16/16 | Loss:0.5387 | MainLoss:0.4580 | Alpha:0.0284 | SPLoss:2.8412 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8399 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5383 | MainLoss:0.5383 | SPLoss:2.8399 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [474 | 1000] LR: 0.002783\n",
      "Train | 16/16 | Loss:0.5413 | MainLoss:0.4581 | Alpha:0.0293 | SPLoss:2.8395 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2244 | MainLoss:0.2244 | SPLoss:2.8395 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5343 | MainLoss:0.5343 | SPLoss:2.8395 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [475 | 1000] LR: 0.002777\n",
      "Train | 16/16 | Loss:0.5420 | MainLoss:0.4622 | Alpha:0.0281 | SPLoss:2.8393 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2212 | MainLoss:0.2212 | SPLoss:2.8415 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5428 | MainLoss:0.5428 | SPLoss:2.8415 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [476 | 1000] LR: 0.002771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5361 | MainLoss:0.4521 | Alpha:0.0296 | SPLoss:2.8414 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.8408 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5398 | MainLoss:0.5398 | SPLoss:2.8408 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [477 | 1000] LR: 0.002765\n",
      "Train | 16/16 | Loss:0.5314 | MainLoss:0.4485 | Alpha:0.0292 | SPLoss:2.8409 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.8387 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5387 | MainLoss:0.5387 | SPLoss:2.8387 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [478 | 1000] LR: 0.002760\n",
      "Train | 16/16 | Loss:0.5350 | MainLoss:0.4533 | Alpha:0.0288 | SPLoss:2.8369 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2228 | MainLoss:0.2228 | SPLoss:2.8360 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5371 | MainLoss:0.5371 | SPLoss:2.8360 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [479 | 1000] LR: 0.002754\n",
      "Train | 16/16 | Loss:0.5332 | MainLoss:0.4515 | Alpha:0.0288 | SPLoss:2.8335 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2247 | MainLoss:0.2247 | SPLoss:2.8323 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5318 | MainLoss:0.5318 | SPLoss:2.8323 | CLSLoss:0.0000 | AUROC:0.9894\n",
      "\n",
      "Epoch: [480 | 1000] LR: 0.002748\n",
      "Train | 16/16 | Loss:0.5354 | MainLoss:0.4557 | Alpha:0.0281 | SPLoss:2.8332 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:2.8342 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5392 | MainLoss:0.5392 | SPLoss:2.8342 | CLSLoss:0.0000 | AUROC:0.9897\n",
      "\n",
      "Epoch: [481 | 1000] LR: 0.002742\n",
      "Train | 16/16 | Loss:0.5393 | MainLoss:0.4583 | Alpha:0.0285 | SPLoss:2.8347 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2218 | MainLoss:0.2218 | SPLoss:2.8337 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5400 | MainLoss:0.5400 | SPLoss:2.8337 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [482 | 1000] LR: 0.002736\n",
      "Train | 16/16 | Loss:0.5355 | MainLoss:0.4552 | Alpha:0.0283 | SPLoss:2.8351 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2213 | MainLoss:0.2213 | SPLoss:2.8344 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5429 | MainLoss:0.5429 | SPLoss:2.8344 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [483 | 1000] LR: 0.002730\n",
      "Train | 16/16 | Loss:0.5405 | MainLoss:0.4628 | Alpha:0.0274 | SPLoss:2.8333 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2221 | MainLoss:0.2221 | SPLoss:2.8341 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5402 | MainLoss:0.5402 | SPLoss:2.8341 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [484 | 1000] LR: 0.002725\n",
      "Train | 16/16 | Loss:0.5408 | MainLoss:0.4586 | Alpha:0.0290 | SPLoss:2.8353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8378 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5385 | MainLoss:0.5385 | SPLoss:2.8378 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [485 | 1000] LR: 0.002719\n",
      "Train | 16/16 | Loss:0.5379 | MainLoss:0.4560 | Alpha:0.0288 | SPLoss:2.8370 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:2.8364 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5327 | MainLoss:0.5327 | SPLoss:2.8364 | CLSLoss:0.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [486 | 1000] LR: 0.002713\n",
      "Train | 16/16 | Loss:0.5338 | MainLoss:0.4526 | Alpha:0.0286 | SPLoss:2.8357 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2223 | MainLoss:0.2223 | SPLoss:2.8371 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5396 | MainLoss:0.5396 | SPLoss:2.8371 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [487 | 1000] LR: 0.002707\n",
      "Train | 16/16 | Loss:0.5331 | MainLoss:0.4512 | Alpha:0.0289 | SPLoss:2.8350 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:2.8324 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5347 | MainLoss:0.5347 | SPLoss:2.8324 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [488 | 1000] LR: 0.002701\n",
      "Train | 16/16 | Loss:0.5352 | MainLoss:0.4540 | Alpha:0.0287 | SPLoss:2.8322 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2255 | MainLoss:0.2255 | SPLoss:2.8307 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5308 | MainLoss:0.5308 | SPLoss:2.8307 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [489 | 1000] LR: 0.002695\n",
      "Train | 16/16 | Loss:0.5340 | MainLoss:0.4569 | Alpha:0.0272 | SPLoss:2.8323 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2243 | MainLoss:0.2243 | SPLoss:2.8325 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5335 | MainLoss:0.5335 | SPLoss:2.8325 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [490 | 1000] LR: 0.002689\n",
      "Train | 16/16 | Loss:0.5273 | MainLoss:0.4456 | Alpha:0.0288 | SPLoss:2.8324 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2243 | MainLoss:0.2243 | SPLoss:2.8315 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5332 | MainLoss:0.5332 | SPLoss:2.8315 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [491 | 1000] LR: 0.002683\n",
      "Train | 16/16 | Loss:0.5389 | MainLoss:0.4572 | Alpha:0.0288 | SPLoss:2.8322 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2242 | MainLoss:0.2242 | SPLoss:2.8319 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5338 | MainLoss:0.5338 | SPLoss:2.8319 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [492 | 1000] LR: 0.002677\n",
      "Train | 16/16 | Loss:0.5344 | MainLoss:0.4513 | Alpha:0.0293 | SPLoss:2.8322 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8324 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5377 | MainLoss:0.5377 | SPLoss:2.8324 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [493 | 1000] LR: 0.002672\n",
      "Train | 16/16 | Loss:0.5367 | MainLoss:0.4578 | Alpha:0.0279 | SPLoss:2.8335 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2226 | MainLoss:0.2226 | SPLoss:2.8348 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5391 | MainLoss:0.5391 | SPLoss:2.8348 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [494 | 1000] LR: 0.002666\n",
      "Train | 16/16 | Loss:0.5348 | MainLoss:0.4538 | Alpha:0.0286 | SPLoss:2.8339 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2235 | MainLoss:0.2235 | SPLoss:2.8350 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5365 | MainLoss:0.5365 | SPLoss:2.8350 | CLSLoss:0.0000 | AUROC:0.9895\n",
      "\n",
      "Epoch: [495 | 1000] LR: 0.002660\n",
      "Train | 16/16 | Loss:0.5454 | MainLoss:0.4612 | Alpha:0.0297 | SPLoss:2.8349 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2242 | MainLoss:0.2242 | SPLoss:2.8331 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5355 | MainLoss:0.5355 | SPLoss:2.8331 | CLSLoss:0.0000 | AUROC:0.9896\n",
      "\n",
      "Epoch: [496 | 1000] LR: 0.002654\n",
      "Train | 16/16 | Loss:0.5432 | MainLoss:0.4619 | Alpha:0.0287 | SPLoss:2.8331 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2276 | MainLoss:0.2276 | SPLoss:2.8297 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5268 | MainLoss:0.5268 | SPLoss:2.8297 | CLSLoss:0.0000 | AUROC:0.9899\n",
      "\n",
      "Epoch: [497 | 1000] LR: 0.002648\n",
      "Train | 16/16 | Loss:0.5382 | MainLoss:0.4581 | Alpha:0.0283 | SPLoss:2.8303 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2280 | MainLoss:0.2280 | SPLoss:2.8290 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5255 | MainLoss:0.5255 | SPLoss:2.8290 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [498 | 1000] LR: 0.002642\n",
      "Train | 16/16 | Loss:0.5422 | MainLoss:0.4616 | Alpha:0.0285 | SPLoss:2.8274 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2292 | MainLoss:0.2292 | SPLoss:2.8289 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5231 | MainLoss:0.5231 | SPLoss:2.8289 | CLSLoss:0.0000 | AUROC:0.9898\n",
      "\n",
      "Epoch: [499 | 1000] LR: 0.002636\n",
      "Train | 16/16 | Loss:0.5454 | MainLoss:0.4621 | Alpha:0.0294 | SPLoss:2.8294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2253 | MainLoss:0.2253 | SPLoss:2.8297 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5333 | MainLoss:0.5333 | SPLoss:2.8297 | CLSLoss:0.0000 | AUROC:0.9901\n",
      "\n",
      "Epoch: [500 | 1000] LR: 0.002630\n",
      "Train | 16/16 | Loss:0.5299 | MainLoss:0.4490 | Alpha:0.0286 | SPLoss:2.8294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2246 | MainLoss:0.2246 | SPLoss:2.8296 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5343 | MainLoss:0.5343 | SPLoss:2.8296 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [501 | 1000] LR: 0.002624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5305 | MainLoss:0.4509 | Alpha:0.0282 | SPLoss:2.8277 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2261 | MainLoss:0.2261 | SPLoss:2.8262 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5290 | MainLoss:0.5290 | SPLoss:2.8262 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [502 | 1000] LR: 0.002618\n",
      "Train | 16/16 | Loss:0.5341 | MainLoss:0.4545 | Alpha:0.0282 | SPLoss:2.8266 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2251 | MainLoss:0.2251 | SPLoss:2.8269 | CLSLoss:0.0000 | AUROC:0.9954\n",
      "Test | 30/16 | Loss:0.5326 | MainLoss:0.5326 | SPLoss:2.8269 | CLSLoss:0.0000 | AUROC:0.9905\n",
      "\n",
      "Epoch: [503 | 1000] LR: 0.002612\n",
      "Train | 16/16 | Loss:0.5361 | MainLoss:0.4562 | Alpha:0.0283 | SPLoss:2.8274 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:2.8292 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5378 | MainLoss:0.5378 | SPLoss:2.8292 | CLSLoss:0.0000 | AUROC:0.9904\n",
      "\n",
      "Epoch: [504 | 1000] LR: 0.002606\n",
      "Train | 16/16 | Loss:0.5406 | MainLoss:0.4590 | Alpha:0.0288 | SPLoss:2.8311 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2199 | MainLoss:0.2199 | SPLoss:2.8329 | CLSLoss:0.0000 | AUROC:0.9956\n",
      "Test | 30/16 | Loss:0.5471 | MainLoss:0.5471 | SPLoss:2.8329 | CLSLoss:0.0000 | AUROC:0.9906\n",
      "\n",
      "Epoch: [505 | 1000] LR: 0.002600\n",
      "Train | 16/16 | Loss:0.5407 | MainLoss:0.4586 | Alpha:0.0290 | SPLoss:2.8323 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2221 | MainLoss:0.2221 | SPLoss:2.8309 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5412 | MainLoss:0.5412 | SPLoss:2.8309 | CLSLoss:0.0000 | AUROC:0.9902\n",
      "\n",
      "Epoch: [506 | 1000] LR: 0.002594\n",
      "Train | 16/16 | Loss:0.5415 | MainLoss:0.4631 | Alpha:0.0277 | SPLoss:2.8313 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 128/16 | Loss:0.2228 | MainLoss:0.2228 | SPLoss:2.8301 | CLSLoss:0.0000 | AUROC:0.9955\n",
      "Test | 30/16 | Loss:0.5387 | MainLoss:0.5387 | SPLoss:2.8301 | CLSLoss:0.0000 | AUROC:0.9903\n",
      "\n",
      "Epoch: [507 | 1000] LR: 0.002588\n",
      "Train | 16/16 | Loss:0.5375 | MainLoss:0.4551 | Alpha:0.0291 | SPLoss:2.8298 | CLSLoss:0.0000 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  \n",
    "                   source_loss, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch+1)%900 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
