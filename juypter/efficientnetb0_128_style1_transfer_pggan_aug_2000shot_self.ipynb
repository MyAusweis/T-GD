{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style1/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.01\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0/to_pggan/2000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style1/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.3, 'drop_connect_rate':0.3})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.001000\n",
      "Train | 20/20 | Loss:1.0901 | MainLoss:1.0901 | Alpha:0.0237 | SPLoss:44.5269 | CLSLoss:4.7168 | top1:50.3250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6942 | MainLoss:0.6942 | SPLoss:95.9125 | CLSLoss:4.4070 | top1:50.8474 | AUROC:0.5211\n",
      "Test | 39/20 | Loss:0.6148 | MainLoss:0.6148 | SPLoss:95.9126 | CLSLoss:4.4069 | top1:85.7564 | AUROC:0.9546\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.001300\n",
      "Train | 20/20 | Loss:0.7014 | MainLoss:0.7014 | Alpha:0.0245 | SPLoss:127.5034 | CLSLoss:4.3124 | top1:50.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.7070 | MainLoss:0.7070 | SPLoss:148.9389 | CLSLoss:4.2572 | top1:50.0031 | AUROC:0.5793\n",
      "Test | 39/20 | Loss:0.6439 | MainLoss:0.6439 | SPLoss:148.9388 | CLSLoss:4.2572 | top1:50.0000 | AUROC:0.9833\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.001600\n",
      "Train | 20/20 | Loss:0.6939 | MainLoss:0.6939 | Alpha:0.0247 | SPLoss:157.4898 | CLSLoss:4.2092 | top1:50.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6893 | MainLoss:0.6893 | SPLoss:164.1765 | CLSLoss:4.1750 | top1:50.6947 | AUROC:0.6135\n",
      "Test | 39/20 | Loss:0.6310 | MainLoss:0.6310 | SPLoss:164.1763 | CLSLoss:4.1750 | top1:96.0256 | AUROC:0.9918\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.001900\n",
      "Train | 20/20 | Loss:0.6958 | MainLoss:0.6958 | Alpha:0.0238 | SPLoss:168.0475 | CLSLoss:4.1683 | top1:50.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6816 | MainLoss:0.6816 | SPLoss:171.6551 | CLSLoss:4.1537 | top1:60.6480 | AUROC:0.6451\n",
      "Test | 39/20 | Loss:0.6156 | MainLoss:0.6156 | SPLoss:171.6552 | CLSLoss:4.1537 | top1:72.9615 | AUROC:0.9932\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.002200\n",
      "Train | 20/20 | Loss:0.6832 | MainLoss:0.6832 | Alpha:0.0235 | SPLoss:177.3085 | CLSLoss:4.1530 | top1:55.4000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.6204 | MainLoss:0.6204 | SPLoss:187.4559 | CLSLoss:4.1654 | top1:68.6729 | AUROC:0.7658\n",
      "Test | 39/20 | Loss:0.4601 | MainLoss:0.4601 | SPLoss:187.4562 | CLSLoss:4.1654 | top1:79.0256 | AUROC:0.9906\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.002500\n",
      "Train | 20/20 | Loss:0.6228 | MainLoss:0.6228 | Alpha:0.0243 | SPLoss:205.8354 | CLSLoss:4.1716 | top1:65.7250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.3946 | MainLoss:0.3946 | SPLoss:231.7577 | CLSLoss:4.2006 | top1:86.1059 | AUROC:0.9352\n",
      "Test | 39/20 | Loss:0.5513 | MainLoss:0.5513 | SPLoss:231.7581 | CLSLoss:4.2006 | top1:67.7051 | AUROC:0.9212\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.002800\n",
      "Train | 20/20 | Loss:0.4880 | MainLoss:0.4880 | Alpha:0.0242 | SPLoss:260.7444 | CLSLoss:4.2703 | top1:76.5000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2762 | MainLoss:0.2762 | SPLoss:291.6102 | CLSLoss:4.3693 | top1:93.8287 | AUROC:0.9945\n",
      "Test | 39/20 | Loss:0.6156 | MainLoss:0.6156 | SPLoss:291.6105 | CLSLoss:4.3693 | top1:62.7692 | AUROC:0.8230\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.003100\n",
      "Train | 20/20 | Loss:0.4515 | MainLoss:0.4515 | Alpha:0.0245 | SPLoss:323.5006 | CLSLoss:4.4558 | top1:79.5250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2624 | MainLoss:0.2624 | SPLoss:359.8646 | CLSLoss:4.5196 | top1:96.7819 | AUROC:0.9983\n",
      "Test | 39/20 | Loss:0.6828 | MainLoss:0.6828 | SPLoss:359.8639 | CLSLoss:4.5196 | top1:55.7821 | AUROC:0.7058\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.003400\n",
      "Train | 20/20 | Loss:0.4298 | MainLoss:0.4298 | Alpha:0.0242 | SPLoss:393.7428 | CLSLoss:4.6661 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1889 | MainLoss:0.1889 | SPLoss:422.4763 | CLSLoss:4.7734 | top1:98.8037 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.7803 | MainLoss:0.7803 | SPLoss:422.4766 | CLSLoss:4.7734 | top1:51.3205 | AUROC:0.6087\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.003700\n",
      "Train | 20/20 | Loss:0.4242 | MainLoss:0.4242 | Alpha:0.0246 | SPLoss:443.9802 | CLSLoss:4.8699 | top1:80.4250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2079 | MainLoss:0.2079 | SPLoss:465.9054 | CLSLoss:4.9431 | top1:98.0530 | AUROC:0.9994\n",
      "Test | 39/20 | Loss:0.7460 | MainLoss:0.7460 | SPLoss:465.9062 | CLSLoss:4.9431 | top1:53.1282 | AUROC:0.6258\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.4122 | MainLoss:0.4122 | Alpha:0.0241 | SPLoss:478.5130 | CLSLoss:5.0300 | top1:81.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1754 | MainLoss:0.1754 | SPLoss:493.9189 | CLSLoss:5.0347 | top1:98.9190 | AUROC:0.9993\n",
      "Test | 39/20 | Loss:0.7669 | MainLoss:0.7669 | SPLoss:493.9185 | CLSLoss:5.0347 | top1:51.2308 | AUROC:0.7084\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.4112 | MainLoss:0.4112 | Alpha:0.0238 | SPLoss:511.9839 | CLSLoss:5.0760 | top1:81.4000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1712 | MainLoss:0.1712 | SPLoss:538.1997 | CLSLoss:5.1324 | top1:98.7383 | AUROC:0.9995\n",
      "Test | 39/20 | Loss:0.7661 | MainLoss:0.7661 | SPLoss:538.1988 | CLSLoss:5.1324 | top1:52.3718 | AUROC:0.6938\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.4007 | MainLoss:0.4007 | Alpha:0.0246 | SPLoss:560.8188 | CLSLoss:5.1967 | top1:81.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1929 | MainLoss:0.1929 | SPLoss:584.4809 | CLSLoss:5.1525 | top1:98.5483 | AUROC:0.9996\n",
      "Test | 39/20 | Loss:0.7363 | MainLoss:0.7363 | SPLoss:584.4814 | CLSLoss:5.1525 | top1:52.3846 | AUROC:0.7135\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.4043 | MainLoss:0.4043 | Alpha:0.0239 | SPLoss:598.3545 | CLSLoss:5.2061 | top1:80.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1813 | MainLoss:0.1813 | SPLoss:610.7601 | CLSLoss:5.2422 | top1:98.4829 | AUROC:0.9996\n",
      "Test | 39/20 | Loss:0.7342 | MainLoss:0.7342 | SPLoss:610.7603 | CLSLoss:5.2422 | top1:53.2179 | AUROC:0.7014\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.3953 | MainLoss:0.3953 | Alpha:0.0239 | SPLoss:619.0497 | CLSLoss:5.3377 | top1:82.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1734 | MainLoss:0.1734 | SPLoss:629.7767 | CLSLoss:5.3171 | top1:97.9751 | AUROC:0.9996\n",
      "Test | 39/20 | Loss:0.7139 | MainLoss:0.7139 | SPLoss:629.7764 | CLSLoss:5.3171 | top1:55.3590 | AUROC:0.7365\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.3932 | MainLoss:0.3932 | Alpha:0.0237 | SPLoss:644.2842 | CLSLoss:5.3361 | top1:82.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2708 | MainLoss:0.2708 | SPLoss:661.4103 | CLSLoss:5.2639 | top1:91.8879 | AUROC:0.9995\n",
      "Test | 39/20 | Loss:0.6446 | MainLoss:0.6446 | SPLoss:661.4109 | CLSLoss:5.2638 | top1:63.1410 | AUROC:0.7160\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.3940 | MainLoss:0.3940 | Alpha:0.0253 | SPLoss:683.2648 | CLSLoss:5.1555 | top1:81.5250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2040 | MainLoss:0.2040 | SPLoss:708.4708 | CLSLoss:5.0762 | top1:94.9626 | AUROC:0.9986\n",
      "Test | 39/20 | Loss:0.6885 | MainLoss:0.6885 | SPLoss:708.4717 | CLSLoss:5.0762 | top1:59.7692 | AUROC:0.7177\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.3909 | MainLoss:0.3909 | Alpha:0.0245 | SPLoss:726.2469 | CLSLoss:5.0540 | top1:81.4250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1419 | MainLoss:0.1419 | SPLoss:747.6467 | CLSLoss:5.1134 | top1:97.6417 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.7718 | MainLoss:0.7718 | SPLoss:747.6459 | CLSLoss:5.1134 | top1:55.9872 | AUROC:0.7306\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.004000\n",
      "Train | 20/20 | Loss:0.3879 | MainLoss:0.3879 | Alpha:0.0232 | SPLoss:766.2445 | CLSLoss:5.1225 | top1:81.9750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2258 | MainLoss:0.2258 | SPLoss:789.2274 | CLSLoss:4.9815 | top1:95.2710 | AUROC:0.9981\n",
      "Test | 39/20 | Loss:0.6798 | MainLoss:0.6798 | SPLoss:789.2268 | CLSLoss:4.9815 | top1:57.9487 | AUROC:0.6844\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.003999\n",
      "Train | 20/20 | Loss:0.3834 | MainLoss:0.3834 | Alpha:0.0250 | SPLoss:810.6954 | CLSLoss:4.9971 | top1:82.5000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1383 | MainLoss:0.1383 | SPLoss:835.7370 | CLSLoss:4.9210 | top1:98.7134 | AUROC:0.9994\n",
      "Test | 39/20 | Loss:0.8155 | MainLoss:0.8155 | SPLoss:835.7352 | CLSLoss:4.9210 | top1:52.4487 | AUROC:0.6547\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.003999\n",
      "Train | 20/20 | Loss:0.3751 | MainLoss:0.3751 | Alpha:0.0237 | SPLoss:852.5975 | CLSLoss:4.8945 | top1:82.8250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1372 | MainLoss:0.1372 | SPLoss:871.4208 | CLSLoss:4.9562 | top1:98.2773 | AUROC:0.9992\n",
      "Test | 39/20 | Loss:0.7929 | MainLoss:0.7929 | SPLoss:871.4225 | CLSLoss:4.9562 | top1:53.8462 | AUROC:0.6852\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.003999\n",
      "Train | 20/20 | Loss:0.3655 | MainLoss:0.3655 | Alpha:0.0235 | SPLoss:890.1762 | CLSLoss:4.9976 | top1:83.2500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1340 | MainLoss:0.1340 | SPLoss:907.2724 | CLSLoss:4.9789 | top1:98.4860 | AUROC:0.9994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 39/20 | Loss:0.7814 | MainLoss:0.7814 | SPLoss:907.2732 | CLSLoss:4.9789 | top1:54.5256 | AUROC:0.6959\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.003999\n",
      "Train | 20/20 | Loss:0.3530 | MainLoss:0.3530 | Alpha:0.0238 | SPLoss:921.1432 | CLSLoss:5.0677 | top1:83.9500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1081 | MainLoss:0.1081 | SPLoss:937.5579 | CLSLoss:5.1204 | top1:97.9377 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.8538 | MainLoss:0.8538 | SPLoss:937.5574 | CLSLoss:5.1204 | top1:55.3205 | AUROC:0.6867\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.003999\n",
      "Train | 20/20 | Loss:0.3647 | MainLoss:0.3647 | Alpha:0.0235 | SPLoss:958.5205 | CLSLoss:4.9970 | top1:83.6500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2742 | MainLoss:0.2742 | SPLoss:985.3179 | CLSLoss:4.7119 | top1:89.5078 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:0.6573 | MainLoss:0.6573 | SPLoss:985.3168 | CLSLoss:4.7119 | top1:63.2179 | AUROC:0.6878\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.003998\n",
      "Train | 20/20 | Loss:0.3439 | MainLoss:0.3439 | Alpha:0.0242 | SPLoss:1003.5248 | CLSLoss:4.7546 | top1:84.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1393 | MainLoss:0.1393 | SPLoss:1022.4606 | CLSLoss:4.6840 | top1:97.5701 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.8216 | MainLoss:0.8216 | SPLoss:1022.4624 | CLSLoss:4.6840 | top1:54.7436 | AUROC:0.6473\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.003998\n",
      "Train | 20/20 | Loss:0.3441 | MainLoss:0.3441 | Alpha:0.0237 | SPLoss:1037.4708 | CLSLoss:4.7679 | top1:84.2500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1525 | MainLoss:0.1525 | SPLoss:1052.9503 | CLSLoss:4.8615 | top1:96.0997 | AUROC:0.9994\n",
      "Test | 39/20 | Loss:0.7728 | MainLoss:0.7728 | SPLoss:1052.9506 | CLSLoss:4.8615 | top1:57.9359 | AUROC:0.6501\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.003998\n",
      "Train | 20/20 | Loss:0.3331 | MainLoss:0.3331 | Alpha:0.0244 | SPLoss:1071.4896 | CLSLoss:4.9501 | top1:84.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1651 | MainLoss:0.1651 | SPLoss:1091.8474 | CLSLoss:4.9327 | top1:95.5016 | AUROC:0.9992\n",
      "Test | 39/20 | Loss:0.7585 | MainLoss:0.7585 | SPLoss:1091.8456 | CLSLoss:4.9327 | top1:57.4103 | AUROC:0.6419\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.003997\n",
      "Train | 20/20 | Loss:0.3217 | MainLoss:0.3217 | Alpha:0.0246 | SPLoss:1111.2354 | CLSLoss:5.0711 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1764 | MainLoss:0.1764 | SPLoss:1130.1709 | CLSLoss:5.2092 | top1:93.6760 | AUROC:0.9985\n",
      "Test | 39/20 | Loss:0.8151 | MainLoss:0.8151 | SPLoss:1130.1707 | CLSLoss:5.2092 | top1:57.8077 | AUROC:0.6174\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.003997\n",
      "Train | 20/20 | Loss:0.3235 | MainLoss:0.3235 | Alpha:0.0236 | SPLoss:1143.7811 | CLSLoss:5.2614 | top1:85.5500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1258 | MainLoss:0.1258 | SPLoss:1160.8528 | CLSLoss:5.2117 | top1:97.2741 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.8344 | MainLoss:0.8344 | SPLoss:1160.8530 | CLSLoss:5.2117 | top1:55.0897 | AUROC:0.6222\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.003997\n",
      "Train | 20/20 | Loss:0.3280 | MainLoss:0.3280 | Alpha:0.0239 | SPLoss:1179.8446 | CLSLoss:5.1353 | top1:85.1250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1624 | MainLoss:0.1624 | SPLoss:1199.7622 | CLSLoss:5.0301 | top1:94.7570 | AUROC:0.9987\n",
      "Test | 39/20 | Loss:0.8166 | MainLoss:0.8166 | SPLoss:1199.7648 | CLSLoss:5.0301 | top1:56.9359 | AUROC:0.6014\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.003996\n",
      "Train | 20/20 | Loss:0.3136 | MainLoss:0.3136 | Alpha:0.0248 | SPLoss:1216.8264 | CLSLoss:5.0344 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2057 | MainLoss:0.2057 | SPLoss:1237.5596 | CLSLoss:5.0187 | top1:92.2368 | AUROC:0.9986\n",
      "Test | 39/20 | Loss:0.7920 | MainLoss:0.7920 | SPLoss:1237.5579 | CLSLoss:5.0187 | top1:56.7564 | AUROC:0.6052\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.003996\n",
      "Train | 20/20 | Loss:0.3030 | MainLoss:0.3030 | Alpha:0.0249 | SPLoss:1255.7701 | CLSLoss:5.0907 | top1:86.5500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1801 | MainLoss:0.1801 | SPLoss:1275.6206 | CLSLoss:5.0567 | top1:93.6480 | AUROC:0.9984\n",
      "Test | 39/20 | Loss:0.8253 | MainLoss:0.8253 | SPLoss:1275.6219 | CLSLoss:5.0567 | top1:56.3846 | AUROC:0.6081\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.003996\n",
      "Train | 20/20 | Loss:0.3164 | MainLoss:0.3164 | Alpha:0.0247 | SPLoss:1292.5770 | CLSLoss:5.1131 | top1:85.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1875 | MainLoss:0.1875 | SPLoss:1309.1021 | CLSLoss:4.9553 | top1:94.7196 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.7375 | MainLoss:0.7375 | SPLoss:1309.1050 | CLSLoss:4.9553 | top1:57.2564 | AUROC:0.6343\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.003995\n",
      "Train | 20/20 | Loss:0.3056 | MainLoss:0.3056 | Alpha:0.0233 | SPLoss:1331.6121 | CLSLoss:5.0812 | top1:86.7500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1364 | MainLoss:0.1364 | SPLoss:1355.8202 | CLSLoss:5.3535 | top1:95.1682 | AUROC:0.9983\n",
      "Test | 39/20 | Loss:0.9354 | MainLoss:0.9354 | SPLoss:1355.8201 | CLSLoss:5.3535 | top1:58.2179 | AUROC:0.6405\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.003995\n",
      "Train | 20/20 | Loss:0.3068 | MainLoss:0.3068 | Alpha:0.0243 | SPLoss:1377.9674 | CLSLoss:5.1882 | top1:86.4000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1341 | MainLoss:0.1341 | SPLoss:1400.2599 | CLSLoss:5.1640 | top1:96.1246 | AUROC:0.9993\n",
      "Test | 39/20 | Loss:0.8398 | MainLoss:0.8398 | SPLoss:1400.2595 | CLSLoss:5.1640 | top1:55.2949 | AUROC:0.5960\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.003994\n",
      "Train | 20/20 | Loss:0.2960 | MainLoss:0.2960 | Alpha:0.0247 | SPLoss:1418.0548 | CLSLoss:5.2528 | top1:86.9000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1042 | MainLoss:0.1042 | SPLoss:1436.0787 | CLSLoss:5.2898 | top1:97.0935 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:0.9394 | MainLoss:0.9394 | SPLoss:1436.0768 | CLSLoss:5.2898 | top1:55.3590 | AUROC:0.6113\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.003994\n",
      "Train | 20/20 | Loss:0.2831 | MainLoss:0.2831 | Alpha:0.0249 | SPLoss:1455.1459 | CLSLoss:5.2972 | top1:87.2250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2140 | MainLoss:0.2140 | SPLoss:1475.6399 | CLSLoss:5.3341 | top1:91.2960 | AUROC:0.9979\n",
      "Test | 39/20 | Loss:0.8571 | MainLoss:0.8571 | SPLoss:1475.6395 | CLSLoss:5.3341 | top1:56.7821 | AUROC:0.5941\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.003993\n",
      "Train | 20/20 | Loss:0.2936 | MainLoss:0.2936 | Alpha:0.0246 | SPLoss:1492.4370 | CLSLoss:5.3957 | top1:86.9750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2004 | MainLoss:0.2004 | SPLoss:1511.6658 | CLSLoss:5.3197 | top1:91.7913 | AUROC:0.9992\n",
      "Test | 39/20 | Loss:0.8043 | MainLoss:0.8043 | SPLoss:1511.6666 | CLSLoss:5.3197 | top1:57.0769 | AUROC:0.5969\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.003993\n",
      "Train | 20/20 | Loss:0.2842 | MainLoss:0.2842 | Alpha:0.0232 | SPLoss:1527.9987 | CLSLoss:5.4074 | top1:87.2750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1727 | MainLoss:0.1727 | SPLoss:1545.3586 | CLSLoss:5.4220 | top1:93.2928 | AUROC:0.9988\n",
      "Test | 39/20 | Loss:0.9122 | MainLoss:0.9122 | SPLoss:1545.3566 | CLSLoss:5.4220 | top1:56.0256 | AUROC:0.5820\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.003992\n",
      "Train | 20/20 | Loss:0.2895 | MainLoss:0.2895 | Alpha:0.0237 | SPLoss:1563.0854 | CLSLoss:5.3200 | top1:87.2500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1644 | MainLoss:0.1644 | SPLoss:1581.3600 | CLSLoss:5.3726 | top1:93.7850 | AUROC:0.9982\n",
      "Test | 39/20 | Loss:0.8899 | MainLoss:0.8899 | SPLoss:1581.3595 | CLSLoss:5.3726 | top1:55.8718 | AUROC:0.5888\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.003992\n",
      "Train | 20/20 | Loss:0.2667 | MainLoss:0.2667 | Alpha:0.0234 | SPLoss:1595.7678 | CLSLoss:5.4038 | top1:88.5000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1948 | MainLoss:0.1948 | SPLoss:1613.3997 | CLSLoss:5.3626 | top1:92.3614 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:0.9233 | MainLoss:0.9233 | SPLoss:1613.3984 | CLSLoss:5.3626 | top1:54.2051 | AUROC:0.5620\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.003991\n",
      "Train | 20/20 | Loss:0.2707 | MainLoss:0.2707 | Alpha:0.0251 | SPLoss:1633.4692 | CLSLoss:5.3911 | top1:88.2000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1268 | MainLoss:0.1268 | SPLoss:1655.4617 | CLSLoss:5.4348 | top1:95.6636 | AUROC:0.9985\n",
      "Test | 39/20 | Loss:0.9262 | MainLoss:0.9262 | SPLoss:1655.4646 | CLSLoss:5.4348 | top1:55.2179 | AUROC:0.5850\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.003991\n",
      "Train | 20/20 | Loss:0.2558 | MainLoss:0.2558 | Alpha:0.0247 | SPLoss:1671.2732 | CLSLoss:5.4997 | top1:88.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1402 | MainLoss:0.1402 | SPLoss:1687.7797 | CLSLoss:5.5507 | top1:94.8037 | AUROC:0.9986\n",
      "Test | 39/20 | Loss:0.9009 | MainLoss:0.9009 | SPLoss:1687.7784 | CLSLoss:5.5507 | top1:56.2564 | AUROC:0.5995\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.003990\n",
      "Train | 20/20 | Loss:0.2538 | MainLoss:0.2538 | Alpha:0.0239 | SPLoss:1703.8778 | CLSLoss:5.6346 | top1:89.0000 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 161/20 | Loss:0.1549 | MainLoss:0.1549 | SPLoss:1722.3398 | CLSLoss:5.6248 | top1:94.2150 | AUROC:0.9980\n",
      "Test | 39/20 | Loss:0.9595 | MainLoss:0.9595 | SPLoss:1722.3396 | CLSLoss:5.6248 | top1:54.9231 | AUROC:0.5691\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.003989\n",
      "Train | 20/20 | Loss:0.2582 | MainLoss:0.2582 | Alpha:0.0244 | SPLoss:1739.0282 | CLSLoss:5.5776 | top1:88.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2006 | MainLoss:0.2006 | SPLoss:1758.1060 | CLSLoss:5.5277 | top1:91.9813 | AUROC:0.9987\n",
      "Test | 39/20 | Loss:0.9216 | MainLoss:0.9216 | SPLoss:1758.1099 | CLSLoss:5.5277 | top1:55.7436 | AUROC:0.5820\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.003989\n",
      "Train | 20/20 | Loss:0.2504 | MainLoss:0.2504 | Alpha:0.0240 | SPLoss:1773.4556 | CLSLoss:5.5903 | top1:89.7000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2107 | MainLoss:0.2107 | SPLoss:1790.6199 | CLSLoss:5.5877 | top1:91.4922 | AUROC:0.9981\n",
      "Test | 39/20 | Loss:0.9186 | MainLoss:0.9186 | SPLoss:1790.6198 | CLSLoss:5.5877 | top1:55.5769 | AUROC:0.5831\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.003988\n",
      "Train | 20/20 | Loss:0.2394 | MainLoss:0.2394 | Alpha:0.0243 | SPLoss:1806.5386 | CLSLoss:5.6557 | top1:89.7500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2116 | MainLoss:0.2116 | SPLoss:1824.4213 | CLSLoss:5.6770 | top1:91.4455 | AUROC:0.9985\n",
      "Test | 39/20 | Loss:1.0182 | MainLoss:1.0182 | SPLoss:1824.4237 | CLSLoss:5.6770 | top1:53.1538 | AUROC:0.5485\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.003987\n",
      "Train | 20/20 | Loss:0.2437 | MainLoss:0.2437 | Alpha:0.0242 | SPLoss:1839.7684 | CLSLoss:5.7134 | top1:89.7250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1363 | MainLoss:0.1363 | SPLoss:1858.3645 | CLSLoss:5.6994 | top1:95.0312 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:0.9561 | MainLoss:0.9561 | SPLoss:1858.3661 | CLSLoss:5.6994 | top1:55.3846 | AUROC:0.5816\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.003987\n",
      "Train | 20/20 | Loss:0.2380 | MainLoss:0.2380 | Alpha:0.0247 | SPLoss:1878.9882 | CLSLoss:5.7369 | top1:90.3250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1016 | MainLoss:0.1016 | SPLoss:1900.3003 | CLSLoss:5.6415 | top1:96.6978 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:0.9721 | MainLoss:0.9721 | SPLoss:1900.3000 | CLSLoss:5.6415 | top1:54.0897 | AUROC:0.5840\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.003986\n",
      "Train | 20/20 | Loss:0.2274 | MainLoss:0.2274 | Alpha:0.0239 | SPLoss:1917.1243 | CLSLoss:5.7675 | top1:90.5250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1382 | MainLoss:0.1382 | SPLoss:1935.3455 | CLSLoss:5.7927 | top1:94.6417 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.0040 | MainLoss:1.0040 | SPLoss:1935.3499 | CLSLoss:5.7927 | top1:55.7564 | AUROC:0.5891\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.003985\n",
      "Train | 20/20 | Loss:0.2331 | MainLoss:0.2331 | Alpha:0.0246 | SPLoss:1959.4718 | CLSLoss:5.7748 | top1:90.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2058 | MainLoss:0.2058 | SPLoss:1986.5651 | CLSLoss:5.7347 | top1:91.4922 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:0.9615 | MainLoss:0.9615 | SPLoss:1986.5688 | CLSLoss:5.7347 | top1:54.4103 | AUROC:0.5584\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.2148 | MainLoss:0.2148 | Alpha:0.0237 | SPLoss:1988.2391 | CLSLoss:5.7678 | top1:91.4000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1339 | MainLoss:0.1339 | SPLoss:1989.7241 | CLSLoss:5.7944 | top1:94.9346 | AUROC:0.9992\n",
      "Test | 39/20 | Loss:1.0126 | MainLoss:1.0126 | SPLoss:1989.7251 | CLSLoss:5.7944 | top1:54.1923 | AUROC:0.5641\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.2179 | MainLoss:0.2179 | Alpha:0.0238 | SPLoss:1989.9963 | CLSLoss:5.8015 | top1:90.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1472 | MainLoss:0.1472 | SPLoss:1989.9604 | CLSLoss:5.8211 | top1:94.2150 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.0065 | MainLoss:1.0065 | SPLoss:1989.9611 | CLSLoss:5.8211 | top1:54.7179 | AUROC:0.5664\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.2013 | MainLoss:0.2013 | Alpha:0.0245 | SPLoss:1990.5775 | CLSLoss:5.8450 | top1:91.7500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1538 | MainLoss:0.1538 | SPLoss:1991.1013 | CLSLoss:5.8791 | top1:93.9720 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.0269 | MainLoss:1.0269 | SPLoss:1991.1039 | CLSLoss:5.8791 | top1:54.7692 | AUROC:0.5683\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.2086 | MainLoss:0.2086 | Alpha:0.0247 | SPLoss:1990.5878 | CLSLoss:5.9085 | top1:91.2000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1607 | MainLoss:0.1607 | SPLoss:1990.1151 | CLSLoss:5.9233 | top1:93.6324 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.0354 | MainLoss:1.0354 | SPLoss:1990.1112 | CLSLoss:5.9233 | top1:54.7692 | AUROC:0.5645\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1946 | MainLoss:0.1946 | Alpha:0.0240 | SPLoss:1989.7998 | CLSLoss:5.9585 | top1:92.1000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1333 | MainLoss:0.1333 | SPLoss:1989.5591 | CLSLoss:5.9806 | top1:94.8910 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.0743 | MainLoss:1.0743 | SPLoss:1989.5562 | CLSLoss:5.9806 | top1:54.6026 | AUROC:0.5663\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1827 | MainLoss:0.1827 | Alpha:0.0247 | SPLoss:1989.9629 | CLSLoss:5.9959 | top1:92.3750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1549 | MainLoss:0.1549 | SPLoss:1990.4451 | CLSLoss:6.0266 | top1:93.9626 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.0871 | MainLoss:1.0871 | SPLoss:1990.4486 | CLSLoss:6.0266 | top1:54.7179 | AUROC:0.5665\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1827 | MainLoss:0.1827 | Alpha:0.0246 | SPLoss:1990.4076 | CLSLoss:6.0608 | top1:92.7250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1551 | MainLoss:0.1551 | SPLoss:1990.6960 | CLSLoss:6.0838 | top1:94.0405 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1230 | MainLoss:1.1230 | SPLoss:1990.6948 | CLSLoss:6.0838 | top1:54.6282 | AUROC:0.5654\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1855 | MainLoss:0.1855 | Alpha:0.0242 | SPLoss:1991.4198 | CLSLoss:6.0961 | top1:92.0750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1300 | MainLoss:0.1300 | SPLoss:1991.8241 | CLSLoss:6.1021 | top1:95.0561 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.1377 | MainLoss:1.1377 | SPLoss:1991.8253 | CLSLoss:6.1021 | top1:54.5256 | AUROC:0.5675\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1878 | MainLoss:0.1878 | Alpha:0.0241 | SPLoss:1993.2921 | CLSLoss:6.0961 | top1:92.7250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1617 | MainLoss:0.1617 | SPLoss:1994.3812 | CLSLoss:6.0975 | top1:93.7196 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1007 | MainLoss:1.1007 | SPLoss:1994.3839 | CLSLoss:6.0975 | top1:54.8333 | AUROC:0.5673\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1731 | MainLoss:0.1731 | Alpha:0.0245 | SPLoss:1994.6400 | CLSLoss:6.1232 | top1:92.8750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1414 | MainLoss:0.1414 | SPLoss:1995.4414 | CLSLoss:6.1503 | top1:94.5919 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1394 | MainLoss:1.1394 | SPLoss:1995.4449 | CLSLoss:6.1503 | top1:54.4487 | AUROC:0.5641\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.000398\n",
      "Train | 20/20 | Loss:0.1800 | MainLoss:0.1800 | Alpha:0.0241 | SPLoss:1995.9072 | CLSLoss:6.1567 | top1:92.9750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1709 | MainLoss:0.1709 | SPLoss:1996.5399 | CLSLoss:6.1517 | top1:93.3645 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1235 | MainLoss:1.1235 | SPLoss:1996.5398 | CLSLoss:6.1517 | top1:54.8846 | AUROC:0.5631\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1752 | MainLoss:0.1752 | Alpha:0.0233 | SPLoss:1996.2620 | CLSLoss:6.1684 | top1:92.9750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1380 | MainLoss:0.1380 | SPLoss:1995.6350 | CLSLoss:6.1809 | top1:94.7913 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1433 | MainLoss:1.1433 | SPLoss:1995.6304 | CLSLoss:6.1809 | top1:54.6154 | AUROC:0.5651\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1804 | MainLoss:0.1804 | Alpha:0.0246 | SPLoss:1995.8210 | CLSLoss:6.1708 | top1:92.6750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1454 | MainLoss:0.1454 | SPLoss:1995.4414 | CLSLoss:6.1777 | top1:94.4611 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1188 | MainLoss:1.1188 | SPLoss:1995.4448 | CLSLoss:6.1777 | top1:54.7564 | AUROC:0.5652\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1679 | MainLoss:0.1679 | Alpha:0.0246 | SPLoss:1995.6562 | CLSLoss:6.1816 | top1:93.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1420 | MainLoss:0.1420 | SPLoss:1995.5641 | CLSLoss:6.2084 | top1:94.6231 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1216 | MainLoss:1.1216 | SPLoss:1995.5652 | CLSLoss:6.2084 | top1:54.8462 | AUROC:0.5677\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.000397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.1684 | MainLoss:0.1684 | Alpha:0.0240 | SPLoss:1995.6190 | CLSLoss:6.2468 | top1:93.3750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1564 | MainLoss:0.1564 | SPLoss:1995.3651 | CLSLoss:6.2811 | top1:94.0748 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.1568 | MainLoss:1.1568 | SPLoss:1995.3687 | CLSLoss:6.2811 | top1:54.7564 | AUROC:0.5670\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1690 | MainLoss:0.1690 | Alpha:0.0236 | SPLoss:1995.0297 | CLSLoss:6.2969 | top1:93.0500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1819 | MainLoss:0.1819 | SPLoss:1995.1442 | CLSLoss:6.2995 | top1:92.9688 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.1454 | MainLoss:1.1454 | SPLoss:1995.1453 | CLSLoss:6.2995 | top1:54.7308 | AUROC:0.5694\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1670 | MainLoss:0.1670 | Alpha:0.0235 | SPLoss:1995.1660 | CLSLoss:6.3104 | top1:93.1000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1474 | MainLoss:0.1474 | SPLoss:1994.7043 | CLSLoss:6.3170 | top1:94.3645 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1463 | MainLoss:1.1463 | SPLoss:1994.7062 | CLSLoss:6.3170 | top1:55.1410 | AUROC:0.5711\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1639 | MainLoss:0.1639 | Alpha:0.0241 | SPLoss:1994.9149 | CLSLoss:6.3290 | top1:93.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1400 | MainLoss:0.1400 | SPLoss:1994.9202 | CLSLoss:6.3441 | top1:94.6729 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1646 | MainLoss:1.1646 | SPLoss:1994.9203 | CLSLoss:6.3442 | top1:55.0641 | AUROC:0.5691\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1654 | MainLoss:0.1654 | Alpha:0.0247 | SPLoss:1994.8075 | CLSLoss:6.3378 | top1:93.2250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1589 | MainLoss:0.1589 | SPLoss:1995.1591 | CLSLoss:6.3392 | top1:93.9502 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1481 | MainLoss:1.1481 | SPLoss:1995.1562 | CLSLoss:6.3392 | top1:55.0000 | AUROC:0.5683\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.000397\n",
      "Train | 20/20 | Loss:0.1804 | MainLoss:0.1804 | Alpha:0.0239 | SPLoss:1995.1744 | CLSLoss:6.3481 | top1:92.8250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1372 | MainLoss:0.1372 | SPLoss:1994.2557 | CLSLoss:6.3407 | top1:94.8037 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1594 | MainLoss:1.1594 | SPLoss:1994.2537 | CLSLoss:6.3407 | top1:54.7692 | AUROC:0.5674\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1769 | MainLoss:0.1769 | Alpha:0.0241 | SPLoss:1993.7672 | CLSLoss:6.3172 | top1:93.1250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1250 | MainLoss:0.1250 | SPLoss:1993.3790 | CLSLoss:6.3249 | top1:95.3022 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.1448 | MainLoss:1.1448 | SPLoss:1993.3762 | CLSLoss:6.3249 | top1:54.7179 | AUROC:0.5716\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1684 | MainLoss:0.1684 | Alpha:0.0244 | SPLoss:1994.5607 | CLSLoss:6.3329 | top1:93.8500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1592 | MainLoss:0.1592 | SPLoss:1995.4653 | CLSLoss:6.3583 | top1:93.9097 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1421 | MainLoss:1.1421 | SPLoss:1995.4698 | CLSLoss:6.3583 | top1:55.0256 | AUROC:0.5701\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1526 | MainLoss:0.1526 | Alpha:0.0247 | SPLoss:1996.1936 | CLSLoss:6.3931 | top1:93.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1424 | MainLoss:0.1424 | SPLoss:1996.9188 | CLSLoss:6.4311 | top1:94.6386 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2012 | MainLoss:1.2012 | SPLoss:1996.9153 | CLSLoss:6.4311 | top1:54.8846 | AUROC:0.5684\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1625 | MainLoss:0.1625 | Alpha:0.0246 | SPLoss:1997.5219 | CLSLoss:6.4453 | top1:93.5250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1708 | MainLoss:0.1708 | SPLoss:1997.8190 | CLSLoss:6.4475 | top1:93.5888 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2042 | MainLoss:1.2042 | SPLoss:1997.8162 | CLSLoss:6.4475 | top1:54.8974 | AUROC:0.5663\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1557 | MainLoss:0.1557 | Alpha:0.0238 | SPLoss:1997.7766 | CLSLoss:6.4639 | top1:93.4750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1759 | MainLoss:0.1759 | SPLoss:1998.1958 | CLSLoss:6.4677 | top1:93.3863 | AUROC:0.9988\n",
      "Test | 39/20 | Loss:1.2054 | MainLoss:1.2054 | SPLoss:1998.1936 | CLSLoss:6.4677 | top1:55.0897 | AUROC:0.5670\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1611 | MainLoss:0.1611 | Alpha:0.0246 | SPLoss:1998.0928 | CLSLoss:6.4658 | top1:93.8750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1566 | MainLoss:0.1566 | SPLoss:1998.7412 | CLSLoss:6.4666 | top1:94.0810 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.1865 | MainLoss:1.1865 | SPLoss:1998.7439 | CLSLoss:6.4666 | top1:54.9231 | AUROC:0.5671\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1611 | MainLoss:0.1611 | Alpha:0.0243 | SPLoss:1999.3707 | CLSLoss:6.4758 | top1:93.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1547 | MainLoss:0.1547 | SPLoss:2000.4188 | CLSLoss:6.4869 | top1:94.1620 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1925 | MainLoss:1.1925 | SPLoss:2000.4150 | CLSLoss:6.4869 | top1:54.4487 | AUROC:0.5668\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.000396\n",
      "Train | 20/20 | Loss:0.1541 | MainLoss:0.1541 | Alpha:0.0241 | SPLoss:2001.0275 | CLSLoss:6.4891 | top1:94.1250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1729 | MainLoss:0.1729 | SPLoss:2001.9414 | CLSLoss:6.4961 | top1:93.4579 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1899 | MainLoss:1.1899 | SPLoss:2001.9446 | CLSLoss:6.4961 | top1:54.7821 | AUROC:0.5649\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1510 | MainLoss:0.1510 | Alpha:0.0238 | SPLoss:2001.6559 | CLSLoss:6.5302 | top1:94.2250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1484 | MainLoss:0.1484 | SPLoss:2001.2214 | CLSLoss:6.5434 | top1:94.4829 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.2302 | MainLoss:1.2302 | SPLoss:2001.2249 | CLSLoss:6.5434 | top1:54.7051 | AUROC:0.5664\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1473 | MainLoss:0.1473 | Alpha:0.0244 | SPLoss:2001.2889 | CLSLoss:6.5661 | top1:94.2000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1433 | MainLoss:0.1433 | SPLoss:2000.9561 | CLSLoss:6.5991 | top1:94.8100 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2747 | MainLoss:1.2747 | SPLoss:2000.9546 | CLSLoss:6.5991 | top1:54.6795 | AUROC:0.5678\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1538 | MainLoss:0.1538 | Alpha:0.0243 | SPLoss:2001.1036 | CLSLoss:6.6061 | top1:94.1250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1604 | MainLoss:0.1604 | SPLoss:2001.4596 | CLSLoss:6.6045 | top1:94.1028 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2559 | MainLoss:1.2559 | SPLoss:2001.4586 | CLSLoss:6.6045 | top1:54.8590 | AUROC:0.5673\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1656 | MainLoss:0.1656 | Alpha:0.0251 | SPLoss:2002.9923 | CLSLoss:6.5859 | top1:93.3750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1768 | MainLoss:0.1768 | SPLoss:2005.1987 | CLSLoss:6.5572 | top1:93.2991 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1981 | MainLoss:1.1981 | SPLoss:2005.1949 | CLSLoss:6.5572 | top1:55.1667 | AUROC:0.5683\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1558 | MainLoss:0.1558 | Alpha:0.0246 | SPLoss:2006.0964 | CLSLoss:6.5621 | top1:93.5500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1547 | MainLoss:0.1547 | SPLoss:2006.8010 | CLSLoss:6.5686 | top1:94.2243 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2090 | MainLoss:1.2090 | SPLoss:2006.8036 | CLSLoss:6.5686 | top1:55.0256 | AUROC:0.5698\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1372 | MainLoss:0.1372 | Alpha:0.0243 | SPLoss:2007.3331 | CLSLoss:6.5826 | top1:94.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:2007.8789 | CLSLoss:6.6049 | top1:92.1620 | AUROC:0.9988\n",
      "Test | 39/20 | Loss:1.2264 | MainLoss:1.2264 | SPLoss:2007.8760 | CLSLoss:6.6049 | top1:54.3462 | AUROC:0.5664\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.000395\n",
      "Train | 20/20 | Loss:0.1609 | MainLoss:0.1609 | Alpha:0.0234 | SPLoss:2006.9291 | CLSLoss:6.6262 | top1:94.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1123 | MainLoss:0.1123 | SPLoss:2006.7200 | CLSLoss:6.6213 | top1:95.8661 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2788 | MainLoss:1.2788 | SPLoss:2006.7195 | CLSLoss:6.6213 | top1:54.2179 | AUROC:0.5663\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1426 | MainLoss:0.1426 | Alpha:0.0243 | SPLoss:2008.8389 | CLSLoss:6.5842 | top1:94.5250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1545 | MainLoss:0.1545 | SPLoss:2010.0242 | CLSLoss:6.6048 | top1:94.2212 | AUROC:0.9989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 39/20 | Loss:1.2188 | MainLoss:1.2188 | SPLoss:2010.0250 | CLSLoss:6.6048 | top1:54.6538 | AUROC:0.5664\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1689 | MainLoss:0.1689 | Alpha:0.0242 | SPLoss:2010.8750 | CLSLoss:6.6110 | top1:93.0250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1432 | MainLoss:0.1432 | SPLoss:2011.8561 | CLSLoss:6.5996 | top1:94.6012 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2149 | MainLoss:1.2149 | SPLoss:2011.8549 | CLSLoss:6.5996 | top1:54.3333 | AUROC:0.5617\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1545 | MainLoss:0.1545 | Alpha:0.0235 | SPLoss:2012.5145 | CLSLoss:6.5900 | top1:94.0500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1621 | MainLoss:0.1621 | SPLoss:2013.4758 | CLSLoss:6.5920 | top1:93.8349 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.1827 | MainLoss:1.1827 | SPLoss:2013.4736 | CLSLoss:6.5920 | top1:54.6154 | AUROC:0.5643\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1519 | MainLoss:0.1519 | Alpha:0.0243 | SPLoss:2013.4255 | CLSLoss:6.6329 | top1:94.1750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1551 | MainLoss:0.1551 | SPLoss:2013.3599 | CLSLoss:6.6458 | top1:94.1620 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2108 | MainLoss:1.2108 | SPLoss:2013.3589 | CLSLoss:6.6458 | top1:54.7949 | AUROC:0.5660\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1440 | MainLoss:0.1440 | Alpha:0.0246 | SPLoss:2013.2775 | CLSLoss:6.6866 | top1:94.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1666 | MainLoss:0.1666 | SPLoss:2013.5249 | CLSLoss:6.7195 | top1:93.9128 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2542 | MainLoss:1.2542 | SPLoss:2013.5288 | CLSLoss:6.7195 | top1:54.7821 | AUROC:0.5644\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1304 | MainLoss:0.1304 | Alpha:0.0240 | SPLoss:2013.9575 | CLSLoss:6.7508 | top1:94.6250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1735 | MainLoss:0.1735 | SPLoss:2015.0653 | CLSLoss:6.7895 | top1:93.7695 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2960 | MainLoss:1.2960 | SPLoss:2015.0697 | CLSLoss:6.7895 | top1:54.7308 | AUROC:0.5672\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.000394\n",
      "Train | 20/20 | Loss:0.1378 | MainLoss:0.1378 | Alpha:0.0241 | SPLoss:2015.7205 | CLSLoss:6.7934 | top1:94.6750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1618 | MainLoss:0.1618 | SPLoss:2016.2650 | CLSLoss:6.7993 | top1:94.0810 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.2711 | MainLoss:1.2711 | SPLoss:2016.2688 | CLSLoss:6.7993 | top1:54.7436 | AUROC:0.5669\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.000393\n",
      "Train | 20/20 | Loss:0.1438 | MainLoss:0.1438 | Alpha:0.0242 | SPLoss:2016.7572 | CLSLoss:6.8201 | top1:94.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1494 | MainLoss:0.1494 | SPLoss:2017.0560 | CLSLoss:6.8266 | top1:94.4579 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2832 | MainLoss:1.2832 | SPLoss:2017.0547 | CLSLoss:6.8266 | top1:54.3462 | AUROC:0.5624\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.000393\n",
      "Train | 20/20 | Loss:0.1398 | MainLoss:0.1398 | Alpha:0.0250 | SPLoss:2018.2738 | CLSLoss:6.8216 | top1:94.3500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1601 | MainLoss:0.1601 | SPLoss:2019.4850 | CLSLoss:6.8389 | top1:94.0374 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.2670 | MainLoss:1.2670 | SPLoss:2019.4886 | CLSLoss:6.8389 | top1:54.4744 | AUROC:0.5645\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.000393\n",
      "Train | 20/20 | Loss:0.1423 | MainLoss:0.1423 | Alpha:0.0238 | SPLoss:2020.1276 | CLSLoss:6.8473 | top1:94.1500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1613 | MainLoss:0.1613 | SPLoss:2020.1205 | CLSLoss:6.8503 | top1:94.0592 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2705 | MainLoss:1.2705 | SPLoss:2020.1212 | CLSLoss:6.8503 | top1:54.2308 | AUROC:0.5623\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.000393\n",
      "Train | 20/20 | Loss:0.1475 | MainLoss:0.1475 | Alpha:0.0248 | SPLoss:2019.3707 | CLSLoss:6.8671 | top1:94.0500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1392 | MainLoss:0.1392 | SPLoss:2018.7443 | CLSLoss:6.8713 | top1:94.8598 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2979 | MainLoss:1.2979 | SPLoss:2018.7460 | CLSLoss:6.8713 | top1:54.4615 | AUROC:0.5667\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.000393\n",
      "Train | 20/20 | Loss:0.1262 | MainLoss:0.1262 | Alpha:0.0239 | SPLoss:2019.2759 | CLSLoss:6.8774 | top1:95.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1541 | MainLoss:0.1541 | SPLoss:2019.9559 | CLSLoss:6.9095 | top1:94.3770 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3113 | MainLoss:1.3113 | SPLoss:2019.9539 | CLSLoss:6.9095 | top1:54.6154 | AUROC:0.5688\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.000393\n",
      "Train | 20/20 | Loss:0.1512 | MainLoss:0.1512 | Alpha:0.0239 | SPLoss:2021.2456 | CLSLoss:6.9172 | top1:93.9750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1542 | MainLoss:0.1542 | SPLoss:2021.5762 | CLSLoss:6.9167 | top1:94.3863 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3083 | MainLoss:1.3083 | SPLoss:2021.5748 | CLSLoss:6.9167 | top1:54.5128 | AUROC:0.5669\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.000392\n",
      "Train | 20/20 | Loss:0.1341 | MainLoss:0.1341 | Alpha:0.0245 | SPLoss:2022.0125 | CLSLoss:6.9223 | top1:95.0500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1590 | MainLoss:0.1590 | SPLoss:2022.2450 | CLSLoss:6.9270 | top1:94.2399 | AUROC:0.9988\n",
      "Test | 39/20 | Loss:1.3206 | MainLoss:1.3206 | SPLoss:2022.2489 | CLSLoss:6.9270 | top1:54.1795 | AUROC:0.5644\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.000392\n",
      "Train | 20/20 | Loss:0.1407 | MainLoss:0.1407 | Alpha:0.0246 | SPLoss:2023.2599 | CLSLoss:6.9045 | top1:94.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1819 | MainLoss:0.1819 | SPLoss:2024.8411 | CLSLoss:6.8766 | top1:93.2679 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2692 | MainLoss:1.2692 | SPLoss:2024.8438 | CLSLoss:6.8766 | top1:54.3974 | AUROC:0.5613\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.000392\n",
      "Train | 20/20 | Loss:0.1375 | MainLoss:0.1375 | Alpha:0.0237 | SPLoss:2025.4626 | CLSLoss:6.8967 | top1:94.5500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1678 | MainLoss:0.1678 | SPLoss:2026.6149 | CLSLoss:6.9135 | top1:93.8536 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2858 | MainLoss:1.2858 | SPLoss:2026.6101 | CLSLoss:6.9135 | top1:54.2051 | AUROC:0.5620\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.000392\n",
      "Train | 20/20 | Loss:0.1448 | MainLoss:0.1448 | Alpha:0.0230 | SPLoss:2026.6593 | CLSLoss:6.9134 | top1:94.4000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1463 | MainLoss:0.1463 | SPLoss:2026.5989 | CLSLoss:6.9264 | top1:94.6075 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.2886 | MainLoss:1.2886 | SPLoss:2026.5951 | CLSLoss:6.9264 | top1:54.1410 | AUROC:0.5591\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.000392\n",
      "Train | 20/20 | Loss:0.1555 | MainLoss:0.1555 | Alpha:0.0251 | SPLoss:2027.5161 | CLSLoss:6.9281 | top1:94.5500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1733 | MainLoss:0.1733 | SPLoss:2028.6962 | CLSLoss:6.8818 | top1:93.6480 | AUROC:0.9991\n",
      "Test | 39/20 | Loss:1.2487 | MainLoss:1.2487 | SPLoss:2028.6948 | CLSLoss:6.8818 | top1:54.1026 | AUROC:0.5597\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.000392\n",
      "Train | 20/20 | Loss:0.1332 | MainLoss:0.1332 | Alpha:0.0251 | SPLoss:2028.4557 | CLSLoss:6.9002 | top1:95.1000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1384 | MainLoss:0.1384 | SPLoss:2028.5452 | CLSLoss:6.9241 | top1:95.0062 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.2936 | MainLoss:1.2936 | SPLoss:2028.5496 | CLSLoss:6.9241 | top1:54.2051 | AUROC:0.5604\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.000391\n",
      "Train | 20/20 | Loss:0.1353 | MainLoss:0.1353 | Alpha:0.0241 | SPLoss:2029.4052 | CLSLoss:6.9490 | top1:94.7750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1941 | MainLoss:0.1941 | SPLoss:2030.1152 | CLSLoss:6.9722 | top1:92.9844 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3019 | MainLoss:1.3019 | SPLoss:2030.1110 | CLSLoss:6.9722 | top1:54.4744 | AUROC:0.5618\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.000391\n",
      "Train | 20/20 | Loss:0.1373 | MainLoss:0.1373 | Alpha:0.0246 | SPLoss:2030.2235 | CLSLoss:6.9921 | top1:94.9750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1509 | MainLoss:0.1509 | SPLoss:2031.1014 | CLSLoss:6.9962 | top1:94.6667 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3319 | MainLoss:1.3319 | SPLoss:2031.1046 | CLSLoss:6.9962 | top1:54.3590 | AUROC:0.5617\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.000391\n",
      "Train | 20/20 | Loss:0.1446 | MainLoss:0.1446 | Alpha:0.0241 | SPLoss:2032.0964 | CLSLoss:6.9733 | top1:94.4750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1721 | MainLoss:0.1721 | SPLoss:2033.3589 | CLSLoss:6.9526 | top1:93.8037 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2902 | MainLoss:1.2902 | SPLoss:2033.3552 | CLSLoss:6.9526 | top1:54.4231 | AUROC:0.5626\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.000391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.1440 | MainLoss:0.1440 | Alpha:0.0241 | SPLoss:2034.2228 | CLSLoss:6.9468 | top1:94.3250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1737 | MainLoss:0.1737 | SPLoss:2035.1388 | CLSLoss:6.9505 | top1:93.6231 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2747 | MainLoss:1.2747 | SPLoss:2035.1350 | CLSLoss:6.9505 | top1:54.3462 | AUROC:0.5611\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.000391\n",
      "Train | 20/20 | Loss:0.1348 | MainLoss:0.1348 | Alpha:0.0249 | SPLoss:2036.5786 | CLSLoss:6.9706 | top1:94.6750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1746 | MainLoss:0.1746 | SPLoss:2038.4398 | CLSLoss:6.9913 | top1:93.6542 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.3044 | MainLoss:1.3044 | SPLoss:2038.4388 | CLSLoss:6.9913 | top1:54.2821 | AUROC:0.5580\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.000390\n",
      "Train | 20/20 | Loss:0.1440 | MainLoss:0.1440 | Alpha:0.0240 | SPLoss:2038.2325 | CLSLoss:7.0163 | top1:94.5750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1617 | MainLoss:0.1617 | SPLoss:2038.2640 | CLSLoss:7.0113 | top1:94.1308 | AUROC:0.9990\n",
      "Test | 39/20 | Loss:1.3064 | MainLoss:1.3064 | SPLoss:2038.2650 | CLSLoss:7.0113 | top1:54.6282 | AUROC:0.5625\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.000390\n",
      "Train | 20/20 | Loss:0.1383 | MainLoss:0.1383 | Alpha:0.0249 | SPLoss:2038.6039 | CLSLoss:7.0037 | top1:94.9500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1750 | MainLoss:0.1750 | SPLoss:2038.9050 | CLSLoss:6.9963 | top1:93.6386 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2800 | MainLoss:1.2800 | SPLoss:2038.9087 | CLSLoss:6.9963 | top1:54.4103 | AUROC:0.5626\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.000390\n",
      "Train | 20/20 | Loss:0.1339 | MainLoss:0.1339 | Alpha:0.0240 | SPLoss:2038.9313 | CLSLoss:7.0117 | top1:95.0000 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1800 | MainLoss:0.1800 | SPLoss:2039.1411 | CLSLoss:7.0262 | top1:93.4829 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.2897 | MainLoss:1.2897 | SPLoss:2039.1438 | CLSLoss:7.0262 | top1:54.4487 | AUROC:0.5635\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.000390\n",
      "Train | 20/20 | Loss:0.1367 | MainLoss:0.1367 | Alpha:0.0240 | SPLoss:2038.2625 | CLSLoss:7.0514 | top1:94.6750 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1460 | MainLoss:0.1460 | SPLoss:2037.1561 | CLSLoss:7.0773 | top1:94.6916 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3333 | MainLoss:1.3333 | SPLoss:2037.1545 | CLSLoss:7.0773 | top1:54.0256 | AUROC:0.5656\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.000390\n",
      "Train | 20/20 | Loss:0.1289 | MainLoss:0.1289 | Alpha:0.0247 | SPLoss:2037.0177 | CLSLoss:7.0907 | top1:94.9250 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1298 | MainLoss:0.1298 | SPLoss:2035.9399 | CLSLoss:7.1208 | top1:95.2835 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3972 | MainLoss:1.3972 | SPLoss:2035.9398 | CLSLoss:7.1208 | top1:54.1154 | AUROC:0.5664\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.000389\n",
      "Train | 20/20 | Loss:0.1356 | MainLoss:0.1356 | Alpha:0.0248 | SPLoss:2037.5524 | CLSLoss:7.1142 | top1:94.9500 | AUROC:0.0000\n",
      "Test | 161/20 | Loss:0.1345 | MainLoss:0.1345 | SPLoss:2038.2452 | CLSLoss:7.1263 | top1:95.1153 | AUROC:0.9989\n",
      "Test | 39/20 | Loss:1.3838 | MainLoss:1.3838 | SPLoss:2038.2498 | CLSLoss:7.1263 | top1:54.3333 | AUROC:0.5686\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.000389\n",
      "Train | 20/20 | Loss:0.1292 | MainLoss:0.1292 | Alpha:0.0241 | SPLoss:2040.3107 | CLSLoss:7.0974 | top1:94.7750 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_auroc+source_auroc > best_acc\n",
    "    best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "#     teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
