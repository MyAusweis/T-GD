{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/style2/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 5000\n",
    "start_epoch = 0\n",
    "train_batch = 250\n",
    "test_batch = 250\n",
    "lr = 0.04\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style2/128/b0/to_star/2000shot/self2' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'star/2000_shot')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/style2/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 500, gamma=0.1, last_epoch=-1)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=100, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(m):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in m.named_parameters():\n",
    "        sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        teacher_outputs = teacher_model(inputs)\n",
    "        teacher_loss = criterion(teacher_outputs, targets)\n",
    "        sp_alpha = 0\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, ac=arc.avg))\n",
    "    return (losses.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, ac=arc.avg))\n",
    "    return (losses.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 5000] LR: 0.040000\n",
      "Train | 16/16 | Loss:1.0295 | MainLoss:0.9157 | Alpha:0.0319 | SPLoss:3.7503 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [2 | 5000] LR: 0.041200\n",
      "Train | 16/16 | Loss:1.0033 | MainLoss:0.6928 | Alpha:0.0318 | SPLoss:9.7631 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [3 | 5000] LR: 0.042400\n",
      "Train | 16/16 | Loss:0.9470 | MainLoss:0.6944 | Alpha:0.0300 | SPLoss:8.4083 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [4 | 5000] LR: 0.043600\n",
      "Train | 16/16 | Loss:0.8587 | MainLoss:0.6937 | Alpha:0.0289 | SPLoss:5.7314 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [5 | 5000] LR: 0.044800\n",
      "Train | 16/16 | Loss:0.8095 | MainLoss:0.6954 | Alpha:0.0315 | SPLoss:3.6073 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [6 | 5000] LR: 0.046000\n",
      "Train | 16/16 | Loss:0.7634 | MainLoss:0.6944 | Alpha:0.0317 | SPLoss:2.1725 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [7 | 5000] LR: 0.047200\n",
      "Train | 16/16 | Loss:0.7332 | MainLoss:0.6944 | Alpha:0.0307 | SPLoss:1.2740 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [8 | 5000] LR: 0.048400\n",
      "Train | 16/16 | Loss:0.7166 | MainLoss:0.6933 | Alpha:0.0315 | SPLoss:0.7456 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [9 | 5000] LR: 0.049600\n",
      "Train | 16/16 | Loss:0.7060 | MainLoss:0.6922 | Alpha:0.0311 | SPLoss:0.4419 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [10 | 5000] LR: 0.050800\n",
      "Train | 16/16 | Loss:0.6970 | MainLoss:0.6878 | Alpha:0.0322 | SPLoss:0.2842 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [11 | 5000] LR: 0.052000\n",
      "Train | 16/16 | Loss:0.6928 | MainLoss:0.6830 | Alpha:0.0315 | SPLoss:0.3125 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [12 | 5000] LR: 0.053200\n",
      "Train | 16/16 | Loss:0.6877 | MainLoss:0.6779 | Alpha:0.0308 | SPLoss:0.3180 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [13 | 5000] LR: 0.054400\n",
      "Train | 16/16 | Loss:0.6835 | MainLoss:0.6751 | Alpha:0.0311 | SPLoss:0.2701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [14 | 5000] LR: 0.055600\n",
      "Train | 16/16 | Loss:0.6851 | MainLoss:0.6756 | Alpha:0.0308 | SPLoss:0.3061 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [15 | 5000] LR: 0.056800\n",
      "Train | 16/16 | Loss:0.6853 | MainLoss:0.6757 | Alpha:0.0314 | SPLoss:0.3048 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [16 | 5000] LR: 0.058000\n",
      "Train | 16/16 | Loss:0.6845 | MainLoss:0.6748 | Alpha:0.0308 | SPLoss:0.3122 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [17 | 5000] LR: 0.059200\n",
      "Train | 16/16 | Loss:0.6889 | MainLoss:0.6774 | Alpha:0.0313 | SPLoss:0.3719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [18 | 5000] LR: 0.060400\n",
      "Train | 16/16 | Loss:0.6863 | MainLoss:0.6724 | Alpha:0.0307 | SPLoss:0.4504 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [19 | 5000] LR: 0.061600\n",
      "Train | 16/16 | Loss:0.6847 | MainLoss:0.6708 | Alpha:0.0299 | SPLoss:0.4652 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [20 | 5000] LR: 0.062800\n",
      "Train | 16/16 | Loss:0.6824 | MainLoss:0.6644 | Alpha:0.0320 | SPLoss:0.5584 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [21 | 5000] LR: 0.064000\n",
      "Train | 16/16 | Loss:0.6876 | MainLoss:0.6671 | Alpha:0.0321 | SPLoss:0.6401 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [22 | 5000] LR: 0.065200\n",
      "Train | 16/16 | Loss:0.6880 | MainLoss:0.6643 | Alpha:0.0303 | SPLoss:0.7769 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [23 | 5000] LR: 0.066400\n",
      "Train | 16/16 | Loss:2.8612 | MainLoss:0.6578 | Alpha:0.0301 | SPLoss:70.5560 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [24 | 5000] LR: 0.067600\n",
      "Train | 16/16 | Loss:21.8835 | MainLoss:0.6511 | Alpha:0.0313 | SPLoss:673.5600 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [25 | 5000] LR: 0.068800\n",
      "Train | 16/16 | Loss:20.6996 | MainLoss:0.6660 | Alpha:0.0304 | SPLoss:663.0459 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [26 | 5000] LR: 0.070000\n",
      "Train | 16/16 | Loss:11.5669 | MainLoss:0.6608 | Alpha:0.0302 | SPLoss:360.3610 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [27 | 5000] LR: 0.071200\n",
      "Train | 16/16 | Loss:5.5715 | MainLoss:0.6546 | Alpha:0.0304 | SPLoss:159.5551 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [28 | 5000] LR: 0.072400\n",
      "Train | 16/16 | Loss:2.6779 | MainLoss:0.6692 | Alpha:0.0310 | SPLoss:65.0666 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [29 | 5000] LR: 0.073600\n",
      "Train | 16/16 | Loss:1.4779 | MainLoss:0.6649 | Alpha:0.0319 | SPLoss:24.8358 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [30 | 5000] LR: 0.074800\n",
      "Train | 16/16 | Loss:0.9515 | MainLoss:0.6606 | Alpha:0.0303 | SPLoss:9.4233 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [31 | 5000] LR: 0.076000\n",
      "Train | 16/16 | Loss:0.7977 | MainLoss:0.6579 | Alpha:0.0303 | SPLoss:4.6320 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [32 | 5000] LR: 0.077200\n",
      "Train | 16/16 | Loss:0.7537 | MainLoss:0.6618 | Alpha:0.0307 | SPLoss:2.9979 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [33 | 5000] LR: 0.078400\n",
      "Train | 16/16 | Loss:0.7138 | MainLoss:0.6492 | Alpha:0.0306 | SPLoss:2.1119 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [34 | 5000] LR: 0.079600\n",
      "Train | 16/16 | Loss:0.7127 | MainLoss:0.6540 | Alpha:0.0317 | SPLoss:1.8464 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [35 | 5000] LR: 0.080800\n",
      "Train | 16/16 | Loss:0.7017 | MainLoss:0.6482 | Alpha:0.0303 | SPLoss:1.7671 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [36 | 5000] LR: 0.082000\n",
      "Train | 16/16 | Loss:0.7249 | MainLoss:0.6693 | Alpha:0.0308 | SPLoss:1.8054 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [37 | 5000] LR: 0.083200\n",
      "Train | 16/16 | Loss:0.7080 | MainLoss:0.6621 | Alpha:0.0292 | SPLoss:1.5727 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [38 | 5000] LR: 0.084400\n",
      "Train | 16/16 | Loss:0.7030 | MainLoss:0.6564 | Alpha:0.0311 | SPLoss:1.4985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [39 | 5000] LR: 0.085600\n",
      "Train | 16/16 | Loss:0.6827 | MainLoss:0.6350 | Alpha:0.0302 | SPLoss:1.5767 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [40 | 5000] LR: 0.086800\n",
      "Train | 16/16 | Loss:0.6899 | MainLoss:0.6330 | Alpha:0.0320 | SPLoss:1.7792 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [41 | 5000] LR: 0.088000\n",
      "Train | 16/16 | Loss:0.7006 | MainLoss:0.6372 | Alpha:0.0313 | SPLoss:2.0341 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [42 | 5000] LR: 0.089200\n",
      "Train | 16/16 | Loss:0.7424 | MainLoss:0.6503 | Alpha:0.0305 | SPLoss:3.0106 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [43 | 5000] LR: 0.090400\n",
      "Train | 16/16 | Loss:0.7214 | MainLoss:0.6369 | Alpha:0.0309 | SPLoss:2.7375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [44 | 5000] LR: 0.091600\n",
      "Train | 16/16 | Loss:0.7965 | MainLoss:0.7075 | Alpha:0.0310 | SPLoss:2.8841 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [45 | 5000] LR: 0.092800\n",
      "Train | 16/16 | Loss:0.7792 | MainLoss:0.6953 | Alpha:0.0305 | SPLoss:2.7905 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [46 | 5000] LR: 0.094000\n",
      "Train | 16/16 | Loss:0.7259 | MainLoss:0.6878 | Alpha:0.0291 | SPLoss:1.3184 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [47 | 5000] LR: 0.095200\n",
      "Train | 16/16 | Loss:0.6950 | MainLoss:0.6770 | Alpha:0.0304 | SPLoss:0.5884 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [48 | 5000] LR: 0.096400\n",
      "Train | 16/16 | Loss:0.6936 | MainLoss:0.6770 | Alpha:0.0305 | SPLoss:0.5420 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [49 | 5000] LR: 0.097600\n",
      "Train | 16/16 | Loss:0.6966 | MainLoss:0.6735 | Alpha:0.0307 | SPLoss:0.7501 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [50 | 5000] LR: 0.098800\n",
      "Train | 16/16 | Loss:0.6965 | MainLoss:0.6723 | Alpha:0.0301 | SPLoss:0.8021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [51 | 5000] LR: 0.100000\n",
      "Train | 16/16 | Loss:0.7023 | MainLoss:0.6731 | Alpha:0.0307 | SPLoss:0.9529 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [52 | 5000] LR: 0.101200\n",
      "Train | 16/16 | Loss:0.6966 | MainLoss:0.6671 | Alpha:0.0308 | SPLoss:0.9578 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [53 | 5000] LR: 0.102400\n",
      "Train | 16/16 | Loss:0.7238 | MainLoss:0.6704 | Alpha:0.0322 | SPLoss:1.6410 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [54 | 5000] LR: 0.103600\n",
      "Train | 16/16 | Loss:0.7280 | MainLoss:0.6755 | Alpha:0.0305 | SPLoss:1.7335 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [55 | 5000] LR: 0.104800\n",
      "Train | 16/16 | Loss:0.7256 | MainLoss:0.6828 | Alpha:0.0313 | SPLoss:1.3696 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [56 | 5000] LR: 0.106000\n",
      "Train | 16/16 | Loss:0.7009 | MainLoss:0.6632 | Alpha:0.0300 | SPLoss:1.2574 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [57 | 5000] LR: 0.107200\n",
      "Train | 16/16 | Loss:0.6962 | MainLoss:0.6553 | Alpha:0.0320 | SPLoss:1.2792 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [58 | 5000] LR: 0.108400\n",
      "Train | 16/16 | Loss:0.7007 | MainLoss:0.6566 | Alpha:0.0297 | SPLoss:1.4834 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [59 | 5000] LR: 0.109600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.7458 | MainLoss:0.6897 | Alpha:0.0311 | SPLoss:1.8126 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [60 | 5000] LR: 0.110800\n",
      "Train | 16/16 | Loss:0.7193 | MainLoss:0.6645 | Alpha:0.0319 | SPLoss:1.7151 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [61 | 5000] LR: 0.112000\n",
      "Train | 16/16 | Loss:0.7294 | MainLoss:0.6668 | Alpha:0.0317 | SPLoss:1.9200 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [62 | 5000] LR: 0.113200\n",
      "Train | 16/16 | Loss:0.7320 | MainLoss:0.6566 | Alpha:0.0302 | SPLoss:2.4807 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [63 | 5000] LR: 0.114400\n",
      "Train | 16/16 | Loss:0.7241 | MainLoss:0.6577 | Alpha:0.0318 | SPLoss:2.0895 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [64 | 5000] LR: 0.115600\n",
      "Train | 16/16 | Loss:0.7346 | MainLoss:0.6732 | Alpha:0.0309 | SPLoss:1.9857 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [65 | 5000] LR: 0.116800\n",
      "Train | 16/16 | Loss:0.7131 | MainLoss:0.6601 | Alpha:0.0321 | SPLoss:1.6434 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [66 | 5000] LR: 0.118000\n",
      "Train | 16/16 | Loss:0.7050 | MainLoss:0.6573 | Alpha:0.0319 | SPLoss:1.4931 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [67 | 5000] LR: 0.119200\n",
      "Train | 16/16 | Loss:0.9217 | MainLoss:0.6651 | Alpha:0.0304 | SPLoss:8.4989 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [68 | 5000] LR: 0.120400\n",
      "Train | 16/16 | Loss:96.7893 | MainLoss:0.6729 | Alpha:0.0315 | SPLoss:3386.0825 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [69 | 5000] LR: 0.121600\n",
      "Train | 16/16 | Loss:883.4248 | MainLoss:0.6878 | Alpha:0.0328 | SPLoss:27045.8594 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [70 | 5000] LR: 0.122800\n",
      "Train | 16/16 | Loss:518.7900 | MainLoss:0.6889 | Alpha:0.0319 | SPLoss:16233.1094 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [71 | 5000] LR: 0.124000\n",
      "Train | 16/16 | Loss:115.1323 | MainLoss:0.6786 | Alpha:0.0309 | SPLoss:3762.9292 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [72 | 5000] LR: 0.125200\n",
      "Train | 16/16 | Loss:13.4911 | MainLoss:0.6767 | Alpha:0.0296 | SPLoss:436.7240 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [73 | 5000] LR: 0.126400\n",
      "Train | 16/16 | Loss:2.2944 | MainLoss:0.6768 | Alpha:0.0310 | SPLoss:50.9577 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [74 | 5000] LR: 0.127600\n",
      "Train | 16/16 | Loss:19.9053 | MainLoss:0.6749 | Alpha:0.0322 | SPLoss:583.1865 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [75 | 5000] LR: 0.128800\n",
      "Train | 16/16 | Loss:14.1835 | MainLoss:0.6788 | Alpha:0.0313 | SPLoss:430.7826 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [76 | 5000] LR: 0.130000\n",
      "Train | 16/16 | Loss:4.1098 | MainLoss:0.6759 | Alpha:0.0319 | SPLoss:105.6072 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [77 | 5000] LR: 0.131200\n",
      "Train | 16/16 | Loss:1.0778 | MainLoss:0.6730 | Alpha:0.0309 | SPLoss:13.1401 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [78 | 5000] LR: 0.132400\n",
      "Train | 16/16 | Loss:1.0314 | MainLoss:0.6901 | Alpha:0.0325 | SPLoss:10.5222 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [79 | 5000] LR: 0.133600\n",
      "Train | 16/16 | Loss:1.7912 | MainLoss:0.6825 | Alpha:0.0317 | SPLoss:33.3142 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [80 | 5000] LR: 0.134800\n",
      "Train | 16/16 | Loss:5.4025 | MainLoss:0.6819 | Alpha:0.0326 | SPLoss:145.6103 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [81 | 5000] LR: 0.136000\n",
      "Train | 16/16 | Loss:3.0287 | MainLoss:0.6785 | Alpha:0.0309 | SPLoss:75.6780 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [82 | 5000] LR: 0.137200\n",
      "Train | 16/16 | Loss:1.1803 | MainLoss:0.6806 | Alpha:0.0311 | SPLoss:15.5516 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [83 | 5000] LR: 0.138400\n",
      "Train | 16/16 | Loss:12.5635 | MainLoss:0.6743 | Alpha:0.0311 | SPLoss:396.2883 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [84 | 5000] LR: 0.139600\n",
      "Train | 16/16 | Loss:71.0454 | MainLoss:0.6766 | Alpha:0.0315 | SPLoss:2242.4124 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [85 | 5000] LR: 0.140800\n",
      "Train | 16/16 | Loss:37.0773 | MainLoss:0.6778 | Alpha:0.0312 | SPLoss:1157.7994 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [86 | 5000] LR: 0.142000\n",
      "Train | 16/16 | Loss:8.1425 | MainLoss:0.6770 | Alpha:0.0301 | SPLoss:254.5431 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [87 | 5000] LR: 0.143200\n",
      "Train | 16/16 | Loss:1.6533 | MainLoss:0.6757 | Alpha:0.0316 | SPLoss:30.4559 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [88 | 5000] LR: 0.144400\n",
      "Train | 16/16 | Loss:0.7898 | MainLoss:0.6760 | Alpha:0.0300 | SPLoss:3.8793 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [89 | 5000] LR: 0.145600\n",
      "Train | 16/16 | Loss:0.7700 | MainLoss:0.6710 | Alpha:0.0303 | SPLoss:3.2837 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [90 | 5000] LR: 0.146800\n",
      "Train | 16/16 | Loss:0.9061 | MainLoss:0.6806 | Alpha:0.0303 | SPLoss:7.1268 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [91 | 5000] LR: 0.148000\n",
      "Train | 16/16 | Loss:1.1174 | MainLoss:0.6750 | Alpha:0.0326 | SPLoss:13.6089 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [92 | 5000] LR: 0.149200\n",
      "Train | 16/16 | Loss:1.2671 | MainLoss:0.6795 | Alpha:0.0320 | SPLoss:18.3619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [93 | 5000] LR: 0.150400\n",
      "Train | 16/16 | Loss:0.9653 | MainLoss:0.6700 | Alpha:0.0315 | SPLoss:9.5446 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [94 | 5000] LR: 0.151600\n",
      "Train | 16/16 | Loss:0.7970 | MainLoss:0.6828 | Alpha:0.0332 | SPLoss:3.4179 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [95 | 5000] LR: 0.152800\n",
      "Train | 16/16 | Loss:0.8457 | MainLoss:0.6984 | Alpha:0.0318 | SPLoss:4.5725 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [96 | 5000] LR: 0.154000\n",
      "Train | 16/16 | Loss:1.1432 | MainLoss:0.6966 | Alpha:0.0304 | SPLoss:14.5921 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [97 | 5000] LR: 0.155200\n",
      "Train | 16/16 | Loss:2.7630 | MainLoss:0.6942 | Alpha:0.0315 | SPLoss:65.2092 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [98 | 5000] LR: 0.156400\n",
      "Train | 16/16 | Loss:1.8689 | MainLoss:0.6894 | Alpha:0.0302 | SPLoss:39.3140 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [99 | 5000] LR: 0.157600\n",
      "Train | 16/16 | Loss:1.7727 | MainLoss:0.6801 | Alpha:0.0302 | SPLoss:36.1900 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [100 | 5000] LR: 0.158800\n",
      "Train | 16/16 | Loss:1.3170 | MainLoss:0.6784 | Alpha:0.0314 | SPLoss:20.4146 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.7015 | MainLoss:0.7015 | SPLoss:12.2707 | CLSLoss:0.0000 | AUROC:0.5278\n",
      "Test | 31/16 | Loss:0.2880 | MainLoss:0.2880 | SPLoss:12.2707 | CLSLoss:0.0000 | AUROC:0.9998\n",
      "\n",
      "Epoch: [101 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:11.9247 | MainLoss:0.6762 | Alpha:0.0305 | SPLoss:388.9420 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [102 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:17.1049 | MainLoss:0.6799 | Alpha:0.0313 | SPLoss:536.9804 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [103 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:7.0155 | MainLoss:0.6836 | Alpha:0.0310 | SPLoss:206.8635 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [104 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:3.0584 | MainLoss:0.6888 | Alpha:0.0308 | SPLoss:76.7089 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [105 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:31.8741 | MainLoss:0.6835 | Alpha:0.0315 | SPLoss:1032.6023 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [106 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:36.1995 | MainLoss:0.6859 | Alpha:0.0318 | SPLoss:1100.8868 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [107 | 5000] LR: 0.160000\n",
      "Train | 16/16 | Loss:10.5438 | MainLoss:0.6983 | Alpha:0.0309 | SPLoss:311.7211 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [108 | 5000] LR: 0.159999\n",
      "Train | 16/16 | Loss:222.7121 | MainLoss:0.6931 | Alpha:0.0297 | SPLoss:7715.3804 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [109 | 5000] LR: 0.159999\n",
      "Train | 16/16 | Loss:393.6344 | MainLoss:0.6863 | Alpha:0.0320 | SPLoss:12845.4619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [110 | 5000] LR: 0.159999\n",
      "Train | 16/16 | Loss:100.3750 | MainLoss:0.6842 | Alpha:0.0309 | SPLoss:3321.1392 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [111 | 5000] LR: 0.159999\n",
      "Train | 16/16 | Loss:7.1738 | MainLoss:0.6799 | Alpha:0.0318 | SPLoss:192.0724 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [112 | 5000] LR: 0.159998\n",
      "Train | 16/16 | Loss:8.5922 | MainLoss:0.6861 | Alpha:0.0321 | SPLoss:253.6178 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [113 | 5000] LR: 0.159998\n",
      "Train | 16/16 | Loss:15.0095 | MainLoss:0.6818 | Alpha:0.0309 | SPLoss:472.3311 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [114 | 5000] LR: 0.159998\n",
      "Train | 16/16 | Loss:5.0169 | MainLoss:0.6816 | Alpha:0.0308 | SPLoss:144.4729 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [115 | 5000] LR: 0.159997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:2.6551 | MainLoss:0.6831 | Alpha:0.0326 | SPLoss:61.2011 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [116 | 5000] LR: 0.159997\n",
      "Train | 16/16 | Loss:2.2505 | MainLoss:0.6889 | Alpha:0.0313 | SPLoss:49.9951 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [117 | 5000] LR: 0.159996\n",
      "Train | 16/16 | Loss:2.4979 | MainLoss:0.6848 | Alpha:0.0310 | SPLoss:57.8768 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [118 | 5000] LR: 0.159996\n",
      "Train | 16/16 | Loss:3.2227 | MainLoss:0.9120 | Alpha:0.0300 | SPLoss:81.3432 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [119 | 5000] LR: 0.159995\n",
      "Train | 16/16 | Loss:9.1708 | MainLoss:0.6948 | Alpha:0.0307 | SPLoss:274.6574 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [120 | 5000] LR: 0.159995\n",
      "Train | 16/16 | Loss:4.8228 | MainLoss:0.6991 | Alpha:0.0316 | SPLoss:127.6301 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [121 | 5000] LR: 0.159994\n",
      "Train | 16/16 | Loss:1.3757 | MainLoss:0.6990 | Alpha:0.0331 | SPLoss:21.4789 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [122 | 5000] LR: 0.159994\n",
      "Train | 16/16 | Loss:1.6237 | MainLoss:0.7234 | Alpha:0.0308 | SPLoss:28.4471 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [123 | 5000] LR: 0.159993\n",
      "Train | 16/16 | Loss:2.5595 | MainLoss:0.6966 | Alpha:0.0311 | SPLoss:60.0889 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [124 | 5000] LR: 0.159992\n",
      "Train | 16/16 | Loss:13.2839 | MainLoss:0.6952 | Alpha:0.0315 | SPLoss:378.6005 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [125 | 5000] LR: 0.159992\n",
      "Train | 16/16 | Loss:36.0941 | MainLoss:0.6958 | Alpha:0.0303 | SPLoss:1154.1819 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [126 | 5000] LR: 0.159991\n",
      "Train | 16/16 | Loss:24.0952 | MainLoss:0.6812 | Alpha:0.0323 | SPLoss:715.5986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [127 | 5000] LR: 0.159990\n",
      "Train | 16/16 | Loss:13.1587 | MainLoss:0.6843 | Alpha:0.0317 | SPLoss:392.2985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [128 | 5000] LR: 0.159989\n",
      "Train | 16/16 | Loss:13.8273 | MainLoss:0.7001 | Alpha:0.0315 | SPLoss:418.7163 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [129 | 5000] LR: 0.159988\n",
      "Train | 16/16 | Loss:7.7238 | MainLoss:0.7007 | Alpha:0.0314 | SPLoss:216.8123 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [130 | 5000] LR: 0.159988\n",
      "Train | 16/16 | Loss:1.8996 | MainLoss:0.6924 | Alpha:0.0307 | SPLoss:40.4209 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [131 | 5000] LR: 0.159987\n",
      "Train | 16/16 | Loss:123.8281 | MainLoss:0.6872 | Alpha:0.0310 | SPLoss:4431.9453 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [132 | 5000] LR: 0.159986\n",
      "Train | 16/16 | Loss:321.9970 | MainLoss:0.6777 | Alpha:0.0313 | SPLoss:10220.6230 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [133 | 5000] LR: 0.159985\n",
      "Train | 16/16 | Loss:95.0220 | MainLoss:0.6856 | Alpha:0.0312 | SPLoss:2888.1426 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [134 | 5000] LR: 0.159984\n",
      "Train | 16/16 | Loss:7.1990 | MainLoss:0.6931 | Alpha:0.0313 | SPLoss:223.9190 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [135 | 5000] LR: 0.159983\n",
      "Train | 16/16 | Loss:1.9934 | MainLoss:0.6866 | Alpha:0.0295 | SPLoss:44.2761 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [136 | 5000] LR: 0.159982\n",
      "Train | 16/16 | Loss:1.6053 | MainLoss:0.6801 | Alpha:0.0302 | SPLoss:30.6418 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [137 | 5000] LR: 0.159981\n",
      "Train | 16/16 | Loss:2.1000 | MainLoss:0.6803 | Alpha:0.0307 | SPLoss:46.1902 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [138 | 5000] LR: 0.159980\n",
      "Train | 16/16 | Loss:180.5640 | MainLoss:0.6814 | Alpha:0.0297 | SPLoss:5664.1416 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [139 | 5000] LR: 0.159978\n",
      "Train | 16/16 | Loss:313.2264 | MainLoss:0.6836 | Alpha:0.0303 | SPLoss:10472.6328 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [140 | 5000] LR: 0.159977\n",
      "Train | 16/16 | Loss:92.8660 | MainLoss:0.6827 | Alpha:0.0322 | SPLoss:2701.8716 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [141 | 5000] LR: 0.159976\n",
      "Train | 16/16 | Loss:9.7718 | MainLoss:0.6910 | Alpha:0.0307 | SPLoss:293.9959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [142 | 5000] LR: 0.159975\n",
      "Train | 16/16 | Loss:286.7220 | MainLoss:0.6953 | Alpha:0.0310 | SPLoss:10362.9131 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [143 | 5000] LR: 0.159973\n",
      "Train | 16/16 | Loss:603.7106 | MainLoss:0.7023 | Alpha:0.0312 | SPLoss:19374.5410 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [144 | 5000] LR: 0.159972\n",
      "Train | 16/16 | Loss:166.1884 | MainLoss:0.6954 | Alpha:0.0316 | SPLoss:5084.3784 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [145 | 5000] LR: 0.159971\n",
      "Train | 16/16 | Loss:11.2323 | MainLoss:0.6829 | Alpha:0.0320 | SPLoss:302.9520 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [146 | 5000] LR: 0.159969\n",
      "Train | 16/16 | Loss:1.7012 | MainLoss:0.6847 | Alpha:0.0320 | SPLoss:33.1876 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [147 | 5000] LR: 0.159968\n",
      "Train | 16/16 | Loss:4.7465 | MainLoss:0.6799 | Alpha:0.0321 | SPLoss:120.9847 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [148 | 5000] LR: 0.159967\n",
      "Train | 16/16 | Loss:8.0836 | MainLoss:0.6822 | Alpha:0.0328 | SPLoss:226.2924 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [149 | 5000] LR: 0.159965\n",
      "Train | 16/16 | Loss:2.8170 | MainLoss:0.6835 | Alpha:0.0304 | SPLoss:70.6072 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [150 | 5000] LR: 0.159964\n",
      "Train | 16/16 | Loss:1.0920 | MainLoss:0.6820 | Alpha:0.0312 | SPLoss:13.3925 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [151 | 5000] LR: 0.159962\n",
      "Train | 16/16 | Loss:0.8506 | MainLoss:0.6787 | Alpha:0.0315 | SPLoss:5.3021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [152 | 5000] LR: 0.159961\n",
      "Train | 16/16 | Loss:1.3552 | MainLoss:0.7425 | Alpha:0.0313 | SPLoss:19.7812 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [153 | 5000] LR: 0.159959\n",
      "Train | 16/16 | Loss:87.2758 | MainLoss:0.8440 | Alpha:0.0301 | SPLoss:2845.9160 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [154 | 5000] LR: 0.159957\n",
      "Train | 16/16 | Loss:85.7406 | MainLoss:0.7063 | Alpha:0.0322 | SPLoss:2575.6677 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [155 | 5000] LR: 0.159956\n",
      "Train | 16/16 | Loss:16.9261 | MainLoss:0.7014 | Alpha:0.0311 | SPLoss:511.5108 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [156 | 5000] LR: 0.159954\n",
      "Train | 16/16 | Loss:45.5470 | MainLoss:0.7116 | Alpha:0.0301 | SPLoss:1492.4230 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [157 | 5000] LR: 0.159952\n",
      "Train | 16/16 | Loss:47.5911 | MainLoss:0.7027 | Alpha:0.0316 | SPLoss:1488.3025 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [158 | 5000] LR: 0.159950\n",
      "Train | 16/16 | Loss:12.8923 | MainLoss:0.6967 | Alpha:0.0317 | SPLoss:394.7176 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [159 | 5000] LR: 0.159949\n",
      "Train | 16/16 | Loss:1.8470 | MainLoss:0.6963 | Alpha:0.0306 | SPLoss:39.6189 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [160 | 5000] LR: 0.159947\n",
      "Train | 16/16 | Loss:3.0405 | MainLoss:0.6943 | Alpha:0.0305 | SPLoss:75.8733 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [161 | 5000] LR: 0.159945\n",
      "Train | 16/16 | Loss:4.0748 | MainLoss:0.6963 | Alpha:0.0322 | SPLoss:106.1031 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [162 | 5000] LR: 0.159943\n",
      "Train | 16/16 | Loss:1.6228 | MainLoss:0.6878 | Alpha:0.0305 | SPLoss:30.8269 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [163 | 5000] LR: 0.159941\n",
      "Train | 16/16 | Loss:0.7952 | MainLoss:0.6873 | Alpha:0.0306 | SPLoss:3.7179 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [164 | 5000] LR: 0.159939\n",
      "Train | 16/16 | Loss:0.7337 | MainLoss:0.6868 | Alpha:0.0302 | SPLoss:1.5635 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [165 | 5000] LR: 0.159937\n",
      "Train | 16/16 | Loss:0.7571 | MainLoss:0.6871 | Alpha:0.0299 | SPLoss:2.4336 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [166 | 5000] LR: 0.159935\n",
      "Train | 16/16 | Loss:0.9337 | MainLoss:0.6859 | Alpha:0.0301 | SPLoss:8.2599 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [167 | 5000] LR: 0.159933\n",
      "Train | 16/16 | Loss:0.9890 | MainLoss:0.6864 | Alpha:0.0297 | SPLoss:10.1762 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [168 | 5000] LR: 0.159931\n",
      "Train | 16/16 | Loss:3.1333 | MainLoss:0.6823 | Alpha:0.0294 | SPLoss:75.0875 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [169 | 5000] LR: 0.159929\n",
      "Train | 16/16 | Loss:54.9260 | MainLoss:0.7927 | Alpha:0.0319 | SPLoss:1644.1415 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [170 | 5000] LR: 0.159927\n",
      "Train | 16/16 | Loss:56.4493 | MainLoss:0.6993 | Alpha:0.0297 | SPLoss:1899.4186 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [171 | 5000] LR: 0.159925\n",
      "Train | 16/16 | Loss:163.1373 | MainLoss:0.6990 | Alpha:0.0299 | SPLoss:8886.0010 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [172 | 5000] LR: 0.159923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:119459.9269 | MainLoss:0.7860 | Alpha:0.0287 | SPLoss:4072575.7500 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [173 | 5000] LR: 0.159920\n",
      "Train | 16/16 | Loss:138559.3936 | MainLoss:0.7043 | Alpha:0.0305 | SPLoss:4524346.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [174 | 5000] LR: 0.159918\n",
      "Train | 16/16 | Loss:76246.9788 | MainLoss:0.7038 | Alpha:0.0310 | SPLoss:2458637.7500 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [175 | 5000] LR: 0.159916\n",
      "Train | 16/16 | Loss:60412.3921 | MainLoss:0.7009 | Alpha:0.0316 | SPLoss:1940121.6250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [176 | 5000] LR: 0.159914\n",
      "Train | 16/16 | Loss:12868.1950 | MainLoss:0.7009 | Alpha:0.0294 | SPLoss:436519.6562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [177 | 5000] LR: 0.159911\n",
      "Train | 16/16 | Loss:1092.2496 | MainLoss:0.6958 | Alpha:0.0318 | SPLoss:34047.6289 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [178 | 5000] LR: 0.159909\n",
      "Train | 16/16 | Loss:130.7355 | MainLoss:0.6946 | Alpha:0.0319 | SPLoss:4120.5903 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [179 | 5000] LR: 0.159906\n",
      "Train | 16/16 | Loss:72.1806 | MainLoss:0.7044 | Alpha:0.0308 | SPLoss:2315.3625 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [180 | 5000] LR: 0.159904\n",
      "Train | 16/16 | Loss:16.2218 | MainLoss:0.6952 | Alpha:0.0308 | SPLoss:500.1923 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [181 | 5000] LR: 0.159901\n",
      "Train | 16/16 | Loss:2.4091 | MainLoss:0.7019 | Alpha:0.0326 | SPLoss:47.8115 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [182 | 5000] LR: 0.159899\n",
      "Train | 16/16 | Loss:8.9700 | MainLoss:0.6946 | Alpha:0.0312 | SPLoss:281.4845 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [183 | 5000] LR: 0.159896\n",
      "Train | 16/16 | Loss:11.8387 | MainLoss:0.6928 | Alpha:0.0323 | SPLoss:338.8102 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [184 | 5000] LR: 0.159894\n",
      "Train | 16/16 | Loss:8.3257 | MainLoss:0.7978 | Alpha:0.0313 | SPLoss:242.3462 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [185 | 5000] LR: 0.159891\n",
      "Train | 16/16 | Loss:8.4620 | MainLoss:0.6999 | Alpha:0.0319 | SPLoss:242.7029 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [186 | 5000] LR: 0.159889\n",
      "Train | 16/16 | Loss:3.2896 | MainLoss:0.6977 | Alpha:0.0297 | SPLoss:89.1183 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [187 | 5000] LR: 0.159886\n",
      "Train | 16/16 | Loss:1.2917 | MainLoss:0.7207 | Alpha:0.0303 | SPLoss:18.0486 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [188 | 5000] LR: 0.159883\n",
      "Train | 16/16 | Loss:1.4583 | MainLoss:0.7100 | Alpha:0.0297 | SPLoss:25.1876 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [189 | 5000] LR: 0.159881\n",
      "Train | 16/16 | Loss:1.5828 | MainLoss:0.6871 | Alpha:0.0305 | SPLoss:29.3668 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [190 | 5000] LR: 0.159878\n",
      "Train | 16/16 | Loss:23.0238 | MainLoss:0.6949 | Alpha:0.0294 | SPLoss:709.8636 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [191 | 5000] LR: 0.159875\n",
      "Train | 16/16 | Loss:116.3287 | MainLoss:0.7112 | Alpha:0.0306 | SPLoss:3777.7864 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [192 | 5000] LR: 0.159872\n",
      "Train | 16/16 | Loss:47.2012 | MainLoss:0.6975 | Alpha:0.0296 | SPLoss:1564.5033 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [193 | 5000] LR: 0.159869\n",
      "Train | 16/16 | Loss:205.4700 | MainLoss:0.7084 | Alpha:0.0315 | SPLoss:7708.1113 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [194 | 5000] LR: 0.159866\n",
      "Train | 16/16 | Loss:5687.3999 | MainLoss:0.6983 | Alpha:0.0305 | SPLoss:185868.6562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [195 | 5000] LR: 0.159863\n",
      "Train | 16/16 | Loss:3712.1727 | MainLoss:0.6897 | Alpha:0.0308 | SPLoss:120726.7422 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [196 | 5000] LR: 0.159861\n",
      "Train | 16/16 | Loss:574.5457 | MainLoss:0.6883 | Alpha:0.0312 | SPLoss:18381.9512 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [197 | 5000] LR: 0.159858\n",
      "Train | 16/16 | Loss:17.9810 | MainLoss:0.6939 | Alpha:0.0311 | SPLoss:583.6016 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [198 | 5000] LR: 0.159855\n",
      "Train | 16/16 | Loss:14.1983 | MainLoss:0.6915 | Alpha:0.0318 | SPLoss:419.2116 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [199 | 5000] LR: 0.159851\n",
      "Train | 16/16 | Loss:14.3122 | MainLoss:0.6891 | Alpha:0.0316 | SPLoss:435.1499 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [200 | 5000] LR: 0.159848\n",
      "Train | 16/16 | Loss:4.8786 | MainLoss:0.6903 | Alpha:0.0300 | SPLoss:138.3589 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.7003 | MainLoss:0.7003 | SPLoss:344.2095 | CLSLoss:0.0000 | AUROC:0.4914\n",
      "Test | 31/16 | Loss:0.7245 | MainLoss:0.7245 | SPLoss:344.2096 | CLSLoss:0.0000 | AUROC:0.0042\n",
      "\n",
      "Epoch: [201 | 5000] LR: 0.159845\n",
      "Train | 16/16 | Loss:27.5907 | MainLoss:0.7025 | Alpha:0.0299 | SPLoss:911.2056 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [202 | 5000] LR: 0.159842\n",
      "Train | 16/16 | Loss:21.8375 | MainLoss:0.6985 | Alpha:0.0313 | SPLoss:662.0165 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [203 | 5000] LR: 0.159839\n",
      "Train | 16/16 | Loss:4.4124 | MainLoss:0.6988 | Alpha:0.0315 | SPLoss:123.9716 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [204 | 5000] LR: 0.159836\n",
      "Train | 16/16 | Loss:1.7114 | MainLoss:0.7195 | Alpha:0.0319 | SPLoss:31.2544 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [205 | 5000] LR: 0.159833\n",
      "Train | 16/16 | Loss:1.2663 | MainLoss:0.6940 | Alpha:0.0316 | SPLoss:18.8692 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [206 | 5000] LR: 0.159829\n",
      "Train | 16/16 | Loss:5.2408 | MainLoss:0.6978 | Alpha:0.0320 | SPLoss:137.4362 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [207 | 5000] LR: 0.159826\n",
      "Train | 16/16 | Loss:6.7496 | MainLoss:0.7100 | Alpha:0.0313 | SPLoss:190.7746 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [208 | 5000] LR: 0.159823\n",
      "Train | 16/16 | Loss:4.1025 | MainLoss:0.7085 | Alpha:0.0326 | SPLoss:104.6502 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [209 | 5000] LR: 0.159819\n",
      "Train | 16/16 | Loss:1.7300 | MainLoss:0.6934 | Alpha:0.0316 | SPLoss:32.3309 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [210 | 5000] LR: 0.159816\n",
      "Train | 16/16 | Loss:0.9151 | MainLoss:0.6860 | Alpha:0.0311 | SPLoss:7.3158 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [211 | 5000] LR: 0.159812\n",
      "Train | 16/16 | Loss:0.8944 | MainLoss:0.6812 | Alpha:0.0323 | SPLoss:6.5624 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [212 | 5000] LR: 0.159809\n",
      "Train | 16/16 | Loss:0.9350 | MainLoss:0.6936 | Alpha:0.0314 | SPLoss:7.7400 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [213 | 5000] LR: 0.159806\n",
      "Train | 16/16 | Loss:0.9246 | MainLoss:0.6965 | Alpha:0.0311 | SPLoss:7.3402 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [214 | 5000] LR: 0.159802\n",
      "Train | 16/16 | Loss:0.9638 | MainLoss:0.7068 | Alpha:0.0308 | SPLoss:8.4835 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [215 | 5000] LR: 0.159798\n",
      "Train | 16/16 | Loss:1.5110 | MainLoss:0.6968 | Alpha:0.0313 | SPLoss:25.6707 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [216 | 5000] LR: 0.159795\n",
      "Train | 16/16 | Loss:1.1220 | MainLoss:0.6956 | Alpha:0.0318 | SPLoss:13.3353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [217 | 5000] LR: 0.159791\n",
      "Train | 16/16 | Loss:0.9677 | MainLoss:0.7075 | Alpha:0.0300 | SPLoss:8.9389 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [218 | 5000] LR: 0.159788\n",
      "Train | 16/16 | Loss:644.4578 | MainLoss:0.7443 | Alpha:0.0309 | SPLoss:22088.3086 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [219 | 5000] LR: 0.159784\n",
      "Train | 16/16 | Loss:1869.5230 | MainLoss:0.7013 | Alpha:0.0314 | SPLoss:59431.9141 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [220 | 5000] LR: 0.159780\n",
      "Train | 16/16 | Loss:577.2032 | MainLoss:0.6953 | Alpha:0.0301 | SPLoss:18297.5098 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [221 | 5000] LR: 0.159776\n",
      "Train | 16/16 | Loss:138.3467 | MainLoss:0.7048 | Alpha:0.0315 | SPLoss:4479.7524 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [222 | 5000] LR: 0.159773\n",
      "Train | 16/16 | Loss:661.0207 | MainLoss:0.6944 | Alpha:0.0309 | SPLoss:21121.9512 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [223 | 5000] LR: 0.159769\n",
      "Train | 16/16 | Loss:297.3661 | MainLoss:0.6890 | Alpha:0.0314 | SPLoss:9877.2061 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [224 | 5000] LR: 0.159765\n",
      "Train | 16/16 | Loss:37.8259 | MainLoss:0.6831 | Alpha:0.0319 | SPLoss:1257.9780 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [225 | 5000] LR: 0.159761\n",
      "Train | 16/16 | Loss:12.4398 | MainLoss:0.7100 | Alpha:0.0321 | SPLoss:372.5630 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [226 | 5000] LR: 0.159757\n",
      "Train | 16/16 | Loss:81.6157 | MainLoss:0.7214 | Alpha:0.0300 | SPLoss:2699.3784 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [227 | 5000] LR: 0.159753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:44.4559 | MainLoss:0.6994 | Alpha:0.0324 | SPLoss:1301.5159 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [228 | 5000] LR: 0.159749\n",
      "Train | 16/16 | Loss:6.8972 | MainLoss:0.7062 | Alpha:0.0312 | SPLoss:190.9130 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [229 | 5000] LR: 0.159745\n",
      "Train | 16/16 | Loss:183.4992 | MainLoss:0.6969 | Alpha:0.0306 | SPLoss:6114.9863 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [230 | 5000] LR: 0.159741\n",
      "Train | 16/16 | Loss:176.6848 | MainLoss:0.6959 | Alpha:0.0316 | SPLoss:5464.2212 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [231 | 5000] LR: 0.159737\n",
      "Train | 16/16 | Loss:34.2445 | MainLoss:0.6886 | Alpha:0.0309 | SPLoss:1048.6112 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [232 | 5000] LR: 0.159733\n",
      "Train | 16/16 | Loss:4.4017 | MainLoss:0.6853 | Alpha:0.0314 | SPLoss:116.7327 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [233 | 5000] LR: 0.159729\n",
      "Train | 16/16 | Loss:203469.6882 | MainLoss:0.9624 | Alpha:0.0305 | SPLoss:7155145.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [234 | 5000] LR: 0.159725\n",
      "Train | 16/16 | Loss:500147.4941 | MainLoss:0.7190 | Alpha:0.0305 | SPLoss:16411449.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [235 | 5000] LR: 0.159721\n",
      "Train | 16/16 | Loss:154557.6248 | MainLoss:0.6965 | Alpha:0.0304 | SPLoss:4888102.5000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [236 | 5000] LR: 0.159717\n",
      "Train | 16/16 | Loss:28172.1105 | MainLoss:0.7058 | Alpha:0.0323 | SPLoss:876081.6875 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [237 | 5000] LR: 0.159712\n",
      "Train | 16/16 | Loss:22104.2831 | MainLoss:0.7114 | Alpha:0.0311 | SPLoss:717584.5625 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [238 | 5000] LR: 0.159708\n",
      "Train | 16/16 | Loss:5990.7754 | MainLoss:0.6981 | Alpha:0.0299 | SPLoss:202063.9062 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [239 | 5000] LR: 0.159704\n",
      "Train | 16/16 | Loss:662.9439 | MainLoss:0.6946 | Alpha:0.0305 | SPLoss:23090.4277 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [240 | 5000] LR: 0.159699\n",
      "Train | 16/16 | Loss:82.2030 | MainLoss:0.7312 | Alpha:0.0307 | SPLoss:2690.8452 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [241 | 5000] LR: 0.159695\n",
      "Train | 16/16 | Loss:51.7255 | MainLoss:0.6997 | Alpha:0.0315 | SPLoss:1564.3943 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [242 | 5000] LR: 0.159691\n",
      "Train | 16/16 | Loss:12.8050 | MainLoss:0.7001 | Alpha:0.0313 | SPLoss:397.7287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [243 | 5000] LR: 0.159686\n",
      "Train | 16/16 | Loss:4.1217 | MainLoss:0.7045 | Alpha:0.0307 | SPLoss:109.4246 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [244 | 5000] LR: 0.159682\n",
      "Train | 16/16 | Loss:13.0364 | MainLoss:0.6873 | Alpha:0.0317 | SPLoss:389.4658 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [245 | 5000] LR: 0.159677\n",
      "Train | 16/16 | Loss:83.2404 | MainLoss:0.7052 | Alpha:0.0312 | SPLoss:2765.4255 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [246 | 5000] LR: 0.159673\n",
      "Train | 16/16 | Loss:184.3041 | MainLoss:0.6926 | Alpha:0.0313 | SPLoss:5797.0742 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [247 | 5000] LR: 0.159668\n",
      "Train | 16/16 | Loss:48.9083 | MainLoss:0.6895 | Alpha:0.0298 | SPLoss:1694.7764 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [248 | 5000] LR: 0.159664\n",
      "Train | 16/16 | Loss:8.6430 | MainLoss:0.6995 | Alpha:0.0322 | SPLoss:261.8933 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [249 | 5000] LR: 0.159659\n",
      "Train | 16/16 | Loss:1164.6055 | MainLoss:0.6909 | Alpha:0.0320 | SPLoss:36481.2656 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [250 | 5000] LR: 0.159654\n",
      "Train | 16/16 | Loss:1065.3580 | MainLoss:0.6917 | Alpha:0.0313 | SPLoss:33674.0312 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [251 | 5000] LR: 0.159650\n",
      "Train | 16/16 | Loss:210.9963 | MainLoss:0.6864 | Alpha:0.0309 | SPLoss:6486.2261 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [252 | 5000] LR: 0.159645\n",
      "Train | 16/16 | Loss:83.6865 | MainLoss:0.6910 | Alpha:0.0334 | SPLoss:2400.7986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [253 | 5000] LR: 0.159640\n",
      "Train | 16/16 | Loss:256.3254 | MainLoss:0.6971 | Alpha:0.0313 | SPLoss:8227.0430 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [254 | 5000] LR: 0.159635\n",
      "Train | 16/16 | Loss:96.2014 | MainLoss:0.6966 | Alpha:0.0311 | SPLoss:3160.5059 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [255 | 5000] LR: 0.159631\n",
      "Train | 16/16 | Loss:1026.0603 | MainLoss:0.6872 | Alpha:0.0305 | SPLoss:30541.0078 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [256 | 5000] LR: 0.159626\n",
      "Train | 16/16 | Loss:12895.1509 | MainLoss:0.6867 | Alpha:0.0296 | SPLoss:436155.8750 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [257 | 5000] LR: 0.159621\n",
      "Train | 16/16 | Loss:8115.9251 | MainLoss:0.6989 | Alpha:0.0322 | SPLoss:255833.8438 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [258 | 5000] LR: 0.159616\n",
      "Train | 16/16 | Loss:1187.5242 | MainLoss:0.7010 | Alpha:0.0326 | SPLoss:35288.9375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [259 | 5000] LR: 0.159611\n",
      "Train | 16/16 | Loss:22.0146 | MainLoss:0.6858 | Alpha:0.0303 | SPLoss:747.0480 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [260 | 5000] LR: 0.159606\n",
      "Train | 16/16 | Loss:40.4166 | MainLoss:0.6818 | Alpha:0.0315 | SPLoss:1262.0531 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [261 | 5000] LR: 0.159601\n",
      "Train | 16/16 | Loss:18.4098 | MainLoss:0.6889 | Alpha:0.0316 | SPLoss:566.5338 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [262 | 5000] LR: 0.159596\n",
      "Train | 16/16 | Loss:2.7455 | MainLoss:0.6878 | Alpha:0.0303 | SPLoss:70.1562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [263 | 5000] LR: 0.159591\n",
      "Train | 16/16 | Loss:6.9491 | MainLoss:0.6866 | Alpha:0.0309 | SPLoss:198.3920 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [264 | 5000] LR: 0.159586\n",
      "Train | 16/16 | Loss:17.0486 | MainLoss:0.6800 | Alpha:0.0323 | SPLoss:501.6211 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [265 | 5000] LR: 0.159581\n",
      "Train | 16/16 | Loss:6.0987 | MainLoss:0.6852 | Alpha:0.0317 | SPLoss:168.2464 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [266 | 5000] LR: 0.159576\n",
      "Train | 16/16 | Loss:4.2014 | MainLoss:0.6886 | Alpha:0.0307 | SPLoss:114.4663 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [267 | 5000] LR: 0.159570\n",
      "Train | 16/16 | Loss:3.8708 | MainLoss:0.6861 | Alpha:0.0313 | SPLoss:101.2601 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [268 | 5000] LR: 0.159565\n",
      "Train | 16/16 | Loss:2.6217 | MainLoss:0.6855 | Alpha:0.0332 | SPLoss:58.6875 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [269 | 5000] LR: 0.159560\n",
      "Train | 16/16 | Loss:1.1113 | MainLoss:0.6792 | Alpha:0.0301 | SPLoss:13.8392 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [270 | 5000] LR: 0.159555\n",
      "Train | 16/16 | Loss:75.8811 | MainLoss:0.6811 | Alpha:0.0312 | SPLoss:2433.7893 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [271 | 5000] LR: 0.159549\n",
      "Train | 16/16 | Loss:909.4352 | MainLoss:0.6815 | Alpha:0.0308 | SPLoss:29580.7227 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [272 | 5000] LR: 0.159544\n",
      "Train | 16/16 | Loss:1070.8971 | MainLoss:0.6828 | Alpha:0.0307 | SPLoss:34588.7305 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [273 | 5000] LR: 0.159539\n",
      "Train | 16/16 | Loss:248.3509 | MainLoss:0.6888 | Alpha:0.0314 | SPLoss:7532.4316 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [274 | 5000] LR: 0.159533\n",
      "Train | 16/16 | Loss:24.9670 | MainLoss:0.6908 | Alpha:0.0321 | SPLoss:752.3134 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [275 | 5000] LR: 0.159528\n",
      "Train | 16/16 | Loss:15.1627 | MainLoss:0.7018 | Alpha:0.0298 | SPLoss:485.7790 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [276 | 5000] LR: 0.159522\n",
      "Train | 16/16 | Loss:8.1244 | MainLoss:0.6941 | Alpha:0.0311 | SPLoss:237.1846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [277 | 5000] LR: 0.159517\n",
      "Train | 16/16 | Loss:55.4009 | MainLoss:0.6947 | Alpha:0.0317 | SPLoss:1905.0117 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [278 | 5000] LR: 0.159511\n",
      "Train | 16/16 | Loss:147.5806 | MainLoss:0.6898 | Alpha:0.0310 | SPLoss:4710.0938 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [279 | 5000] LR: 0.159506\n",
      "Train | 16/16 | Loss:247.9249 | MainLoss:0.6956 | Alpha:0.0330 | SPLoss:7522.8213 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [280 | 5000] LR: 0.159500\n",
      "Train | 16/16 | Loss:114.6633 | MainLoss:0.7068 | Alpha:0.0294 | SPLoss:3805.7437 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [281 | 5000] LR: 0.159495\n",
      "Train | 16/16 | Loss:23.0726 | MainLoss:0.6994 | Alpha:0.0296 | SPLoss:739.7360 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [282 | 5000] LR: 0.159489\n",
      "Train | 16/16 | Loss:10.6863 | MainLoss:0.6985 | Alpha:0.0310 | SPLoss:323.1692 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [283 | 5000] LR: 0.159483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:31.9439 | MainLoss:0.7513 | Alpha:0.0306 | SPLoss:1028.1626 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [284 | 5000] LR: 0.159477\n",
      "Train | 16/16 | Loss:48.0137 | MainLoss:0.6988 | Alpha:0.0307 | SPLoss:1566.1288 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [285 | 5000] LR: 0.159472\n",
      "Train | 16/16 | Loss:829.4462 | MainLoss:0.7273 | Alpha:0.0303 | SPLoss:26710.8809 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [286 | 5000] LR: 0.159466\n",
      "Train | 16/16 | Loss:1690.0718 | MainLoss:0.7030 | Alpha:0.0311 | SPLoss:54908.5547 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [287 | 5000] LR: 0.159460\n",
      "Train | 16/16 | Loss:1817.6112 | MainLoss:0.6995 | Alpha:0.0304 | SPLoss:58863.8984 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [288 | 5000] LR: 0.159454\n",
      "Train | 16/16 | Loss:2017.5407 | MainLoss:0.6929 | Alpha:0.0297 | SPLoss:68347.5781 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [289 | 5000] LR: 0.159448\n",
      "Train | 16/16 | Loss:546.0475 | MainLoss:0.6999 | Alpha:0.0308 | SPLoss:17665.9922 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [290 | 5000] LR: 0.159443\n",
      "Train | 16/16 | Loss:39.5711 | MainLoss:0.6950 | Alpha:0.0299 | SPLoss:1315.7400 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [291 | 5000] LR: 0.159437\n",
      "Train | 16/16 | Loss:174.6926 | MainLoss:0.6851 | Alpha:0.0312 | SPLoss:5224.0127 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [292 | 5000] LR: 0.159431\n",
      "Train | 16/16 | Loss:240.1524 | MainLoss:0.6913 | Alpha:0.0316 | SPLoss:7402.9683 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [293 | 5000] LR: 0.159425\n",
      "Train | 16/16 | Loss:55.8777 | MainLoss:0.6988 | Alpha:0.0308 | SPLoss:1859.1951 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [294 | 5000] LR: 0.159419\n",
      "Train | 16/16 | Loss:6.8594 | MainLoss:0.6983 | Alpha:0.0313 | SPLoss:217.5311 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [295 | 5000] LR: 0.159413\n",
      "Train | 16/16 | Loss:1.7341 | MainLoss:0.7007 | Alpha:0.0323 | SPLoss:33.4054 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [296 | 5000] LR: 0.159406\n",
      "Train | 16/16 | Loss:1.1817 | MainLoss:0.6955 | Alpha:0.0319 | SPLoss:16.3267 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [297 | 5000] LR: 0.159400\n",
      "Train | 16/16 | Loss:15.5174 | MainLoss:0.6952 | Alpha:0.0327 | SPLoss:446.1612 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [298 | 5000] LR: 0.159394\n",
      "Train | 16/16 | Loss:14.1984 | MainLoss:0.7003 | Alpha:0.0314 | SPLoss:431.2088 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [299 | 5000] LR: 0.159388\n",
      "Train | 16/16 | Loss:19.1112 | MainLoss:0.6990 | Alpha:0.0320 | SPLoss:591.7637 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [300 | 5000] LR: 0.159382\n",
      "Train | 16/16 | Loss:23.5745 | MainLoss:0.7149 | Alpha:0.0318 | SPLoss:718.0956 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.7033 | MainLoss:0.7033 | SPLoss:427.3228 | CLSLoss:0.0000 | AUROC:0.5006\n",
      "Test | 31/16 | Loss:0.7041 | MainLoss:0.7041 | SPLoss:427.3223 | CLSLoss:0.0000 | AUROC:0.0251\n",
      "\n",
      "Epoch: [301 | 5000] LR: 0.159375\n",
      "Train | 16/16 | Loss:7.8128 | MainLoss:0.7094 | Alpha:0.0314 | SPLoss:232.4225 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [302 | 5000] LR: 0.159369\n",
      "Train | 16/16 | Loss:275433.5289 | MainLoss:0.7542 | Alpha:0.0319 | SPLoss:8563392.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [303 | 5000] LR: 0.159363\n",
      "Train | 16/16 | Loss:397409.0889 | MainLoss:0.6984 | Alpha:0.0305 | SPLoss:12904638.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [304 | 5000] LR: 0.159357\n",
      "Train | 16/16 | Loss:97885.9078 | MainLoss:0.6989 | Alpha:0.0307 | SPLoss:3177658.5000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [305 | 5000] LR: 0.159350\n",
      "Train | 16/16 | Loss:7391.7801 | MainLoss:0.7162 | Alpha:0.0332 | SPLoss:207967.0469 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [306 | 5000] LR: 0.159344\n",
      "Train | 16/16 | Loss:2026.4192 | MainLoss:0.7144 | Alpha:0.0306 | SPLoss:65609.0469 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [307 | 5000] LR: 0.159337\n",
      "Train | 16/16 | Loss:1576.9860 | MainLoss:0.6937 | Alpha:0.0309 | SPLoss:50716.3711 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [308 | 5000] LR: 0.159331\n",
      "Train | 16/16 | Loss:303.5668 | MainLoss:0.6923 | Alpha:0.0320 | SPLoss:9154.1240 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [309 | 5000] LR: 0.159324\n",
      "Train | 16/16 | Loss:16.1887 | MainLoss:0.6982 | Alpha:0.0298 | SPLoss:508.3597 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [310 | 5000] LR: 0.159318\n",
      "Train | 16/16 | Loss:24.1932 | MainLoss:0.6906 | Alpha:0.0312 | SPLoss:746.8889 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [311 | 5000] LR: 0.159311\n",
      "Train | 16/16 | Loss:11.5273 | MainLoss:0.6839 | Alpha:0.0313 | SPLoss:347.9080 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [312 | 5000] LR: 0.159305\n",
      "Train | 16/16 | Loss:2.1689 | MainLoss:0.6836 | Alpha:0.0310 | SPLoss:49.0359 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [313 | 5000] LR: 0.159298\n",
      "Train | 16/16 | Loss:547.0706 | MainLoss:0.7129 | Alpha:0.0300 | SPLoss:20318.1582 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [314 | 5000] LR: 0.159291\n",
      "Train | 16/16 | Loss:2753.8471 | MainLoss:0.6972 | Alpha:0.0311 | SPLoss:88129.5000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [315 | 5000] LR: 0.159285\n",
      "Train | 16/16 | Loss:1017.6542 | MainLoss:0.6957 | Alpha:0.0302 | SPLoss:32802.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [316 | 5000] LR: 0.159278\n",
      "Train | 16/16 | Loss:591.2066 | MainLoss:0.6864 | Alpha:0.0306 | SPLoss:20360.1543 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [317 | 5000] LR: 0.159271\n",
      "Train | 16/16 | Loss:1499.0001 | MainLoss:0.6823 | Alpha:0.0312 | SPLoss:48243.8633 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [318 | 5000] LR: 0.159264\n",
      "Train | 16/16 | Loss:752.7172 | MainLoss:0.6843 | Alpha:0.0304 | SPLoss:25449.7656 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [319 | 5000] LR: 0.159258\n",
      "Train | 16/16 | Loss:200.9916 | MainLoss:0.6929 | Alpha:0.0306 | SPLoss:6317.7080 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [320 | 5000] LR: 0.159251\n",
      "Train | 16/16 | Loss:21.3682 | MainLoss:0.6947 | Alpha:0.0314 | SPLoss:687.7078 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [321 | 5000] LR: 0.159244\n",
      "Train | 16/16 | Loss:3.3758 | MainLoss:0.6847 | Alpha:0.0311 | SPLoss:87.7169 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [322 | 5000] LR: 0.159237\n",
      "Train | 16/16 | Loss:2.4396 | MainLoss:0.7027 | Alpha:0.0297 | SPLoss:58.8062 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [323 | 5000] LR: 0.159230\n",
      "Train | 16/16 | Loss:3.6051 | MainLoss:0.7067 | Alpha:0.0313 | SPLoss:93.1638 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [324 | 5000] LR: 0.159223\n",
      "Train | 16/16 | Loss:2.1798 | MainLoss:0.6989 | Alpha:0.0297 | SPLoss:49.8022 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [325 | 5000] LR: 0.159216\n",
      "Train | 16/16 | Loss:1.1972 | MainLoss:0.6890 | Alpha:0.0306 | SPLoss:16.5259 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [326 | 5000] LR: 0.159209\n",
      "Train | 16/16 | Loss:2.5645 | MainLoss:0.6909 | Alpha:0.0318 | SPLoss:63.8866 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [327 | 5000] LR: 0.159202\n",
      "Train | 16/16 | Loss:19.2693 | MainLoss:0.6957 | Alpha:0.0310 | SPLoss:607.0938 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [328 | 5000] LR: 0.159195\n",
      "Train | 16/16 | Loss:11.7718 | MainLoss:0.6949 | Alpha:0.0316 | SPLoss:355.7198 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [329 | 5000] LR: 0.159188\n",
      "Train | 16/16 | Loss:2.6572 | MainLoss:0.6889 | Alpha:0.0300 | SPLoss:67.3447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [330 | 5000] LR: 0.159181\n",
      "Train | 16/16 | Loss:1.5114 | MainLoss:0.6802 | Alpha:0.0316 | SPLoss:25.9684 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [331 | 5000] LR: 0.159173\n",
      "Train | 16/16 | Loss:4.2855 | MainLoss:0.6798 | Alpha:0.0310 | SPLoss:70.4156 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [332 | 5000] LR: 0.159166\n",
      "Train | 16/16 | Loss:1108.5517 | MainLoss:0.6851 | Alpha:0.0303 | SPLoss:35792.6680 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [333 | 5000] LR: 0.159159\n",
      "Train | 16/16 | Loss:1566.5314 | MainLoss:0.7145 | Alpha:0.0318 | SPLoss:48317.8086 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [334 | 5000] LR: 0.159152\n",
      "Train | 16/16 | Loss:370.9790 | MainLoss:0.6903 | Alpha:0.0307 | SPLoss:11754.7578 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [335 | 5000] LR: 0.159144\n",
      "Train | 16/16 | Loss:26.6966 | MainLoss:0.6838 | Alpha:0.0314 | SPLoss:842.2646 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [336 | 5000] LR: 0.159137\n",
      "Train | 16/16 | Loss:2.2818 | MainLoss:0.6824 | Alpha:0.0313 | SPLoss:51.7219 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [337 | 5000] LR: 0.159130\n",
      "Train | 16/16 | Loss:6.9431 | MainLoss:0.6852 | Alpha:0.0316 | SPLoss:202.9726 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [338 | 5000] LR: 0.159122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:10.7337 | MainLoss:0.6890 | Alpha:0.0323 | SPLoss:313.8523 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [339 | 5000] LR: 0.159115\n",
      "Train | 16/16 | Loss:3.6363 | MainLoss:0.6856 | Alpha:0.0315 | SPLoss:98.1804 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [340 | 5000] LR: 0.159107\n",
      "Train | 16/16 | Loss:2.4693 | MainLoss:0.6886 | Alpha:0.0308 | SPLoss:55.7826 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [341 | 5000] LR: 0.159100\n",
      "Train | 16/16 | Loss:3.2690 | MainLoss:0.6977 | Alpha:0.0321 | SPLoss:79.2791 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [342 | 5000] LR: 0.159092\n",
      "Train | 16/16 | Loss:1.3995 | MainLoss:0.6969 | Alpha:0.0318 | SPLoss:21.8399 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [343 | 5000] LR: 0.159085\n",
      "Train | 16/16 | Loss:0.8413 | MainLoss:0.6880 | Alpha:0.0333 | SPLoss:4.6764 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [344 | 5000] LR: 0.159077\n",
      "Train | 16/16 | Loss:1.6310 | MainLoss:0.6836 | Alpha:0.0330 | SPLoss:28.9838 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [345 | 5000] LR: 0.159069\n",
      "Train | 16/16 | Loss:1.9396 | MainLoss:0.7130 | Alpha:0.0320 | SPLoss:38.0340 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [346 | 5000] LR: 0.159062\n",
      "Train | 16/16 | Loss:1.2912 | MainLoss:0.6992 | Alpha:0.0293 | SPLoss:19.8846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [347 | 5000] LR: 0.159054\n",
      "Train | 16/16 | Loss:0.8273 | MainLoss:0.6969 | Alpha:0.0311 | SPLoss:4.2266 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [348 | 5000] LR: 0.159046\n",
      "Train | 16/16 | Loss:1.3718 | MainLoss:0.7012 | Alpha:0.0320 | SPLoss:23.6946 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [349 | 5000] LR: 0.159039\n",
      "Train | 16/16 | Loss:5.2690 | MainLoss:0.7015 | Alpha:0.0305 | SPLoss:152.6138 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [350 | 5000] LR: 0.159031\n",
      "Train | 16/16 | Loss:8.0533 | MainLoss:0.6887 | Alpha:0.0303 | SPLoss:240.7393 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [351 | 5000] LR: 0.159023\n",
      "Train | 16/16 | Loss:17.5191 | MainLoss:0.8030 | Alpha:0.0310 | SPLoss:561.8920 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [352 | 5000] LR: 0.159015\n",
      "Train | 16/16 | Loss:136.4353 | MainLoss:0.7070 | Alpha:0.0308 | SPLoss:4383.1440 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [353 | 5000] LR: 0.159007\n",
      "Train | 16/16 | Loss:88.9007 | MainLoss:0.6991 | Alpha:0.0306 | SPLoss:2935.4719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [354 | 5000] LR: 0.158999\n",
      "Train | 16/16 | Loss:18.9196 | MainLoss:0.6978 | Alpha:0.0309 | SPLoss:573.5494 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [355 | 5000] LR: 0.158991\n",
      "Train | 16/16 | Loss:8397.2181 | MainLoss:0.7260 | Alpha:0.0316 | SPLoss:253991.5625 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [356 | 5000] LR: 0.158983\n",
      "Train | 16/16 | Loss:8557.6670 | MainLoss:0.7022 | Alpha:0.0314 | SPLoss:271623.4375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [357 | 5000] LR: 0.158975\n",
      "Train | 16/16 | Loss:1591.0702 | MainLoss:0.6945 | Alpha:0.0302 | SPLoss:53423.8594 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [358 | 5000] LR: 0.158967\n",
      "Train | 16/16 | Loss:73.4649 | MainLoss:0.6862 | Alpha:0.0313 | SPLoss:2509.3665 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [359 | 5000] LR: 0.158959\n",
      "Train | 16/16 | Loss:15.1096 | MainLoss:0.6880 | Alpha:0.0319 | SPLoss:462.9914 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [360 | 5000] LR: 0.158951\n",
      "Train | 16/16 | Loss:40.3550 | MainLoss:0.6874 | Alpha:0.0326 | SPLoss:1230.1300 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [361 | 5000] LR: 0.158943\n",
      "Train | 16/16 | Loss:14.3072 | MainLoss:0.6814 | Alpha:0.0328 | SPLoss:410.6961 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [362 | 5000] LR: 0.158935\n",
      "Train | 16/16 | Loss:2.0319 | MainLoss:0.6854 | Alpha:0.0302 | SPLoss:42.7664 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [363 | 5000] LR: 0.158927\n",
      "Train | 16/16 | Loss:19.9231 | MainLoss:0.8867 | Alpha:0.0300 | SPLoss:584.3013 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [364 | 5000] LR: 0.158918\n",
      "Train | 16/16 | Loss:51.6815 | MainLoss:0.7234 | Alpha:0.0326 | SPLoss:1581.8436 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [365 | 5000] LR: 0.158910\n",
      "Train | 16/16 | Loss:16.2312 | MainLoss:0.7059 | Alpha:0.0316 | SPLoss:488.2693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [366 | 5000] LR: 0.158902\n",
      "Train | 16/16 | Loss:2.0026 | MainLoss:0.7105 | Alpha:0.0318 | SPLoss:41.5149 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [367 | 5000] LR: 0.158894\n",
      "Train | 16/16 | Loss:6.9935 | MainLoss:0.7146 | Alpha:0.0324 | SPLoss:194.0831 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [368 | 5000] LR: 0.158885\n",
      "Train | 16/16 | Loss:28.3764 | MainLoss:0.6967 | Alpha:0.0309 | SPLoss:888.4414 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [369 | 5000] LR: 0.158877\n",
      "Train | 16/16 | Loss:13.2612 | MainLoss:0.6912 | Alpha:0.0328 | SPLoss:395.0156 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [370 | 5000] LR: 0.158868\n",
      "Train | 16/16 | Loss:21.8988 | MainLoss:0.7266 | Alpha:0.0322 | SPLoss:641.4398 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [371 | 5000] LR: 0.158860\n",
      "Train | 16/16 | Loss:35.6273 | MainLoss:0.6986 | Alpha:0.0294 | SPLoss:1194.9369 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [372 | 5000] LR: 0.158852\n",
      "Train | 16/16 | Loss:11.8289 | MainLoss:0.6954 | Alpha:0.0311 | SPLoss:370.4104 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [373 | 5000] LR: 0.158843\n",
      "Train | 16/16 | Loss:2.1689 | MainLoss:0.6935 | Alpha:0.0307 | SPLoss:49.6324 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [374 | 5000] LR: 0.158835\n",
      "Train | 16/16 | Loss:1.3517 | MainLoss:0.6802 | Alpha:0.0313 | SPLoss:21.5809 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [375 | 5000] LR: 0.158826\n",
      "Train | 16/16 | Loss:1.7331 | MainLoss:0.6809 | Alpha:0.0311 | SPLoss:33.6256 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [376 | 5000] LR: 0.158817\n",
      "Train | 16/16 | Loss:87.1464 | MainLoss:0.6955 | Alpha:0.0324 | SPLoss:3402.6870 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [377 | 5000] LR: 0.158809\n",
      "Train | 16/16 | Loss:48957.1970 | MainLoss:0.6936 | Alpha:0.0317 | SPLoss:1503579.0000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [378 | 5000] LR: 0.158800\n",
      "Train | 16/16 | Loss:41563.6694 | MainLoss:0.6959 | Alpha:0.0300 | SPLoss:1401538.5000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [379 | 5000] LR: 0.158791\n",
      "Train | 16/16 | Loss:8861.6324 | MainLoss:0.6908 | Alpha:0.0307 | SPLoss:285950.0938 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [380 | 5000] LR: 0.158783\n",
      "Train | 16/16 | Loss:532.3119 | MainLoss:0.6932 | Alpha:0.0323 | SPLoss:14588.5234 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [381 | 5000] LR: 0.158774\n",
      "Train | 16/16 | Loss:84.3352 | MainLoss:0.8655 | Alpha:0.0332 | SPLoss:2523.8008 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [382 | 5000] LR: 0.158765\n",
      "Train | 16/16 | Loss:96.8888 | MainLoss:0.6973 | Alpha:0.0330 | SPLoss:2883.9321 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [383 | 5000] LR: 0.158756\n",
      "Train | 16/16 | Loss:27.7148 | MainLoss:0.7004 | Alpha:0.0307 | SPLoss:858.1780 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [384 | 5000] LR: 0.158747\n",
      "Train | 16/16 | Loss:11.3068 | MainLoss:0.7120 | Alpha:0.0307 | SPLoss:344.4331 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [385 | 5000] LR: 0.158739\n",
      "Train | 16/16 | Loss:7.3539 | MainLoss:0.6952 | Alpha:0.0315 | SPLoss:210.7217 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [386 | 5000] LR: 0.158730\n",
      "Train | 16/16 | Loss:2.1591 | MainLoss:0.6872 | Alpha:0.0311 | SPLoss:47.1108 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [387 | 5000] LR: 0.158721\n",
      "Train | 16/16 | Loss:0.8267 | MainLoss:0.6802 | Alpha:0.0305 | SPLoss:5.0330 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [388 | 5000] LR: 0.158712\n",
      "Train | 16/16 | Loss:26.4439 | MainLoss:0.6941 | Alpha:0.0313 | SPLoss:846.3482 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [389 | 5000] LR: 0.158703\n",
      "Train | 16/16 | Loss:72.2271 | MainLoss:0.6883 | Alpha:0.0317 | SPLoss:2265.4539 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [390 | 5000] LR: 0.158694\n",
      "Train | 16/16 | Loss:22.0277 | MainLoss:0.6801 | Alpha:0.0307 | SPLoss:705.5545 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [391 | 5000] LR: 0.158685\n",
      "Train | 16/16 | Loss:3.2213 | MainLoss:0.6853 | Alpha:0.0306 | SPLoss:80.2769 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [392 | 5000] LR: 0.158676\n",
      "Train | 16/16 | Loss:43.7544 | MainLoss:0.8894 | Alpha:0.0334 | SPLoss:1485.7566 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [393 | 5000] LR: 0.158666\n",
      "Train | 16/16 | Loss:24216.1941 | MainLoss:0.7192 | Alpha:0.0319 | SPLoss:739906.6250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [394 | 5000] LR: 0.158657\n",
      "Train | 16/16 | Loss:48497.9762 | MainLoss:0.6967 | Alpha:0.0313 | SPLoss:1550874.5000 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [395 | 5000] LR: 0.158648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:13834.8464 | MainLoss:0.6945 | Alpha:0.0318 | SPLoss:421471.1875 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [396 | 5000] LR: 0.158639\n",
      "Train | 16/16 | Loss:913.5132 | MainLoss:0.7186 | Alpha:0.0315 | SPLoss:26779.9727 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [397 | 5000] LR: 0.158630\n",
      "Train | 16/16 | Loss:52.3957 | MainLoss:0.7058 | Alpha:0.0312 | SPLoss:1646.3549 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [398 | 5000] LR: 0.158620\n",
      "Train | 16/16 | Loss:72.2282 | MainLoss:0.7034 | Alpha:0.0326 | SPLoss:2168.4756 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [399 | 5000] LR: 0.158611\n",
      "Train | 16/16 | Loss:15.3601 | MainLoss:0.6883 | Alpha:0.0315 | SPLoss:472.4902 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [400 | 5000] LR: 0.158602\n",
      "Train | 16/16 | Loss:31.7722 | MainLoss:0.8141 | Alpha:0.0319 | SPLoss:836.5258 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.6980 | MainLoss:0.6980 | SPLoss:5177.6270 | CLSLoss:0.0000 | AUROC:0.5027\n",
      "Test | 31/16 | Loss:0.6980 | MainLoss:0.6980 | SPLoss:5177.6313 | CLSLoss:0.0000 | AUROC:0.6677\n",
      "\n",
      "Epoch: [401 | 5000] LR: 0.158592\n",
      "Train | 16/16 | Loss:305.2864 | MainLoss:0.7013 | Alpha:0.0311 | SPLoss:9766.7529 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [402 | 5000] LR: 0.158583\n",
      "Train | 16/16 | Loss:537.1655 | MainLoss:0.6949 | Alpha:0.0317 | SPLoss:16891.4219 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [403 | 5000] LR: 0.158574\n",
      "Train | 16/16 | Loss:197.7488 | MainLoss:0.6943 | Alpha:0.0321 | SPLoss:6402.2837 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [404 | 5000] LR: 0.158564\n",
      "Train | 16/16 | Loss:22.8911 | MainLoss:0.7233 | Alpha:0.0295 | SPLoss:728.1686 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [405 | 5000] LR: 0.158555\n",
      "Train | 16/16 | Loss:2.1768 | MainLoss:0.6946 | Alpha:0.0309 | SPLoss:48.6278 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [406 | 5000] LR: 0.158545\n",
      "Train | 16/16 | Loss:1.7255 | MainLoss:0.6951 | Alpha:0.0299 | SPLoss:34.1659 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [407 | 5000] LR: 0.158535\n",
      "Train | 16/16 | Loss:10.4589 | MainLoss:0.9016 | Alpha:0.0310 | SPLoss:296.5534 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [408 | 5000] LR: 0.158526\n",
      "Train | 16/16 | Loss:33.0199 | MainLoss:0.7122 | Alpha:0.0300 | SPLoss:1076.9993 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [409 | 5000] LR: 0.158516\n",
      "Train | 16/16 | Loss:13.1536 | MainLoss:0.6985 | Alpha:0.0312 | SPLoss:408.8285 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [410 | 5000] LR: 0.158507\n",
      "Train | 16/16 | Loss:6.8739 | MainLoss:0.7231 | Alpha:0.0321 | SPLoss:177.5695 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [411 | 5000] LR: 0.158497\n",
      "Train | 16/16 | Loss:62.0812 | MainLoss:0.7008 | Alpha:0.0312 | SPLoss:2000.9816 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [412 | 5000] LR: 0.158487\n",
      "Train | 16/16 | Loss:42.3858 | MainLoss:0.6951 | Alpha:0.0305 | SPLoss:1387.1298 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [413 | 5000] LR: 0.158477\n",
      "Train | 16/16 | Loss:244.8612 | MainLoss:0.6997 | Alpha:0.0315 | SPLoss:7767.5781 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [414 | 5000] LR: 0.158468\n",
      "Train | 16/16 | Loss:290.3888 | MainLoss:0.7104 | Alpha:0.0310 | SPLoss:9316.2852 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [415 | 5000] LR: 0.158458\n",
      "Train | 16/16 | Loss:13341.2206 | MainLoss:0.7912 | Alpha:0.0313 | SPLoss:377089.3125 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [416 | 5000] LR: 0.158448\n",
      "Train | 16/16 | Loss:61510.3442 | MainLoss:0.7124 | Alpha:0.0307 | SPLoss:2003836.7500 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [417 | 5000] LR: 0.158438\n",
      "Train | 16/16 | Loss:25471.6852 | MainLoss:0.7009 | Alpha:0.0298 | SPLoss:801761.1875 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [418 | 5000] LR: 0.158428\n",
      "Train | 16/16 | Loss:2953.7863 | MainLoss:0.6971 | Alpha:0.0308 | SPLoss:94948.7969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [419 | 5000] LR: 0.158418\n",
      "Train | 16/16 | Loss:160.9001 | MainLoss:0.7105 | Alpha:0.0305 | SPLoss:5198.8989 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [420 | 5000] LR: 0.158408\n",
      "Train | 16/16 | Loss:82.6079 | MainLoss:0.7015 | Alpha:0.0303 | SPLoss:2733.4045 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [421 | 5000] LR: 0.158398\n",
      "Train | 16/16 | Loss:42.9717 | MainLoss:0.7035 | Alpha:0.0298 | SPLoss:1419.2684 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [422 | 5000] LR: 0.158388\n",
      "Train | 16/16 | Loss:26.6868 | MainLoss:0.6933 | Alpha:0.0309 | SPLoss:848.8528 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [423 | 5000] LR: 0.158378\n",
      "Train | 16/16 | Loss:21.8106 | MainLoss:0.6994 | Alpha:0.0315 | SPLoss:670.8846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [424 | 5000] LR: 0.158368\n",
      "Train | 16/16 | Loss:12.4462 | MainLoss:0.6982 | Alpha:0.0312 | SPLoss:372.4649 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [425 | 5000] LR: 0.158358\n",
      "Train | 16/16 | Loss:17.3344 | MainLoss:0.6961 | Alpha:0.0312 | SPLoss:532.1101 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [426 | 5000] LR: 0.158348\n",
      "Train | 16/16 | Loss:24.6869 | MainLoss:0.8355 | Alpha:0.0316 | SPLoss:789.7784 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [427 | 5000] LR: 0.158338\n",
      "Train | 16/16 | Loss:118.5522 | MainLoss:0.7085 | Alpha:0.0309 | SPLoss:3800.8770 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [428 | 5000] LR: 0.158328\n",
      "Train | 16/16 | Loss:16044.8427 | MainLoss:0.6999 | Alpha:0.0317 | SPLoss:478015.0312 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [429 | 5000] LR: 0.158317\n",
      "Train | 16/16 | Loss:21772.0727 | MainLoss:0.6970 | Alpha:0.0307 | SPLoss:706083.0625 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [430 | 5000] LR: 0.158307\n",
      "Train | 16/16 | Loss:36175.1998 | MainLoss:0.8521 | Alpha:0.0310 | SPLoss:1185362.6250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [431 | 5000] LR: 0.158297\n",
      "Train | 16/16 | Loss:31859.1252 | MainLoss:0.6957 | Alpha:0.0316 | SPLoss:988316.3750 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [432 | 5000] LR: 0.158286\n",
      "Train | 16/16 | Loss:5599.4990 | MainLoss:0.7063 | Alpha:0.0299 | SPLoss:189572.3906 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [433 | 5000] LR: 0.158276\n",
      "Train | 16/16 | Loss:420.3345 | MainLoss:0.7020 | Alpha:0.0324 | SPLoss:13278.8135 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [434 | 5000] LR: 0.158266\n",
      "Train | 16/16 | Loss:111.1704 | MainLoss:0.9203 | Alpha:0.0314 | SPLoss:3498.6680 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [435 | 5000] LR: 0.158255\n",
      "Train | 16/16 | Loss:68.1212 | MainLoss:0.7424 | Alpha:0.0300 | SPLoss:2226.9377 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [436 | 5000] LR: 0.158245\n",
      "Train | 16/16 | Loss:16.7489 | MainLoss:0.7026 | Alpha:0.0307 | SPLoss:534.4125 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [437 | 5000] LR: 0.158234\n",
      "Train | 16/16 | Loss:7.5049 | MainLoss:0.7275 | Alpha:0.0326 | SPLoss:190.4397 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [438 | 5000] LR: 0.158224\n",
      "Train | 16/16 | Loss:65.2385 | MainLoss:0.7065 | Alpha:0.0316 | SPLoss:2044.7385 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [439 | 5000] LR: 0.158213\n",
      "Train | 16/16 | Loss:39.0316 | MainLoss:0.6984 | Alpha:0.0311 | SPLoss:1236.2013 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [440 | 5000] LR: 0.158203\n",
      "Train | 16/16 | Loss:6.6076 | MainLoss:0.7143 | Alpha:0.0296 | SPLoss:195.4718 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [441 | 5000] LR: 0.158192\n",
      "Train | 16/16 | Loss:13.6286 | MainLoss:0.7238 | Alpha:0.0313 | SPLoss:416.7765 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [442 | 5000] LR: 0.158181\n",
      "Train | 16/16 | Loss:12.8503 | MainLoss:0.6950 | Alpha:0.0313 | SPLoss:381.4008 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [443 | 5000] LR: 0.158171\n",
      "Train | 16/16 | Loss:5.1905 | MainLoss:0.7051 | Alpha:0.0316 | SPLoss:148.9477 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [444 | 5000] LR: 0.158160\n",
      "Train | 16/16 | Loss:34.8320 | MainLoss:0.7000 | Alpha:0.0313 | SPLoss:1084.2944 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [445 | 5000] LR: 0.158149\n",
      "Train | 16/16 | Loss:55.3642 | MainLoss:0.7102 | Alpha:0.0305 | SPLoss:1790.0370 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [446 | 5000] LR: 0.158139\n",
      "Train | 16/16 | Loss:42.2559 | MainLoss:0.6956 | Alpha:0.0313 | SPLoss:1346.3237 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [447 | 5000] LR: 0.158128\n",
      "Train | 16/16 | Loss:9.3123 | MainLoss:0.6998 | Alpha:0.0320 | SPLoss:273.1624 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [448 | 5000] LR: 0.158117\n",
      "Train | 16/16 | Loss:1.6892 | MainLoss:0.6942 | Alpha:0.0300 | SPLoss:31.7476 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [449 | 5000] LR: 0.158106\n",
      "Train | 16/16 | Loss:35.1025 | MainLoss:0.7130 | Alpha:0.0320 | SPLoss:1018.7559 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [450 | 5000] LR: 0.158095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:44.6835 | MainLoss:0.6943 | Alpha:0.0308 | SPLoss:1404.2216 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [451 | 5000] LR: 0.158084\n",
      "Train | 16/16 | Loss:11.4943 | MainLoss:0.6972 | Alpha:0.0301 | SPLoss:363.7216 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [452 | 5000] LR: 0.158073\n",
      "Train | 16/16 | Loss:4.4025 | MainLoss:0.7360 | Alpha:0.0310 | SPLoss:120.7082 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [453 | 5000] LR: 0.158062\n",
      "Train | 16/16 | Loss:6.0700 | MainLoss:0.7815 | Alpha:0.0319 | SPLoss:165.6892 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [454 | 5000] LR: 0.158051\n",
      "Train | 16/16 | Loss:4.0726 | MainLoss:0.7033 | Alpha:0.0304 | SPLoss:111.1315 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [455 | 5000] LR: 0.158040\n",
      "Train | 16/16 | Loss:4.1749 | MainLoss:0.6999 | Alpha:0.0315 | SPLoss:107.5772 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [456 | 5000] LR: 0.158029\n",
      "Train | 16/16 | Loss:148.8926 | MainLoss:0.6880 | Alpha:0.0324 | SPLoss:4071.8362 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [457 | 5000] LR: 0.158018\n",
      "Train | 16/16 | Loss:178.3464 | MainLoss:0.6845 | Alpha:0.0311 | SPLoss:5592.7935 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [458 | 5000] LR: 0.158007\n",
      "Train | 16/16 | Loss:55.1411 | MainLoss:0.8542 | Alpha:0.0292 | SPLoss:1862.2836 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [459 | 5000] LR: 0.157996\n",
      "Train | 16/16 | Loss:27.6723 | MainLoss:0.6944 | Alpha:0.0299 | SPLoss:890.8814 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [460 | 5000] LR: 0.157985\n",
      "Train | 16/16 | Loss:7.3939 | MainLoss:0.6960 | Alpha:0.0309 | SPLoss:219.7053 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [461 | 5000] LR: 0.157973\n",
      "Train | 16/16 | Loss:4.3999 | MainLoss:0.6995 | Alpha:0.0300 | SPLoss:123.3492 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [462 | 5000] LR: 0.157962\n",
      "Train | 16/16 | Loss:3.0414 | MainLoss:0.6995 | Alpha:0.0320 | SPLoss:74.2979 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [463 | 5000] LR: 0.157951\n",
      "Train | 16/16 | Loss:8.6112 | MainLoss:0.6994 | Alpha:0.0322 | SPLoss:258.5002 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [464 | 5000] LR: 0.157940\n",
      "Train | 16/16 | Loss:698.9373 | MainLoss:0.6978 | Alpha:0.0303 | SPLoss:21944.5664 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [465 | 5000] LR: 0.157928\n",
      "Train | 16/16 | Loss:638.6559 | MainLoss:0.8151 | Alpha:0.0310 | SPLoss:20801.9297 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [466 | 5000] LR: 0.157917\n",
      "Train | 16/16 | Loss:125.4865 | MainLoss:0.6989 | Alpha:0.0304 | SPLoss:3950.7029 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [467 | 5000] LR: 0.157905\n",
      "Train | 16/16 | Loss:7.0867 | MainLoss:0.6947 | Alpha:0.0306 | SPLoss:214.6741 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [468 | 5000] LR: 0.157894\n",
      "Train | 16/16 | Loss:1.9551 | MainLoss:0.6970 | Alpha:0.0304 | SPLoss:40.8796 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [469 | 5000] LR: 0.157882\n",
      "Train | 16/16 | Loss:545.9812 | MainLoss:0.7136 | Alpha:0.0314 | SPLoss:18412.5449 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [470 | 5000] LR: 0.157871\n",
      "Train | 16/16 | Loss:909.0638 | MainLoss:0.6960 | Alpha:0.0312 | SPLoss:29030.5215 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [471 | 5000] LR: 0.157859\n",
      "Train | 16/16 | Loss:286.4912 | MainLoss:0.7040 | Alpha:0.0346 | SPLoss:8499.0400 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [472 | 5000] LR: 0.157848\n",
      "Train | 16/16 | Loss:207.9607 | MainLoss:0.6972 | Alpha:0.0298 | SPLoss:7010.1265 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [473 | 5000] LR: 0.157836\n",
      "Train | 16/16 | Loss:89.8435 | MainLoss:0.6978 | Alpha:0.0310 | SPLoss:2867.7781 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [474 | 5000] LR: 0.157825\n",
      "Train | 16/16 | Loss:12.1620 | MainLoss:0.7078 | Alpha:0.0312 | SPLoss:407.1381 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [475 | 5000] LR: 0.157813\n",
      "Train | 16/16 | Loss:25.7427 | MainLoss:0.8427 | Alpha:0.0315 | SPLoss:769.1373 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [476 | 5000] LR: 0.157801\n",
      "Train | 16/16 | Loss:49.0540 | MainLoss:0.6976 | Alpha:0.0308 | SPLoss:1578.2269 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [477 | 5000] LR: 0.157790\n",
      "Train | 16/16 | Loss:2676.8350 | MainLoss:0.7074 | Alpha:0.0312 | SPLoss:91429.9922 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [478 | 5000] LR: 0.157778\n",
      "Train | 16/16 | Loss:15953.5737 | MainLoss:0.7631 | Alpha:0.0326 | SPLoss:493900.3125 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [479 | 5000] LR: 0.157766\n",
      "Train | 16/16 | Loss:6866.2831 | MainLoss:0.7062 | Alpha:0.0326 | SPLoss:217327.2969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [480 | 5000] LR: 0.157754\n",
      "Train | 16/16 | Loss:900.8783 | MainLoss:0.6989 | Alpha:0.0324 | SPLoss:27106.2969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [481 | 5000] LR: 0.157742\n",
      "Train | 16/16 | Loss:52.8451 | MainLoss:0.6955 | Alpha:0.0316 | SPLoss:1560.7297 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [482 | 5000] LR: 0.157731\n",
      "Train | 16/16 | Loss:56.8773 | MainLoss:0.8165 | Alpha:0.0325 | SPLoss:1881.6909 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [483 | 5000] LR: 0.157719\n",
      "Train | 16/16 | Loss:406.1436 | MainLoss:0.7185 | Alpha:0.0322 | SPLoss:12585.7861 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [484 | 5000] LR: 0.157707\n",
      "Train | 16/16 | Loss:205.4147 | MainLoss:0.7358 | Alpha:0.0310 | SPLoss:6682.2437 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [485 | 5000] LR: 0.157695\n",
      "Train | 16/16 | Loss:34.2614 | MainLoss:0.6934 | Alpha:0.0317 | SPLoss:1048.4285 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [486 | 5000] LR: 0.157683\n",
      "Train | 16/16 | Loss:5.9163 | MainLoss:0.7213 | Alpha:0.0304 | SPLoss:169.0150 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [487 | 5000] LR: 0.157671\n",
      "Train | 16/16 | Loss:11.3889 | MainLoss:0.7011 | Alpha:0.0312 | SPLoss:346.4600 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [488 | 5000] LR: 0.157659\n",
      "Train | 16/16 | Loss:7.8997 | MainLoss:0.6962 | Alpha:0.0307 | SPLoss:226.2150 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [489 | 5000] LR: 0.157647\n",
      "Train | 16/16 | Loss:95.9318 | MainLoss:0.6958 | Alpha:0.0306 | SPLoss:3072.7031 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [490 | 5000] LR: 0.157634\n",
      "Train | 16/16 | Loss:81.4138 | MainLoss:0.7016 | Alpha:0.0311 | SPLoss:2576.1655 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [491 | 5000] LR: 0.157622\n",
      "Train | 16/16 | Loss:18.3818 | MainLoss:0.6975 | Alpha:0.0305 | SPLoss:594.4531 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [492 | 5000] LR: 0.157610\n",
      "Train | 16/16 | Loss:5.7985 | MainLoss:0.6944 | Alpha:0.0306 | SPLoss:163.3739 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [493 | 5000] LR: 0.157598\n",
      "Train | 16/16 | Loss:812.1418 | MainLoss:0.6971 | Alpha:0.0324 | SPLoss:25364.9004 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [494 | 5000] LR: 0.157586\n",
      "Train | 16/16 | Loss:1474.8522 | MainLoss:0.7109 | Alpha:0.0326 | SPLoss:44656.7148 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [495 | 5000] LR: 0.157573\n",
      "Train | 16/16 | Loss:339.2903 | MainLoss:0.6982 | Alpha:0.0303 | SPLoss:11109.4746 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [496 | 5000] LR: 0.157561\n",
      "Train | 16/16 | Loss:28.9698 | MainLoss:0.7049 | Alpha:0.0320 | SPLoss:895.9531 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [497 | 5000] LR: 0.157549\n",
      "Train | 16/16 | Loss:8.1544 | MainLoss:0.6944 | Alpha:0.0323 | SPLoss:230.0234 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [498 | 5000] LR: 0.157536\n",
      "Train | 16/16 | Loss:6.0134 | MainLoss:0.6984 | Alpha:0.0312 | SPLoss:168.9913 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [499 | 5000] LR: 0.157524\n",
      "Train | 16/16 | Loss:1156.4467 | MainLoss:0.7127 | Alpha:0.0309 | SPLoss:36765.4297 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [500 | 5000] LR: 0.157512\n",
      "Train | 16/16 | Loss:11232.4452 | MainLoss:0.7056 | Alpha:0.0311 | SPLoss:360425.6562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.6987 | MainLoss:0.6987 | SPLoss:317257.5312 | CLSLoss:0.0000 | AUROC:0.5087\n",
      "Test | 31/16 | Loss:0.6964 | MainLoss:0.6964 | SPLoss:317257.6875 | CLSLoss:0.0000 | AUROC:0.9997\n",
      "\n",
      "Epoch: [501 | 5000] LR: 0.015750\n",
      "Train | 16/16 | Loss:9144.4459 | MainLoss:0.6949 | Alpha:0.0304 | SPLoss:300817.9375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [502 | 5000] LR: 0.015749\n",
      "Train | 16/16 | Loss:8004.5232 | MainLoss:0.6943 | Alpha:0.0305 | SPLoss:260820.7188 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [503 | 5000] LR: 0.015747\n",
      "Train | 16/16 | Loss:6867.1285 | MainLoss:0.6930 | Alpha:0.0308 | SPLoss:222281.9375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [504 | 5000] LR: 0.015746\n",
      "Train | 16/16 | Loss:5818.1786 | MainLoss:0.6927 | Alpha:0.0307 | SPLoss:189689.1719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [505 | 5000] LR: 0.015745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:5188.7643 | MainLoss:0.6934 | Alpha:0.0321 | SPLoss:161273.7031 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [506 | 5000] LR: 0.015744\n",
      "Train | 16/16 | Loss:4229.2354 | MainLoss:0.6931 | Alpha:0.0310 | SPLoss:136361.6562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [507 | 5000] LR: 0.015742\n",
      "Train | 16/16 | Loss:3555.6655 | MainLoss:0.6927 | Alpha:0.0308 | SPLoss:115761.8516 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [508 | 5000] LR: 0.015741\n",
      "Train | 16/16 | Loss:3166.5906 | MainLoss:0.6913 | Alpha:0.0322 | SPLoss:97984.2656 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [509 | 5000] LR: 0.015740\n",
      "Train | 16/16 | Loss:2524.1900 | MainLoss:0.6900 | Alpha:0.0304 | SPLoss:82829.3672 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [510 | 5000] LR: 0.015739\n",
      "Train | 16/16 | Loss:2249.6578 | MainLoss:0.6878 | Alpha:0.0320 | SPLoss:70264.7812 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [511 | 5000] LR: 0.015737\n",
      "Train | 16/16 | Loss:1894.4795 | MainLoss:0.6927 | Alpha:0.0318 | SPLoss:59602.3242 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [512 | 5000] LR: 0.015736\n",
      "Train | 16/16 | Loss:1592.2254 | MainLoss:0.6910 | Alpha:0.0316 | SPLoss:50298.4258 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [513 | 5000] LR: 0.015735\n",
      "Train | 16/16 | Loss:1271.2002 | MainLoss:0.6857 | Alpha:0.0298 | SPLoss:42511.6172 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [514 | 5000] LR: 0.015733\n",
      "Train | 16/16 | Loss:1146.0840 | MainLoss:0.6845 | Alpha:0.0317 | SPLoss:36169.7734 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [515 | 5000] LR: 0.015732\n",
      "Train | 16/16 | Loss:991.4581 | MainLoss:0.6844 | Alpha:0.0323 | SPLoss:30703.3027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [516 | 5000] LR: 0.015731\n",
      "Train | 16/16 | Loss:795.0338 | MainLoss:0.6805 | Alpha:0.0307 | SPLoss:25954.2441 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [517 | 5000] LR: 0.015730\n",
      "Train | 16/16 | Loss:691.0681 | MainLoss:0.6849 | Alpha:0.0314 | SPLoss:22006.1191 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [518 | 5000] LR: 0.015728\n",
      "Train | 16/16 | Loss:577.6005 | MainLoss:0.6865 | Alpha:0.0309 | SPLoss:18656.1230 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [519 | 5000] LR: 0.015727\n",
      "Train | 16/16 | Loss:478.3886 | MainLoss:0.6824 | Alpha:0.0301 | SPLoss:15796.5518 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [520 | 5000] LR: 0.015726\n",
      "Train | 16/16 | Loss:428.7697 | MainLoss:0.6820 | Alpha:0.0319 | SPLoss:13445.8389 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [521 | 5000] LR: 0.015724\n",
      "Train | 16/16 | Loss:355.3339 | MainLoss:0.6831 | Alpha:0.0311 | SPLoss:11431.5986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [522 | 5000] LR: 0.015723\n",
      "Train | 16/16 | Loss:299.4461 | MainLoss:0.6817 | Alpha:0.0309 | SPLoss:9688.8584 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [523 | 5000] LR: 0.015722\n",
      "Train | 16/16 | Loss:257.4039 | MainLoss:0.6796 | Alpha:0.0314 | SPLoss:8221.2490 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [524 | 5000] LR: 0.015720\n",
      "Train | 16/16 | Loss:214.9875 | MainLoss:0.6759 | Alpha:0.0308 | SPLoss:6982.8032 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [525 | 5000] LR: 0.015719\n",
      "Train | 16/16 | Loss:191.2803 | MainLoss:0.6737 | Alpha:0.0322 | SPLoss:5909.7637 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [526 | 5000] LR: 0.015718\n",
      "Train | 16/16 | Loss:158.0336 | MainLoss:0.6741 | Alpha:0.0316 | SPLoss:4990.9014 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [527 | 5000] LR: 0.015716\n",
      "Train | 16/16 | Loss:131.6709 | MainLoss:0.6754 | Alpha:0.0311 | SPLoss:4226.4033 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [528 | 5000] LR: 0.015715\n",
      "Train | 16/16 | Loss:110.8674 | MainLoss:0.6770 | Alpha:0.0307 | SPLoss:3573.4839 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [529 | 5000] LR: 0.015714\n",
      "Train | 16/16 | Loss:96.3153 | MainLoss:0.6692 | Alpha:0.0315 | SPLoss:3028.2898 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [530 | 5000] LR: 0.015712\n",
      "Train | 16/16 | Loss:82.3441 | MainLoss:0.6742 | Alpha:0.0318 | SPLoss:2566.0981 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [531 | 5000] LR: 0.015711\n",
      "Train | 16/16 | Loss:69.3385 | MainLoss:0.6679 | Alpha:0.0316 | SPLoss:2174.5471 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [532 | 5000] LR: 0.015710\n",
      "Train | 16/16 | Loss:59.4542 | MainLoss:0.6725 | Alpha:0.0319 | SPLoss:1835.6514 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [533 | 5000] LR: 0.015708\n",
      "Train | 16/16 | Loss:50.5741 | MainLoss:0.6691 | Alpha:0.0321 | SPLoss:1552.4552 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [534 | 5000] LR: 0.015707\n",
      "Train | 16/16 | Loss:43.6913 | MainLoss:0.6656 | Alpha:0.0327 | SPLoss:1306.6759 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [535 | 5000] LR: 0.015706\n",
      "Train | 16/16 | Loss:34.5251 | MainLoss:0.6645 | Alpha:0.0307 | SPLoss:1106.0251 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [536 | 5000] LR: 0.015704\n",
      "Train | 16/16 | Loss:27.4336 | MainLoss:0.6570 | Alpha:0.0284 | SPLoss:942.0518 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [537 | 5000] LR: 0.015703\n",
      "Train | 16/16 | Loss:25.3298 | MainLoss:0.6593 | Alpha:0.0306 | SPLoss:805.8673 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [538 | 5000] LR: 0.015702\n",
      "Train | 16/16 | Loss:21.2889 | MainLoss:0.6588 | Alpha:0.0301 | SPLoss:688.8343 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [539 | 5000] LR: 0.015700\n",
      "Train | 16/16 | Loss:19.7599 | MainLoss:0.6425 | Alpha:0.0326 | SPLoss:587.6375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [540 | 5000] LR: 0.015699\n",
      "Train | 16/16 | Loss:16.4513 | MainLoss:0.6455 | Alpha:0.0318 | SPLoss:494.5353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [541 | 5000] LR: 0.015698\n",
      "Train | 16/16 | Loss:13.7485 | MainLoss:0.6387 | Alpha:0.0314 | SPLoss:418.4183 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [542 | 5000] LR: 0.015696\n",
      "Train | 16/16 | Loss:11.6798 | MainLoss:0.6460 | Alpha:0.0311 | SPLoss:355.7836 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [543 | 5000] LR: 0.015695\n",
      "Train | 16/16 | Loss:10.1671 | MainLoss:0.6395 | Alpha:0.0315 | SPLoss:301.1758 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [544 | 5000] LR: 0.015693\n",
      "Train | 16/16 | Loss:8.3519 | MainLoss:0.6276 | Alpha:0.0303 | SPLoss:255.9635 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [545 | 5000] LR: 0.015692\n",
      "Train | 16/16 | Loss:7.1967 | MainLoss:0.6206 | Alpha:0.0301 | SPLoss:217.5822 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [546 | 5000] LR: 0.015691\n",
      "Train | 16/16 | Loss:6.5742 | MainLoss:0.6139 | Alpha:0.0322 | SPLoss:185.1752 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [547 | 5000] LR: 0.015689\n",
      "Train | 16/16 | Loss:5.5688 | MainLoss:0.6705 | Alpha:0.0311 | SPLoss:157.6791 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [548 | 5000] LR: 0.015688\n",
      "Train | 16/16 | Loss:4.6846 | MainLoss:0.6656 | Alpha:0.0299 | SPLoss:134.4823 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [549 | 5000] LR: 0.015687\n",
      "Train | 16/16 | Loss:4.0667 | MainLoss:0.6300 | Alpha:0.0299 | SPLoss:114.8907 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [550 | 5000] LR: 0.015685\n",
      "Train | 16/16 | Loss:3.6812 | MainLoss:0.6157 | Alpha:0.0312 | SPLoss:98.2988 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [551 | 5000] LR: 0.015684\n",
      "Train | 16/16 | Loss:3.2423 | MainLoss:0.6073 | Alpha:0.0313 | SPLoss:83.9843 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [552 | 5000] LR: 0.015682\n",
      "Train | 16/16 | Loss:2.9806 | MainLoss:0.6005 | Alpha:0.0321 | SPLoss:73.9807 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [553 | 5000] LR: 0.015681\n",
      "Train | 16/16 | Loss:2.6092 | MainLoss:0.6057 | Alpha:0.0311 | SPLoss:64.4414 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [554 | 5000] LR: 0.015680\n",
      "Train | 16/16 | Loss:2.2672 | MainLoss:0.5951 | Alpha:0.0302 | SPLoss:55.2175 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [555 | 5000] LR: 0.015678\n",
      "Train | 16/16 | Loss:2.0346 | MainLoss:0.6075 | Alpha:0.0301 | SPLoss:47.3085 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [556 | 5000] LR: 0.015677\n",
      "Train | 16/16 | Loss:1.7906 | MainLoss:0.5960 | Alpha:0.0293 | SPLoss:40.7682 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [557 | 5000] LR: 0.015675\n",
      "Train | 16/16 | Loss:1.7249 | MainLoss:0.6129 | Alpha:0.0307 | SPLoss:36.1057 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [558 | 5000] LR: 0.015674\n",
      "Train | 16/16 | Loss:2.1712 | MainLoss:0.6504 | Alpha:0.0309 | SPLoss:49.1050 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [559 | 5000] LR: 0.015672\n",
      "Train | 16/16 | Loss:2.2900 | MainLoss:0.6086 | Alpha:0.0310 | SPLoss:54.2957 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [560 | 5000] LR: 0.015671\n",
      "Train | 16/16 | Loss:2.1474 | MainLoss:0.5961 | Alpha:0.0316 | SPLoss:49.0873 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [561 | 5000] LR: 0.015670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.8787 | MainLoss:0.5848 | Alpha:0.0306 | SPLoss:42.3693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [562 | 5000] LR: 0.015668\n",
      "Train | 16/16 | Loss:1.7141 | MainLoss:0.5931 | Alpha:0.0308 | SPLoss:36.3461 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [563 | 5000] LR: 0.015667\n",
      "Train | 16/16 | Loss:1.5532 | MainLoss:0.5848 | Alpha:0.0311 | SPLoss:31.1735 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [564 | 5000] LR: 0.015665\n",
      "Train | 16/16 | Loss:1.4348 | MainLoss:0.5785 | Alpha:0.0320 | SPLoss:26.7409 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [565 | 5000] LR: 0.015664\n",
      "Train | 16/16 | Loss:1.3037 | MainLoss:0.5826 | Alpha:0.0307 | SPLoss:23.5048 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [566 | 5000] LR: 0.015662\n",
      "Train | 16/16 | Loss:1.2496 | MainLoss:0.5889 | Alpha:0.0320 | SPLoss:20.6537 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [567 | 5000] LR: 0.015661\n",
      "Train | 16/16 | Loss:1.1737 | MainLoss:0.5862 | Alpha:0.0307 | SPLoss:19.1145 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [568 | 5000] LR: 0.015660\n",
      "Train | 16/16 | Loss:1.2858 | MainLoss:0.5571 | Alpha:0.0306 | SPLoss:23.9908 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [569 | 5000] LR: 0.015658\n",
      "Train | 16/16 | Loss:1.3327 | MainLoss:0.5569 | Alpha:0.0299 | SPLoss:25.9296 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [570 | 5000] LR: 0.015657\n",
      "Train | 16/16 | Loss:1.3214 | MainLoss:0.5658 | Alpha:0.0319 | SPLoss:23.6691 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [571 | 5000] LR: 0.015655\n",
      "Train | 16/16 | Loss:1.2036 | MainLoss:0.5825 | Alpha:0.0300 | SPLoss:20.7836 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [572 | 5000] LR: 0.015654\n",
      "Train | 16/16 | Loss:1.1477 | MainLoss:0.5762 | Alpha:0.0317 | SPLoss:18.0347 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [573 | 5000] LR: 0.015652\n",
      "Train | 16/16 | Loss:1.0351 | MainLoss:0.5604 | Alpha:0.0304 | SPLoss:15.6194 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [574 | 5000] LR: 0.015651\n",
      "Train | 16/16 | Loss:0.9721 | MainLoss:0.5504 | Alpha:0.0309 | SPLoss:13.6884 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [575 | 5000] LR: 0.015649\n",
      "Train | 16/16 | Loss:0.9143 | MainLoss:0.5435 | Alpha:0.0308 | SPLoss:11.9563 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [576 | 5000] LR: 0.015648\n",
      "Train | 16/16 | Loss:0.8669 | MainLoss:0.5550 | Alpha:0.0295 | SPLoss:10.5646 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [577 | 5000] LR: 0.015646\n",
      "Train | 16/16 | Loss:0.8266 | MainLoss:0.5382 | Alpha:0.0305 | SPLoss:9.4566 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [578 | 5000] LR: 0.015645\n",
      "Train | 16/16 | Loss:0.8165 | MainLoss:0.5552 | Alpha:0.0306 | SPLoss:8.5133 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [579 | 5000] LR: 0.015643\n",
      "Train | 16/16 | Loss:0.8025 | MainLoss:0.5663 | Alpha:0.0307 | SPLoss:7.6756 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [580 | 5000] LR: 0.015642\n",
      "Train | 16/16 | Loss:0.7592 | MainLoss:0.5453 | Alpha:0.0310 | SPLoss:6.9121 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [581 | 5000] LR: 0.015640\n",
      "Train | 16/16 | Loss:0.7368 | MainLoss:0.5430 | Alpha:0.0308 | SPLoss:6.2938 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [582 | 5000] LR: 0.015639\n",
      "Train | 16/16 | Loss:0.7135 | MainLoss:0.5339 | Alpha:0.0313 | SPLoss:5.7395 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [583 | 5000] LR: 0.015637\n",
      "Train | 16/16 | Loss:0.6928 | MainLoss:0.5280 | Alpha:0.0310 | SPLoss:5.3145 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [584 | 5000] LR: 0.015636\n",
      "Train | 16/16 | Loss:0.6930 | MainLoss:0.5356 | Alpha:0.0318 | SPLoss:4.9603 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [585 | 5000] LR: 0.015634\n",
      "Train | 16/16 | Loss:0.6760 | MainLoss:0.5364 | Alpha:0.0304 | SPLoss:4.5949 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [586 | 5000] LR: 0.015633\n",
      "Train | 16/16 | Loss:0.6488 | MainLoss:0.5206 | Alpha:0.0295 | SPLoss:4.3496 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [587 | 5000] LR: 0.015631\n",
      "Train | 16/16 | Loss:0.7271 | MainLoss:0.5957 | Alpha:0.0311 | SPLoss:4.2208 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [588 | 5000] LR: 0.015630\n",
      "Train | 16/16 | Loss:0.6864 | MainLoss:0.5615 | Alpha:0.0306 | SPLoss:4.0834 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [589 | 5000] LR: 0.015628\n",
      "Train | 16/16 | Loss:0.9316 | MainLoss:0.5377 | Alpha:0.0295 | SPLoss:14.4559 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [590 | 5000] LR: 0.015627\n",
      "Train | 16/16 | Loss:2.5690 | MainLoss:0.5657 | Alpha:0.0315 | SPLoss:63.1337 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [591 | 5000] LR: 0.015625\n",
      "Train | 16/16 | Loss:3.3203 | MainLoss:0.5897 | Alpha:0.0308 | SPLoss:88.5315 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [592 | 5000] LR: 0.015624\n",
      "Train | 16/16 | Loss:3.1792 | MainLoss:0.5730 | Alpha:0.0309 | SPLoss:84.4399 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [593 | 5000] LR: 0.015622\n",
      "Train | 16/16 | Loss:2.8867 | MainLoss:0.5649 | Alpha:0.0310 | SPLoss:74.8422 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [594 | 5000] LR: 0.015621\n",
      "Train | 16/16 | Loss:2.5920 | MainLoss:0.5562 | Alpha:0.0315 | SPLoss:64.5532 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [595 | 5000] LR: 0.015619\n",
      "Train | 16/16 | Loss:2.2658 | MainLoss:0.5351 | Alpha:0.0314 | SPLoss:55.3619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [596 | 5000] LR: 0.015618\n",
      "Train | 16/16 | Loss:2.0817 | MainLoss:0.5749 | Alpha:0.0317 | SPLoss:47.3509 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [597 | 5000] LR: 0.015616\n",
      "Train | 16/16 | Loss:1.8137 | MainLoss:0.5424 | Alpha:0.0312 | SPLoss:40.7226 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [598 | 5000] LR: 0.015615\n",
      "Train | 16/16 | Loss:1.5853 | MainLoss:0.5266 | Alpha:0.0302 | SPLoss:35.2006 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [599 | 5000] LR: 0.015613\n",
      "Train | 16/16 | Loss:1.4807 | MainLoss:0.5312 | Alpha:0.0312 | SPLoss:30.3881 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [600 | 5000] LR: 0.015612\n",
      "Train | 16/16 | Loss:1.3659 | MainLoss:0.5299 | Alpha:0.0319 | SPLoss:26.0864 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.5266 | MainLoss:0.5266 | SPLoss:24.0823 | CLSLoss:0.0000 | AUROC:0.8144\n",
      "Test | 31/16 | Loss:0.1780 | MainLoss:0.1780 | SPLoss:24.0823 | CLSLoss:0.0000 | AUROC:0.9919\n",
      "\n",
      "Epoch: [601 | 5000] LR: 0.015610\n",
      "Train | 16/16 | Loss:1.2267 | MainLoss:0.5171 | Alpha:0.0314 | SPLoss:22.6326 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [602 | 5000] LR: 0.015608\n",
      "Train | 16/16 | Loss:1.1637 | MainLoss:0.5369 | Alpha:0.0319 | SPLoss:19.5955 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [603 | 5000] LR: 0.015607\n",
      "Train | 16/16 | Loss:1.0564 | MainLoss:0.5217 | Alpha:0.0311 | SPLoss:17.1438 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [604 | 5000] LR: 0.015605\n",
      "Train | 16/16 | Loss:1.0223 | MainLoss:0.5175 | Alpha:0.0328 | SPLoss:15.3984 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [605 | 5000] LR: 0.015604\n",
      "Train | 16/16 | Loss:0.9659 | MainLoss:0.5295 | Alpha:0.0322 | SPLoss:13.6294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [606 | 5000] LR: 0.015602\n",
      "Train | 16/16 | Loss:0.9146 | MainLoss:0.5281 | Alpha:0.0324 | SPLoss:11.9127 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [607 | 5000] LR: 0.015601\n",
      "Train | 16/16 | Loss:0.8697 | MainLoss:0.5407 | Alpha:0.0311 | SPLoss:10.5976 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [608 | 5000] LR: 0.015599\n",
      "Train | 16/16 | Loss:0.8075 | MainLoss:0.5123 | Alpha:0.0310 | SPLoss:9.5108 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [609 | 5000] LR: 0.015598\n",
      "Train | 16/16 | Loss:0.7643 | MainLoss:0.5026 | Alpha:0.0308 | SPLoss:8.4985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [610 | 5000] LR: 0.015596\n",
      "Train | 16/16 | Loss:0.7550 | MainLoss:0.5085 | Alpha:0.0319 | SPLoss:7.7084 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [611 | 5000] LR: 0.015594\n",
      "Train | 16/16 | Loss:0.7107 | MainLoss:0.4896 | Alpha:0.0317 | SPLoss:6.9919 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [612 | 5000] LR: 0.015593\n",
      "Train | 16/16 | Loss:0.7097 | MainLoss:0.5078 | Alpha:0.0314 | SPLoss:6.4378 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [613 | 5000] LR: 0.015591\n",
      "Train | 16/16 | Loss:0.7033 | MainLoss:0.5118 | Alpha:0.0320 | SPLoss:5.9991 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [614 | 5000] LR: 0.015590\n",
      "Train | 16/16 | Loss:0.6738 | MainLoss:0.5107 | Alpha:0.0292 | SPLoss:5.5923 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [615 | 5000] LR: 0.015588\n",
      "Train | 16/16 | Loss:0.6286 | MainLoss:0.4711 | Alpha:0.0296 | SPLoss:5.3198 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [616 | 5000] LR: 0.015586\n",
      "Train | 16/16 | Loss:0.7072 | MainLoss:0.5492 | Alpha:0.0314 | SPLoss:5.0290 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [617 | 5000] LR: 0.015585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6757 | MainLoss:0.5215 | Alpha:0.0320 | SPLoss:4.8144 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [618 | 5000] LR: 0.015583\n",
      "Train | 16/16 | Loss:0.6637 | MainLoss:0.5159 | Alpha:0.0323 | SPLoss:4.5707 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [619 | 5000] LR: 0.015582\n",
      "Train | 16/16 | Loss:0.6205 | MainLoss:0.4856 | Alpha:0.0310 | SPLoss:4.3419 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [620 | 5000] LR: 0.015580\n",
      "Train | 16/16 | Loss:0.6544 | MainLoss:0.5205 | Alpha:0.0320 | SPLoss:4.1756 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [621 | 5000] LR: 0.015578\n",
      "Train | 16/16 | Loss:0.6469 | MainLoss:0.5146 | Alpha:0.0322 | SPLoss:4.1072 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [622 | 5000] LR: 0.015577\n",
      "Train | 16/16 | Loss:0.6304 | MainLoss:0.5090 | Alpha:0.0303 | SPLoss:3.9984 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [623 | 5000] LR: 0.015575\n",
      "Train | 16/16 | Loss:2.2536 | MainLoss:0.5129 | Alpha:0.0310 | SPLoss:56.5403 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [624 | 5000] LR: 0.015574\n",
      "Train | 16/16 | Loss:5.9729 | MainLoss:0.5014 | Alpha:0.0307 | SPLoss:177.3657 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [625 | 5000] LR: 0.015572\n",
      "Train | 16/16 | Loss:6.5029 | MainLoss:0.5056 | Alpha:0.0313 | SPLoss:191.7814 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [626 | 5000] LR: 0.015570\n",
      "Train | 16/16 | Loss:5.8295 | MainLoss:0.5000 | Alpha:0.0310 | SPLoss:171.7102 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [627 | 5000] LR: 0.015569\n",
      "Train | 16/16 | Loss:5.0435 | MainLoss:0.4966 | Alpha:0.0307 | SPLoss:147.8488 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [628 | 5000] LR: 0.015567\n",
      "Train | 16/16 | Loss:4.4440 | MainLoss:0.4913 | Alpha:0.0312 | SPLoss:126.7252 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [629 | 5000] LR: 0.015565\n",
      "Train | 16/16 | Loss:4.0364 | MainLoss:0.5490 | Alpha:0.0322 | SPLoss:108.7206 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [630 | 5000] LR: 0.015564\n",
      "Train | 16/16 | Loss:3.2804 | MainLoss:0.5131 | Alpha:0.0298 | SPLoss:92.5375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [631 | 5000] LR: 0.015562\n",
      "Train | 16/16 | Loss:2.9748 | MainLoss:0.4843 | Alpha:0.0314 | SPLoss:79.1920 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [632 | 5000] LR: 0.015561\n",
      "Train | 16/16 | Loss:2.5865 | MainLoss:0.4902 | Alpha:0.0309 | SPLoss:67.8798 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [633 | 5000] LR: 0.015559\n",
      "Train | 16/16 | Loss:2.3189 | MainLoss:0.4968 | Alpha:0.0313 | SPLoss:58.2675 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [634 | 5000] LR: 0.015557\n",
      "Train | 16/16 | Loss:2.0800 | MainLoss:0.5491 | Alpha:0.0305 | SPLoss:49.9572 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [635 | 5000] LR: 0.015556\n",
      "Train | 16/16 | Loss:1.8322 | MainLoss:0.5141 | Alpha:0.0305 | SPLoss:43.1662 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [636 | 5000] LR: 0.015554\n",
      "Train | 16/16 | Loss:1.5967 | MainLoss:0.4911 | Alpha:0.0296 | SPLoss:37.4410 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [637 | 5000] LR: 0.015552\n",
      "Train | 16/16 | Loss:1.4778 | MainLoss:0.4968 | Alpha:0.0302 | SPLoss:32.6181 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [638 | 5000] LR: 0.015551\n",
      "Train | 16/16 | Loss:1.3445 | MainLoss:0.4803 | Alpha:0.0304 | SPLoss:28.4132 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [639 | 5000] LR: 0.015549\n",
      "Train | 16/16 | Loss:1.2783 | MainLoss:0.4919 | Alpha:0.0318 | SPLoss:24.6996 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [640 | 5000] LR: 0.015547\n",
      "Train | 16/16 | Loss:1.1649 | MainLoss:0.5172 | Alpha:0.0301 | SPLoss:21.4470 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [641 | 5000] LR: 0.015546\n",
      "Train | 16/16 | Loss:1.0819 | MainLoss:0.4997 | Alpha:0.0309 | SPLoss:18.8389 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [642 | 5000] LR: 0.015544\n",
      "Train | 16/16 | Loss:1.0044 | MainLoss:0.4917 | Alpha:0.0309 | SPLoss:16.5834 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [643 | 5000] LR: 0.015542\n",
      "Train | 16/16 | Loss:0.9631 | MainLoss:0.5051 | Alpha:0.0311 | SPLoss:14.7316 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [644 | 5000] LR: 0.015541\n",
      "Train | 16/16 | Loss:0.9149 | MainLoss:0.5037 | Alpha:0.0307 | SPLoss:13.3635 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [645 | 5000] LR: 0.015539\n",
      "Train | 16/16 | Loss:0.8978 | MainLoss:0.5003 | Alpha:0.0325 | SPLoss:12.2019 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [646 | 5000] LR: 0.015537\n",
      "Train | 16/16 | Loss:130.9768 | MainLoss:0.4913 | Alpha:0.0309 | SPLoss:4131.4585 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [647 | 5000] LR: 0.015536\n",
      "Train | 16/16 | Loss:358.3928 | MainLoss:0.4874 | Alpha:0.0301 | SPLoss:11944.1338 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [648 | 5000] LR: 0.015534\n",
      "Train | 16/16 | Loss:380.6875 | MainLoss:0.4736 | Alpha:0.0299 | SPLoss:12683.8027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [649 | 5000] LR: 0.015532\n",
      "Train | 16/16 | Loss:349.8313 | MainLoss:0.4852 | Alpha:0.0307 | SPLoss:11317.4766 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [650 | 5000] LR: 0.015530\n",
      "Train | 16/16 | Loss:311.4075 | MainLoss:0.4822 | Alpha:0.0319 | SPLoss:9734.8613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [651 | 5000] LR: 0.015529\n",
      "Train | 16/16 | Loss:255.7997 | MainLoss:0.4713 | Alpha:0.0307 | SPLoss:8296.7217 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [652 | 5000] LR: 0.015527\n",
      "Train | 16/16 | Loss:219.0476 | MainLoss:0.5134 | Alpha:0.0309 | SPLoss:7053.1465 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [653 | 5000] LR: 0.015525\n",
      "Train | 16/16 | Loss:186.3968 | MainLoss:0.4754 | Alpha:0.0309 | SPLoss:6027.6714 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [654 | 5000] LR: 0.015524\n",
      "Train | 16/16 | Loss:157.6198 | MainLoss:0.4883 | Alpha:0.0306 | SPLoss:5137.6196 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [655 | 5000] LR: 0.015522\n",
      "Train | 16/16 | Loss:134.8159 | MainLoss:0.4761 | Alpha:0.0308 | SPLoss:4377.5483 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [656 | 5000] LR: 0.015520\n",
      "Train | 16/16 | Loss:119.1970 | MainLoss:0.5257 | Alpha:0.0319 | SPLoss:3732.4150 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [657 | 5000] LR: 0.015518\n",
      "Train | 16/16 | Loss:100.2403 | MainLoss:0.5231 | Alpha:0.0316 | SPLoss:3171.3950 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [658 | 5000] LR: 0.015517\n",
      "Train | 16/16 | Loss:81.3535 | MainLoss:0.4627 | Alpha:0.0301 | SPLoss:2692.7930 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [659 | 5000] LR: 0.015515\n",
      "Train | 16/16 | Loss:73.8700 | MainLoss:0.4635 | Alpha:0.0321 | SPLoss:2289.7185 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [660 | 5000] LR: 0.015513\n",
      "Train | 16/16 | Loss:61.5199 | MainLoss:0.4679 | Alpha:0.0314 | SPLoss:1936.0936 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [661 | 5000] LR: 0.015512\n",
      "Train | 16/16 | Loss:51.4954 | MainLoss:0.4899 | Alpha:0.0310 | SPLoss:1643.0295 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [662 | 5000] LR: 0.015510\n",
      "Train | 16/16 | Loss:45.7313 | MainLoss:0.5126 | Alpha:0.0324 | SPLoss:1396.4928 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [663 | 5000] LR: 0.015508\n",
      "Train | 16/16 | Loss:39.5553 | MainLoss:0.5451 | Alpha:0.0329 | SPLoss:1178.8387 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [664 | 5000] LR: 0.015506\n",
      "Train | 16/16 | Loss:32.1111 | MainLoss:0.4998 | Alpha:0.0318 | SPLoss:997.3519 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [665 | 5000] LR: 0.015505\n",
      "Train | 16/16 | Loss:27.4633 | MainLoss:0.4981 | Alpha:0.0319 | SPLoss:845.5747 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [666 | 5000] LR: 0.015503\n",
      "Train | 16/16 | Loss:22.2554 | MainLoss:0.4683 | Alpha:0.0304 | SPLoss:717.0126 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [667 | 5000] LR: 0.015501\n",
      "Train | 16/16 | Loss:19.7992 | MainLoss:0.4697 | Alpha:0.0317 | SPLoss:610.2213 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [668 | 5000] LR: 0.015499\n",
      "Train | 16/16 | Loss:16.9258 | MainLoss:0.4771 | Alpha:0.0316 | SPLoss:520.1523 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [669 | 5000] LR: 0.015498\n",
      "Train | 16/16 | Loss:14.7568 | MainLoss:0.4719 | Alpha:0.0306 | SPLoss:466.4248 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [670 | 5000] LR: 0.015496\n",
      "Train | 16/16 | Loss:13.9060 | MainLoss:0.4734 | Alpha:0.0324 | SPLoss:412.7287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [671 | 5000] LR: 0.015494\n",
      "Train | 16/16 | Loss:11.5351 | MainLoss:0.5011 | Alpha:0.0310 | SPLoss:353.9392 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [672 | 5000] LR: 0.015492\n",
      "Train | 16/16 | Loss:10.2658 | MainLoss:0.5719 | Alpha:0.0319 | SPLoss:302.7801 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [673 | 5000] LR: 0.015491\n",
      "Train | 16/16 | Loss:8.8228 | MainLoss:0.5821 | Alpha:0.0318 | SPLoss:260.0645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [674 | 5000] LR: 0.015489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:7.9119 | MainLoss:0.5458 | Alpha:0.0304 | SPLoss:241.8959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [675 | 5000] LR: 0.015487\n",
      "Train | 16/16 | Loss:7.5889 | MainLoss:0.5009 | Alpha:0.0318 | SPLoss:223.6926 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [676 | 5000] LR: 0.015485\n",
      "Train | 16/16 | Loss:6.1961 | MainLoss:0.4805 | Alpha:0.0293 | SPLoss:194.9883 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [677 | 5000] LR: 0.015484\n",
      "Train | 16/16 | Loss:5.8833 | MainLoss:0.4837 | Alpha:0.0321 | SPLoss:168.1564 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [678 | 5000] LR: 0.015482\n",
      "Train | 16/16 | Loss:4.8857 | MainLoss:0.4687 | Alpha:0.0307 | SPLoss:144.0443 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [679 | 5000] LR: 0.015480\n",
      "Train | 16/16 | Loss:4.4209 | MainLoss:0.4911 | Alpha:0.0319 | SPLoss:123.1959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [680 | 5000] LR: 0.015478\n",
      "Train | 16/16 | Loss:3.8143 | MainLoss:0.5050 | Alpha:0.0315 | SPLoss:105.3312 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [681 | 5000] LR: 0.015476\n",
      "Train | 16/16 | Loss:3.3266 | MainLoss:0.5095 | Alpha:0.0314 | SPLoss:89.9910 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [682 | 5000] LR: 0.015475\n",
      "Train | 16/16 | Loss:2.9337 | MainLoss:0.4972 | Alpha:0.0306 | SPLoss:79.5056 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [683 | 5000] LR: 0.015473\n",
      "Train | 16/16 | Loss:2.5972 | MainLoss:0.4844 | Alpha:0.0294 | SPLoss:72.1158 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [684 | 5000] LR: 0.015471\n",
      "Train | 16/16 | Loss:2.4827 | MainLoss:0.4873 | Alpha:0.0316 | SPLoss:63.1457 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [685 | 5000] LR: 0.015469\n",
      "Train | 16/16 | Loss:2.1713 | MainLoss:0.4742 | Alpha:0.0311 | SPLoss:54.5504 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [686 | 5000] LR: 0.015467\n",
      "Train | 16/16 | Loss:1.9116 | MainLoss:0.4601 | Alpha:0.0309 | SPLoss:46.7572 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [687 | 5000] LR: 0.015466\n",
      "Train | 16/16 | Loss:1.7160 | MainLoss:0.4813 | Alpha:0.0306 | SPLoss:40.5265 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [688 | 5000] LR: 0.015464\n",
      "Train | 16/16 | Loss:1.5405 | MainLoss:0.4831 | Alpha:0.0301 | SPLoss:35.2151 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [689 | 5000] LR: 0.015462\n",
      "Train | 16/16 | Loss:1.3851 | MainLoss:0.4797 | Alpha:0.0295 | SPLoss:30.6459 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [690 | 5000] LR: 0.015460\n",
      "Train | 16/16 | Loss:1.3074 | MainLoss:0.4906 | Alpha:0.0304 | SPLoss:26.9380 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [691 | 5000] LR: 0.015458\n",
      "Train | 16/16 | Loss:1.1902 | MainLoss:0.4694 | Alpha:0.0305 | SPLoss:23.6396 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [692 | 5000] LR: 0.015457\n",
      "Train | 16/16 | Loss:1.0841 | MainLoss:0.4720 | Alpha:0.0295 | SPLoss:20.7748 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [693 | 5000] LR: 0.015455\n",
      "Train | 16/16 | Loss:1.0691 | MainLoss:0.4998 | Alpha:0.0311 | SPLoss:18.3670 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [694 | 5000] LR: 0.015453\n",
      "Train | 16/16 | Loss:0.9987 | MainLoss:0.5007 | Alpha:0.0306 | SPLoss:16.2186 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [695 | 5000] LR: 0.015451\n",
      "Train | 16/16 | Loss:0.9521 | MainLoss:0.5052 | Alpha:0.0310 | SPLoss:14.3955 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [696 | 5000] LR: 0.015449\n",
      "Train | 16/16 | Loss:0.8827 | MainLoss:0.4808 | Alpha:0.0310 | SPLoss:13.0011 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [697 | 5000] LR: 0.015447\n",
      "Train | 16/16 | Loss:0.8336 | MainLoss:0.4657 | Alpha:0.0311 | SPLoss:11.8475 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [698 | 5000] LR: 0.015446\n",
      "Train | 16/16 | Loss:0.7924 | MainLoss:0.4665 | Alpha:0.0304 | SPLoss:10.7012 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [699 | 5000] LR: 0.015444\n",
      "Train | 16/16 | Loss:0.8218 | MainLoss:0.4744 | Alpha:0.0322 | SPLoss:10.7990 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [700 | 5000] LR: 0.015442\n",
      "Train | 16/16 | Loss:0.9200 | MainLoss:0.4566 | Alpha:0.0301 | SPLoss:15.5093 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4570 | MainLoss:0.4570 | SPLoss:16.6491 | CLSLoss:0.0000 | AUROC:0.8662\n",
      "Test | 31/16 | Loss:0.1767 | MainLoss:0.1767 | SPLoss:16.6491 | CLSLoss:0.0000 | AUROC:0.9862\n",
      "\n",
      "Epoch: [701 | 5000] LR: 0.015440\n",
      "Train | 16/16 | Loss:0.9824 | MainLoss:0.4897 | Alpha:0.0299 | SPLoss:16.4489 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [702 | 5000] LR: 0.015438\n",
      "Train | 16/16 | Loss:0.9437 | MainLoss:0.4708 | Alpha:0.0311 | SPLoss:15.1854 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [703 | 5000] LR: 0.015436\n",
      "Train | 16/16 | Loss:0.8985 | MainLoss:0.4784 | Alpha:0.0307 | SPLoss:13.7233 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [704 | 5000] LR: 0.015435\n",
      "Train | 16/16 | Loss:0.8983 | MainLoss:0.5053 | Alpha:0.0308 | SPLoss:12.7682 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [705 | 5000] LR: 0.015433\n",
      "Train | 16/16 | Loss:0.8356 | MainLoss:0.4814 | Alpha:0.0300 | SPLoss:11.7538 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [706 | 5000] LR: 0.015431\n",
      "Train | 16/16 | Loss:0.8339 | MainLoss:0.4865 | Alpha:0.0326 | SPLoss:10.6763 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [707 | 5000] LR: 0.015429\n",
      "Train | 16/16 | Loss:0.7756 | MainLoss:0.4770 | Alpha:0.0308 | SPLoss:9.7020 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [708 | 5000] LR: 0.015427\n",
      "Train | 16/16 | Loss:0.8186 | MainLoss:0.5479 | Alpha:0.0306 | SPLoss:8.8387 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [709 | 5000] LR: 0.015425\n",
      "Train | 16/16 | Loss:0.7382 | MainLoss:0.4886 | Alpha:0.0305 | SPLoss:8.1888 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [710 | 5000] LR: 0.015423\n",
      "Train | 16/16 | Loss:0.8285 | MainLoss:0.4712 | Alpha:0.0297 | SPLoss:12.1672 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [711 | 5000] LR: 0.015421\n",
      "Train | 16/16 | Loss:2.1806 | MainLoss:0.5019 | Alpha:0.0298 | SPLoss:55.2262 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [712 | 5000] LR: 0.015420\n",
      "Train | 16/16 | Loss:2.8964 | MainLoss:0.5373 | Alpha:0.0306 | SPLoss:77.0550 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [713 | 5000] LR: 0.015418\n",
      "Train | 16/16 | Loss:2.6625 | MainLoss:0.4857 | Alpha:0.0298 | SPLoss:73.0549 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [714 | 5000] LR: 0.015416\n",
      "Train | 16/16 | Loss:2.4355 | MainLoss:0.4867 | Alpha:0.0303 | SPLoss:64.1934 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [715 | 5000] LR: 0.015414\n",
      "Train | 16/16 | Loss:2.1450 | MainLoss:0.4411 | Alpha:0.0306 | SPLoss:55.6521 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [716 | 5000] LR: 0.015412\n",
      "Train | 16/16 | Loss:1.9157 | MainLoss:0.4873 | Alpha:0.0296 | SPLoss:48.1780 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [717 | 5000] LR: 0.015410\n",
      "Train | 16/16 | Loss:1.7723 | MainLoss:0.4907 | Alpha:0.0306 | SPLoss:41.8597 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [718 | 5000] LR: 0.015408\n",
      "Train | 16/16 | Loss:1.5718 | MainLoss:0.4995 | Alpha:0.0295 | SPLoss:36.4055 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [719 | 5000] LR: 0.015406\n",
      "Train | 16/16 | Loss:1.5228 | MainLoss:0.4948 | Alpha:0.0324 | SPLoss:31.7879 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [720 | 5000] LR: 0.015404\n",
      "Train | 16/16 | Loss:1.3685 | MainLoss:0.4926 | Alpha:0.0317 | SPLoss:27.5695 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [721 | 5000] LR: 0.015403\n",
      "Train | 16/16 | Loss:1.2020 | MainLoss:0.4626 | Alpha:0.0308 | SPLoss:23.9359 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [722 | 5000] LR: 0.015401\n",
      "Train | 16/16 | Loss:1.1277 | MainLoss:0.4648 | Alpha:0.0317 | SPLoss:20.8870 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [723 | 5000] LR: 0.015399\n",
      "Train | 16/16 | Loss:1.0201 | MainLoss:0.4566 | Alpha:0.0307 | SPLoss:18.3734 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [724 | 5000] LR: 0.015397\n",
      "Train | 16/16 | Loss:0.9958 | MainLoss:0.4679 | Alpha:0.0323 | SPLoss:16.2792 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [725 | 5000] LR: 0.015395\n",
      "Train | 16/16 | Loss:1.0501 | MainLoss:0.4969 | Alpha:0.0328 | SPLoss:16.8351 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [726 | 5000] LR: 0.015393\n",
      "Train | 16/16 | Loss:1.1088 | MainLoss:0.5331 | Alpha:0.0324 | SPLoss:17.7648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [727 | 5000] LR: 0.015391\n",
      "Train | 16/16 | Loss:1.0031 | MainLoss:0.4853 | Alpha:0.0316 | SPLoss:16.3782 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [728 | 5000] LR: 0.015389\n",
      "Train | 16/16 | Loss:0.8921 | MainLoss:0.4598 | Alpha:0.0294 | SPLoss:14.6733 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [729 | 5000] LR: 0.015387\n",
      "Train | 16/16 | Loss:0.8802 | MainLoss:0.4647 | Alpha:0.0318 | SPLoss:13.0574 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [730 | 5000] LR: 0.015385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.8473 | MainLoss:0.4945 | Alpha:0.0300 | SPLoss:11.7636 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [731 | 5000] LR: 0.015383\n",
      "Train | 16/16 | Loss:0.7886 | MainLoss:0.4735 | Alpha:0.0296 | SPLoss:10.6435 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [732 | 5000] LR: 0.015381\n",
      "Train | 16/16 | Loss:0.7809 | MainLoss:0.4784 | Alpha:0.0311 | SPLoss:9.6912 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [733 | 5000] LR: 0.015379\n",
      "Train | 16/16 | Loss:0.7529 | MainLoss:0.4683 | Alpha:0.0320 | SPLoss:8.8749 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [734 | 5000] LR: 0.015377\n",
      "Train | 16/16 | Loss:0.6813 | MainLoss:0.4406 | Alpha:0.0297 | SPLoss:8.0918 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [735 | 5000] LR: 0.015376\n",
      "Train | 16/16 | Loss:0.7048 | MainLoss:0.4721 | Alpha:0.0310 | SPLoss:7.4955 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [736 | 5000] LR: 0.015374\n",
      "Train | 16/16 | Loss:0.6949 | MainLoss:0.4731 | Alpha:0.0307 | SPLoss:7.2317 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [737 | 5000] LR: 0.015372\n",
      "Train | 16/16 | Loss:0.6921 | MainLoss:0.4663 | Alpha:0.0316 | SPLoss:7.1424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [738 | 5000] LR: 0.015370\n",
      "Train | 16/16 | Loss:0.6863 | MainLoss:0.4779 | Alpha:0.0297 | SPLoss:7.0133 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [739 | 5000] LR: 0.015368\n",
      "Train | 16/16 | Loss:0.7474 | MainLoss:0.4962 | Alpha:0.0298 | SPLoss:8.4094 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [740 | 5000] LR: 0.015366\n",
      "Train | 16/16 | Loss:0.7409 | MainLoss:0.4638 | Alpha:0.0310 | SPLoss:8.9272 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [741 | 5000] LR: 0.015364\n",
      "Train | 16/16 | Loss:0.7244 | MainLoss:0.4708 | Alpha:0.0300 | SPLoss:8.4457 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [742 | 5000] LR: 0.015362\n",
      "Train | 16/16 | Loss:0.7274 | MainLoss:0.4757 | Alpha:0.0320 | SPLoss:7.8741 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [743 | 5000] LR: 0.015360\n",
      "Train | 16/16 | Loss:0.7548 | MainLoss:0.5220 | Alpha:0.0318 | SPLoss:7.3065 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [744 | 5000] LR: 0.015358\n",
      "Train | 16/16 | Loss:0.6826 | MainLoss:0.4733 | Alpha:0.0309 | SPLoss:6.7809 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [745 | 5000] LR: 0.015356\n",
      "Train | 16/16 | Loss:0.6961 | MainLoss:0.4949 | Alpha:0.0316 | SPLoss:6.3889 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [746 | 5000] LR: 0.015354\n",
      "Train | 16/16 | Loss:0.6670 | MainLoss:0.4776 | Alpha:0.0314 | SPLoss:6.0389 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [747 | 5000] LR: 0.015352\n",
      "Train | 16/16 | Loss:0.6308 | MainLoss:0.4543 | Alpha:0.0311 | SPLoss:5.6765 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [748 | 5000] LR: 0.015350\n",
      "Train | 16/16 | Loss:0.6598 | MainLoss:0.4831 | Alpha:0.0323 | SPLoss:5.4667 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [749 | 5000] LR: 0.015348\n",
      "Train | 16/16 | Loss:0.6314 | MainLoss:0.4655 | Alpha:0.0317 | SPLoss:5.2282 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [750 | 5000] LR: 0.015346\n",
      "Train | 16/16 | Loss:0.6118 | MainLoss:0.4649 | Alpha:0.0291 | SPLoss:5.0406 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [751 | 5000] LR: 0.015344\n",
      "Train | 16/16 | Loss:0.6024 | MainLoss:0.4529 | Alpha:0.0303 | SPLoss:4.9346 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [752 | 5000] LR: 0.015342\n",
      "Train | 16/16 | Loss:0.6211 | MainLoss:0.4652 | Alpha:0.0324 | SPLoss:4.8174 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [753 | 5000] LR: 0.015340\n",
      "Train | 16/16 | Loss:0.5827 | MainLoss:0.4390 | Alpha:0.0309 | SPLoss:4.6484 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [754 | 5000] LR: 0.015338\n",
      "Train | 16/16 | Loss:0.5871 | MainLoss:0.4434 | Alpha:0.0309 | SPLoss:4.6569 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [755 | 5000] LR: 0.015336\n",
      "Train | 16/16 | Loss:0.6259 | MainLoss:0.4754 | Alpha:0.0322 | SPLoss:4.6772 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [756 | 5000] LR: 0.015334\n",
      "Train | 16/16 | Loss:33.5366 | MainLoss:0.4684 | Alpha:0.0329 | SPLoss:916.3254 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [757 | 5000] LR: 0.015332\n",
      "Train | 16/16 | Loss:64.2555 | MainLoss:0.4693 | Alpha:0.0306 | SPLoss:2076.7061 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [758 | 5000] LR: 0.015330\n",
      "Train | 16/16 | Loss:64.7262 | MainLoss:0.5007 | Alpha:0.0304 | SPLoss:2114.6162 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [759 | 5000] LR: 0.015328\n",
      "Train | 16/16 | Loss:58.0862 | MainLoss:0.4874 | Alpha:0.0308 | SPLoss:1877.4867 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [760 | 5000] LR: 0.015326\n",
      "Train | 16/16 | Loss:50.6081 | MainLoss:0.4598 | Alpha:0.0311 | SPLoss:1617.3564 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [761 | 5000] LR: 0.015324\n",
      "Train | 16/16 | Loss:42.4126 | MainLoss:0.4712 | Alpha:0.0303 | SPLoss:1376.6057 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [762 | 5000] LR: 0.015322\n",
      "Train | 16/16 | Loss:36.0188 | MainLoss:0.4622 | Alpha:0.0302 | SPLoss:1178.8213 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [763 | 5000] LR: 0.015320\n",
      "Train | 16/16 | Loss:31.4970 | MainLoss:0.4987 | Alpha:0.0306 | SPLoss:1008.2798 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [764 | 5000] LR: 0.015318\n",
      "Train | 16/16 | Loss:27.1187 | MainLoss:0.4683 | Alpha:0.0309 | SPLoss:863.0928 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [765 | 5000] LR: 0.015316\n",
      "Train | 16/16 | Loss:22.6348 | MainLoss:0.4536 | Alpha:0.0300 | SPLoss:738.0547 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [766 | 5000] LR: 0.015314\n",
      "Train | 16/16 | Loss:19.7089 | MainLoss:0.4613 | Alpha:0.0304 | SPLoss:631.8850 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [767 | 5000] LR: 0.015312\n",
      "Train | 16/16 | Loss:16.8807 | MainLoss:0.4361 | Alpha:0.0303 | SPLoss:540.6755 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [768 | 5000] LR: 0.015310\n",
      "Train | 16/16 | Loss:14.7091 | MainLoss:0.4477 | Alpha:0.0309 | SPLoss:464.7585 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [769 | 5000] LR: 0.015308\n",
      "Train | 16/16 | Loss:12.5949 | MainLoss:0.4688 | Alpha:0.0305 | SPLoss:397.8701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [770 | 5000] LR: 0.015306\n",
      "Train | 16/16 | Loss:10.6811 | MainLoss:0.4843 | Alpha:0.0299 | SPLoss:340.3131 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [771 | 5000] LR: 0.015304\n",
      "Train | 16/16 | Loss:8.9797 | MainLoss:0.4617 | Alpha:0.0292 | SPLoss:292.4865 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [772 | 5000] LR: 0.015302\n",
      "Train | 16/16 | Loss:8.1450 | MainLoss:0.4744 | Alpha:0.0305 | SPLoss:252.8477 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [773 | 5000] LR: 0.015299\n",
      "Train | 16/16 | Loss:7.4287 | MainLoss:0.5038 | Alpha:0.0320 | SPLoss:216.4234 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [774 | 5000] LR: 0.015297\n",
      "Train | 16/16 | Loss:6.2919 | MainLoss:0.4877 | Alpha:0.0314 | SPLoss:184.4673 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [775 | 5000] LR: 0.015295\n",
      "Train | 16/16 | Loss:5.2731 | MainLoss:0.4608 | Alpha:0.0305 | SPLoss:157.4261 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [776 | 5000] LR: 0.015293\n",
      "Train | 16/16 | Loss:4.9387 | MainLoss:0.4478 | Alpha:0.0333 | SPLoss:134.8725 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [777 | 5000] LR: 0.015291\n",
      "Train | 16/16 | Loss:3.8810 | MainLoss:0.4640 | Alpha:0.0297 | SPLoss:114.9854 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [778 | 5000] LR: 0.015289\n",
      "Train | 16/16 | Loss:3.5826 | MainLoss:0.4870 | Alpha:0.0313 | SPLoss:98.6282 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [779 | 5000] LR: 0.015287\n",
      "Train | 16/16 | Loss:3.0365 | MainLoss:0.4533 | Alpha:0.0304 | SPLoss:84.7642 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [780 | 5000] LR: 0.015285\n",
      "Train | 16/16 | Loss:2.7999 | MainLoss:0.4588 | Alpha:0.0320 | SPLoss:72.8697 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [781 | 5000] LR: 0.015283\n",
      "Train | 16/16 | Loss:2.3516 | MainLoss:0.4621 | Alpha:0.0302 | SPLoss:62.7579 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [782 | 5000] LR: 0.015281\n",
      "Train | 16/16 | Loss:2.1366 | MainLoss:0.4768 | Alpha:0.0306 | SPLoss:54.2850 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [783 | 5000] LR: 0.015279\n",
      "Train | 16/16 | Loss:1.8952 | MainLoss:0.4443 | Alpha:0.0308 | SPLoss:47.1583 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [784 | 5000] LR: 0.015277\n",
      "Train | 16/16 | Loss:1.7041 | MainLoss:0.4447 | Alpha:0.0308 | SPLoss:40.8068 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [785 | 5000] LR: 0.015275\n",
      "Train | 16/16 | Loss:1.5739 | MainLoss:0.4621 | Alpha:0.0315 | SPLoss:35.4305 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [786 | 5000] LR: 0.015272\n",
      "Train | 16/16 | Loss:1.4253 | MainLoss:0.4457 | Alpha:0.0319 | SPLoss:30.7497 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [787 | 5000] LR: 0.015270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.3640 | MainLoss:0.4847 | Alpha:0.0328 | SPLoss:26.6751 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [788 | 5000] LR: 0.015268\n",
      "Train | 16/16 | Loss:1.2596 | MainLoss:0.4896 | Alpha:0.0333 | SPLoss:23.1556 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [789 | 5000] LR: 0.015266\n",
      "Train | 16/16 | Loss:1.0778 | MainLoss:0.4607 | Alpha:0.0305 | SPLoss:20.1841 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [790 | 5000] LR: 0.015264\n",
      "Train | 16/16 | Loss:1.0519 | MainLoss:0.4863 | Alpha:0.0318 | SPLoss:17.8037 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [791 | 5000] LR: 0.015262\n",
      "Train | 16/16 | Loss:1.0076 | MainLoss:0.4874 | Alpha:0.0329 | SPLoss:15.7557 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [792 | 5000] LR: 0.015260\n",
      "Train | 16/16 | Loss:0.8946 | MainLoss:0.4519 | Alpha:0.0316 | SPLoss:13.9783 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [793 | 5000] LR: 0.015258\n",
      "Train | 16/16 | Loss:0.8335 | MainLoss:0.4415 | Alpha:0.0314 | SPLoss:12.4829 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [794 | 5000] LR: 0.015256\n",
      "Train | 16/16 | Loss:0.8095 | MainLoss:0.4607 | Alpha:0.0310 | SPLoss:11.2457 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [795 | 5000] LR: 0.015254\n",
      "Train | 16/16 | Loss:1.2419 | MainLoss:0.4727 | Alpha:0.0332 | SPLoss:23.5965 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [796 | 5000] LR: 0.015251\n",
      "Train | 16/16 | Loss:2.4637 | MainLoss:0.4708 | Alpha:0.0335 | SPLoss:59.6957 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [797 | 5000] LR: 0.015249\n",
      "Train | 16/16 | Loss:2.5819 | MainLoss:0.4959 | Alpha:0.0325 | SPLoss:64.1308 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [798 | 5000] LR: 0.015247\n",
      "Train | 16/16 | Loss:2.3769 | MainLoss:0.4765 | Alpha:0.0329 | SPLoss:57.8022 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [799 | 5000] LR: 0.015245\n",
      "Train | 16/16 | Loss:2.0642 | MainLoss:0.4620 | Alpha:0.0309 | SPLoss:51.8061 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [800 | 5000] LR: 0.015243\n",
      "Train | 16/16 | Loss:1.9008 | MainLoss:0.4784 | Alpha:0.0311 | SPLoss:45.8318 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4807 | MainLoss:0.4807 | SPLoss:42.7197 | CLSLoss:0.0000 | AUROC:0.8686\n",
      "Test | 31/16 | Loss:0.2757 | MainLoss:0.2757 | SPLoss:42.7196 | CLSLoss:0.0000 | AUROC:0.9522\n",
      "\n",
      "Epoch: [801 | 5000] LR: 0.015241\n",
      "Train | 16/16 | Loss:1.7401 | MainLoss:0.4917 | Alpha:0.0313 | SPLoss:40.0646 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [802 | 5000] LR: 0.015239\n",
      "Train | 16/16 | Loss:1.5068 | MainLoss:0.4567 | Alpha:0.0302 | SPLoss:34.7848 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [803 | 5000] LR: 0.015236\n",
      "Train | 16/16 | Loss:1.4472 | MainLoss:0.4887 | Alpha:0.0316 | SPLoss:30.2503 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [804 | 5000] LR: 0.015234\n",
      "Train | 16/16 | Loss:1.3101 | MainLoss:0.4837 | Alpha:0.0313 | SPLoss:26.4319 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [805 | 5000] LR: 0.015232\n",
      "Train | 16/16 | Loss:1.2574 | MainLoss:0.4999 | Alpha:0.0328 | SPLoss:23.0773 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [806 | 5000] LR: 0.015230\n",
      "Train | 16/16 | Loss:1.1187 | MainLoss:0.4901 | Alpha:0.0310 | SPLoss:20.2969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [807 | 5000] LR: 0.015228\n",
      "Train | 16/16 | Loss:1.0565 | MainLoss:0.4520 | Alpha:0.0306 | SPLoss:19.7892 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [808 | 5000] LR: 0.015226\n",
      "Train | 16/16 | Loss:1.0485 | MainLoss:0.4673 | Alpha:0.0312 | SPLoss:18.6600 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [809 | 5000] LR: 0.015224\n",
      "Train | 16/16 | Loss:0.9808 | MainLoss:0.4827 | Alpha:0.0298 | SPLoss:16.7008 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [810 | 5000] LR: 0.015221\n",
      "Train | 16/16 | Loss:0.9558 | MainLoss:0.5052 | Alpha:0.0301 | SPLoss:14.9562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [811 | 5000] LR: 0.015219\n",
      "Train | 16/16 | Loss:0.8623 | MainLoss:0.4593 | Alpha:0.0299 | SPLoss:13.4529 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [812 | 5000] LR: 0.015217\n",
      "Train | 16/16 | Loss:0.9008 | MainLoss:0.5017 | Alpha:0.0324 | SPLoss:12.3184 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [813 | 5000] LR: 0.015215\n",
      "Train | 16/16 | Loss:0.9914 | MainLoss:0.5732 | Alpha:0.0301 | SPLoss:13.9005 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [814 | 5000] LR: 0.015213\n",
      "Train | 16/16 | Loss:0.9537 | MainLoss:0.5096 | Alpha:0.0318 | SPLoss:13.9414 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [815 | 5000] LR: 0.015211\n",
      "Train | 16/16 | Loss:0.9050 | MainLoss:0.5112 | Alpha:0.0312 | SPLoss:12.5944 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [816 | 5000] LR: 0.015208\n",
      "Train | 16/16 | Loss:0.8386 | MainLoss:0.4778 | Alpha:0.0320 | SPLoss:11.2663 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [817 | 5000] LR: 0.015206\n",
      "Train | 16/16 | Loss:0.7873 | MainLoss:0.4615 | Alpha:0.0323 | SPLoss:10.0734 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [818 | 5000] LR: 0.015204\n",
      "Train | 16/16 | Loss:0.7321 | MainLoss:0.4644 | Alpha:0.0295 | SPLoss:9.0890 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [819 | 5000] LR: 0.015202\n",
      "Train | 16/16 | Loss:0.7553 | MainLoss:0.4870 | Alpha:0.0322 | SPLoss:8.3138 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [820 | 5000] LR: 0.015200\n",
      "Train | 16/16 | Loss:0.7147 | MainLoss:0.4861 | Alpha:0.0300 | SPLoss:7.6019 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [821 | 5000] LR: 0.015197\n",
      "Train | 16/16 | Loss:0.7663 | MainLoss:0.5330 | Alpha:0.0314 | SPLoss:7.4499 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [822 | 5000] LR: 0.015195\n",
      "Train | 16/16 | Loss:0.7807 | MainLoss:0.4774 | Alpha:0.0295 | SPLoss:10.2611 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [823 | 5000] LR: 0.015193\n",
      "Train | 16/16 | Loss:0.7977 | MainLoss:0.4524 | Alpha:0.0312 | SPLoss:11.0743 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [824 | 5000] LR: 0.015191\n",
      "Train | 16/16 | Loss:0.8640 | MainLoss:0.5147 | Alpha:0.0317 | SPLoss:11.0328 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [825 | 5000] LR: 0.015189\n",
      "Train | 16/16 | Loss:0.7932 | MainLoss:0.4710 | Alpha:0.0304 | SPLoss:10.6193 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [826 | 5000] LR: 0.015186\n",
      "Train | 16/16 | Loss:0.7517 | MainLoss:0.4531 | Alpha:0.0304 | SPLoss:9.7880 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [827 | 5000] LR: 0.015184\n",
      "Train | 16/16 | Loss:0.7549 | MainLoss:0.4697 | Alpha:0.0319 | SPLoss:8.9535 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [828 | 5000] LR: 0.015182\n",
      "Train | 16/16 | Loss:0.7075 | MainLoss:0.4567 | Alpha:0.0305 | SPLoss:8.2093 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [829 | 5000] LR: 0.015180\n",
      "Train | 16/16 | Loss:0.7631 | MainLoss:0.5225 | Alpha:0.0312 | SPLoss:7.7017 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [830 | 5000] LR: 0.015178\n",
      "Train | 16/16 | Loss:3.2529 | MainLoss:0.5194 | Alpha:0.0314 | SPLoss:85.2697 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [831 | 5000] LR: 0.015175\n",
      "Train | 16/16 | Loss:7.4042 | MainLoss:0.4984 | Alpha:0.0333 | SPLoss:206.7390 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [832 | 5000] LR: 0.015173\n",
      "Train | 16/16 | Loss:6.9939 | MainLoss:0.4679 | Alpha:0.0307 | SPLoss:212.4810 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [833 | 5000] LR: 0.015171\n",
      "Train | 16/16 | Loss:8.8709 | MainLoss:0.4447 | Alpha:0.0302 | SPLoss:281.9474 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [834 | 5000] LR: 0.015169\n",
      "Train | 16/16 | Loss:18.2002 | MainLoss:0.4339 | Alpha:0.0306 | SPLoss:578.4915 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [835 | 5000] LR: 0.015166\n",
      "Train | 16/16 | Loss:19.9576 | MainLoss:0.4450 | Alpha:0.0316 | SPLoss:617.7074 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [836 | 5000] LR: 0.015164\n",
      "Train | 16/16 | Loss:17.7376 | MainLoss:0.4575 | Alpha:0.0312 | SPLoss:552.4791 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [837 | 5000] LR: 0.015162\n",
      "Train | 16/16 | Loss:15.3411 | MainLoss:0.5740 | Alpha:0.0309 | SPLoss:478.0598 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [838 | 5000] LR: 0.015160\n",
      "Train | 16/16 | Loss:13.7644 | MainLoss:0.5512 | Alpha:0.0322 | SPLoss:410.9020 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [839 | 5000] LR: 0.015157\n",
      "Train | 16/16 | Loss:11.1681 | MainLoss:0.4800 | Alpha:0.0304 | SPLoss:350.9352 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [840 | 5000] LR: 0.015155\n",
      "Train | 16/16 | Loss:9.7847 | MainLoss:0.5055 | Alpha:0.0309 | SPLoss:301.0715 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [841 | 5000] LR: 0.015153\n",
      "Train | 16/16 | Loss:8.7060 | MainLoss:0.4701 | Alpha:0.0318 | SPLoss:256.9122 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [842 | 5000] LR: 0.015151\n",
      "Train | 16/16 | Loss:7.1601 | MainLoss:0.4886 | Alpha:0.0304 | SPLoss:219.7065 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [843 | 5000] LR: 0.015148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:6.3031 | MainLoss:0.4670 | Alpha:0.0309 | SPLoss:189.1608 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [844 | 5000] LR: 0.015146\n",
      "Train | 16/16 | Loss:5.5932 | MainLoss:0.4833 | Alpha:0.0315 | SPLoss:162.1399 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [845 | 5000] LR: 0.015144\n",
      "Train | 16/16 | Loss:4.8518 | MainLoss:0.4746 | Alpha:0.0316 | SPLoss:138.7272 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [846 | 5000] LR: 0.015142\n",
      "Train | 16/16 | Loss:4.0681 | MainLoss:0.5007 | Alpha:0.0300 | SPLoss:118.8056 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [847 | 5000] LR: 0.015139\n",
      "Train | 16/16 | Loss:3.6508 | MainLoss:0.4663 | Alpha:0.0304 | SPLoss:104.9168 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [848 | 5000] LR: 0.015137\n",
      "Train | 16/16 | Loss:3.3321 | MainLoss:0.4474 | Alpha:0.0307 | SPLoss:93.9011 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [849 | 5000] LR: 0.015135\n",
      "Train | 16/16 | Loss:3.1005 | MainLoss:0.4762 | Alpha:0.0318 | SPLoss:82.7132 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [850 | 5000] LR: 0.015133\n",
      "Train | 16/16 | Loss:2.6244 | MainLoss:0.4590 | Alpha:0.0303 | SPLoss:71.7519 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [851 | 5000] LR: 0.015130\n",
      "Train | 16/16 | Loss:2.4329 | MainLoss:0.5159 | Alpha:0.0309 | SPLoss:61.9042 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [852 | 5000] LR: 0.015128\n",
      "Train | 16/16 | Loss:2.2335 | MainLoss:0.4797 | Alpha:0.0328 | SPLoss:53.5383 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [853 | 5000] LR: 0.015126\n",
      "Train | 16/16 | Loss:1.8164 | MainLoss:0.4622 | Alpha:0.0294 | SPLoss:46.1294 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [854 | 5000] LR: 0.015123\n",
      "Train | 16/16 | Loss:1.6899 | MainLoss:0.4512 | Alpha:0.0309 | SPLoss:40.1159 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [855 | 5000] LR: 0.015121\n",
      "Train | 16/16 | Loss:1.6148 | MainLoss:0.4726 | Alpha:0.0328 | SPLoss:34.8296 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [856 | 5000] LR: 0.015119\n",
      "Train | 16/16 | Loss:1.3919 | MainLoss:0.4901 | Alpha:0.0299 | SPLoss:30.2268 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [857 | 5000] LR: 0.015117\n",
      "Train | 16/16 | Loss:1.3095 | MainLoss:0.4819 | Alpha:0.0313 | SPLoss:26.5738 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [858 | 5000] LR: 0.015114\n",
      "Train | 16/16 | Loss:1.2083 | MainLoss:0.4814 | Alpha:0.0312 | SPLoss:23.2590 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [859 | 5000] LR: 0.015112\n",
      "Train | 16/16 | Loss:1.1306 | MainLoss:0.4784 | Alpha:0.0319 | SPLoss:20.4832 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [860 | 5000] LR: 0.015110\n",
      "Train | 16/16 | Loss:1.0399 | MainLoss:0.4914 | Alpha:0.0303 | SPLoss:18.1217 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [861 | 5000] LR: 0.015107\n",
      "Train | 16/16 | Loss:1.1545 | MainLoss:0.5211 | Alpha:0.0310 | SPLoss:20.4105 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [862 | 5000] LR: 0.015105\n",
      "Train | 16/16 | Loss:1.7332 | MainLoss:0.4888 | Alpha:0.0296 | SPLoss:42.0903 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [863 | 5000] LR: 0.015103\n",
      "Train | 16/16 | Loss:1.9052 | MainLoss:0.4612 | Alpha:0.0306 | SPLoss:47.1379 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [864 | 5000] LR: 0.015100\n",
      "Train | 16/16 | Loss:1.7729 | MainLoss:0.4516 | Alpha:0.0306 | SPLoss:43.2472 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [865 | 5000] LR: 0.015098\n",
      "Train | 16/16 | Loss:1.6550 | MainLoss:0.4753 | Alpha:0.0310 | SPLoss:37.9986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [866 | 5000] LR: 0.015096\n",
      "Train | 16/16 | Loss:1.5051 | MainLoss:0.4671 | Alpha:0.0313 | SPLoss:33.1648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [867 | 5000] LR: 0.015094\n",
      "Train | 16/16 | Loss:1.3309 | MainLoss:0.4462 | Alpha:0.0305 | SPLoss:28.9061 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [868 | 5000] LR: 0.015091\n",
      "Train | 16/16 | Loss:1.2358 | MainLoss:0.4535 | Alpha:0.0308 | SPLoss:25.3318 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [869 | 5000] LR: 0.015089\n",
      "Train | 16/16 | Loss:1.2001 | MainLoss:0.4817 | Alpha:0.0322 | SPLoss:22.2431 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [870 | 5000] LR: 0.015087\n",
      "Train | 16/16 | Loss:1.0845 | MainLoss:0.4851 | Alpha:0.0306 | SPLoss:19.5365 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [871 | 5000] LR: 0.015084\n",
      "Train | 16/16 | Loss:1.0068 | MainLoss:0.4824 | Alpha:0.0302 | SPLoss:17.3589 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [872 | 5000] LR: 0.015082\n",
      "Train | 16/16 | Loss:0.9724 | MainLoss:0.4787 | Alpha:0.0317 | SPLoss:15.6201 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [873 | 5000] LR: 0.015080\n",
      "Train | 16/16 | Loss:0.9149 | MainLoss:0.4708 | Alpha:0.0319 | SPLoss:13.9970 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [874 | 5000] LR: 0.015077\n",
      "Train | 16/16 | Loss:0.8542 | MainLoss:0.4579 | Alpha:0.0318 | SPLoss:12.4835 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [875 | 5000] LR: 0.015075\n",
      "Train | 16/16 | Loss:0.8722 | MainLoss:0.5046 | Alpha:0.0329 | SPLoss:11.1592 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [876 | 5000] LR: 0.015072\n",
      "Train | 16/16 | Loss:0.8252 | MainLoss:0.5032 | Alpha:0.0320 | SPLoss:10.0859 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [877 | 5000] LR: 0.015070\n",
      "Train | 16/16 | Loss:1.0733 | MainLoss:0.4717 | Alpha:0.0320 | SPLoss:19.6332 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [878 | 5000] LR: 0.015068\n",
      "Train | 16/16 | Loss:1.9854 | MainLoss:0.4674 | Alpha:0.0318 | SPLoss:47.8521 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [879 | 5000] LR: 0.015065\n",
      "Train | 16/16 | Loss:2.1791 | MainLoss:0.4462 | Alpha:0.0322 | SPLoss:53.7586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [880 | 5000] LR: 0.015063\n",
      "Train | 16/16 | Loss:1.9726 | MainLoss:0.4915 | Alpha:0.0302 | SPLoss:49.0178 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [881 | 5000] LR: 0.015061\n",
      "Train | 16/16 | Loss:1.8917 | MainLoss:0.5097 | Alpha:0.0321 | SPLoss:43.0156 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [882 | 5000] LR: 0.015058\n",
      "Train | 16/16 | Loss:1.7758 | MainLoss:0.6540 | Alpha:0.0294 | SPLoss:38.1595 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [883 | 5000] LR: 0.015056\n",
      "Train | 16/16 | Loss:1.7079 | MainLoss:0.6466 | Alpha:0.0297 | SPLoss:35.7167 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [884 | 5000] LR: 0.015054\n",
      "Train | 16/16 | Loss:1.7120 | MainLoss:0.6115 | Alpha:0.0301 | SPLoss:36.4447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [885 | 5000] LR: 0.015051\n",
      "Train | 16/16 | Loss:1.7301 | MainLoss:0.5774 | Alpha:0.0324 | SPLoss:35.6647 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [886 | 5000] LR: 0.015049\n",
      "Train | 16/16 | Loss:1.5174 | MainLoss:0.5458 | Alpha:0.0307 | SPLoss:31.6378 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [887 | 5000] LR: 0.015046\n",
      "Train | 16/16 | Loss:1.3529 | MainLoss:0.5147 | Alpha:0.0303 | SPLoss:27.6560 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [888 | 5000] LR: 0.015044\n",
      "Train | 16/16 | Loss:1.2031 | MainLoss:0.4818 | Alpha:0.0297 | SPLoss:24.2794 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [889 | 5000] LR: 0.015042\n",
      "Train | 16/16 | Loss:1.1350 | MainLoss:0.4843 | Alpha:0.0303 | SPLoss:21.5253 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [890 | 5000] LR: 0.015039\n",
      "Train | 16/16 | Loss:1.1106 | MainLoss:0.5201 | Alpha:0.0310 | SPLoss:18.9953 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [891 | 5000] LR: 0.015037\n",
      "Train | 16/16 | Loss:1.0332 | MainLoss:0.5108 | Alpha:0.0311 | SPLoss:16.8386 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [892 | 5000] LR: 0.015035\n",
      "Train | 16/16 | Loss:0.9530 | MainLoss:0.5013 | Alpha:0.0302 | SPLoss:14.9567 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [893 | 5000] LR: 0.015032\n",
      "Train | 16/16 | Loss:0.9045 | MainLoss:0.4842 | Alpha:0.0315 | SPLoss:13.3811 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [894 | 5000] LR: 0.015030\n",
      "Train | 16/16 | Loss:0.8249 | MainLoss:0.4621 | Alpha:0.0302 | SPLoss:11.9951 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [895 | 5000] LR: 0.015027\n",
      "Train | 16/16 | Loss:0.7903 | MainLoss:0.4618 | Alpha:0.0302 | SPLoss:10.8701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [896 | 5000] LR: 0.015025\n",
      "Train | 16/16 | Loss:0.7783 | MainLoss:0.4754 | Alpha:0.0307 | SPLoss:9.8840 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [897 | 5000] LR: 0.015023\n",
      "Train | 16/16 | Loss:0.8279 | MainLoss:0.5348 | Alpha:0.0312 | SPLoss:9.3999 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [898 | 5000] LR: 0.015020\n",
      "Train | 16/16 | Loss:0.8282 | MainLoss:0.4967 | Alpha:0.0304 | SPLoss:10.9727 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [899 | 5000] LR: 0.015018\n",
      "Train | 16/16 | Loss:4.4418 | MainLoss:0.4713 | Alpha:0.0314 | SPLoss:129.1376 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [900 | 5000] LR: 0.015015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:6.9512 | MainLoss:0.5195 | Alpha:0.0296 | SPLoss:217.7766 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.5159 | MainLoss:0.5159 | SPLoss:222.0620 | CLSLoss:0.0000 | AUROC:0.8421\n",
      "Test | 31/16 | Loss:0.2559 | MainLoss:0.2559 | SPLoss:222.0620 | CLSLoss:0.0000 | AUROC:0.9730\n",
      "\n",
      "Epoch: [901 | 5000] LR: 0.015013\n",
      "Train | 16/16 | Loss:7.0720 | MainLoss:0.5658 | Alpha:0.0305 | SPLoss:213.9075 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [902 | 5000] LR: 0.015010\n",
      "Train | 16/16 | Loss:6.3690 | MainLoss:0.5409 | Alpha:0.0305 | SPLoss:191.3833 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [903 | 5000] LR: 0.015008\n",
      "Train | 16/16 | Loss:5.6573 | MainLoss:0.5190 | Alpha:0.0299 | SPLoss:172.2565 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [904 | 5000] LR: 0.015006\n",
      "Train | 16/16 | Loss:5.2706 | MainLoss:0.4990 | Alpha:0.0316 | SPLoss:150.7742 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [905 | 5000] LR: 0.015003\n",
      "Train | 16/16 | Loss:4.5403 | MainLoss:0.4678 | Alpha:0.0314 | SPLoss:129.7649 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [906 | 5000] LR: 0.015001\n",
      "Train | 16/16 | Loss:3.8563 | MainLoss:0.4590 | Alpha:0.0304 | SPLoss:111.5123 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [907 | 5000] LR: 0.014998\n",
      "Train | 16/16 | Loss:3.4924 | MainLoss:0.4840 | Alpha:0.0312 | SPLoss:96.3577 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [908 | 5000] LR: 0.014996\n",
      "Train | 16/16 | Loss:2.9333 | MainLoss:0.4919 | Alpha:0.0293 | SPLoss:83.3197 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [909 | 5000] LR: 0.014993\n",
      "Train | 16/16 | Loss:2.6035 | MainLoss:0.4562 | Alpha:0.0297 | SPLoss:72.4534 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [910 | 5000] LR: 0.014991\n",
      "Train | 16/16 | Loss:2.4856 | MainLoss:0.4339 | Alpha:0.0327 | SPLoss:63.1136 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [911 | 5000] LR: 0.014989\n",
      "Train | 16/16 | Loss:2.2425 | MainLoss:0.4952 | Alpha:0.0322 | SPLoss:54.3297 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [912 | 5000] LR: 0.014986\n",
      "Train | 16/16 | Loss:1.9752 | MainLoss:0.4814 | Alpha:0.0320 | SPLoss:46.6848 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [913 | 5000] LR: 0.014984\n",
      "Train | 16/16 | Loss:1.7143 | MainLoss:0.4796 | Alpha:0.0306 | SPLoss:40.3537 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [914 | 5000] LR: 0.014981\n",
      "Train | 16/16 | Loss:1.6592 | MainLoss:0.5075 | Alpha:0.0324 | SPLoss:35.5747 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [915 | 5000] LR: 0.014979\n",
      "Train | 16/16 | Loss:1.7869 | MainLoss:0.5159 | Alpha:0.0305 | SPLoss:42.0292 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [916 | 5000] LR: 0.014976\n",
      "Train | 16/16 | Loss:2.9795 | MainLoss:0.4790 | Alpha:0.0293 | SPLoss:84.8628 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [917 | 5000] LR: 0.014974\n",
      "Train | 16/16 | Loss:3.2867 | MainLoss:0.4618 | Alpha:0.0304 | SPLoss:92.6835 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [918 | 5000] LR: 0.014971\n",
      "Train | 16/16 | Loss:3.0496 | MainLoss:0.4412 | Alpha:0.0310 | SPLoss:83.9726 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [919 | 5000] LR: 0.014969\n",
      "Train | 16/16 | Loss:2.6307 | MainLoss:0.4727 | Alpha:0.0293 | SPLoss:73.3021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [920 | 5000] LR: 0.014966\n",
      "Train | 16/16 | Loss:2.3851 | MainLoss:0.4579 | Alpha:0.0301 | SPLoss:63.8360 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [921 | 5000] LR: 0.014964\n",
      "Train | 16/16 | Loss:2.1507 | MainLoss:0.4666 | Alpha:0.0302 | SPLoss:55.6222 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [922 | 5000] LR: 0.014961\n",
      "Train | 16/16 | Loss:1.9385 | MainLoss:0.4461 | Alpha:0.0308 | SPLoss:48.4200 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [923 | 5000] LR: 0.014959\n",
      "Train | 16/16 | Loss:1.7523 | MainLoss:0.4577 | Alpha:0.0308 | SPLoss:42.1772 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [924 | 5000] LR: 0.014957\n",
      "Train | 16/16 | Loss:1.5909 | MainLoss:0.4545 | Alpha:0.0309 | SPLoss:36.6908 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [925 | 5000] LR: 0.014954\n",
      "Train | 16/16 | Loss:1.4627 | MainLoss:0.4660 | Alpha:0.0312 | SPLoss:31.9237 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [926 | 5000] LR: 0.014952\n",
      "Train | 16/16 | Loss:1.3499 | MainLoss:0.4797 | Alpha:0.0311 | SPLoss:27.9024 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [927 | 5000] LR: 0.014949\n",
      "Train | 16/16 | Loss:1.2556 | MainLoss:0.4818 | Alpha:0.0316 | SPLoss:24.4956 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [928 | 5000] LR: 0.014947\n",
      "Train | 16/16 | Loss:1.1107 | MainLoss:0.4527 | Alpha:0.0305 | SPLoss:21.4817 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [929 | 5000] LR: 0.014944\n",
      "Train | 16/16 | Loss:1.0484 | MainLoss:0.4480 | Alpha:0.0316 | SPLoss:18.9904 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [930 | 5000] LR: 0.014942\n",
      "Train | 16/16 | Loss:0.9685 | MainLoss:0.4393 | Alpha:0.0314 | SPLoss:16.7964 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [931 | 5000] LR: 0.014939\n",
      "Train | 16/16 | Loss:0.9070 | MainLoss:0.4315 | Alpha:0.0320 | SPLoss:14.9103 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [932 | 5000] LR: 0.014937\n",
      "Train | 16/16 | Loss:0.8771 | MainLoss:0.4642 | Alpha:0.0309 | SPLoss:13.3790 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [933 | 5000] LR: 0.014934\n",
      "Train | 16/16 | Loss:0.8680 | MainLoss:0.4682 | Alpha:0.0316 | SPLoss:12.6325 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [934 | 5000] LR: 0.014932\n",
      "Train | 16/16 | Loss:0.8278 | MainLoss:0.4537 | Alpha:0.0322 | SPLoss:11.6447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [935 | 5000] LR: 0.014929\n",
      "Train | 16/16 | Loss:0.8064 | MainLoss:0.4747 | Alpha:0.0316 | SPLoss:10.5113 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [936 | 5000] LR: 0.014927\n",
      "Train | 16/16 | Loss:0.7605 | MainLoss:0.4662 | Alpha:0.0307 | SPLoss:9.5709 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [937 | 5000] LR: 0.014924\n",
      "Train | 16/16 | Loss:0.7299 | MainLoss:0.4591 | Alpha:0.0308 | SPLoss:8.7975 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [938 | 5000] LR: 0.014921\n",
      "Train | 16/16 | Loss:0.7377 | MainLoss:0.4814 | Alpha:0.0315 | SPLoss:8.1618 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [939 | 5000] LR: 0.014919\n",
      "Train | 16/16 | Loss:0.7169 | MainLoss:0.4779 | Alpha:0.0317 | SPLoss:7.5357 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [940 | 5000] LR: 0.014916\n",
      "Train | 16/16 | Loss:0.6710 | MainLoss:0.4566 | Alpha:0.0304 | SPLoss:7.0509 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [941 | 5000] LR: 0.014914\n",
      "Train | 16/16 | Loss:0.7039 | MainLoss:0.4957 | Alpha:0.0315 | SPLoss:6.6117 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [942 | 5000] LR: 0.014911\n",
      "Train | 16/16 | Loss:0.6584 | MainLoss:0.4649 | Alpha:0.0309 | SPLoss:6.2613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [943 | 5000] LR: 0.014909\n",
      "Train | 16/16 | Loss:0.6711 | MainLoss:0.4873 | Alpha:0.0300 | SPLoss:6.1221 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [944 | 5000] LR: 0.014906\n",
      "Train | 16/16 | Loss:0.7026 | MainLoss:0.4568 | Alpha:0.0322 | SPLoss:7.6387 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [945 | 5000] LR: 0.014904\n",
      "Train | 16/16 | Loss:0.6723 | MainLoss:0.4421 | Alpha:0.0286 | SPLoss:8.0370 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [946 | 5000] LR: 0.014901\n",
      "Train | 16/16 | Loss:0.6946 | MainLoss:0.4664 | Alpha:0.0297 | SPLoss:7.6681 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [947 | 5000] LR: 0.014899\n",
      "Train | 16/16 | Loss:0.6821 | MainLoss:0.4588 | Alpha:0.0306 | SPLoss:7.3069 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [948 | 5000] LR: 0.014896\n",
      "Train | 16/16 | Loss:0.7051 | MainLoss:0.4884 | Alpha:0.0313 | SPLoss:6.9214 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [949 | 5000] LR: 0.014894\n",
      "Train | 16/16 | Loss:0.6567 | MainLoss:0.4430 | Alpha:0.0319 | SPLoss:6.7034 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [950 | 5000] LR: 0.014891\n",
      "Train | 16/16 | Loss:0.6508 | MainLoss:0.4569 | Alpha:0.0301 | SPLoss:6.4387 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [951 | 5000] LR: 0.014888\n",
      "Train | 16/16 | Loss:0.6485 | MainLoss:0.4572 | Alpha:0.0315 | SPLoss:6.0821 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [952 | 5000] LR: 0.014886\n",
      "Train | 16/16 | Loss:0.6099 | MainLoss:0.4403 | Alpha:0.0293 | SPLoss:5.7755 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [953 | 5000] LR: 0.014883\n",
      "Train | 16/16 | Loss:0.6195 | MainLoss:0.4495 | Alpha:0.0307 | SPLoss:5.5346 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [954 | 5000] LR: 0.014881\n",
      "Train | 16/16 | Loss:0.6316 | MainLoss:0.4597 | Alpha:0.0307 | SPLoss:5.5871 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [955 | 5000] LR: 0.014878\n",
      "Train | 16/16 | Loss:0.6349 | MainLoss:0.4566 | Alpha:0.0308 | SPLoss:5.7920 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [956 | 5000] LR: 0.014876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6405 | MainLoss:0.4740 | Alpha:0.0292 | SPLoss:5.7022 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [957 | 5000] LR: 0.014873\n",
      "Train | 16/16 | Loss:0.6339 | MainLoss:0.4605 | Alpha:0.0311 | SPLoss:5.5719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [958 | 5000] LR: 0.014871\n",
      "Train | 16/16 | Loss:0.6148 | MainLoss:0.4488 | Alpha:0.0307 | SPLoss:5.4047 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [959 | 5000] LR: 0.014868\n",
      "Train | 16/16 | Loss:0.6310 | MainLoss:0.4767 | Alpha:0.0295 | SPLoss:5.2207 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [960 | 5000] LR: 0.014865\n",
      "Train | 16/16 | Loss:0.6162 | MainLoss:0.4591 | Alpha:0.0310 | SPLoss:5.0754 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [961 | 5000] LR: 0.014863\n",
      "Train | 16/16 | Loss:0.6015 | MainLoss:0.4514 | Alpha:0.0302 | SPLoss:4.9720 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [962 | 5000] LR: 0.014860\n",
      "Train | 16/16 | Loss:0.6653 | MainLoss:0.4818 | Alpha:0.0312 | SPLoss:5.8577 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [963 | 5000] LR: 0.014858\n",
      "Train | 16/16 | Loss:0.6621 | MainLoss:0.4738 | Alpha:0.0293 | SPLoss:6.4162 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [964 | 5000] LR: 0.014855\n",
      "Train | 16/16 | Loss:0.6528 | MainLoss:0.4595 | Alpha:0.0306 | SPLoss:6.3253 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [965 | 5000] LR: 0.014852\n",
      "Train | 16/16 | Loss:0.6833 | MainLoss:0.4911 | Alpha:0.0304 | SPLoss:6.3461 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [966 | 5000] LR: 0.014850\n",
      "Train | 16/16 | Loss:0.6995 | MainLoss:0.4390 | Alpha:0.0319 | SPLoss:8.2125 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [967 | 5000] LR: 0.014847\n",
      "Train | 16/16 | Loss:0.7292 | MainLoss:0.4576 | Alpha:0.0312 | SPLoss:8.7050 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [968 | 5000] LR: 0.014845\n",
      "Train | 16/16 | Loss:233.2160 | MainLoss:0.4500 | Alpha:0.0299 | SPLoss:7724.0322 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [969 | 5000] LR: 0.014842\n",
      "Train | 16/16 | Loss:612.7187 | MainLoss:0.4452 | Alpha:0.0306 | SPLoss:19906.9883 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [970 | 5000] LR: 0.014839\n",
      "Train | 16/16 | Loss:620.4745 | MainLoss:0.4357 | Alpha:0.0298 | SPLoss:20761.9375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [971 | 5000] LR: 0.014837\n",
      "Train | 16/16 | Loss:585.7836 | MainLoss:0.4649 | Alpha:0.0315 | SPLoss:18615.4805 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [972 | 5000] LR: 0.014834\n",
      "Train | 16/16 | Loss:468.4643 | MainLoss:0.4499 | Alpha:0.0291 | SPLoss:16135.5645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [973 | 5000] LR: 0.014832\n",
      "Train | 16/16 | Loss:420.2135 | MainLoss:0.4472 | Alpha:0.0301 | SPLoss:13925.8457 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [974 | 5000] LR: 0.014829\n",
      "Train | 16/16 | Loss:415.1684 | MainLoss:0.4488 | Alpha:0.0314 | SPLoss:13165.7578 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [975 | 5000] LR: 0.014826\n",
      "Train | 16/16 | Loss:410.3937 | MainLoss:0.4392 | Alpha:0.0300 | SPLoss:13687.5928 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [976 | 5000] LR: 0.014824\n",
      "Train | 16/16 | Loss:369.8816 | MainLoss:0.4465 | Alpha:0.0297 | SPLoss:12468.5107 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [977 | 5000] LR: 0.014821\n",
      "Train | 16/16 | Loss:338.0506 | MainLoss:0.4338 | Alpha:0.0311 | SPLoss:10841.6445 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [978 | 5000] LR: 0.014818\n",
      "Train | 16/16 | Loss:288.9600 | MainLoss:0.4860 | Alpha:0.0310 | SPLoss:9335.4102 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [979 | 5000] LR: 0.014816\n",
      "Train | 16/16 | Loss:261.7312 | MainLoss:0.4570 | Alpha:0.0327 | SPLoss:8012.1841 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [980 | 5000] LR: 0.014813\n",
      "Train | 16/16 | Loss:207.0423 | MainLoss:0.4775 | Alpha:0.0302 | SPLoss:6831.2988 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [981 | 5000] LR: 0.014811\n",
      "Train | 16/16 | Loss:180.1405 | MainLoss:0.4874 | Alpha:0.0306 | SPLoss:5857.5391 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [982 | 5000] LR: 0.014808\n",
      "Train | 16/16 | Loss:155.6324 | MainLoss:0.4777 | Alpha:0.0309 | SPLoss:5040.3823 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [983 | 5000] LR: 0.014805\n",
      "Train | 16/16 | Loss:135.3932 | MainLoss:0.4367 | Alpha:0.0312 | SPLoss:4320.8438 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [984 | 5000] LR: 0.014803\n",
      "Train | 16/16 | Loss:118.4733 | MainLoss:0.4461 | Alpha:0.0319 | SPLoss:3690.6018 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [985 | 5000] LR: 0.014800\n",
      "Train | 16/16 | Loss:95.5294 | MainLoss:0.4449 | Alpha:0.0300 | SPLoss:3158.7109 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [986 | 5000] LR: 0.014797\n",
      "Train | 16/16 | Loss:84.1044 | MainLoss:0.4479 | Alpha:0.0308 | SPLoss:2707.9138 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [987 | 5000] LR: 0.014795\n",
      "Train | 16/16 | Loss:76.2707 | MainLoss:0.4855 | Alpha:0.0312 | SPLoss:2431.3496 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [988 | 5000] LR: 0.014792\n",
      "Train | 16/16 | Loss:69.8517 | MainLoss:0.4808 | Alpha:0.0311 | SPLoss:2230.1064 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [989 | 5000] LR: 0.014789\n",
      "Train | 16/16 | Loss:59.5931 | MainLoss:0.4387 | Alpha:0.0303 | SPLoss:1950.2931 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [990 | 5000] LR: 0.014787\n",
      "Train | 16/16 | Loss:53.2566 | MainLoss:0.4385 | Alpha:0.0314 | SPLoss:1683.0865 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [991 | 5000] LR: 0.014784\n",
      "Train | 16/16 | Loss:45.6110 | MainLoss:0.4937 | Alpha:0.0312 | SPLoss:1445.1736 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [992 | 5000] LR: 0.014781\n",
      "Train | 16/16 | Loss:39.5139 | MainLoss:0.4762 | Alpha:0.0316 | SPLoss:1241.3744 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [993 | 5000] LR: 0.014779\n",
      "Train | 16/16 | Loss:31.7831 | MainLoss:0.4754 | Alpha:0.0295 | SPLoss:1064.2148 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [994 | 5000] LR: 0.014776\n",
      "Train | 16/16 | Loss:30.4086 | MainLoss:0.4878 | Alpha:0.0327 | SPLoss:913.3746 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [995 | 5000] LR: 0.014773\n",
      "Train | 16/16 | Loss:25.2029 | MainLoss:0.4539 | Alpha:0.0317 | SPLoss:778.8346 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [996 | 5000] LR: 0.014771\n",
      "Train | 16/16 | Loss:21.9867 | MainLoss:0.4539 | Alpha:0.0323 | SPLoss:666.0281 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [997 | 5000] LR: 0.014768\n",
      "Train | 16/16 | Loss:18.2941 | MainLoss:0.4519 | Alpha:0.0314 | SPLoss:570.5057 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [998 | 5000] LR: 0.014765\n",
      "Train | 16/16 | Loss:16.0452 | MainLoss:0.4475 | Alpha:0.0321 | SPLoss:489.8382 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [999 | 5000] LR: 0.014763\n",
      "Train | 16/16 | Loss:13.8116 | MainLoss:0.4565 | Alpha:0.0319 | SPLoss:417.7170 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1000 | 5000] LR: 0.014760\n",
      "Train | 16/16 | Loss:12.0032 | MainLoss:0.4676 | Alpha:0.0310 | SPLoss:371.3874 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4557 | MainLoss:0.4557 | SPLoss:452.1649 | CLSLoss:0.0000 | AUROC:0.8731\n",
      "Test | 31/16 | Loss:0.2995 | MainLoss:0.2995 | SPLoss:452.1650 | CLSLoss:0.0000 | AUROC:0.9546\n",
      "\n",
      "Epoch: [1001 | 5000] LR: 0.001476\n",
      "Train | 16/16 | Loss:15.1460 | MainLoss:0.4377 | Alpha:0.0318 | SPLoss:463.4003 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1002 | 5000] LR: 0.001475\n",
      "Train | 16/16 | Loss:15.6535 | MainLoss:0.4142 | Alpha:0.0325 | SPLoss:469.2684 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1003 | 5000] LR: 0.001475\n",
      "Train | 16/16 | Loss:14.2204 | MainLoss:0.4152 | Alpha:0.0297 | SPLoss:464.6446 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1004 | 5000] LR: 0.001475\n",
      "Train | 16/16 | Loss:14.7478 | MainLoss:0.4102 | Alpha:0.0313 | SPLoss:458.3562 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1005 | 5000] LR: 0.001475\n",
      "Train | 16/16 | Loss:14.5142 | MainLoss:0.4126 | Alpha:0.0312 | SPLoss:451.6995 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1006 | 5000] LR: 0.001474\n",
      "Train | 16/16 | Loss:14.5578 | MainLoss:0.3920 | Alpha:0.0318 | SPLoss:445.0637 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1007 | 5000] LR: 0.001474\n",
      "Train | 16/16 | Loss:13.9291 | MainLoss:0.3758 | Alpha:0.0309 | SPLoss:438.5650 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1008 | 5000] LR: 0.001474\n",
      "Train | 16/16 | Loss:13.3834 | MainLoss:0.3776 | Alpha:0.0301 | SPLoss:432.2652 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1009 | 5000] LR: 0.001474\n",
      "Train | 16/16 | Loss:13.5959 | MainLoss:0.3988 | Alpha:0.0310 | SPLoss:426.1243 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1010 | 5000] LR: 0.001473\n",
      "Train | 16/16 | Loss:13.0999 | MainLoss:0.3952 | Alpha:0.0302 | SPLoss:420.0058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1011 | 5000] LR: 0.001473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:12.8024 | MainLoss:0.3838 | Alpha:0.0300 | SPLoss:414.0955 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1012 | 5000] LR: 0.001473\n",
      "Train | 16/16 | Loss:13.0353 | MainLoss:0.3722 | Alpha:0.0310 | SPLoss:408.3250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1013 | 5000] LR: 0.001472\n",
      "Train | 16/16 | Loss:12.8956 | MainLoss:0.3914 | Alpha:0.0311 | SPLoss:402.4864 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1014 | 5000] LR: 0.001472\n",
      "Train | 16/16 | Loss:12.7250 | MainLoss:0.3904 | Alpha:0.0311 | SPLoss:396.6273 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1015 | 5000] LR: 0.001472\n",
      "Train | 16/16 | Loss:12.1569 | MainLoss:0.3770 | Alpha:0.0301 | SPLoss:390.9055 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1016 | 5000] LR: 0.001472\n",
      "Train | 16/16 | Loss:12.5359 | MainLoss:0.3780 | Alpha:0.0315 | SPLoss:385.2758 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1017 | 5000] LR: 0.001471\n",
      "Train | 16/16 | Loss:12.4110 | MainLoss:0.3869 | Alpha:0.0317 | SPLoss:379.6980 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1018 | 5000] LR: 0.001471\n",
      "Train | 16/16 | Loss:11.7226 | MainLoss:0.3626 | Alpha:0.0304 | SPLoss:374.2395 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1019 | 5000] LR: 0.001471\n",
      "Train | 16/16 | Loss:12.1444 | MainLoss:0.3736 | Alpha:0.0319 | SPLoss:368.7071 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1020 | 5000] LR: 0.001471\n",
      "Train | 16/16 | Loss:11.5619 | MainLoss:0.3642 | Alpha:0.0308 | SPLoss:363.4259 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1021 | 5000] LR: 0.001470\n",
      "Train | 16/16 | Loss:11.1008 | MainLoss:0.3779 | Alpha:0.0299 | SPLoss:358.2714 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1022 | 5000] LR: 0.001470\n",
      "Train | 16/16 | Loss:11.7737 | MainLoss:0.3940 | Alpha:0.0322 | SPLoss:353.0642 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1023 | 5000] LR: 0.001470\n",
      "Train | 16/16 | Loss:11.4528 | MainLoss:0.3672 | Alpha:0.0319 | SPLoss:347.8521 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1024 | 5000] LR: 0.001469\n",
      "Train | 16/16 | Loss:11.2663 | MainLoss:0.3788 | Alpha:0.0318 | SPLoss:342.7645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1025 | 5000] LR: 0.001469\n",
      "Train | 16/16 | Loss:11.0953 | MainLoss:0.3800 | Alpha:0.0317 | SPLoss:337.7032 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1026 | 5000] LR: 0.001469\n",
      "Train | 16/16 | Loss:11.2967 | MainLoss:0.3934 | Alpha:0.0328 | SPLoss:332.5760 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1027 | 5000] LR: 0.001469\n",
      "Train | 16/16 | Loss:10.6960 | MainLoss:0.3789 | Alpha:0.0315 | SPLoss:327.6697 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1028 | 5000] LR: 0.001468\n",
      "Train | 16/16 | Loss:10.1428 | MainLoss:0.3632 | Alpha:0.0303 | SPLoss:322.9751 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1029 | 5000] LR: 0.001468\n",
      "Train | 16/16 | Loss:10.5755 | MainLoss:0.3713 | Alpha:0.0321 | SPLoss:318.3280 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1030 | 5000] LR: 0.001468\n",
      "Train | 16/16 | Loss:10.3924 | MainLoss:0.3666 | Alpha:0.0320 | SPLoss:313.7174 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1031 | 5000] LR: 0.001468\n",
      "Train | 16/16 | Loss:10.2876 | MainLoss:0.3754 | Alpha:0.0321 | SPLoss:308.9554 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1032 | 5000] LR: 0.001467\n",
      "Train | 16/16 | Loss:10.2069 | MainLoss:0.4020 | Alpha:0.0322 | SPLoss:304.4018 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1033 | 5000] LR: 0.001467\n",
      "Train | 16/16 | Loss:9.8433 | MainLoss:0.3775 | Alpha:0.0316 | SPLoss:299.9492 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1034 | 5000] LR: 0.001467\n",
      "Train | 16/16 | Loss:9.5167 | MainLoss:0.3626 | Alpha:0.0310 | SPLoss:295.6205 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1035 | 5000] LR: 0.001466\n",
      "Train | 16/16 | Loss:8.9645 | MainLoss:0.3761 | Alpha:0.0295 | SPLoss:291.3553 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1036 | 5000] LR: 0.001466\n",
      "Train | 16/16 | Loss:8.9382 | MainLoss:0.3693 | Alpha:0.0298 | SPLoss:287.3389 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1037 | 5000] LR: 0.001466\n",
      "Train | 16/16 | Loss:9.3508 | MainLoss:0.3607 | Alpha:0.0317 | SPLoss:283.2902 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1038 | 5000] LR: 0.001466\n",
      "Train | 16/16 | Loss:9.2014 | MainLoss:0.3741 | Alpha:0.0316 | SPLoss:279.1481 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1039 | 5000] LR: 0.001465\n",
      "Train | 16/16 | Loss:8.7382 | MainLoss:0.3729 | Alpha:0.0304 | SPLoss:275.0866 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1040 | 5000] LR: 0.001465\n",
      "Train | 16/16 | Loss:8.8597 | MainLoss:0.3752 | Alpha:0.0313 | SPLoss:271.1709 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1041 | 5000] LR: 0.001465\n",
      "Train | 16/16 | Loss:8.7851 | MainLoss:0.3720 | Alpha:0.0315 | SPLoss:267.2942 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1042 | 5000] LR: 0.001464\n",
      "Train | 16/16 | Loss:8.6188 | MainLoss:0.3716 | Alpha:0.0313 | SPLoss:263.4778 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1043 | 5000] LR: 0.001464\n",
      "Train | 16/16 | Loss:8.5386 | MainLoss:0.3724 | Alpha:0.0314 | SPLoss:259.6610 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1044 | 5000] LR: 0.001464\n",
      "Train | 16/16 | Loss:8.5144 | MainLoss:0.3679 | Alpha:0.0318 | SPLoss:255.8669 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1045 | 5000] LR: 0.001464\n",
      "Train | 16/16 | Loss:8.0641 | MainLoss:0.3709 | Alpha:0.0305 | SPLoss:252.2101 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1046 | 5000] LR: 0.001463\n",
      "Train | 16/16 | Loss:8.1216 | MainLoss:0.3695 | Alpha:0.0312 | SPLoss:248.5846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1047 | 5000] LR: 0.001463\n",
      "Train | 16/16 | Loss:8.2229 | MainLoss:0.3592 | Alpha:0.0321 | SPLoss:244.9447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1048 | 5000] LR: 0.001463\n",
      "Train | 16/16 | Loss:7.7212 | MainLoss:0.3773 | Alpha:0.0304 | SPLoss:241.4874 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1049 | 5000] LR: 0.001463\n",
      "Train | 16/16 | Loss:7.8291 | MainLoss:0.3677 | Alpha:0.0313 | SPLoss:238.0489 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1050 | 5000] LR: 0.001462\n",
      "Train | 16/16 | Loss:7.9849 | MainLoss:0.3708 | Alpha:0.0325 | SPLoss:234.6166 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1051 | 5000] LR: 0.001462\n",
      "Train | 16/16 | Loss:7.7345 | MainLoss:0.3633 | Alpha:0.0319 | SPLoss:231.2276 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1052 | 5000] LR: 0.001462\n",
      "Train | 16/16 | Loss:7.3767 | MainLoss:0.3453 | Alpha:0.0309 | SPLoss:227.8324 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1053 | 5000] LR: 0.001461\n",
      "Train | 16/16 | Loss:7.4502 | MainLoss:0.3911 | Alpha:0.0314 | SPLoss:224.5608 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1054 | 5000] LR: 0.001461\n",
      "Train | 16/16 | Loss:7.1551 | MainLoss:0.3819 | Alpha:0.0306 | SPLoss:221.3250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1055 | 5000] LR: 0.001461\n",
      "Train | 16/16 | Loss:7.1138 | MainLoss:0.3595 | Alpha:0.0310 | SPLoss:218.1721 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1056 | 5000] LR: 0.001461\n",
      "Train | 16/16 | Loss:7.0673 | MainLoss:0.3636 | Alpha:0.0312 | SPLoss:215.0235 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1057 | 5000] LR: 0.001460\n",
      "Train | 16/16 | Loss:6.8085 | MainLoss:0.3503 | Alpha:0.0305 | SPLoss:212.0263 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1058 | 5000] LR: 0.001460\n",
      "Train | 16/16 | Loss:6.8031 | MainLoss:0.3759 | Alpha:0.0307 | SPLoss:209.0529 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1059 | 5000] LR: 0.001460\n",
      "Train | 16/16 | Loss:6.6504 | MainLoss:0.3702 | Alpha:0.0305 | SPLoss:206.1061 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1060 | 5000] LR: 0.001459\n",
      "Train | 16/16 | Loss:6.4787 | MainLoss:0.3757 | Alpha:0.0300 | SPLoss:203.2536 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1061 | 5000] LR: 0.001459\n",
      "Train | 16/16 | Loss:6.6194 | MainLoss:0.3770 | Alpha:0.0311 | SPLoss:200.4126 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1062 | 5000] LR: 0.001459\n",
      "Train | 16/16 | Loss:6.4423 | MainLoss:0.3611 | Alpha:0.0308 | SPLoss:197.6194 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1063 | 5000] LR: 0.001459\n",
      "Train | 16/16 | Loss:6.3820 | MainLoss:0.3538 | Alpha:0.0309 | SPLoss:194.8651 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1064 | 5000] LR: 0.001458\n",
      "Train | 16/16 | Loss:6.2560 | MainLoss:0.3488 | Alpha:0.0308 | SPLoss:192.1553 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1065 | 5000] LR: 0.001458\n",
      "Train | 16/16 | Loss:6.1140 | MainLoss:0.3623 | Alpha:0.0304 | SPLoss:189.4132 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1066 | 5000] LR: 0.001458\n",
      "Train | 16/16 | Loss:5.9755 | MainLoss:0.3655 | Alpha:0.0300 | SPLoss:186.7892 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1067 | 5000] LR: 0.001457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:5.9295 | MainLoss:0.3448 | Alpha:0.0303 | SPLoss:184.2019 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1068 | 5000] LR: 0.001457\n",
      "Train | 16/16 | Loss:5.9511 | MainLoss:0.3621 | Alpha:0.0308 | SPLoss:181.6561 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1069 | 5000] LR: 0.001457\n",
      "Train | 16/16 | Loss:5.9639 | MainLoss:0.3653 | Alpha:0.0313 | SPLoss:179.1141 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1070 | 5000] LR: 0.001457\n",
      "Train | 16/16 | Loss:6.0389 | MainLoss:0.3473 | Alpha:0.0322 | SPLoss:176.5589 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1071 | 5000] LR: 0.001456\n",
      "Train | 16/16 | Loss:6.0305 | MainLoss:0.3517 | Alpha:0.0326 | SPLoss:173.9874 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1072 | 5000] LR: 0.001456\n",
      "Train | 16/16 | Loss:5.7465 | MainLoss:0.3652 | Alpha:0.0314 | SPLoss:171.4114 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1073 | 5000] LR: 0.001456\n",
      "Train | 16/16 | Loss:5.6274 | MainLoss:0.3629 | Alpha:0.0312 | SPLoss:168.9685 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1074 | 5000] LR: 0.001455\n",
      "Train | 16/16 | Loss:5.4901 | MainLoss:0.3544 | Alpha:0.0308 | SPLoss:166.5975 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1075 | 5000] LR: 0.001455\n",
      "Train | 16/16 | Loss:5.5245 | MainLoss:0.3494 | Alpha:0.0315 | SPLoss:164.2870 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1076 | 5000] LR: 0.001455\n",
      "Train | 16/16 | Loss:5.2832 | MainLoss:0.3798 | Alpha:0.0303 | SPLoss:161.9717 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1077 | 5000] LR: 0.001455\n",
      "Train | 16/16 | Loss:5.1343 | MainLoss:0.3410 | Alpha:0.0300 | SPLoss:159.7157 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1078 | 5000] LR: 0.001454\n",
      "Train | 16/16 | Loss:5.3073 | MainLoss:0.3549 | Alpha:0.0314 | SPLoss:157.5206 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1079 | 5000] LR: 0.001454\n",
      "Train | 16/16 | Loss:5.0441 | MainLoss:0.3577 | Alpha:0.0302 | SPLoss:155.3184 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1080 | 5000] LR: 0.001454\n",
      "Train | 16/16 | Loss:5.0801 | MainLoss:0.3643 | Alpha:0.0308 | SPLoss:153.1183 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1081 | 5000] LR: 0.001453\n",
      "Train | 16/16 | Loss:5.0710 | MainLoss:0.3573 | Alpha:0.0312 | SPLoss:150.9963 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1082 | 5000] LR: 0.001453\n",
      "Train | 16/16 | Loss:4.8679 | MainLoss:0.3862 | Alpha:0.0301 | SPLoss:148.8999 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1083 | 5000] LR: 0.001453\n",
      "Train | 16/16 | Loss:5.0635 | MainLoss:0.3628 | Alpha:0.0320 | SPLoss:146.8627 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1084 | 5000] LR: 0.001452\n",
      "Train | 16/16 | Loss:4.6582 | MainLoss:0.3630 | Alpha:0.0297 | SPLoss:144.8180 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1085 | 5000] LR: 0.001452\n",
      "Train | 16/16 | Loss:4.7677 | MainLoss:0.3489 | Alpha:0.0309 | SPLoss:142.8343 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1086 | 5000] LR: 0.001452\n",
      "Train | 16/16 | Loss:4.6207 | MainLoss:0.3557 | Alpha:0.0303 | SPLoss:140.8437 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1087 | 5000] LR: 0.001452\n",
      "Train | 16/16 | Loss:4.6395 | MainLoss:0.3635 | Alpha:0.0308 | SPLoss:138.9121 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1088 | 5000] LR: 0.001451\n",
      "Train | 16/16 | Loss:4.6851 | MainLoss:0.3554 | Alpha:0.0316 | SPLoss:136.9601 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1089 | 5000] LR: 0.001451\n",
      "Train | 16/16 | Loss:4.4194 | MainLoss:0.3677 | Alpha:0.0300 | SPLoss:135.0641 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1090 | 5000] LR: 0.001451\n",
      "Train | 16/16 | Loss:4.3475 | MainLoss:0.3444 | Alpha:0.0300 | SPLoss:133.2620 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1091 | 5000] LR: 0.001450\n",
      "Train | 16/16 | Loss:4.4903 | MainLoss:0.3596 | Alpha:0.0314 | SPLoss:131.4514 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1092 | 5000] LR: 0.001450\n",
      "Train | 16/16 | Loss:4.3104 | MainLoss:0.3542 | Alpha:0.0305 | SPLoss:129.6147 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1093 | 5000] LR: 0.001450\n",
      "Train | 16/16 | Loss:4.4012 | MainLoss:0.3511 | Alpha:0.0317 | SPLoss:127.7967 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1094 | 5000] LR: 0.001450\n",
      "Train | 16/16 | Loss:4.3058 | MainLoss:0.3560 | Alpha:0.0314 | SPLoss:125.9827 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1095 | 5000] LR: 0.001449\n",
      "Train | 16/16 | Loss:4.2626 | MainLoss:0.3502 | Alpha:0.0315 | SPLoss:124.2293 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1096 | 5000] LR: 0.001449\n",
      "Train | 16/16 | Loss:4.3225 | MainLoss:0.3635 | Alpha:0.0323 | SPLoss:122.4923 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1097 | 5000] LR: 0.001449\n",
      "Train | 16/16 | Loss:4.0963 | MainLoss:0.3669 | Alpha:0.0309 | SPLoss:120.7293 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1098 | 5000] LR: 0.001448\n",
      "Train | 16/16 | Loss:4.0020 | MainLoss:0.3525 | Alpha:0.0307 | SPLoss:119.0373 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1099 | 5000] LR: 0.001448\n",
      "Train | 16/16 | Loss:3.9335 | MainLoss:0.3501 | Alpha:0.0305 | SPLoss:117.3931 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1100 | 5000] LR: 0.001448\n",
      "Train | 16/16 | Loss:3.7586 | MainLoss:0.3593 | Alpha:0.0294 | SPLoss:115.8148 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4062 | MainLoss:0.4062 | SPLoss:114.9991 | CLSLoss:0.0000 | AUROC:0.9047\n",
      "Test | 31/16 | Loss:0.1611 | MainLoss:0.1611 | SPLoss:114.9992 | CLSLoss:0.0000 | AUROC:0.9842\n",
      "\n",
      "Epoch: [1101 | 5000] LR: 0.001448\n",
      "Train | 16/16 | Loss:3.9447 | MainLoss:0.3679 | Alpha:0.0313 | SPLoss:114.2541 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1102 | 5000] LR: 0.001447\n",
      "Train | 16/16 | Loss:3.9598 | MainLoss:0.3652 | Alpha:0.0319 | SPLoss:112.6820 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1103 | 5000] LR: 0.001447\n",
      "Train | 16/16 | Loss:3.6721 | MainLoss:0.3552 | Alpha:0.0299 | SPLoss:111.1292 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1104 | 5000] LR: 0.001447\n",
      "Train | 16/16 | Loss:3.6410 | MainLoss:0.3368 | Alpha:0.0301 | SPLoss:109.6164 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1105 | 5000] LR: 0.001446\n",
      "Train | 16/16 | Loss:3.7079 | MainLoss:0.3573 | Alpha:0.0310 | SPLoss:108.1161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1106 | 5000] LR: 0.001446\n",
      "Train | 16/16 | Loss:3.4843 | MainLoss:0.3543 | Alpha:0.0293 | SPLoss:106.6619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1107 | 5000] LR: 0.001446\n",
      "Train | 16/16 | Loss:3.5844 | MainLoss:0.3657 | Alpha:0.0306 | SPLoss:105.2502 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1108 | 5000] LR: 0.001445\n",
      "Train | 16/16 | Loss:3.5413 | MainLoss:0.3505 | Alpha:0.0307 | SPLoss:103.8412 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1109 | 5000] LR: 0.001445\n",
      "Train | 16/16 | Loss:3.5596 | MainLoss:0.3730 | Alpha:0.0311 | SPLoss:102.4323 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1110 | 5000] LR: 0.001445\n",
      "Train | 16/16 | Loss:3.5203 | MainLoss:0.3461 | Alpha:0.0314 | SPLoss:100.9901 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1111 | 5000] LR: 0.001445\n",
      "Train | 16/16 | Loss:3.5853 | MainLoss:0.3813 | Alpha:0.0322 | SPLoss:99.6130 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1112 | 5000] LR: 0.001444\n",
      "Train | 16/16 | Loss:3.4772 | MainLoss:0.3813 | Alpha:0.0315 | SPLoss:98.2225 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1113 | 5000] LR: 0.001444\n",
      "Train | 16/16 | Loss:3.3075 | MainLoss:0.3520 | Alpha:0.0305 | SPLoss:96.8104 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1114 | 5000] LR: 0.001444\n",
      "Train | 16/16 | Loss:3.2769 | MainLoss:0.3477 | Alpha:0.0307 | SPLoss:95.4695 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1115 | 5000] LR: 0.001443\n",
      "Train | 16/16 | Loss:3.2112 | MainLoss:0.3477 | Alpha:0.0304 | SPLoss:94.2001 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1116 | 5000] LR: 0.001443\n",
      "Train | 16/16 | Loss:3.2091 | MainLoss:0.3657 | Alpha:0.0306 | SPLoss:92.9228 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1117 | 5000] LR: 0.001443\n",
      "Train | 16/16 | Loss:3.2546 | MainLoss:0.3643 | Alpha:0.0315 | SPLoss:91.6929 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1118 | 5000] LR: 0.001442\n",
      "Train | 16/16 | Loss:3.2375 | MainLoss:0.3457 | Alpha:0.0320 | SPLoss:90.4591 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1119 | 5000] LR: 0.001442\n",
      "Train | 16/16 | Loss:3.0541 | MainLoss:0.3493 | Alpha:0.0303 | SPLoss:89.1999 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1120 | 5000] LR: 0.001442\n",
      "Train | 16/16 | Loss:3.0238 | MainLoss:0.3429 | Alpha:0.0305 | SPLoss:87.9864 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1121 | 5000] LR: 0.001442\n",
      "Train | 16/16 | Loss:3.0565 | MainLoss:0.3535 | Alpha:0.0311 | SPLoss:86.8003 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1122 | 5000] LR: 0.001441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:3.1468 | MainLoss:0.3798 | Alpha:0.0323 | SPLoss:85.6093 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1123 | 5000] LR: 0.001441\n",
      "Train | 16/16 | Loss:2.9857 | MainLoss:0.3364 | Alpha:0.0314 | SPLoss:84.4114 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1124 | 5000] LR: 0.001441\n",
      "Train | 16/16 | Loss:2.8814 | MainLoss:0.3491 | Alpha:0.0304 | SPLoss:83.2542 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1125 | 5000] LR: 0.001440\n",
      "Train | 16/16 | Loss:2.8258 | MainLoss:0.3536 | Alpha:0.0301 | SPLoss:82.1341 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1126 | 5000] LR: 0.001440\n",
      "Train | 16/16 | Loss:2.7882 | MainLoss:0.3486 | Alpha:0.0301 | SPLoss:81.0628 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1127 | 5000] LR: 0.001440\n",
      "Train | 16/16 | Loss:2.8329 | MainLoss:0.3438 | Alpha:0.0311 | SPLoss:80.0086 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1128 | 5000] LR: 0.001439\n",
      "Train | 16/16 | Loss:2.7596 | MainLoss:0.3647 | Alpha:0.0303 | SPLoss:78.9323 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1129 | 5000] LR: 0.001439\n",
      "Train | 16/16 | Loss:2.5428 | MainLoss:0.3210 | Alpha:0.0285 | SPLoss:77.9022 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1130 | 5000] LR: 0.001439\n",
      "Train | 16/16 | Loss:2.5331 | MainLoss:0.3305 | Alpha:0.0286 | SPLoss:76.9244 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1131 | 5000] LR: 0.001439\n",
      "Train | 16/16 | Loss:2.7884 | MainLoss:0.3547 | Alpha:0.0320 | SPLoss:75.9509 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1132 | 5000] LR: 0.001438\n",
      "Train | 16/16 | Loss:2.6791 | MainLoss:0.3612 | Alpha:0.0309 | SPLoss:74.9176 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1133 | 5000] LR: 0.001438\n",
      "Train | 16/16 | Loss:2.5858 | MainLoss:0.3519 | Alpha:0.0302 | SPLoss:73.8942 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1134 | 5000] LR: 0.001438\n",
      "Train | 16/16 | Loss:2.6226 | MainLoss:0.3574 | Alpha:0.0311 | SPLoss:72.9051 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1135 | 5000] LR: 0.001437\n",
      "Train | 16/16 | Loss:2.5207 | MainLoss:0.3393 | Alpha:0.0303 | SPLoss:71.9406 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1136 | 5000] LR: 0.001437\n",
      "Train | 16/16 | Loss:2.6669 | MainLoss:0.3371 | Alpha:0.0328 | SPLoss:71.0058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1137 | 5000] LR: 0.001437\n",
      "Train | 16/16 | Loss:2.5434 | MainLoss:0.3478 | Alpha:0.0314 | SPLoss:70.0142 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1138 | 5000] LR: 0.001436\n",
      "Train | 16/16 | Loss:2.5952 | MainLoss:0.3692 | Alpha:0.0323 | SPLoss:69.0329 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1139 | 5000] LR: 0.001436\n",
      "Train | 16/16 | Loss:2.5898 | MainLoss:0.3612 | Alpha:0.0327 | SPLoss:68.0666 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1140 | 5000] LR: 0.001436\n",
      "Train | 16/16 | Loss:2.3980 | MainLoss:0.3501 | Alpha:0.0305 | SPLoss:67.0958 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1141 | 5000] LR: 0.001435\n",
      "Train | 16/16 | Loss:2.4248 | MainLoss:0.3291 | Alpha:0.0317 | SPLoss:66.2022 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1142 | 5000] LR: 0.001435\n",
      "Train | 16/16 | Loss:2.4064 | MainLoss:0.3448 | Alpha:0.0316 | SPLoss:65.3205 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1143 | 5000] LR: 0.001435\n",
      "Train | 16/16 | Loss:2.2912 | MainLoss:0.3435 | Alpha:0.0302 | SPLoss:64.4340 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1144 | 5000] LR: 0.001435\n",
      "Train | 16/16 | Loss:2.2789 | MainLoss:0.3652 | Alpha:0.0301 | SPLoss:63.5869 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1145 | 5000] LR: 0.001434\n",
      "Train | 16/16 | Loss:2.3431 | MainLoss:0.3607 | Alpha:0.0316 | SPLoss:62.7552 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1146 | 5000] LR: 0.001434\n",
      "Train | 16/16 | Loss:2.2519 | MainLoss:0.3654 | Alpha:0.0305 | SPLoss:61.9082 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1147 | 5000] LR: 0.001434\n",
      "Train | 16/16 | Loss:2.2696 | MainLoss:0.3480 | Alpha:0.0314 | SPLoss:61.0790 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1148 | 5000] LR: 0.001433\n",
      "Train | 16/16 | Loss:2.2055 | MainLoss:0.3500 | Alpha:0.0308 | SPLoss:60.2710 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1149 | 5000] LR: 0.001433\n",
      "Train | 16/16 | Loss:2.1364 | MainLoss:0.3544 | Alpha:0.0300 | SPLoss:59.5040 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1150 | 5000] LR: 0.001433\n",
      "Train | 16/16 | Loss:2.0764 | MainLoss:0.3517 | Alpha:0.0294 | SPLoss:58.7531 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1151 | 5000] LR: 0.001432\n",
      "Train | 16/16 | Loss:2.1341 | MainLoss:0.3812 | Alpha:0.0302 | SPLoss:58.0027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1152 | 5000] LR: 0.001432\n",
      "Train | 16/16 | Loss:2.0590 | MainLoss:0.3504 | Alpha:0.0298 | SPLoss:57.2745 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1153 | 5000] LR: 0.001432\n",
      "Train | 16/16 | Loss:2.0957 | MainLoss:0.3522 | Alpha:0.0308 | SPLoss:56.5590 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1154 | 5000] LR: 0.001432\n",
      "Train | 16/16 | Loss:2.0778 | MainLoss:0.3651 | Alpha:0.0307 | SPLoss:55.8245 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1155 | 5000] LR: 0.001431\n",
      "Train | 16/16 | Loss:2.0713 | MainLoss:0.3529 | Alpha:0.0312 | SPLoss:55.0998 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1156 | 5000] LR: 0.001431\n",
      "Train | 16/16 | Loss:2.0597 | MainLoss:0.3546 | Alpha:0.0314 | SPLoss:54.3739 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1157 | 5000] LR: 0.001431\n",
      "Train | 16/16 | Loss:2.1491 | MainLoss:0.3537 | Alpha:0.0335 | SPLoss:53.6071 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1158 | 5000] LR: 0.001430\n",
      "Train | 16/16 | Loss:1.9679 | MainLoss:0.3537 | Alpha:0.0305 | SPLoss:52.8669 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1159 | 5000] LR: 0.001430\n",
      "Train | 16/16 | Loss:1.9762 | MainLoss:0.3540 | Alpha:0.0311 | SPLoss:52.1840 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1160 | 5000] LR: 0.001430\n",
      "Train | 16/16 | Loss:1.9449 | MainLoss:0.3529 | Alpha:0.0309 | SPLoss:51.5081 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1161 | 5000] LR: 0.001429\n",
      "Train | 16/16 | Loss:1.9305 | MainLoss:0.3396 | Alpha:0.0313 | SPLoss:50.8228 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1162 | 5000] LR: 0.001429\n",
      "Train | 16/16 | Loss:1.9077 | MainLoss:0.3586 | Alpha:0.0309 | SPLoss:50.1586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1163 | 5000] LR: 0.001429\n",
      "Train | 16/16 | Loss:1.8599 | MainLoss:0.3621 | Alpha:0.0303 | SPLoss:49.5146 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1164 | 5000] LR: 0.001428\n",
      "Train | 16/16 | Loss:1.9105 | MainLoss:0.3545 | Alpha:0.0318 | SPLoss:48.8813 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1165 | 5000] LR: 0.001428\n",
      "Train | 16/16 | Loss:1.8551 | MainLoss:0.3285 | Alpha:0.0317 | SPLoss:48.2436 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1166 | 5000] LR: 0.001428\n",
      "Train | 16/16 | Loss:1.8462 | MainLoss:0.3397 | Alpha:0.0316 | SPLoss:47.5962 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1167 | 5000] LR: 0.001427\n",
      "Train | 16/16 | Loss:1.8426 | MainLoss:0.3579 | Alpha:0.0316 | SPLoss:46.9725 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1168 | 5000] LR: 0.001427\n",
      "Train | 16/16 | Loss:1.7824 | MainLoss:0.3631 | Alpha:0.0306 | SPLoss:46.3607 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1169 | 5000] LR: 0.001427\n",
      "Train | 16/16 | Loss:1.7676 | MainLoss:0.3495 | Alpha:0.0310 | SPLoss:45.7466 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1170 | 5000] LR: 0.001427\n",
      "Train | 16/16 | Loss:1.7557 | MainLoss:0.3433 | Alpha:0.0313 | SPLoss:45.1616 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1171 | 5000] LR: 0.001426\n",
      "Train | 16/16 | Loss:1.7032 | MainLoss:0.3405 | Alpha:0.0306 | SPLoss:44.5833 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1172 | 5000] LR: 0.001426\n",
      "Train | 16/16 | Loss:1.7213 | MainLoss:0.3447 | Alpha:0.0313 | SPLoss:44.0139 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1173 | 5000] LR: 0.001426\n",
      "Train | 16/16 | Loss:1.6895 | MainLoss:0.3333 | Alpha:0.0312 | SPLoss:43.4377 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1174 | 5000] LR: 0.001425\n",
      "Train | 16/16 | Loss:1.7268 | MainLoss:0.3312 | Alpha:0.0325 | SPLoss:42.8773 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1175 | 5000] LR: 0.001425\n",
      "Train | 16/16 | Loss:1.6730 | MainLoss:0.3556 | Alpha:0.0311 | SPLoss:42.3175 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1176 | 5000] LR: 0.001425\n",
      "Train | 16/16 | Loss:1.6292 | MainLoss:0.3462 | Alpha:0.0307 | SPLoss:41.7820 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1177 | 5000] LR: 0.001424\n",
      "Train | 16/16 | Loss:1.6013 | MainLoss:0.3354 | Alpha:0.0307 | SPLoss:41.2552 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1178 | 5000] LR: 0.001424\n",
      "Train | 16/16 | Loss:1.6384 | MainLoss:0.3416 | Alpha:0.0318 | SPLoss:40.7357 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1179 | 5000] LR: 0.001424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.5754 | MainLoss:0.3421 | Alpha:0.0307 | SPLoss:40.2088 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1180 | 5000] LR: 0.001423\n",
      "Train | 16/16 | Loss:1.5677 | MainLoss:0.3507 | Alpha:0.0307 | SPLoss:39.7066 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1181 | 5000] LR: 0.001423\n",
      "Train | 16/16 | Loss:1.5861 | MainLoss:0.3539 | Alpha:0.0314 | SPLoss:39.2111 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1182 | 5000] LR: 0.001423\n",
      "Train | 16/16 | Loss:1.5453 | MainLoss:0.3299 | Alpha:0.0314 | SPLoss:38.6952 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1183 | 5000] LR: 0.001422\n",
      "Train | 16/16 | Loss:1.5888 | MainLoss:0.3492 | Alpha:0.0325 | SPLoss:38.1868 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1184 | 5000] LR: 0.001422\n",
      "Train | 16/16 | Loss:1.5512 | MainLoss:0.3415 | Alpha:0.0321 | SPLoss:37.6783 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1185 | 5000] LR: 0.001422\n",
      "Train | 16/16 | Loss:1.5230 | MainLoss:0.3581 | Alpha:0.0313 | SPLoss:37.1928 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1186 | 5000] LR: 0.001422\n",
      "Train | 16/16 | Loss:1.5006 | MainLoss:0.3449 | Alpha:0.0315 | SPLoss:36.7235 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1187 | 5000] LR: 0.001421\n",
      "Train | 16/16 | Loss:1.5174 | MainLoss:0.3413 | Alpha:0.0324 | SPLoss:36.2649 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1188 | 5000] LR: 0.001421\n",
      "Train | 16/16 | Loss:1.5115 | MainLoss:0.3717 | Alpha:0.0319 | SPLoss:35.8047 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1189 | 5000] LR: 0.001421\n",
      "Train | 16/16 | Loss:1.4213 | MainLoss:0.3332 | Alpha:0.0308 | SPLoss:35.3442 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1190 | 5000] LR: 0.001420\n",
      "Train | 16/16 | Loss:1.4225 | MainLoss:0.3271 | Alpha:0.0314 | SPLoss:34.8927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1191 | 5000] LR: 0.001420\n",
      "Train | 16/16 | Loss:1.4535 | MainLoss:0.3386 | Alpha:0.0323 | SPLoss:34.4650 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1192 | 5000] LR: 0.001420\n",
      "Train | 16/16 | Loss:1.3913 | MainLoss:0.3215 | Alpha:0.0314 | SPLoss:34.0351 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1193 | 5000] LR: 0.001419\n",
      "Train | 16/16 | Loss:1.4016 | MainLoss:0.3441 | Alpha:0.0315 | SPLoss:33.6014 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1194 | 5000] LR: 0.001419\n",
      "Train | 16/16 | Loss:1.3790 | MainLoss:0.3461 | Alpha:0.0311 | SPLoss:33.1772 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1195 | 5000] LR: 0.001419\n",
      "Train | 16/16 | Loss:1.3459 | MainLoss:0.3473 | Alpha:0.0305 | SPLoss:32.7728 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1196 | 5000] LR: 0.001418\n",
      "Train | 16/16 | Loss:1.3073 | MainLoss:0.3314 | Alpha:0.0301 | SPLoss:32.3728 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1197 | 5000] LR: 0.001418\n",
      "Train | 16/16 | Loss:1.3226 | MainLoss:0.3377 | Alpha:0.0308 | SPLoss:31.9833 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1198 | 5000] LR: 0.001418\n",
      "Train | 16/16 | Loss:1.3376 | MainLoss:0.3413 | Alpha:0.0315 | SPLoss:31.5954 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1199 | 5000] LR: 0.001417\n",
      "Train | 16/16 | Loss:1.3225 | MainLoss:0.4024 | Alpha:0.0295 | SPLoss:31.2175 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1200 | 5000] LR: 0.001417\n",
      "Train | 16/16 | Loss:1.3544 | MainLoss:0.3488 | Alpha:0.0326 | SPLoss:30.8512 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4436 | MainLoss:0.4436 | SPLoss:30.6405 | CLSLoss:0.0000 | AUROC:0.9044\n",
      "Test | 31/16 | Loss:0.1598 | MainLoss:0.1598 | SPLoss:30.6405 | CLSLoss:0.0000 | AUROC:0.9838\n",
      "\n",
      "Epoch: [1201 | 5000] LR: 0.001417\n",
      "Train | 16/16 | Loss:1.2376 | MainLoss:0.3406 | Alpha:0.0294 | SPLoss:30.4579 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1202 | 5000] LR: 0.001416\n",
      "Train | 16/16 | Loss:1.2737 | MainLoss:0.3663 | Alpha:0.0302 | SPLoss:30.0918 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1203 | 5000] LR: 0.001416\n",
      "Train | 16/16 | Loss:1.3084 | MainLoss:0.3448 | Alpha:0.0324 | SPLoss:29.7413 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1204 | 5000] LR: 0.001416\n",
      "Train | 16/16 | Loss:1.2534 | MainLoss:0.3276 | Alpha:0.0315 | SPLoss:29.3789 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1205 | 5000] LR: 0.001415\n",
      "Train | 16/16 | Loss:1.2799 | MainLoss:0.3587 | Alpha:0.0317 | SPLoss:29.0211 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1206 | 5000] LR: 0.001415\n",
      "Train | 16/16 | Loss:1.2336 | MainLoss:0.3352 | Alpha:0.0313 | SPLoss:28.6671 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1207 | 5000] LR: 0.001415\n",
      "Train | 16/16 | Loss:1.2531 | MainLoss:0.3555 | Alpha:0.0317 | SPLoss:28.3224 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1208 | 5000] LR: 0.001414\n",
      "Train | 16/16 | Loss:1.1693 | MainLoss:0.3390 | Alpha:0.0297 | SPLoss:27.9860 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1209 | 5000] LR: 0.001414\n",
      "Train | 16/16 | Loss:1.1987 | MainLoss:0.3476 | Alpha:0.0308 | SPLoss:27.6576 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1210 | 5000] LR: 0.001414\n",
      "Train | 16/16 | Loss:1.1597 | MainLoss:0.3216 | Alpha:0.0307 | SPLoss:27.3304 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1211 | 5000] LR: 0.001414\n",
      "Train | 16/16 | Loss:1.1614 | MainLoss:0.3383 | Alpha:0.0305 | SPLoss:27.0105 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1212 | 5000] LR: 0.001413\n",
      "Train | 16/16 | Loss:1.1583 | MainLoss:0.3337 | Alpha:0.0309 | SPLoss:26.7029 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1213 | 5000] LR: 0.001413\n",
      "Train | 16/16 | Loss:1.1634 | MainLoss:0.3305 | Alpha:0.0316 | SPLoss:26.3806 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1214 | 5000] LR: 0.001413\n",
      "Train | 16/16 | Loss:1.1415 | MainLoss:0.3255 | Alpha:0.0313 | SPLoss:26.0697 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1215 | 5000] LR: 0.001412\n",
      "Train | 16/16 | Loss:1.1537 | MainLoss:0.3228 | Alpha:0.0323 | SPLoss:25.7509 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1216 | 5000] LR: 0.001412\n",
      "Train | 16/16 | Loss:1.1533 | MainLoss:0.3423 | Alpha:0.0319 | SPLoss:25.4379 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1217 | 5000] LR: 0.001412\n",
      "Train | 16/16 | Loss:1.1164 | MainLoss:0.3447 | Alpha:0.0307 | SPLoss:25.1358 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1218 | 5000] LR: 0.001411\n",
      "Train | 16/16 | Loss:1.1753 | MainLoss:0.3640 | Alpha:0.0327 | SPLoss:24.8384 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1219 | 5000] LR: 0.001411\n",
      "Train | 16/16 | Loss:1.1973 | MainLoss:0.3712 | Alpha:0.0337 | SPLoss:24.5304 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1220 | 5000] LR: 0.001411\n",
      "Train | 16/16 | Loss:1.1032 | MainLoss:0.3409 | Alpha:0.0315 | SPLoss:24.2236 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1221 | 5000] LR: 0.001410\n",
      "Train | 16/16 | Loss:1.1052 | MainLoss:0.3677 | Alpha:0.0308 | SPLoss:23.9439 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1222 | 5000] LR: 0.001410\n",
      "Train | 16/16 | Loss:1.0784 | MainLoss:0.3452 | Alpha:0.0310 | SPLoss:23.6797 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1223 | 5000] LR: 0.001410\n",
      "Train | 16/16 | Loss:1.0701 | MainLoss:0.3426 | Alpha:0.0311 | SPLoss:23.4039 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1224 | 5000] LR: 0.001409\n",
      "Train | 16/16 | Loss:1.0526 | MainLoss:0.3354 | Alpha:0.0310 | SPLoss:23.1352 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1225 | 5000] LR: 0.001409\n",
      "Train | 16/16 | Loss:1.0648 | MainLoss:0.3367 | Alpha:0.0318 | SPLoss:22.8613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1226 | 5000] LR: 0.001409\n",
      "Train | 16/16 | Loss:1.0105 | MainLoss:0.3246 | Alpha:0.0304 | SPLoss:22.5914 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1227 | 5000] LR: 0.001408\n",
      "Train | 16/16 | Loss:1.0770 | MainLoss:0.3429 | Alpha:0.0329 | SPLoss:22.3423 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1228 | 5000] LR: 0.001408\n",
      "Train | 16/16 | Loss:1.0515 | MainLoss:0.3379 | Alpha:0.0323 | SPLoss:22.0795 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1229 | 5000] LR: 0.001408\n",
      "Train | 16/16 | Loss:0.9958 | MainLoss:0.3345 | Alpha:0.0303 | SPLoss:21.8269 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1230 | 5000] LR: 0.001407\n",
      "Train | 16/16 | Loss:1.0022 | MainLoss:0.3265 | Alpha:0.0313 | SPLoss:21.5796 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1231 | 5000] LR: 0.001407\n",
      "Train | 16/16 | Loss:0.9975 | MainLoss:0.3351 | Alpha:0.0310 | SPLoss:21.3291 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1232 | 5000] LR: 0.001407\n",
      "Train | 16/16 | Loss:0.9883 | MainLoss:0.3329 | Alpha:0.0311 | SPLoss:21.0893 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1233 | 5000] LR: 0.001406\n",
      "Train | 16/16 | Loss:0.9803 | MainLoss:0.3310 | Alpha:0.0311 | SPLoss:20.8563 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1234 | 5000] LR: 0.001406\n",
      "Train | 16/16 | Loss:0.9854 | MainLoss:0.3412 | Alpha:0.0312 | SPLoss:20.6144 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1235 | 5000] LR: 0.001406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:1.0036 | MainLoss:0.3287 | Alpha:0.0331 | SPLoss:20.3690 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1236 | 5000] LR: 0.001405\n",
      "Train | 16/16 | Loss:1.0092 | MainLoss:0.3534 | Alpha:0.0326 | SPLoss:20.1304 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1237 | 5000] LR: 0.001405\n",
      "Train | 16/16 | Loss:0.9452 | MainLoss:0.3501 | Alpha:0.0299 | SPLoss:19.9045 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1238 | 5000] LR: 0.001405\n",
      "Train | 16/16 | Loss:0.9418 | MainLoss:0.3233 | Alpha:0.0314 | SPLoss:19.6944 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1239 | 5000] LR: 0.001404\n",
      "Train | 16/16 | Loss:0.9261 | MainLoss:0.3266 | Alpha:0.0308 | SPLoss:19.4880 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1240 | 5000] LR: 0.001404\n",
      "Train | 16/16 | Loss:0.9224 | MainLoss:0.3543 | Alpha:0.0295 | SPLoss:19.2652 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1241 | 5000] LR: 0.001404\n",
      "Train | 16/16 | Loss:0.9659 | MainLoss:0.3484 | Alpha:0.0324 | SPLoss:19.0551 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1242 | 5000] LR: 0.001403\n",
      "Train | 16/16 | Loss:0.9093 | MainLoss:0.3352 | Alpha:0.0305 | SPLoss:18.8374 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1243 | 5000] LR: 0.001403\n",
      "Train | 16/16 | Loss:0.9105 | MainLoss:0.3424 | Alpha:0.0305 | SPLoss:18.6491 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1244 | 5000] LR: 0.001403\n",
      "Train | 16/16 | Loss:0.8884 | MainLoss:0.3287 | Alpha:0.0303 | SPLoss:18.4601 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1245 | 5000] LR: 0.001402\n",
      "Train | 16/16 | Loss:0.8760 | MainLoss:0.3239 | Alpha:0.0302 | SPLoss:18.2663 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1246 | 5000] LR: 0.001402\n",
      "Train | 16/16 | Loss:0.8729 | MainLoss:0.3286 | Alpha:0.0301 | SPLoss:18.0788 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1247 | 5000] LR: 0.001402\n",
      "Train | 16/16 | Loss:0.8995 | MainLoss:0.3375 | Alpha:0.0314 | SPLoss:17.8953 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1248 | 5000] LR: 0.001401\n",
      "Train | 16/16 | Loss:0.8545 | MainLoss:0.3204 | Alpha:0.0302 | SPLoss:17.7018 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1249 | 5000] LR: 0.001401\n",
      "Train | 16/16 | Loss:0.8768 | MainLoss:0.3404 | Alpha:0.0306 | SPLoss:17.5140 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1250 | 5000] LR: 0.001401\n",
      "Train | 16/16 | Loss:0.8557 | MainLoss:0.3271 | Alpha:0.0305 | SPLoss:17.3272 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1251 | 5000] LR: 0.001400\n",
      "Train | 16/16 | Loss:0.8759 | MainLoss:0.3196 | Alpha:0.0324 | SPLoss:17.1534 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1252 | 5000] LR: 0.001400\n",
      "Train | 16/16 | Loss:0.8788 | MainLoss:0.3520 | Alpha:0.0310 | SPLoss:16.9679 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1253 | 5000] LR: 0.001400\n",
      "Train | 16/16 | Loss:0.8456 | MainLoss:0.3358 | Alpha:0.0304 | SPLoss:16.7869 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1254 | 5000] LR: 0.001399\n",
      "Train | 16/16 | Loss:0.8624 | MainLoss:0.3515 | Alpha:0.0307 | SPLoss:16.6182 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1255 | 5000] LR: 0.001399\n",
      "Train | 16/16 | Loss:0.8686 | MainLoss:0.3533 | Alpha:0.0313 | SPLoss:16.4570 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1256 | 5000] LR: 0.001399\n",
      "Train | 16/16 | Loss:0.8542 | MainLoss:0.3463 | Alpha:0.0312 | SPLoss:16.2850 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1257 | 5000] LR: 0.001398\n",
      "Train | 16/16 | Loss:0.8713 | MainLoss:0.3551 | Alpha:0.0320 | SPLoss:16.1209 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1258 | 5000] LR: 0.001398\n",
      "Train | 16/16 | Loss:0.8228 | MainLoss:0.3368 | Alpha:0.0305 | SPLoss:15.9522 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1259 | 5000] LR: 0.001398\n",
      "Train | 16/16 | Loss:0.8102 | MainLoss:0.3299 | Alpha:0.0304 | SPLoss:15.7958 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1260 | 5000] LR: 0.001397\n",
      "Train | 16/16 | Loss:0.8321 | MainLoss:0.3516 | Alpha:0.0307 | SPLoss:15.6297 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1261 | 5000] LR: 0.001397\n",
      "Train | 16/16 | Loss:0.8027 | MainLoss:0.3349 | Alpha:0.0302 | SPLoss:15.4656 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1262 | 5000] LR: 0.001397\n",
      "Train | 16/16 | Loss:0.8577 | MainLoss:0.3619 | Alpha:0.0324 | SPLoss:15.3140 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1263 | 5000] LR: 0.001396\n",
      "Train | 16/16 | Loss:0.8102 | MainLoss:0.3490 | Alpha:0.0304 | SPLoss:15.1655 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1264 | 5000] LR: 0.001396\n",
      "Train | 16/16 | Loss:0.7779 | MainLoss:0.3255 | Alpha:0.0301 | SPLoss:15.0199 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1265 | 5000] LR: 0.001396\n",
      "Train | 16/16 | Loss:0.8054 | MainLoss:0.3458 | Alpha:0.0309 | SPLoss:14.8663 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1266 | 5000] LR: 0.001395\n",
      "Train | 16/16 | Loss:0.7954 | MainLoss:0.3243 | Alpha:0.0320 | SPLoss:14.7084 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1267 | 5000] LR: 0.001395\n",
      "Train | 16/16 | Loss:0.7860 | MainLoss:0.3353 | Alpha:0.0310 | SPLoss:14.5593 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1268 | 5000] LR: 0.001395\n",
      "Train | 16/16 | Loss:0.7979 | MainLoss:0.3377 | Alpha:0.0319 | SPLoss:14.4305 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1269 | 5000] LR: 0.001394\n",
      "Train | 16/16 | Loss:0.7561 | MainLoss:0.3272 | Alpha:0.0300 | SPLoss:14.2960 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1270 | 5000] LR: 0.001394\n",
      "Train | 16/16 | Loss:0.7966 | MainLoss:0.3376 | Alpha:0.0324 | SPLoss:14.1564 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1271 | 5000] LR: 0.001394\n",
      "Train | 16/16 | Loss:0.7628 | MainLoss:0.3331 | Alpha:0.0307 | SPLoss:14.0072 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1272 | 5000] LR: 0.001393\n",
      "Train | 16/16 | Loss:0.7540 | MainLoss:0.3377 | Alpha:0.0300 | SPLoss:13.8670 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1273 | 5000] LR: 0.001393\n",
      "Train | 16/16 | Loss:0.8067 | MainLoss:0.3588 | Alpha:0.0326 | SPLoss:13.7424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1274 | 5000] LR: 0.001393\n",
      "Train | 16/16 | Loss:0.7641 | MainLoss:0.3393 | Alpha:0.0312 | SPLoss:13.6107 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1275 | 5000] LR: 0.001392\n",
      "Train | 16/16 | Loss:0.7680 | MainLoss:0.3551 | Alpha:0.0307 | SPLoss:13.4641 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1276 | 5000] LR: 0.001392\n",
      "Train | 16/16 | Loss:0.7788 | MainLoss:0.3455 | Alpha:0.0325 | SPLoss:13.3262 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1277 | 5000] LR: 0.001392\n",
      "Train | 16/16 | Loss:0.7335 | MainLoss:0.3279 | Alpha:0.0307 | SPLoss:13.2004 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1278 | 5000] LR: 0.001391\n",
      "Train | 16/16 | Loss:0.7532 | MainLoss:0.3395 | Alpha:0.0316 | SPLoss:13.0824 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1279 | 5000] LR: 0.001391\n",
      "Train | 16/16 | Loss:0.7074 | MainLoss:0.3118 | Alpha:0.0305 | SPLoss:12.9670 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1280 | 5000] LR: 0.001391\n",
      "Train | 16/16 | Loss:0.7188 | MainLoss:0.3259 | Alpha:0.0306 | SPLoss:12.8474 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1281 | 5000] LR: 0.001390\n",
      "Train | 16/16 | Loss:0.7380 | MainLoss:0.3346 | Alpha:0.0317 | SPLoss:12.7261 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1282 | 5000] LR: 0.001390\n",
      "Train | 16/16 | Loss:0.7325 | MainLoss:0.3442 | Alpha:0.0308 | SPLoss:12.6118 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1283 | 5000] LR: 0.001390\n",
      "Train | 16/16 | Loss:0.7286 | MainLoss:0.3506 | Alpha:0.0303 | SPLoss:12.4933 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1284 | 5000] LR: 0.001389\n",
      "Train | 16/16 | Loss:0.7075 | MainLoss:0.3296 | Alpha:0.0305 | SPLoss:12.3687 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1285 | 5000] LR: 0.001389\n",
      "Train | 16/16 | Loss:0.6844 | MainLoss:0.3157 | Alpha:0.0301 | SPLoss:12.2575 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1286 | 5000] LR: 0.001389\n",
      "Train | 16/16 | Loss:0.7348 | MainLoss:0.3565 | Alpha:0.0311 | SPLoss:12.1447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1287 | 5000] LR: 0.001388\n",
      "Train | 16/16 | Loss:0.6919 | MainLoss:0.3296 | Alpha:0.0301 | SPLoss:12.0327 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1288 | 5000] LR: 0.001388\n",
      "Train | 16/16 | Loss:0.7170 | MainLoss:0.3557 | Alpha:0.0303 | SPLoss:11.9308 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1289 | 5000] LR: 0.001388\n",
      "Train | 16/16 | Loss:0.7470 | MainLoss:0.3596 | Alpha:0.0327 | SPLoss:11.8289 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1290 | 5000] LR: 0.001387\n",
      "Train | 16/16 | Loss:0.7014 | MainLoss:0.3337 | Alpha:0.0314 | SPLoss:11.7195 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1291 | 5000] LR: 0.001387\n",
      "Train | 16/16 | Loss:0.6801 | MainLoss:0.3128 | Alpha:0.0316 | SPLoss:11.6112 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1292 | 5000] LR: 0.001387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.6998 | MainLoss:0.3396 | Alpha:0.0313 | SPLoss:11.5052 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1293 | 5000] LR: 0.001386\n",
      "Train | 16/16 | Loss:0.7268 | MainLoss:0.3599 | Alpha:0.0322 | SPLoss:11.4020 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1294 | 5000] LR: 0.001386\n",
      "Train | 16/16 | Loss:0.7111 | MainLoss:0.3503 | Alpha:0.0319 | SPLoss:11.2965 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1295 | 5000] LR: 0.001386\n",
      "Train | 16/16 | Loss:0.6702 | MainLoss:0.3218 | Alpha:0.0311 | SPLoss:11.1959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1296 | 5000] LR: 0.001385\n",
      "Train | 16/16 | Loss:0.7095 | MainLoss:0.3486 | Alpha:0.0325 | SPLoss:11.0959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1297 | 5000] LR: 0.001385\n",
      "Train | 16/16 | Loss:0.6647 | MainLoss:0.3260 | Alpha:0.0308 | SPLoss:10.9954 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1298 | 5000] LR: 0.001385\n",
      "Train | 16/16 | Loss:0.6728 | MainLoss:0.3366 | Alpha:0.0308 | SPLoss:10.9012 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1299 | 5000] LR: 0.001384\n",
      "Train | 16/16 | Loss:0.6535 | MainLoss:0.3264 | Alpha:0.0302 | SPLoss:10.8162 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1300 | 5000] LR: 0.001384\n",
      "Train | 16/16 | Loss:0.6456 | MainLoss:0.3187 | Alpha:0.0304 | SPLoss:10.7368 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4874 | MainLoss:0.4874 | SPLoss:10.6957 | CLSLoss:0.0000 | AUROC:0.9022\n",
      "Test | 31/16 | Loss:0.2608 | MainLoss:0.2608 | SPLoss:10.6957 | CLSLoss:0.0000 | AUROC:0.9788\n",
      "\n",
      "Epoch: [1301 | 5000] LR: 0.001384\n",
      "Train | 16/16 | Loss:0.6729 | MainLoss:0.3468 | Alpha:0.0306 | SPLoss:10.6521 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1302 | 5000] LR: 0.001383\n",
      "Train | 16/16 | Loss:0.6347 | MainLoss:0.3226 | Alpha:0.0295 | SPLoss:10.5740 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1303 | 5000] LR: 0.001383\n",
      "Train | 16/16 | Loss:0.6867 | MainLoss:0.3621 | Alpha:0.0310 | SPLoss:10.4891 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1304 | 5000] LR: 0.001382\n",
      "Train | 16/16 | Loss:0.6770 | MainLoss:0.3517 | Alpha:0.0313 | SPLoss:10.3948 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1305 | 5000] LR: 0.001382\n",
      "Train | 16/16 | Loss:0.6506 | MainLoss:0.3223 | Alpha:0.0318 | SPLoss:10.3091 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1306 | 5000] LR: 0.001382\n",
      "Train | 16/16 | Loss:0.6501 | MainLoss:0.3296 | Alpha:0.0313 | SPLoss:10.2233 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1307 | 5000] LR: 0.001381\n",
      "Train | 16/16 | Loss:0.6654 | MainLoss:0.3466 | Alpha:0.0314 | SPLoss:10.1349 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1308 | 5000] LR: 0.001381\n",
      "Train | 16/16 | Loss:0.6575 | MainLoss:0.3393 | Alpha:0.0316 | SPLoss:10.0566 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1309 | 5000] LR: 0.001381\n",
      "Train | 16/16 | Loss:0.6520 | MainLoss:0.3241 | Alpha:0.0328 | SPLoss:9.9849 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1310 | 5000] LR: 0.001380\n",
      "Train | 16/16 | Loss:0.6459 | MainLoss:0.3369 | Alpha:0.0312 | SPLoss:9.8945 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1311 | 5000] LR: 0.001380\n",
      "Train | 16/16 | Loss:0.6445 | MainLoss:0.3338 | Alpha:0.0317 | SPLoss:9.8083 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1312 | 5000] LR: 0.001380\n",
      "Train | 16/16 | Loss:0.6148 | MainLoss:0.3143 | Alpha:0.0309 | SPLoss:9.7314 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1313 | 5000] LR: 0.001379\n",
      "Train | 16/16 | Loss:0.6468 | MainLoss:0.3388 | Alpha:0.0319 | SPLoss:9.6526 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1314 | 5000] LR: 0.001379\n",
      "Train | 16/16 | Loss:0.6153 | MainLoss:0.3242 | Alpha:0.0304 | SPLoss:9.5721 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1315 | 5000] LR: 0.001379\n",
      "Train | 16/16 | Loss:0.6179 | MainLoss:0.3244 | Alpha:0.0309 | SPLoss:9.5084 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1316 | 5000] LR: 0.001378\n",
      "Train | 16/16 | Loss:0.6037 | MainLoss:0.3233 | Alpha:0.0297 | SPLoss:9.4441 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1317 | 5000] LR: 0.001378\n",
      "Train | 16/16 | Loss:0.6145 | MainLoss:0.3269 | Alpha:0.0306 | SPLoss:9.3852 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1318 | 5000] LR: 0.001378\n",
      "Train | 16/16 | Loss:0.6273 | MainLoss:0.3298 | Alpha:0.0320 | SPLoss:9.3102 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1319 | 5000] LR: 0.001377\n",
      "Train | 16/16 | Loss:0.6046 | MainLoss:0.3307 | Alpha:0.0296 | SPLoss:9.2451 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1320 | 5000] LR: 0.001377\n",
      "Train | 16/16 | Loss:0.6144 | MainLoss:0.3277 | Alpha:0.0312 | SPLoss:9.1873 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1321 | 5000] LR: 0.001377\n",
      "Train | 16/16 | Loss:0.6139 | MainLoss:0.3302 | Alpha:0.0311 | SPLoss:9.1173 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1322 | 5000] LR: 0.001376\n",
      "Train | 16/16 | Loss:0.5973 | MainLoss:0.3289 | Alpha:0.0297 | SPLoss:9.0439 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1323 | 5000] LR: 0.001376\n",
      "Train | 16/16 | Loss:0.6002 | MainLoss:0.3325 | Alpha:0.0298 | SPLoss:8.9835 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1324 | 5000] LR: 0.001376\n",
      "Train | 16/16 | Loss:0.6079 | MainLoss:0.3365 | Alpha:0.0304 | SPLoss:8.9228 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1325 | 5000] LR: 0.001375\n",
      "Train | 16/16 | Loss:0.6074 | MainLoss:0.3409 | Alpha:0.0301 | SPLoss:8.8602 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1326 | 5000] LR: 0.001375\n",
      "Train | 16/16 | Loss:0.5706 | MainLoss:0.3061 | Alpha:0.0300 | SPLoss:8.8021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1327 | 5000] LR: 0.001375\n",
      "Train | 16/16 | Loss:0.5915 | MainLoss:0.3237 | Alpha:0.0306 | SPLoss:8.7406 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1328 | 5000] LR: 0.001374\n",
      "Train | 16/16 | Loss:0.6145 | MainLoss:0.3468 | Alpha:0.0308 | SPLoss:8.6816 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1329 | 5000] LR: 0.001374\n",
      "Train | 16/16 | Loss:0.5827 | MainLoss:0.3223 | Alpha:0.0302 | SPLoss:8.6207 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1330 | 5000] LR: 0.001373\n",
      "Train | 16/16 | Loss:0.5592 | MainLoss:0.2934 | Alpha:0.0310 | SPLoss:8.5645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1331 | 5000] LR: 0.001373\n",
      "Train | 16/16 | Loss:0.5973 | MainLoss:0.3216 | Alpha:0.0324 | SPLoss:8.5055 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1332 | 5000] LR: 0.001373\n",
      "Train | 16/16 | Loss:0.6445 | MainLoss:0.3725 | Alpha:0.0322 | SPLoss:8.4491 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1333 | 5000] LR: 0.001372\n",
      "Train | 16/16 | Loss:0.6053 | MainLoss:0.3364 | Alpha:0.0321 | SPLoss:8.3853 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1334 | 5000] LR: 0.001372\n",
      "Train | 16/16 | Loss:0.5693 | MainLoss:0.3194 | Alpha:0.0300 | SPLoss:8.3337 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1335 | 5000] LR: 0.001372\n",
      "Train | 16/16 | Loss:0.6033 | MainLoss:0.3409 | Alpha:0.0317 | SPLoss:8.2821 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1336 | 5000] LR: 0.001371\n",
      "Train | 16/16 | Loss:0.5579 | MainLoss:0.3103 | Alpha:0.0301 | SPLoss:8.2269 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1337 | 5000] LR: 0.001371\n",
      "Train | 16/16 | Loss:0.5856 | MainLoss:0.3262 | Alpha:0.0317 | SPLoss:8.1756 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1338 | 5000] LR: 0.001371\n",
      "Train | 16/16 | Loss:0.5733 | MainLoss:0.3218 | Alpha:0.0310 | SPLoss:8.1216 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1339 | 5000] LR: 0.001370\n",
      "Train | 16/16 | Loss:0.5738 | MainLoss:0.3341 | Alpha:0.0297 | SPLoss:8.0716 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1340 | 5000] LR: 0.001370\n",
      "Train | 16/16 | Loss:0.5618 | MainLoss:0.3213 | Alpha:0.0300 | SPLoss:8.0165 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1341 | 5000] LR: 0.001370\n",
      "Train | 16/16 | Loss:0.5510 | MainLoss:0.3055 | Alpha:0.0308 | SPLoss:7.9661 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1342 | 5000] LR: 0.001369\n",
      "Train | 16/16 | Loss:0.5579 | MainLoss:0.3196 | Alpha:0.0301 | SPLoss:7.9179 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1343 | 5000] LR: 0.001369\n",
      "Train | 16/16 | Loss:0.5394 | MainLoss:0.2951 | Alpha:0.0310 | SPLoss:7.8730 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1344 | 5000] LR: 0.001369\n",
      "Train | 16/16 | Loss:0.5673 | MainLoss:0.3233 | Alpha:0.0312 | SPLoss:7.8228 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1345 | 5000] LR: 0.001368\n",
      "Train | 16/16 | Loss:0.5951 | MainLoss:0.3424 | Alpha:0.0325 | SPLoss:7.7790 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1346 | 5000] LR: 0.001368\n",
      "Train | 16/16 | Loss:0.5887 | MainLoss:0.3388 | Alpha:0.0324 | SPLoss:7.7174 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1347 | 5000] LR: 0.001367\n",
      "Train | 16/16 | Loss:0.5447 | MainLoss:0.3132 | Alpha:0.0302 | SPLoss:7.6684 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1348 | 5000] LR: 0.001367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5679 | MainLoss:0.3230 | Alpha:0.0321 | SPLoss:7.6212 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1349 | 5000] LR: 0.001367\n",
      "Train | 16/16 | Loss:0.5912 | MainLoss:0.3484 | Alpha:0.0321 | SPLoss:7.5661 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1350 | 5000] LR: 0.001366\n",
      "Train | 16/16 | Loss:0.5441 | MainLoss:0.3150 | Alpha:0.0305 | SPLoss:7.5152 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1351 | 5000] LR: 0.001366\n",
      "Train | 16/16 | Loss:0.5549 | MainLoss:0.3239 | Alpha:0.0309 | SPLoss:7.4709 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1352 | 5000] LR: 0.001366\n",
      "Train | 16/16 | Loss:0.5334 | MainLoss:0.3077 | Alpha:0.0304 | SPLoss:7.4321 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1353 | 5000] LR: 0.001365\n",
      "Train | 16/16 | Loss:0.5813 | MainLoss:0.3432 | Alpha:0.0322 | SPLoss:7.3980 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1354 | 5000] LR: 0.001365\n",
      "Train | 16/16 | Loss:0.5757 | MainLoss:0.3422 | Alpha:0.0317 | SPLoss:7.3651 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1355 | 5000] LR: 0.001365\n",
      "Train | 16/16 | Loss:0.5473 | MainLoss:0.3263 | Alpha:0.0302 | SPLoss:7.3276 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1356 | 5000] LR: 0.001364\n",
      "Train | 16/16 | Loss:0.5439 | MainLoss:0.3248 | Alpha:0.0301 | SPLoss:7.2827 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1357 | 5000] LR: 0.001364\n",
      "Train | 16/16 | Loss:0.5868 | MainLoss:0.3533 | Alpha:0.0322 | SPLoss:7.2468 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1358 | 5000] LR: 0.001364\n",
      "Train | 16/16 | Loss:0.5643 | MainLoss:0.3406 | Alpha:0.0310 | SPLoss:7.2048 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1359 | 5000] LR: 0.001363\n",
      "Train | 16/16 | Loss:0.5476 | MainLoss:0.3298 | Alpha:0.0304 | SPLoss:7.1634 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1360 | 5000] LR: 0.001363\n",
      "Train | 16/16 | Loss:0.5579 | MainLoss:0.3349 | Alpha:0.0313 | SPLoss:7.1210 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1361 | 5000] LR: 0.001362\n",
      "Train | 16/16 | Loss:0.5510 | MainLoss:0.3371 | Alpha:0.0302 | SPLoss:7.0811 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1362 | 5000] LR: 0.001362\n",
      "Train | 16/16 | Loss:0.5205 | MainLoss:0.3106 | Alpha:0.0298 | SPLoss:7.0448 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1363 | 5000] LR: 0.001362\n",
      "Train | 16/16 | Loss:0.5428 | MainLoss:0.3168 | Alpha:0.0322 | SPLoss:7.0085 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1364 | 5000] LR: 0.001361\n",
      "Train | 16/16 | Loss:0.5668 | MainLoss:0.3395 | Alpha:0.0326 | SPLoss:6.9730 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1365 | 5000] LR: 0.001361\n",
      "Train | 16/16 | Loss:0.5441 | MainLoss:0.3265 | Alpha:0.0314 | SPLoss:6.9369 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1366 | 5000] LR: 0.001361\n",
      "Train | 16/16 | Loss:0.5589 | MainLoss:0.3433 | Alpha:0.0313 | SPLoss:6.8905 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1367 | 5000] LR: 0.001360\n",
      "Train | 16/16 | Loss:0.5383 | MainLoss:0.3194 | Alpha:0.0319 | SPLoss:6.8544 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1368 | 5000] LR: 0.001360\n",
      "Train | 16/16 | Loss:0.5094 | MainLoss:0.3094 | Alpha:0.0293 | SPLoss:6.8199 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1369 | 5000] LR: 0.001360\n",
      "Train | 16/16 | Loss:0.5504 | MainLoss:0.3371 | Alpha:0.0314 | SPLoss:6.7937 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1370 | 5000] LR: 0.001359\n",
      "Train | 16/16 | Loss:0.5415 | MainLoss:0.3214 | Alpha:0.0326 | SPLoss:6.7613 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1371 | 5000] LR: 0.001359\n",
      "Train | 16/16 | Loss:0.5323 | MainLoss:0.3269 | Alpha:0.0305 | SPLoss:6.7318 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1372 | 5000] LR: 0.001359\n",
      "Train | 16/16 | Loss:0.5752 | MainLoss:0.3542 | Alpha:0.0330 | SPLoss:6.6995 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1373 | 5000] LR: 0.001358\n",
      "Train | 16/16 | Loss:0.5228 | MainLoss:0.3243 | Alpha:0.0298 | SPLoss:6.6678 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1374 | 5000] LR: 0.001358\n",
      "Train | 16/16 | Loss:0.5284 | MainLoss:0.3371 | Alpha:0.0289 | SPLoss:6.6279 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1375 | 5000] LR: 0.001357\n",
      "Train | 16/16 | Loss:0.5494 | MainLoss:0.3431 | Alpha:0.0313 | SPLoss:6.5921 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1376 | 5000] LR: 0.001357\n",
      "Train | 16/16 | Loss:0.5483 | MainLoss:0.3418 | Alpha:0.0315 | SPLoss:6.5582 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1377 | 5000] LR: 0.001357\n",
      "Train | 16/16 | Loss:0.5492 | MainLoss:0.3373 | Alpha:0.0325 | SPLoss:6.5161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1378 | 5000] LR: 0.001356\n",
      "Train | 16/16 | Loss:0.5523 | MainLoss:0.3500 | Alpha:0.0312 | SPLoss:6.4945 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1379 | 5000] LR: 0.001356\n",
      "Train | 16/16 | Loss:0.5115 | MainLoss:0.3083 | Alpha:0.0314 | SPLoss:6.4745 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1380 | 5000] LR: 0.001356\n",
      "Train | 16/16 | Loss:0.5085 | MainLoss:0.3159 | Alpha:0.0299 | SPLoss:6.4442 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1381 | 5000] LR: 0.001355\n",
      "Train | 16/16 | Loss:0.5260 | MainLoss:0.3291 | Alpha:0.0307 | SPLoss:6.4150 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1382 | 5000] LR: 0.001355\n",
      "Train | 16/16 | Loss:0.5395 | MainLoss:0.3314 | Alpha:0.0326 | SPLoss:6.3844 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1383 | 5000] LR: 0.001355\n",
      "Train | 16/16 | Loss:0.5367 | MainLoss:0.3404 | Alpha:0.0309 | SPLoss:6.3457 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1384 | 5000] LR: 0.001354\n",
      "Train | 16/16 | Loss:0.5262 | MainLoss:0.3328 | Alpha:0.0306 | SPLoss:6.3168 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1385 | 5000] LR: 0.001354\n",
      "Train | 16/16 | Loss:0.5018 | MainLoss:0.3107 | Alpha:0.0304 | SPLoss:6.2843 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1386 | 5000] LR: 0.001353\n",
      "Train | 16/16 | Loss:0.5112 | MainLoss:0.3154 | Alpha:0.0313 | SPLoss:6.2565 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1387 | 5000] LR: 0.001353\n",
      "Train | 16/16 | Loss:0.5130 | MainLoss:0.3181 | Alpha:0.0313 | SPLoss:6.2324 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1388 | 5000] LR: 0.001353\n",
      "Train | 16/16 | Loss:0.4905 | MainLoss:0.2966 | Alpha:0.0312 | SPLoss:6.2238 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1389 | 5000] LR: 0.001352\n",
      "Train | 16/16 | Loss:0.5139 | MainLoss:0.3260 | Alpha:0.0303 | SPLoss:6.1985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1390 | 5000] LR: 0.001352\n",
      "Train | 16/16 | Loss:0.5229 | MainLoss:0.3198 | Alpha:0.0329 | SPLoss:6.1731 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1391 | 5000] LR: 0.001352\n",
      "Train | 16/16 | Loss:0.4946 | MainLoss:0.3064 | Alpha:0.0307 | SPLoss:6.1379 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1392 | 5000] LR: 0.001351\n",
      "Train | 16/16 | Loss:0.5171 | MainLoss:0.3273 | Alpha:0.0310 | SPLoss:6.1109 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1393 | 5000] LR: 0.001351\n",
      "Train | 16/16 | Loss:0.5241 | MainLoss:0.3430 | Alpha:0.0298 | SPLoss:6.0813 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1394 | 5000] LR: 0.001351\n",
      "Train | 16/16 | Loss:0.5094 | MainLoss:0.3301 | Alpha:0.0296 | SPLoss:6.0647 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1395 | 5000] LR: 0.001350\n",
      "Train | 16/16 | Loss:0.5077 | MainLoss:0.3213 | Alpha:0.0308 | SPLoss:6.0510 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1396 | 5000] LR: 0.001350\n",
      "Train | 16/16 | Loss:0.5325 | MainLoss:0.3459 | Alpha:0.0310 | SPLoss:6.0251 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1397 | 5000] LR: 0.001349\n",
      "Train | 16/16 | Loss:0.5373 | MainLoss:0.3523 | Alpha:0.0307 | SPLoss:6.0197 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1398 | 5000] LR: 0.001349\n",
      "Train | 16/16 | Loss:0.5444 | MainLoss:0.3578 | Alpha:0.0310 | SPLoss:6.0086 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1399 | 5000] LR: 0.001349\n",
      "Train | 16/16 | Loss:0.5450 | MainLoss:0.3638 | Alpha:0.0302 | SPLoss:5.9890 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1400 | 5000] LR: 0.001348\n",
      "Train | 16/16 | Loss:0.5258 | MainLoss:0.3364 | Alpha:0.0317 | SPLoss:5.9739 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4216 | MainLoss:0.4216 | SPLoss:5.9626 | CLSLoss:0.0000 | AUROC:0.9031\n",
      "Test | 31/16 | Loss:0.2004 | MainLoss:0.2004 | SPLoss:5.9626 | CLSLoss:0.0000 | AUROC:0.9779\n",
      "\n",
      "Epoch: [1401 | 5000] LR: 0.001348\n",
      "Train | 16/16 | Loss:0.5096 | MainLoss:0.3319 | Alpha:0.0299 | SPLoss:5.9548 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1402 | 5000] LR: 0.001348\n",
      "Train | 16/16 | Loss:0.5155 | MainLoss:0.3297 | Alpha:0.0314 | SPLoss:5.9250 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1403 | 5000] LR: 0.001347\n",
      "Train | 16/16 | Loss:0.4962 | MainLoss:0.3139 | Alpha:0.0309 | SPLoss:5.8944 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1404 | 5000] LR: 0.001347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5109 | MainLoss:0.3216 | Alpha:0.0323 | SPLoss:5.8693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1405 | 5000] LR: 0.001347\n",
      "Train | 16/16 | Loss:0.4954 | MainLoss:0.3210 | Alpha:0.0298 | SPLoss:5.8456 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1406 | 5000] LR: 0.001346\n",
      "Train | 16/16 | Loss:0.5273 | MainLoss:0.3458 | Alpha:0.0311 | SPLoss:5.8263 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1407 | 5000] LR: 0.001346\n",
      "Train | 16/16 | Loss:0.5262 | MainLoss:0.3465 | Alpha:0.0310 | SPLoss:5.8038 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1408 | 5000] LR: 0.001345\n",
      "Train | 16/16 | Loss:0.4982 | MainLoss:0.3256 | Alpha:0.0299 | SPLoss:5.7822 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1409 | 5000] LR: 0.001345\n",
      "Train | 16/16 | Loss:0.5245 | MainLoss:0.3458 | Alpha:0.0310 | SPLoss:5.7652 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1410 | 5000] LR: 0.001345\n",
      "Train | 16/16 | Loss:0.5553 | MainLoss:0.3737 | Alpha:0.0316 | SPLoss:5.7458 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1411 | 5000] LR: 0.001344\n",
      "Train | 16/16 | Loss:0.5394 | MainLoss:0.3586 | Alpha:0.0316 | SPLoss:5.7197 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1412 | 5000] LR: 0.001344\n",
      "Train | 16/16 | Loss:0.5245 | MainLoss:0.3487 | Alpha:0.0308 | SPLoss:5.7030 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1413 | 5000] LR: 0.001344\n",
      "Train | 16/16 | Loss:0.4693 | MainLoss:0.3054 | Alpha:0.0288 | SPLoss:5.6845 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1414 | 5000] LR: 0.001343\n",
      "Train | 16/16 | Loss:0.5080 | MainLoss:0.3302 | Alpha:0.0314 | SPLoss:5.6710 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1415 | 5000] LR: 0.001343\n",
      "Train | 16/16 | Loss:0.5178 | MainLoss:0.3362 | Alpha:0.0322 | SPLoss:5.6460 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1416 | 5000] LR: 0.001342\n",
      "Train | 16/16 | Loss:0.5113 | MainLoss:0.3284 | Alpha:0.0325 | SPLoss:5.6232 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1417 | 5000] LR: 0.001342\n",
      "Train | 16/16 | Loss:0.5008 | MainLoss:0.3284 | Alpha:0.0308 | SPLoss:5.6003 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1418 | 5000] LR: 0.001342\n",
      "Train | 16/16 | Loss:0.4726 | MainLoss:0.3038 | Alpha:0.0302 | SPLoss:5.5917 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1419 | 5000] LR: 0.001341\n",
      "Train | 16/16 | Loss:0.4723 | MainLoss:0.3021 | Alpha:0.0305 | SPLoss:5.5859 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1420 | 5000] LR: 0.001341\n",
      "Train | 16/16 | Loss:0.4885 | MainLoss:0.3175 | Alpha:0.0307 | SPLoss:5.5748 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1421 | 5000] LR: 0.001341\n",
      "Train | 16/16 | Loss:0.4898 | MainLoss:0.3126 | Alpha:0.0319 | SPLoss:5.5510 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1422 | 5000] LR: 0.001340\n",
      "Train | 16/16 | Loss:0.5097 | MainLoss:0.3316 | Alpha:0.0322 | SPLoss:5.5368 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1423 | 5000] LR: 0.001340\n",
      "Train | 16/16 | Loss:0.4943 | MainLoss:0.3171 | Alpha:0.0321 | SPLoss:5.5150 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1424 | 5000] LR: 0.001340\n",
      "Train | 16/16 | Loss:0.4992 | MainLoss:0.3306 | Alpha:0.0307 | SPLoss:5.4949 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1425 | 5000] LR: 0.001339\n",
      "Train | 16/16 | Loss:0.4976 | MainLoss:0.3310 | Alpha:0.0304 | SPLoss:5.4752 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1426 | 5000] LR: 0.001339\n",
      "Train | 16/16 | Loss:0.4856 | MainLoss:0.3151 | Alpha:0.0312 | SPLoss:5.4631 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1427 | 5000] LR: 0.001338\n",
      "Train | 16/16 | Loss:0.4792 | MainLoss:0.3116 | Alpha:0.0307 | SPLoss:5.4546 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1428 | 5000] LR: 0.001338\n",
      "Train | 16/16 | Loss:0.4915 | MainLoss:0.3214 | Alpha:0.0313 | SPLoss:5.4401 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1429 | 5000] LR: 0.001338\n",
      "Train | 16/16 | Loss:0.4907 | MainLoss:0.3275 | Alpha:0.0301 | SPLoss:5.4220 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1430 | 5000] LR: 0.001337\n",
      "Train | 16/16 | Loss:0.5067 | MainLoss:0.3402 | Alpha:0.0308 | SPLoss:5.4138 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1431 | 5000] LR: 0.001337\n",
      "Train | 16/16 | Loss:0.4978 | MainLoss:0.3279 | Alpha:0.0314 | SPLoss:5.4108 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1432 | 5000] LR: 0.001337\n",
      "Train | 16/16 | Loss:0.4936 | MainLoss:0.3261 | Alpha:0.0310 | SPLoss:5.3968 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1433 | 5000] LR: 0.001336\n",
      "Train | 16/16 | Loss:0.4690 | MainLoss:0.3013 | Alpha:0.0312 | SPLoss:5.3793 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1434 | 5000] LR: 0.001336\n",
      "Train | 16/16 | Loss:0.4881 | MainLoss:0.3179 | Alpha:0.0317 | SPLoss:5.3641 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1435 | 5000] LR: 0.001335\n",
      "Train | 16/16 | Loss:0.5038 | MainLoss:0.3322 | Alpha:0.0307 | SPLoss:5.6151 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1436 | 5000] LR: 0.001335\n",
      "Train | 16/16 | Loss:0.5834 | MainLoss:0.3299 | Alpha:0.0308 | SPLoss:8.2597 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1437 | 5000] LR: 0.001335\n",
      "Train | 16/16 | Loss:0.6188 | MainLoss:0.3174 | Alpha:0.0313 | SPLoss:9.6381 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1438 | 5000] LR: 0.001334\n",
      "Train | 16/16 | Loss:0.6082 | MainLoss:0.2920 | Alpha:0.0320 | SPLoss:9.8761 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1439 | 5000] LR: 0.001334\n",
      "Train | 16/16 | Loss:0.6209 | MainLoss:0.3202 | Alpha:0.0305 | SPLoss:9.8546 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1440 | 5000] LR: 0.001334\n",
      "Train | 16/16 | Loss:0.6282 | MainLoss:0.3184 | Alpha:0.0316 | SPLoss:9.7955 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1441 | 5000] LR: 0.001333\n",
      "Train | 16/16 | Loss:0.6469 | MainLoss:0.3342 | Alpha:0.0322 | SPLoss:9.7141 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1442 | 5000] LR: 0.001333\n",
      "Train | 16/16 | Loss:0.6369 | MainLoss:0.3462 | Alpha:0.0301 | SPLoss:9.6450 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1443 | 5000] LR: 0.001332\n",
      "Train | 16/16 | Loss:0.6046 | MainLoss:0.3183 | Alpha:0.0299 | SPLoss:9.5834 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1444 | 5000] LR: 0.001332\n",
      "Train | 16/16 | Loss:0.6038 | MainLoss:0.3099 | Alpha:0.0309 | SPLoss:9.5209 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1445 | 5000] LR: 0.001332\n",
      "Train | 16/16 | Loss:0.6160 | MainLoss:0.3318 | Alpha:0.0300 | SPLoss:9.4619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1446 | 5000] LR: 0.001331\n",
      "Train | 16/16 | Loss:0.6150 | MainLoss:0.3324 | Alpha:0.0301 | SPLoss:9.3929 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1447 | 5000] LR: 0.001331\n",
      "Train | 16/16 | Loss:0.6011 | MainLoss:0.3113 | Alpha:0.0311 | SPLoss:9.3303 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1448 | 5000] LR: 0.001331\n",
      "Train | 16/16 | Loss:0.6122 | MainLoss:0.3261 | Alpha:0.0309 | SPLoss:9.2654 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1449 | 5000] LR: 0.001330\n",
      "Train | 16/16 | Loss:0.5964 | MainLoss:0.3240 | Alpha:0.0296 | SPLoss:9.1939 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1450 | 5000] LR: 0.001330\n",
      "Train | 16/16 | Loss:0.6090 | MainLoss:0.3227 | Alpha:0.0314 | SPLoss:9.1259 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1451 | 5000] LR: 0.001329\n",
      "Train | 16/16 | Loss:0.6171 | MainLoss:0.3344 | Alpha:0.0312 | SPLoss:9.0611 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1452 | 5000] LR: 0.001329\n",
      "Train | 16/16 | Loss:0.6004 | MainLoss:0.3252 | Alpha:0.0306 | SPLoss:9.0049 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1453 | 5000] LR: 0.001329\n",
      "Train | 16/16 | Loss:0.6444 | MainLoss:0.3557 | Alpha:0.0323 | SPLoss:8.9507 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1454 | 5000] LR: 0.001328\n",
      "Train | 16/16 | Loss:0.5947 | MainLoss:0.3266 | Alpha:0.0302 | SPLoss:8.8749 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1455 | 5000] LR: 0.001328\n",
      "Train | 16/16 | Loss:0.5741 | MainLoss:0.3049 | Alpha:0.0305 | SPLoss:8.8203 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1456 | 5000] LR: 0.001328\n",
      "Train | 16/16 | Loss:0.5884 | MainLoss:0.3285 | Alpha:0.0296 | SPLoss:8.7673 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1457 | 5000] LR: 0.001327\n",
      "Train | 16/16 | Loss:0.5319 | MainLoss:0.2853 | Alpha:0.0283 | SPLoss:8.7198 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1458 | 5000] LR: 0.001327\n",
      "Train | 16/16 | Loss:0.5930 | MainLoss:0.3271 | Alpha:0.0307 | SPLoss:8.6691 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1459 | 5000] LR: 0.001326\n",
      "Train | 16/16 | Loss:0.5923 | MainLoss:0.3259 | Alpha:0.0309 | SPLoss:8.6171 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1460 | 5000] LR: 0.001326\n",
      "Train | 16/16 | Loss:0.5744 | MainLoss:0.3174 | Alpha:0.0300 | SPLoss:8.5666 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1461 | 5000] LR: 0.001326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5850 | MainLoss:0.3158 | Alpha:0.0316 | SPLoss:8.5033 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1462 | 5000] LR: 0.001325\n",
      "Train | 16/16 | Loss:0.6004 | MainLoss:0.3283 | Alpha:0.0322 | SPLoss:8.4408 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1463 | 5000] LR: 0.001325\n",
      "Train | 16/16 | Loss:0.5698 | MainLoss:0.3078 | Alpha:0.0312 | SPLoss:8.3886 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1464 | 5000] LR: 0.001325\n",
      "Train | 16/16 | Loss:0.5635 | MainLoss:0.3077 | Alpha:0.0307 | SPLoss:8.3412 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1465 | 5000] LR: 0.001324\n",
      "Train | 16/16 | Loss:0.5662 | MainLoss:0.3079 | Alpha:0.0311 | SPLoss:8.2961 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1466 | 5000] LR: 0.001324\n",
      "Train | 16/16 | Loss:0.6019 | MainLoss:0.3406 | Alpha:0.0317 | SPLoss:8.2452 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1467 | 5000] LR: 0.001323\n",
      "Train | 16/16 | Loss:0.5553 | MainLoss:0.3158 | Alpha:0.0293 | SPLoss:8.1874 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1468 | 5000] LR: 0.001323\n",
      "Train | 16/16 | Loss:0.5784 | MainLoss:0.3173 | Alpha:0.0321 | SPLoss:8.1369 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1469 | 5000] LR: 0.001323\n",
      "Train | 16/16 | Loss:0.5808 | MainLoss:0.3181 | Alpha:0.0325 | SPLoss:8.0799 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1470 | 5000] LR: 0.001322\n",
      "Train | 16/16 | Loss:0.5519 | MainLoss:0.2954 | Alpha:0.0319 | SPLoss:8.0332 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1471 | 5000] LR: 0.001322\n",
      "Train | 16/16 | Loss:0.5491 | MainLoss:0.3056 | Alpha:0.0305 | SPLoss:7.9824 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1472 | 5000] LR: 0.001321\n",
      "Train | 16/16 | Loss:0.5602 | MainLoss:0.3143 | Alpha:0.0310 | SPLoss:7.9376 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1473 | 5000] LR: 0.001321\n",
      "Train | 16/16 | Loss:0.5675 | MainLoss:0.3100 | Alpha:0.0326 | SPLoss:7.8945 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1474 | 5000] LR: 0.001321\n",
      "Train | 16/16 | Loss:0.5740 | MainLoss:0.3316 | Alpha:0.0309 | SPLoss:7.8425 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1475 | 5000] LR: 0.001320\n",
      "Train | 16/16 | Loss:0.5618 | MainLoss:0.3161 | Alpha:0.0315 | SPLoss:7.7910 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1476 | 5000] LR: 0.001320\n",
      "Train | 16/16 | Loss:0.5634 | MainLoss:0.3175 | Alpha:0.0318 | SPLoss:7.7391 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1477 | 5000] LR: 0.001320\n",
      "Train | 16/16 | Loss:0.5652 | MainLoss:0.3226 | Alpha:0.0315 | SPLoss:7.7004 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1478 | 5000] LR: 0.001319\n",
      "Train | 16/16 | Loss:0.5791 | MainLoss:0.3375 | Alpha:0.0315 | SPLoss:7.6588 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1479 | 5000] LR: 0.001319\n",
      "Train | 16/16 | Loss:0.5861 | MainLoss:0.3504 | Alpha:0.0310 | SPLoss:7.6148 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1480 | 5000] LR: 0.001318\n",
      "Train | 16/16 | Loss:0.5600 | MainLoss:0.3334 | Alpha:0.0299 | SPLoss:7.5756 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1481 | 5000] LR: 0.001318\n",
      "Train | 16/16 | Loss:0.5129 | MainLoss:0.2912 | Alpha:0.0295 | SPLoss:7.5287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1482 | 5000] LR: 0.001318\n",
      "Train | 16/16 | Loss:0.5607 | MainLoss:0.3234 | Alpha:0.0317 | SPLoss:7.4869 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1483 | 5000] LR: 0.001317\n",
      "Train | 16/16 | Loss:0.5393 | MainLoss:0.3156 | Alpha:0.0300 | SPLoss:7.4426 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1484 | 5000] LR: 0.001317\n",
      "Train | 16/16 | Loss:0.5751 | MainLoss:0.3396 | Alpha:0.0318 | SPLoss:7.4040 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1485 | 5000] LR: 0.001316\n",
      "Train | 16/16 | Loss:0.5456 | MainLoss:0.3284 | Alpha:0.0295 | SPLoss:7.3757 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1486 | 5000] LR: 0.001316\n",
      "Train | 16/16 | Loss:0.5519 | MainLoss:0.3292 | Alpha:0.0303 | SPLoss:7.3479 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1487 | 5000] LR: 0.001316\n",
      "Train | 16/16 | Loss:0.5618 | MainLoss:0.3383 | Alpha:0.0306 | SPLoss:7.3150 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1488 | 5000] LR: 0.001315\n",
      "Train | 16/16 | Loss:0.5662 | MainLoss:0.3297 | Alpha:0.0325 | SPLoss:7.2712 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1489 | 5000] LR: 0.001315\n",
      "Train | 16/16 | Loss:0.5509 | MainLoss:0.3191 | Alpha:0.0321 | SPLoss:7.2220 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1490 | 5000] LR: 0.001315\n",
      "Train | 16/16 | Loss:0.5307 | MainLoss:0.3123 | Alpha:0.0305 | SPLoss:7.1693 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1491 | 5000] LR: 0.001314\n",
      "Train | 16/16 | Loss:0.5645 | MainLoss:0.3375 | Alpha:0.0319 | SPLoss:7.1211 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1492 | 5000] LR: 0.001314\n",
      "Train | 16/16 | Loss:0.5637 | MainLoss:0.3461 | Alpha:0.0308 | SPLoss:7.0777 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1493 | 5000] LR: 0.001313\n",
      "Train | 16/16 | Loss:0.5335 | MainLoss:0.3169 | Alpha:0.0308 | SPLoss:7.0374 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1494 | 5000] LR: 0.001313\n",
      "Train | 16/16 | Loss:0.5610 | MainLoss:0.3300 | Alpha:0.0330 | SPLoss:6.9959 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1495 | 5000] LR: 0.001313\n",
      "Train | 16/16 | Loss:0.5174 | MainLoss:0.2942 | Alpha:0.0321 | SPLoss:6.9486 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1496 | 5000] LR: 0.001312\n",
      "Train | 16/16 | Loss:0.5374 | MainLoss:0.3310 | Alpha:0.0299 | SPLoss:6.9107 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1497 | 5000] LR: 0.001312\n",
      "Train | 16/16 | Loss:0.5622 | MainLoss:0.3412 | Alpha:0.0321 | SPLoss:6.8743 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1498 | 5000] LR: 0.001311\n",
      "Train | 16/16 | Loss:0.5352 | MainLoss:0.3209 | Alpha:0.0313 | SPLoss:6.8413 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1499 | 5000] LR: 0.001311\n",
      "Train | 16/16 | Loss:0.5286 | MainLoss:0.3126 | Alpha:0.0317 | SPLoss:6.8140 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1500 | 5000] LR: 0.001311\n",
      "Train | 16/16 | Loss:0.5423 | MainLoss:0.3325 | Alpha:0.0309 | SPLoss:6.7865 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.5373 | MainLoss:0.5373 | SPLoss:6.7729 | CLSLoss:0.0000 | AUROC:0.8991\n",
      "Test | 31/16 | Loss:0.1724 | MainLoss:0.1724 | SPLoss:6.7729 | CLSLoss:0.0000 | AUROC:0.9813\n",
      "\n",
      "Epoch: [1501 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5153 | MainLoss:0.3059 | Alpha:0.0309 | SPLoss:6.7763 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1502 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.2958 | Alpha:0.0302 | SPLoss:6.7743 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1503 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5452 | MainLoss:0.3244 | Alpha:0.0326 | SPLoss:6.7710 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1504 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5181 | MainLoss:0.3034 | Alpha:0.0317 | SPLoss:6.7659 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1505 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5101 | MainLoss:0.2960 | Alpha:0.0317 | SPLoss:6.7615 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1506 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5015 | MainLoss:0.3030 | Alpha:0.0294 | SPLoss:6.7573 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1507 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5257 | MainLoss:0.3082 | Alpha:0.0322 | SPLoss:6.7541 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1508 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5030 | MainLoss:0.2946 | Alpha:0.0309 | SPLoss:6.7492 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1509 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5000 | MainLoss:0.2939 | Alpha:0.0306 | SPLoss:6.7454 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1510 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5211 | MainLoss:0.3095 | Alpha:0.0314 | SPLoss:6.7429 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1511 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5367 | MainLoss:0.3219 | Alpha:0.0319 | SPLoss:6.7380 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1512 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5337 | MainLoss:0.3172 | Alpha:0.0321 | SPLoss:6.7348 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1513 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5242 | MainLoss:0.3046 | Alpha:0.0326 | SPLoss:6.7308 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1514 | 5000] LR: 0.000131\n",
      "Train | 16/16 | Loss:0.5167 | MainLoss:0.3046 | Alpha:0.0315 | SPLoss:6.7266 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1515 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5099 | MainLoss:0.3086 | Alpha:0.0300 | SPLoss:6.7222 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1516 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5279 | MainLoss:0.3160 | Alpha:0.0315 | SPLoss:6.7201 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1517 | 5000] LR: 0.000130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5148 | MainLoss:0.3038 | Alpha:0.0314 | SPLoss:6.7163 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1518 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5057 | MainLoss:0.2968 | Alpha:0.0311 | SPLoss:6.7132 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1519 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5142 | MainLoss:0.3028 | Alpha:0.0315 | SPLoss:6.7100 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1520 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5191 | MainLoss:0.3048 | Alpha:0.0320 | SPLoss:6.7065 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1521 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5138 | MainLoss:0.3045 | Alpha:0.0312 | SPLoss:6.7019 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1522 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4992 | MainLoss:0.2908 | Alpha:0.0311 | SPLoss:6.6985 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1523 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4864 | MainLoss:0.2819 | Alpha:0.0306 | SPLoss:6.6948 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1524 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4778 | MainLoss:0.2779 | Alpha:0.0299 | SPLoss:6.6919 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1525 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4837 | MainLoss:0.2848 | Alpha:0.0297 | SPLoss:6.6891 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1526 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.2931 | Alpha:0.0310 | SPLoss:6.6866 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1527 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4856 | MainLoss:0.2923 | Alpha:0.0289 | SPLoss:6.6830 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1528 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5049 | MainLoss:0.2966 | Alpha:0.0312 | SPLoss:6.6802 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1529 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4935 | MainLoss:0.2944 | Alpha:0.0298 | SPLoss:6.6777 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1530 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4934 | MainLoss:0.2899 | Alpha:0.0305 | SPLoss:6.6749 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1531 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5157 | MainLoss:0.3025 | Alpha:0.0320 | SPLoss:6.6721 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1532 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5167 | MainLoss:0.3108 | Alpha:0.0309 | SPLoss:6.6684 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1533 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5243 | MainLoss:0.3177 | Alpha:0.0310 | SPLoss:6.6651 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1534 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5147 | MainLoss:0.3011 | Alpha:0.0321 | SPLoss:6.6621 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1535 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5184 | MainLoss:0.3077 | Alpha:0.0317 | SPLoss:6.6582 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1536 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5092 | MainLoss:0.2857 | Alpha:0.0336 | SPLoss:6.6546 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1537 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5051 | MainLoss:0.2983 | Alpha:0.0311 | SPLoss:6.6516 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1538 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5371 | MainLoss:0.3185 | Alpha:0.0329 | SPLoss:6.6480 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1539 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.5116 | MainLoss:0.2967 | Alpha:0.0323 | SPLoss:6.6447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1540 | 5000] LR: 0.000130\n",
      "Train | 16/16 | Loss:0.4903 | MainLoss:0.2843 | Alpha:0.0310 | SPLoss:6.6408 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1541 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5070 | MainLoss:0.3077 | Alpha:0.0300 | SPLoss:6.6376 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1542 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5200 | MainLoss:0.3137 | Alpha:0.0311 | SPLoss:6.6348 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1543 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4951 | MainLoss:0.2857 | Alpha:0.0316 | SPLoss:6.6306 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1544 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.2864 | Alpha:0.0323 | SPLoss:6.6270 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1545 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5060 | MainLoss:0.2993 | Alpha:0.0312 | SPLoss:6.6247 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1546 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4847 | MainLoss:0.2854 | Alpha:0.0301 | SPLoss:6.6204 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1547 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5219 | MainLoss:0.3158 | Alpha:0.0311 | SPLoss:6.6167 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1548 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4992 | MainLoss:0.3016 | Alpha:0.0299 | SPLoss:6.6138 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1549 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5166 | MainLoss:0.3075 | Alpha:0.0316 | SPLoss:6.6109 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1550 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4845 | MainLoss:0.2811 | Alpha:0.0308 | SPLoss:6.6078 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1551 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5146 | MainLoss:0.3119 | Alpha:0.0307 | SPLoss:6.6064 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1552 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4945 | MainLoss:0.2890 | Alpha:0.0311 | SPLoss:6.6038 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1553 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5383 | MainLoss:0.3229 | Alpha:0.0326 | SPLoss:6.6006 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1554 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5205 | MainLoss:0.3158 | Alpha:0.0310 | SPLoss:6.5972 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1555 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4918 | MainLoss:0.2956 | Alpha:0.0298 | SPLoss:6.5927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1556 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5183 | MainLoss:0.3169 | Alpha:0.0306 | SPLoss:6.5889 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1557 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5468 | MainLoss:0.3368 | Alpha:0.0319 | SPLoss:6.5856 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1558 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4969 | MainLoss:0.2929 | Alpha:0.0310 | SPLoss:6.5830 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1559 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5060 | MainLoss:0.3028 | Alpha:0.0309 | SPLoss:6.5808 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1560 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.2998 | Alpha:0.0305 | SPLoss:6.5775 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1561 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5027 | MainLoss:0.2953 | Alpha:0.0315 | SPLoss:6.5750 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1562 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5204 | MainLoss:0.3149 | Alpha:0.0313 | SPLoss:6.5718 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1563 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4864 | MainLoss:0.2880 | Alpha:0.0302 | SPLoss:6.5683 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1564 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.5051 | MainLoss:0.3022 | Alpha:0.0309 | SPLoss:6.5645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1565 | 5000] LR: 0.000129\n",
      "Train | 16/16 | Loss:0.4771 | MainLoss:0.2735 | Alpha:0.0310 | SPLoss:6.5612 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1566 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4976 | MainLoss:0.2962 | Alpha:0.0307 | SPLoss:6.5586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1567 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4865 | MainLoss:0.2815 | Alpha:0.0313 | SPLoss:6.5554 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1568 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4909 | MainLoss:0.2906 | Alpha:0.0306 | SPLoss:6.5526 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1569 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5048 | MainLoss:0.2963 | Alpha:0.0318 | SPLoss:6.5497 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1570 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4820 | MainLoss:0.2823 | Alpha:0.0305 | SPLoss:6.5462 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1571 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4780 | MainLoss:0.2847 | Alpha:0.0295 | SPLoss:6.5438 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1572 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5062 | MainLoss:0.3027 | Alpha:0.0311 | SPLoss:6.5416 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1573 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5065 | MainLoss:0.3062 | Alpha:0.0306 | SPLoss:6.5390 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1574 | 5000] LR: 0.000128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.4990 | MainLoss:0.2952 | Alpha:0.0312 | SPLoss:6.5353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1575 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4991 | MainLoss:0.3000 | Alpha:0.0305 | SPLoss:6.5310 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1576 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5359 | MainLoss:0.3226 | Alpha:0.0327 | SPLoss:6.5282 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1577 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4922 | MainLoss:0.2906 | Alpha:0.0309 | SPLoss:6.5252 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1578 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4991 | MainLoss:0.2941 | Alpha:0.0314 | SPLoss:6.5219 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1579 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5058 | MainLoss:0.3034 | Alpha:0.0310 | SPLoss:6.5187 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1580 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5129 | MainLoss:0.3138 | Alpha:0.0305 | SPLoss:6.5168 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1581 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4966 | MainLoss:0.2888 | Alpha:0.0319 | SPLoss:6.5141 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1582 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5242 | MainLoss:0.3168 | Alpha:0.0319 | SPLoss:6.5106 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1583 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4904 | MainLoss:0.2904 | Alpha:0.0307 | SPLoss:6.5071 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1584 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5210 | MainLoss:0.3139 | Alpha:0.0319 | SPLoss:6.5036 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1585 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4914 | MainLoss:0.2949 | Alpha:0.0302 | SPLoss:6.5002 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1586 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.2984 | Alpha:0.0311 | SPLoss:6.4973 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1587 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4834 | MainLoss:0.2798 | Alpha:0.0314 | SPLoss:6.4950 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1588 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4941 | MainLoss:0.2940 | Alpha:0.0308 | SPLoss:6.4935 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1589 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.4983 | MainLoss:0.2968 | Alpha:0.0310 | SPLoss:6.4899 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1590 | 5000] LR: 0.000128\n",
      "Train | 16/16 | Loss:0.5056 | MainLoss:0.2995 | Alpha:0.0318 | SPLoss:6.4862 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1591 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4961 | MainLoss:0.3037 | Alpha:0.0297 | SPLoss:6.4838 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1592 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4796 | MainLoss:0.2823 | Alpha:0.0304 | SPLoss:6.4814 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1593 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4670 | MainLoss:0.2758 | Alpha:0.0295 | SPLoss:6.4787 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1594 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4910 | MainLoss:0.2907 | Alpha:0.0309 | SPLoss:6.4754 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1595 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4755 | MainLoss:0.2820 | Alpha:0.0299 | SPLoss:6.4735 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1596 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5125 | MainLoss:0.3121 | Alpha:0.0310 | SPLoss:6.4715 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1597 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4769 | MainLoss:0.2788 | Alpha:0.0306 | SPLoss:6.4687 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1598 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4755 | MainLoss:0.2865 | Alpha:0.0292 | SPLoss:6.4663 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1599 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4810 | MainLoss:0.2844 | Alpha:0.0304 | SPLoss:6.4648 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1600 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4948 | MainLoss:0.2867 | Alpha:0.0322 | SPLoss:6.4615 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4722 | MainLoss:0.4722 | SPLoss:6.4604 | CLSLoss:0.0000 | AUROC:0.9064\n",
      "Test | 31/16 | Loss:0.2472 | MainLoss:0.2472 | SPLoss:6.4604 | CLSLoss:0.0000 | AUROC:0.9775\n",
      "\n",
      "Epoch: [1601 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5269 | MainLoss:0.3313 | Alpha:0.0303 | SPLoss:6.4577 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1602 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4950 | MainLoss:0.3005 | Alpha:0.0301 | SPLoss:6.4545 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1603 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5108 | MainLoss:0.3121 | Alpha:0.0308 | SPLoss:6.4516 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1604 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4915 | MainLoss:0.2980 | Alpha:0.0300 | SPLoss:6.4492 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1605 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5021 | MainLoss:0.2982 | Alpha:0.0316 | SPLoss:6.4470 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1606 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4695 | MainLoss:0.2704 | Alpha:0.0309 | SPLoss:6.4436 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1607 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4973 | MainLoss:0.2890 | Alpha:0.0323 | SPLoss:6.4409 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1608 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4961 | MainLoss:0.2957 | Alpha:0.0311 | SPLoss:6.4383 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1609 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5235 | MainLoss:0.3095 | Alpha:0.0333 | SPLoss:6.4353 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1610 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5135 | MainLoss:0.3047 | Alpha:0.0325 | SPLoss:6.4319 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1611 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4970 | MainLoss:0.2902 | Alpha:0.0322 | SPLoss:6.4279 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1612 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.5006 | MainLoss:0.2990 | Alpha:0.0314 | SPLoss:6.4246 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1613 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4777 | MainLoss:0.2832 | Alpha:0.0303 | SPLoss:6.4212 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1614 | 5000] LR: 0.000127\n",
      "Train | 16/16 | Loss:0.4894 | MainLoss:0.2917 | Alpha:0.0308 | SPLoss:6.4182 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1615 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4802 | MainLoss:0.2830 | Alpha:0.0307 | SPLoss:6.4161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1616 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4966 | MainLoss:0.2965 | Alpha:0.0312 | SPLoss:6.4140 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1617 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4900 | MainLoss:0.2812 | Alpha:0.0326 | SPLoss:6.4114 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1618 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4963 | MainLoss:0.3022 | Alpha:0.0303 | SPLoss:6.4087 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1619 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4831 | MainLoss:0.2931 | Alpha:0.0296 | SPLoss:6.4068 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1620 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4627 | MainLoss:0.2729 | Alpha:0.0296 | SPLoss:6.4046 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1621 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.3087 | Alpha:0.0300 | SPLoss:6.4026 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1622 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4781 | MainLoss:0.2851 | Alpha:0.0302 | SPLoss:6.3996 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1623 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4855 | MainLoss:0.2859 | Alpha:0.0312 | SPLoss:6.3975 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1624 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4774 | MainLoss:0.2766 | Alpha:0.0314 | SPLoss:6.3950 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1625 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.5253 | MainLoss:0.3158 | Alpha:0.0328 | SPLoss:6.3915 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1626 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.5002 | MainLoss:0.2962 | Alpha:0.0319 | SPLoss:6.3883 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1627 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4720 | MainLoss:0.2786 | Alpha:0.0303 | SPLoss:6.3855 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1628 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4891 | MainLoss:0.2948 | Alpha:0.0304 | SPLoss:6.3821 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1629 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4913 | MainLoss:0.2987 | Alpha:0.0302 | SPLoss:6.3792 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1630 | 5000] LR: 0.000126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.4710 | MainLoss:0.2774 | Alpha:0.0304 | SPLoss:6.3765 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1631 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4765 | MainLoss:0.2878 | Alpha:0.0296 | SPLoss:6.3743 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1632 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4958 | MainLoss:0.2934 | Alpha:0.0318 | SPLoss:6.3719 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1633 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4943 | MainLoss:0.2951 | Alpha:0.0313 | SPLoss:6.3672 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1634 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.5034 | MainLoss:0.3062 | Alpha:0.0310 | SPLoss:6.3636 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1635 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.5159 | MainLoss:0.3206 | Alpha:0.0307 | SPLoss:6.3610 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1636 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4789 | MainLoss:0.2794 | Alpha:0.0314 | SPLoss:6.3582 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1637 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.5342 | MainLoss:0.3278 | Alpha:0.0325 | SPLoss:6.3550 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1638 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4803 | MainLoss:0.2866 | Alpha:0.0305 | SPLoss:6.3516 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1639 | 5000] LR: 0.000126\n",
      "Train | 16/16 | Loss:0.4948 | MainLoss:0.2962 | Alpha:0.0313 | SPLoss:6.3483 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1640 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4844 | MainLoss:0.2823 | Alpha:0.0319 | SPLoss:6.3447 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1641 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5229 | MainLoss:0.3093 | Alpha:0.0337 | SPLoss:6.3411 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1642 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4868 | MainLoss:0.2872 | Alpha:0.0315 | SPLoss:6.3375 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1643 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5056 | MainLoss:0.3004 | Alpha:0.0324 | SPLoss:6.3347 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1644 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4868 | MainLoss:0.2867 | Alpha:0.0316 | SPLoss:6.3323 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1645 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5026 | MainLoss:0.2954 | Alpha:0.0327 | SPLoss:6.3288 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1646 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4930 | MainLoss:0.2851 | Alpha:0.0329 | SPLoss:6.3265 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1647 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5262 | MainLoss:0.3194 | Alpha:0.0327 | SPLoss:6.3243 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1648 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4740 | MainLoss:0.2765 | Alpha:0.0312 | SPLoss:6.3218 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1649 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5014 | MainLoss:0.3037 | Alpha:0.0313 | SPLoss:6.3196 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1650 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5072 | MainLoss:0.3079 | Alpha:0.0315 | SPLoss:6.3174 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1651 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4883 | MainLoss:0.2837 | Alpha:0.0324 | SPLoss:6.3148 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1652 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4723 | MainLoss:0.2754 | Alpha:0.0312 | SPLoss:6.3115 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1653 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4921 | MainLoss:0.2968 | Alpha:0.0310 | SPLoss:6.3089 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1654 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4668 | MainLoss:0.2723 | Alpha:0.0308 | SPLoss:6.3058 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1655 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4752 | MainLoss:0.2809 | Alpha:0.0308 | SPLoss:6.3015 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1656 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4726 | MainLoss:0.2845 | Alpha:0.0299 | SPLoss:6.2986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1657 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5027 | MainLoss:0.2940 | Alpha:0.0331 | SPLoss:6.2970 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1658 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4902 | MainLoss:0.2995 | Alpha:0.0303 | SPLoss:6.2938 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1659 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4755 | MainLoss:0.2901 | Alpha:0.0295 | SPLoss:6.2920 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1660 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5040 | MainLoss:0.3159 | Alpha:0.0299 | SPLoss:6.2899 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1661 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4718 | MainLoss:0.2750 | Alpha:0.0313 | SPLoss:6.2884 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1662 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.4731 | MainLoss:0.2849 | Alpha:0.0299 | SPLoss:6.2857 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1663 | 5000] LR: 0.000125\n",
      "Train | 16/16 | Loss:0.5259 | MainLoss:0.3335 | Alpha:0.0306 | SPLoss:6.2838 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1664 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5020 | MainLoss:0.3072 | Alpha:0.0310 | SPLoss:6.2831 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1665 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4934 | MainLoss:0.2914 | Alpha:0.0322 | SPLoss:6.2802 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1666 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4690 | MainLoss:0.2733 | Alpha:0.0312 | SPLoss:6.2772 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1667 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5054 | MainLoss:0.3136 | Alpha:0.0306 | SPLoss:6.2739 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1668 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4993 | MainLoss:0.2993 | Alpha:0.0319 | SPLoss:6.2711 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1669 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4773 | MainLoss:0.2904 | Alpha:0.0298 | SPLoss:6.2680 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1670 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4835 | MainLoss:0.2875 | Alpha:0.0313 | SPLoss:6.2637 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1671 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4708 | MainLoss:0.2817 | Alpha:0.0302 | SPLoss:6.2607 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1672 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4687 | MainLoss:0.2754 | Alpha:0.0309 | SPLoss:6.2581 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1673 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4772 | MainLoss:0.2901 | Alpha:0.0299 | SPLoss:6.2543 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1674 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5076 | MainLoss:0.3122 | Alpha:0.0312 | SPLoss:6.2526 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1675 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4764 | MainLoss:0.2843 | Alpha:0.0307 | SPLoss:6.2507 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1676 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4998 | MainLoss:0.3114 | Alpha:0.0302 | SPLoss:6.2488 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1677 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4916 | MainLoss:0.2909 | Alpha:0.0321 | SPLoss:6.2473 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1678 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4820 | MainLoss:0.2885 | Alpha:0.0310 | SPLoss:6.2446 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1679 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5227 | MainLoss:0.3202 | Alpha:0.0324 | SPLoss:6.2425 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1680 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4942 | MainLoss:0.2991 | Alpha:0.0313 | SPLoss:6.2395 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1681 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5002 | MainLoss:0.3013 | Alpha:0.0319 | SPLoss:6.2357 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1682 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4893 | MainLoss:0.2943 | Alpha:0.0313 | SPLoss:6.2333 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1683 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4952 | MainLoss:0.3009 | Alpha:0.0312 | SPLoss:6.2309 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1684 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5102 | MainLoss:0.3115 | Alpha:0.0319 | SPLoss:6.2287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1685 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.4991 | MainLoss:0.2971 | Alpha:0.0325 | SPLoss:6.2249 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1686 | 5000] LR: 0.000124\n",
      "Train | 16/16 | Loss:0.5032 | MainLoss:0.3030 | Alpha:0.0322 | SPLoss:6.2220 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1687 | 5000] LR: 0.000124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.4886 | MainLoss:0.2869 | Alpha:0.0324 | SPLoss:6.2179 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1688 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4636 | MainLoss:0.2742 | Alpha:0.0305 | SPLoss:6.2151 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1689 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4952 | MainLoss:0.3001 | Alpha:0.0314 | SPLoss:6.2130 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1690 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4952 | MainLoss:0.2919 | Alpha:0.0327 | SPLoss:6.2101 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1691 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4877 | MainLoss:0.2820 | Alpha:0.0331 | SPLoss:6.2070 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1692 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4765 | MainLoss:0.2918 | Alpha:0.0298 | SPLoss:6.2042 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1693 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4933 | MainLoss:0.3060 | Alpha:0.0302 | SPLoss:6.2027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1694 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4946 | MainLoss:0.2984 | Alpha:0.0316 | SPLoss:6.2006 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1695 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4732 | MainLoss:0.2765 | Alpha:0.0317 | SPLoss:6.1986 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1696 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4846 | MainLoss:0.2907 | Alpha:0.0313 | SPLoss:6.1955 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1697 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4614 | MainLoss:0.2772 | Alpha:0.0297 | SPLoss:6.1927 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1698 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4986 | MainLoss:0.3043 | Alpha:0.0314 | SPLoss:6.1894 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1699 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.5134 | MainLoss:0.3218 | Alpha:0.0310 | SPLoss:6.1859 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1700 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4829 | MainLoss:0.2943 | Alpha:0.0305 | SPLoss:6.1824 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4538 | MainLoss:0.4538 | SPLoss:6.1815 | CLSLoss:0.0000 | AUROC:0.9042\n",
      "Test | 31/16 | Loss:0.2177 | MainLoss:0.2177 | SPLoss:6.1815 | CLSLoss:0.0000 | AUROC:0.9798\n",
      "\n",
      "Epoch: [1701 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4888 | MainLoss:0.2900 | Alpha:0.0322 | SPLoss:6.1810 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1702 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4828 | MainLoss:0.2953 | Alpha:0.0304 | SPLoss:6.1786 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1703 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4822 | MainLoss:0.2862 | Alpha:0.0317 | SPLoss:6.1758 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1704 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4891 | MainLoss:0.2967 | Alpha:0.0312 | SPLoss:6.1727 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1705 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4904 | MainLoss:0.2989 | Alpha:0.0310 | SPLoss:6.1699 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1706 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4508 | MainLoss:0.2675 | Alpha:0.0297 | SPLoss:6.1675 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1707 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4791 | MainLoss:0.2937 | Alpha:0.0301 | SPLoss:6.1654 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1708 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4905 | MainLoss:0.3043 | Alpha:0.0302 | SPLoss:6.1640 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1709 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.5106 | MainLoss:0.3112 | Alpha:0.0324 | SPLoss:6.1623 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1710 | 5000] LR: 0.000123\n",
      "Train | 16/16 | Loss:0.4789 | MainLoss:0.2863 | Alpha:0.0313 | SPLoss:6.1607 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1711 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4800 | MainLoss:0.2854 | Alpha:0.0316 | SPLoss:6.1586 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1712 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4677 | MainLoss:0.2832 | Alpha:0.0300 | SPLoss:6.1558 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1713 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4699 | MainLoss:0.2836 | Alpha:0.0303 | SPLoss:6.1545 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1714 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4966 | MainLoss:0.3054 | Alpha:0.0311 | SPLoss:6.1509 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1715 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4736 | MainLoss:0.2875 | Alpha:0.0303 | SPLoss:6.1479 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1716 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4907 | MainLoss:0.2928 | Alpha:0.0322 | SPLoss:6.1444 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1717 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4944 | MainLoss:0.2972 | Alpha:0.0321 | SPLoss:6.1413 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1718 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.5262 | MainLoss:0.3274 | Alpha:0.0324 | SPLoss:6.1380 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1719 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4846 | MainLoss:0.2850 | Alpha:0.0325 | SPLoss:6.1346 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1720 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4905 | MainLoss:0.2932 | Alpha:0.0322 | SPLoss:6.1314 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1721 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4589 | MainLoss:0.2724 | Alpha:0.0304 | SPLoss:6.1279 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1722 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4919 | MainLoss:0.2979 | Alpha:0.0317 | SPLoss:6.1260 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1723 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4896 | MainLoss:0.2988 | Alpha:0.0312 | SPLoss:6.1238 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1724 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4621 | MainLoss:0.2877 | Alpha:0.0285 | SPLoss:6.1224 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1725 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4889 | MainLoss:0.3032 | Alpha:0.0303 | SPLoss:6.1220 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1726 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4602 | MainLoss:0.2843 | Alpha:0.0287 | SPLoss:6.1202 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1727 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4745 | MainLoss:0.2857 | Alpha:0.0309 | SPLoss:6.1191 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1728 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4934 | MainLoss:0.2992 | Alpha:0.0318 | SPLoss:6.1163 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1729 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4782 | MainLoss:0.2927 | Alpha:0.0303 | SPLoss:6.1137 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1730 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.5100 | MainLoss:0.3146 | Alpha:0.0320 | SPLoss:6.1108 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1731 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4729 | MainLoss:0.2911 | Alpha:0.0298 | SPLoss:6.1092 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1732 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4659 | MainLoss:0.2898 | Alpha:0.0288 | SPLoss:6.1068 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1733 | 5000] LR: 0.000122\n",
      "Train | 16/16 | Loss:0.4950 | MainLoss:0.3037 | Alpha:0.0313 | SPLoss:6.1045 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1734 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4649 | MainLoss:0.2813 | Alpha:0.0301 | SPLoss:6.1021 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1735 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4657 | MainLoss:0.2825 | Alpha:0.0300 | SPLoss:6.0992 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1736 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4947 | MainLoss:0.2994 | Alpha:0.0320 | SPLoss:6.0969 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1737 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4868 | MainLoss:0.2990 | Alpha:0.0308 | SPLoss:6.0956 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1738 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4803 | MainLoss:0.2864 | Alpha:0.0318 | SPLoss:6.0933 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1739 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4999 | MainLoss:0.3076 | Alpha:0.0316 | SPLoss:6.0906 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1740 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4715 | MainLoss:0.2812 | Alpha:0.0313 | SPLoss:6.0883 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1741 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.5007 | MainLoss:0.3017 | Alpha:0.0327 | SPLoss:6.0868 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1742 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4959 | MainLoss:0.3033 | Alpha:0.0317 | SPLoss:6.0844 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1743 | 5000] LR: 0.000121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.5209 | MainLoss:0.3209 | Alpha:0.0329 | SPLoss:6.0809 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1744 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4762 | MainLoss:0.2929 | Alpha:0.0302 | SPLoss:6.0769 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1745 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4878 | MainLoss:0.2974 | Alpha:0.0313 | SPLoss:6.0752 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1746 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.5147 | MainLoss:0.3210 | Alpha:0.0319 | SPLoss:6.0735 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1747 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4960 | MainLoss:0.3023 | Alpha:0.0319 | SPLoss:6.0705 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1748 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4805 | MainLoss:0.2890 | Alpha:0.0316 | SPLoss:6.0682 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1749 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4745 | MainLoss:0.2770 | Alpha:0.0326 | SPLoss:6.0655 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1750 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4757 | MainLoss:0.2864 | Alpha:0.0312 | SPLoss:6.0620 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1751 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4831 | MainLoss:0.2911 | Alpha:0.0317 | SPLoss:6.0596 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1752 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4622 | MainLoss:0.2734 | Alpha:0.0312 | SPLoss:6.0576 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1753 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4709 | MainLoss:0.2798 | Alpha:0.0316 | SPLoss:6.0550 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1754 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4841 | MainLoss:0.2915 | Alpha:0.0318 | SPLoss:6.0527 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1755 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4672 | MainLoss:0.2854 | Alpha:0.0300 | SPLoss:6.0518 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1756 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4859 | MainLoss:0.2957 | Alpha:0.0314 | SPLoss:6.0501 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1757 | 5000] LR: 0.000121\n",
      "Train | 16/16 | Loss:0.4629 | MainLoss:0.2718 | Alpha:0.0316 | SPLoss:6.0473 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1758 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.5062 | MainLoss:0.3082 | Alpha:0.0328 | SPLoss:6.0443 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1759 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4835 | MainLoss:0.2874 | Alpha:0.0325 | SPLoss:6.0413 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1760 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4999 | MainLoss:0.3084 | Alpha:0.0317 | SPLoss:6.0390 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1761 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.5116 | MainLoss:0.3240 | Alpha:0.0311 | SPLoss:6.0362 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1762 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4829 | MainLoss:0.2943 | Alpha:0.0313 | SPLoss:6.0343 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1763 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4924 | MainLoss:0.2972 | Alpha:0.0324 | SPLoss:6.0318 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1764 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4920 | MainLoss:0.2994 | Alpha:0.0320 | SPLoss:6.0287 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1765 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4650 | MainLoss:0.2784 | Alpha:0.0310 | SPLoss:6.0263 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1766 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4633 | MainLoss:0.2809 | Alpha:0.0303 | SPLoss:6.0236 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1767 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.5005 | MainLoss:0.3132 | Alpha:0.0311 | SPLoss:6.0212 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1768 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4729 | MainLoss:0.2933 | Alpha:0.0298 | SPLoss:6.0202 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1769 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4709 | MainLoss:0.2884 | Alpha:0.0303 | SPLoss:6.0188 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1770 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4800 | MainLoss:0.2915 | Alpha:0.0313 | SPLoss:6.0161 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1771 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4613 | MainLoss:0.2791 | Alpha:0.0303 | SPLoss:6.0139 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1772 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4673 | MainLoss:0.2853 | Alpha:0.0303 | SPLoss:6.0111 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1773 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4706 | MainLoss:0.2898 | Alpha:0.0301 | SPLoss:6.0077 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1774 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.5112 | MainLoss:0.3177 | Alpha:0.0322 | SPLoss:6.0052 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1775 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4679 | MainLoss:0.2924 | Alpha:0.0292 | SPLoss:6.0027 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1776 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4924 | MainLoss:0.3043 | Alpha:0.0313 | SPLoss:6.0008 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1777 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4655 | MainLoss:0.2900 | Alpha:0.0292 | SPLoss:5.9994 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1778 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4838 | MainLoss:0.3003 | Alpha:0.0306 | SPLoss:5.9971 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1779 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4823 | MainLoss:0.2975 | Alpha:0.0308 | SPLoss:5.9941 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1780 | 5000] LR: 0.000120\n",
      "Train | 16/16 | Loss:0.4846 | MainLoss:0.3019 | Alpha:0.0305 | SPLoss:5.9922 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1781 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4621 | MainLoss:0.2821 | Alpha:0.0301 | SPLoss:5.9900 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1782 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4692 | MainLoss:0.2878 | Alpha:0.0303 | SPLoss:5.9885 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1783 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4758 | MainLoss:0.2852 | Alpha:0.0318 | SPLoss:5.9867 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1784 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4911 | MainLoss:0.3029 | Alpha:0.0314 | SPLoss:5.9846 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1785 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4920 | MainLoss:0.3069 | Alpha:0.0309 | SPLoss:5.9816 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1786 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4853 | MainLoss:0.2986 | Alpha:0.0312 | SPLoss:5.9787 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1787 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4807 | MainLoss:0.2939 | Alpha:0.0312 | SPLoss:5.9768 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1788 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4582 | MainLoss:0.2771 | Alpha:0.0303 | SPLoss:5.9750 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1789 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4821 | MainLoss:0.2967 | Alpha:0.0310 | SPLoss:5.9734 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1790 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4772 | MainLoss:0.2921 | Alpha:0.0310 | SPLoss:5.9727 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1791 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4532 | MainLoss:0.2806 | Alpha:0.0289 | SPLoss:5.9711 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1792 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4656 | MainLoss:0.2773 | Alpha:0.0315 | SPLoss:5.9701 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1793 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4770 | MainLoss:0.2900 | Alpha:0.0313 | SPLoss:5.9687 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1794 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4643 | MainLoss:0.2894 | Alpha:0.0293 | SPLoss:5.9668 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1795 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4624 | MainLoss:0.2788 | Alpha:0.0308 | SPLoss:5.9645 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1796 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4868 | MainLoss:0.3002 | Alpha:0.0313 | SPLoss:5.9619 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1797 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4551 | MainLoss:0.2845 | Alpha:0.0286 | SPLoss:5.9588 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1798 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4764 | MainLoss:0.2943 | Alpha:0.0306 | SPLoss:5.9575 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1799 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4906 | MainLoss:0.3003 | Alpha:0.0319 | SPLoss:5.9560 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1800 | 5000] LR: 0.000119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 16/16 | Loss:0.4817 | MainLoss:0.3013 | Alpha:0.0303 | SPLoss:5.9543 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "Test | 122/16 | Loss:0.4578 | MainLoss:0.4578 | SPLoss:5.9533 | CLSLoss:0.0000 | AUROC:0.9072\n",
      "Test | 31/16 | Loss:0.2281 | MainLoss:0.2281 | SPLoss:5.9533 | CLSLoss:0.0000 | AUROC:0.9792\n",
      "\n",
      "Epoch: [1801 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4647 | MainLoss:0.2789 | Alpha:0.0312 | SPLoss:5.9521 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1802 | 5000] LR: 0.000119\n",
      "Train | 16/16 | Loss:0.4976 | MainLoss:0.3088 | Alpha:0.0317 | SPLoss:5.9495 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1803 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4871 | MainLoss:0.3010 | Alpha:0.0313 | SPLoss:5.9470 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1804 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4988 | MainLoss:0.3113 | Alpha:0.0315 | SPLoss:5.9445 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1805 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4628 | MainLoss:0.2757 | Alpha:0.0315 | SPLoss:5.9424 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1806 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4674 | MainLoss:0.2861 | Alpha:0.0305 | SPLoss:5.9402 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1807 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4876 | MainLoss:0.2967 | Alpha:0.0321 | SPLoss:5.9381 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1808 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4800 | MainLoss:0.2940 | Alpha:0.0313 | SPLoss:5.9358 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1809 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4800 | MainLoss:0.2977 | Alpha:0.0307 | SPLoss:5.9338 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1810 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4845 | MainLoss:0.2978 | Alpha:0.0315 | SPLoss:5.9312 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1811 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4909 | MainLoss:0.3057 | Alpha:0.0312 | SPLoss:5.9292 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1812 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4922 | MainLoss:0.2989 | Alpha:0.0326 | SPLoss:5.9273 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1813 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4790 | MainLoss:0.2845 | Alpha:0.0328 | SPLoss:5.9259 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1814 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4622 | MainLoss:0.2735 | Alpha:0.0319 | SPLoss:5.9229 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1815 | 5000] LR: 0.000118\n",
      "Train | 16/16 | Loss:0.4779 | MainLoss:0.2894 | Alpha:0.0318 | SPLoss:5.9211 | CLSLoss:0.0000 | AUROC:0.0000\n",
      "\n",
      "Epoch: [1816 | 5000] LR: 0.000118\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    \n",
    "    if(epoch+1)%100 == 0:\n",
    "        test_loss, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "        source_loss, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "        logger.append([state['lr'], train_loss, test_loss,  source_loss, train_auroc, test_auroc, source_auroc])\n",
    "        is_best = test_auroc+source_auroc > best_acc\n",
    "        best_acc = max(test_auroc+source_auroc, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict' : student_model.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer': optimizer.state_dict(),    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    scheduler.step()\n",
    "\n",
    "#     if (epoch+1)%4000 == 0:\n",
    "#         teacher_model.load_state_dict(student_model.state_dict())\n",
    "#     teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
