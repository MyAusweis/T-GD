{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import os, shutil\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "import numpy as np\n",
    "# declare batch size for act function\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0) \n",
    "\n",
    "batch_size = 1148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 0: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '/home/kim1'\n",
    "data_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "d_dir = home_dir+data_dir\n",
    "\n",
    "resume = './log/pggan/128/foresic/aug/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = './log/pggan/128/foresic/aug' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder part\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        #should change channel 3\n",
    "        self.conv_1_1 = nn.Conv2d(in_channels=3,out_channels=8,kernel_size=3,padding=1)\n",
    "        self.relu_1_2 = nn.ReLU(True)\n",
    "            \n",
    "        self.conv_2_1 = nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn_2_2 = nn.BatchNorm2d(16)\n",
    "        self.relu_2_3 = nn.ReLU(True)\n",
    "        \n",
    "        self.conv_3_1 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn_3_2 = nn.BatchNorm2d(32)\n",
    "        self.relu_3_3 = nn.ReLU(True)\n",
    "        \n",
    "        self.conv_4_1=nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn_4_2=nn.BatchNorm2d(64)\n",
    "        self.relu_4_3=nn.ReLU(True)\n",
    "        \n",
    "        self.conv_5_1 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=2,padding=1)\n",
    "        self.bn_5_2 =nn.BatchNorm2d(128)\n",
    "        self.relu_5_3 = nn.ReLU(True)\n",
    "        \n",
    "        ## decoder\n",
    "        self.upsample_6_1 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "        self.convtranspose_6_2 = nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.bn_6_3 = nn.BatchNorm2d(64)\n",
    "        self.relu_6_4 = nn.ReLU(True)\n",
    "        \n",
    "        self.upsample_7_1 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "        self.convtranspose_7_2 = nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.bn_7_3 = nn.BatchNorm2d(32)\n",
    "        self.relu_7_4 = nn.ReLU(True)\n",
    "        \n",
    "        self.upsample_8_1 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "        self.convtranspose_8_2 = nn.Conv2d(in_channels=32,out_channels=16,kernel_size=3,padding=1)\n",
    "        self.bn_8_3 = nn.BatchNorm2d(16)\n",
    "        self.relu_8_4 = nn.ReLU(True)\n",
    "        \n",
    "        self.upsample_9_1 = nn.Upsample(scale_factor=2,mode='nearest')\n",
    "        self.convtranspose_9_2 = nn.Conv2d(in_channels=16,out_channels=8,kernel_size=3,padding=1)\n",
    "        self.bn_9_3 = nn.BatchNorm2d(8)\n",
    "        self.relu_9_4 = nn.ReLU(True)\n",
    "        #should change channel 6\n",
    "        # 혹시 안되면\n",
    "        # self.conv_10_1 = nn.ConvTranspose2d(in_channels=8,out_channels=3,kernel_size=3,padding=1)\n",
    "        #이걸로 마지막 conv 계층을 바꿔주세요 \n",
    "        self.conv_10_1 = nn.Conv2d(in_channels=8,out_channels=3,kernel_size=3,padding=1)\n",
    "        self.tanh_10_2=nn.Tanh()\n",
    "        \n",
    "        # no masking no *0 or assign value zero not done .        \n",
    "        \n",
    "    def forward(self, x,label):\n",
    "        x1 = self.conv_1_1(x)\n",
    "        x2 = self.relu_1_2(x1)\n",
    "\n",
    "        x3 = self.conv_2_1(x2)\n",
    "        x4 = self.bn_2_2(x3)\n",
    "        x5 = self.relu_2_3(x4)\n",
    "\n",
    "        \n",
    "        x6 = self.conv_3_1(x5)\n",
    "        x7 = self.bn_3_2(x6)\n",
    "        x8 = self.relu_3_3(x7)\n",
    "\n",
    "        \n",
    "        x9 = self.conv_4_1(x8)\n",
    "        x10 = self.bn_4_2(x9)\n",
    "        x11 = self.relu_4_3(x10)\n",
    "\n",
    "        \n",
    "        x12 = self.conv_5_1(x11)\n",
    "        x13 = self.bn_5_2(x12)\n",
    "        x14 = self.relu_5_3(x13)\n",
    "        \n",
    "        act = x14.clone()\n",
    "        dep = x14.clone()\n",
    "\n",
    "        # Selection block setting zero values based on label\n",
    "        # [:64] -> fake data latent space \n",
    "        # [64:] -> real data latent space\n",
    "        # 0->fake 1 ->real\n",
    "        # 15 15\n",
    "        A = torch.nn.Parameter(torch.zeros(64,8,8))\n",
    "        for i in range(len(label)):\n",
    "            #real \n",
    "            if label[i].item():\n",
    "                #setting fake latent space into zero\n",
    "                dep[i,:64] = A\n",
    "            else:\n",
    "                dep[i,64:]=A\n",
    "                \n",
    "        x15 = self.upsample_6_1(dep) \n",
    "        x16 = self.convtranspose_6_2(x15)\n",
    "        x17 = self.bn_6_3(x16)\n",
    "        x18 = self.relu_6_4(x17) \n",
    "        x19 = self.upsample_7_1(x18)\n",
    "        x20 = self.convtranspose_7_2(x19)\n",
    "        x21 = self.bn_7_3(x20)\n",
    "        x22 = self.relu_7_4(x21)\n",
    "        x23 = self.upsample_8_1(x22)\n",
    "        x24 = self.convtranspose_8_2(x23)\n",
    "        x25 = self.bn_8_3(x24)\n",
    "        x26 = self.relu_8_4(x25)\n",
    "        x27 = self.upsample_9_1(x26)\n",
    "        x28 = self.convtranspose_9_2(x27) \n",
    "        x29 = self.bn_9_3(x28)\n",
    "        x30 = self.relu_9_4(x29)\n",
    "        x31 = self.conv_10_1(x30)\n",
    "        x32 = self.tanh_10_2(x31)\n",
    "\n",
    "        return  x32 , act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (conv_1_1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_1_2): ReLU(inplace=True)\n",
       "  (conv_2_1): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn_2_2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_2_3): ReLU(inplace=True)\n",
       "  (conv_3_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn_3_2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_3_3): ReLU(inplace=True)\n",
       "  (conv_4_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn_4_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_4_3): ReLU(inplace=True)\n",
       "  (conv_5_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (bn_5_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_5_3): ReLU(inplace=True)\n",
       "  (upsample_6_1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (convtranspose_6_2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_6_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_6_4): ReLU(inplace=True)\n",
       "  (upsample_7_1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (convtranspose_7_2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_7_3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_7_4): ReLU(inplace=True)\n",
       "  (upsample_8_1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (convtranspose_8_2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_8_3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_8_4): ReLU(inplace=True)\n",
       "  (upsample_9_1): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  (convtranspose_9_2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn_9_3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu_9_4): ReLU(inplace=True)\n",
       "  (conv_10_1): Conv2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (tanh_10_2): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,eps=1e-7)\n",
    "\n",
    "#configuration\n",
    "\n",
    "num_epochs = 100\n",
    "criterion1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_dir = os.path.join(d_dir, 'train')\n",
    "val_dir = os.path.join(d_dir, 'validation')\n",
    "\n",
    "train_data =torchvision.datasets.ImageFolder(root=train_dir,\n",
    "                                                transform = transforms.Compose([transforms.Resize((128 ,128 )),transforms.ToTensor()]))\n",
    "validation_dataset =torchvision.datasets.ImageFolder(root=val_dir,\n",
    "                                                transform = transforms.Compose([transforms.Resize((128 ,128 )),transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader.classes : ['0_real', '1_fake']\n",
      "validation_dataloader.classes : ['0_real', '1_fake']\n",
      "train_dataloader.classes : {'0_real': 0, '1_fake': 1}\n",
      "validation_dataloader.classes : {'0_real': 0, '1_fake': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataloader.classes : %s\" % train_data.classes)\n",
    "print(\"validation_dataloader.classes : %s\" % validation_dataset.classes)\n",
    "print(\"train_dataloader.classes : %s\" % train_data.class_to_idx)\n",
    "print(\"validation_dataloader.classes : %s\" % validation_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = 0\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_loss_func(outputs, labels):\n",
    "    batch_size = outputs.size()[0]\n",
    "    loss_list = torch.zeros([batch_size])\n",
    "    loss_list = loss_list.to(device)\n",
    "    for i in range(batch_size):\n",
    "        #fake\n",
    "        total_loss =torch.zeros([1],dtype=torch.float32)\n",
    "        total_loss = total_loss.to(device)\n",
    "        #real\n",
    "        total_loss_1 =torch.zeros([1],dtype=torch.float32)\n",
    "        total_loss_1 = total_loss.to(device)\n",
    "        # real\n",
    "        if labels[i].item():\n",
    "            #fake\n",
    "            for latent_index in range(64):\n",
    "                temp= torch.sum(torch.abs(outputs[i,latent_index,:,:]))/225\n",
    "                total_loss = torch.sum(total_loss+temp)\n",
    "            #real\n",
    "            for latent_index in range(64,128):\n",
    "                temp1= torch.abs(1 -torch.sum(torch.abs(outputs[i,latent_index,:,:]))/225)\n",
    "                total_loss_1 = torch.sum(total_loss_1+temp1)\n",
    "        #fake\n",
    "        else:\n",
    "            #fake\n",
    "            for latent_index in range(64):\n",
    "                temp= torch.abs(1- torch.sum(torch.abs(outputs[i,latent_index,:,:]))/225)\n",
    "                total_loss = torch.sum(total_loss+temp)\n",
    "            #real\n",
    "            for latent_index in range(64,128):\n",
    "                temp1= torch.sum(torch.abs(outputs[i,latent_index,:,:]))/225\n",
    "                total_loss_1 = torch.sum(total_loss_1+temp1)\n",
    "        \n",
    "        loss_list[i]=total_loss+total_loss_1\n",
    "\n",
    "        \n",
    "    return torch.sum(loss_list)\n",
    "\n",
    "#%%\n",
    "\n",
    "# test\n",
    "def act_loss_test(outputs):\n",
    "    batch_size = outputs.size()[0]\n",
    "    answer = torch.zeros([batch_size,2])\n",
    "    answer.cuda()\n",
    "    for i in range(batch_size):\n",
    "        fake = torch.zeros([1], dtype=torch.float32).to(device)\n",
    "        real = torch.zeros([1], dtype=torch.float32).to(device)\n",
    "        # fake latent space\n",
    "        for latent_index in range(64):\n",
    "            fake = fake + torch.sum(torch.abs(outputs[i, latent_index]))\n",
    "        # real latent space\n",
    "        for latent_index in range(64, 128):\n",
    "            real = real + torch.sum(torch.abs(outputs[i, latent_index]))\n",
    "\n",
    "\n",
    "        answer[i][0] = fake.item() / (fake.item() + real.item())\n",
    "        answer[i][1] = real.item() / (fake.item() + real.item())\n",
    "        \n",
    "            \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:62089.8711\n",
      "epoch [1/100], loss:61337.8359\n",
      "epoch [1/100], loss:61067.9336\n",
      "epoch [1/100], loss:59800.0938\n",
      "epoch [1/100], loss:59955.4453\n",
      "epoch [1/100], loss:59633.6719\n",
      "epoch [1/100], loss:58978.9922\n",
      "epoch [1/100], loss:58351.2383\n",
      "epoch [1/100], loss:58291.7188\n",
      "epoch [1/100], loss:58068.7578\n",
      "epoch [1/100], loss:57453.6875\n",
      "epoch [1/100], loss:49054.6016\n",
      "validation loss is 1610891.625000\n",
      "Test Accuracy 0.807445 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.74      0.95      0.83     16050\n",
      "        real       0.93      0.66      0.77     16050\n",
      "\n",
      "    accuracy                           0.81     32100\n",
      "   macro avg       0.84      0.81      0.80     32100\n",
      "weighted avg       0.84      0.81      0.80     32100\n",
      "\n",
      "epoch [2/100], loss:58187.3984\n",
      "epoch [2/100], loss:57385.2852\n",
      "epoch [2/100], loss:56132.3555\n",
      "epoch [2/100], loss:55934.4844\n",
      "epoch [2/100], loss:55570.1484\n",
      "epoch [2/100], loss:55118.6289\n",
      "epoch [2/100], loss:55085.8555\n",
      "epoch [2/100], loss:54572.7969\n",
      "epoch [2/100], loss:53793.6016\n",
      "epoch [2/100], loss:53484.9844\n",
      "epoch [2/100], loss:54435.1953\n",
      "epoch [2/100], loss:45405.8203\n",
      "validation loss is 1446582.250000\n",
      "Test Accuracy 0.943676 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.92      0.97      0.95     16050\n",
      "        real       0.97      0.91      0.94     16050\n",
      "\n",
      "    accuracy                           0.94     32100\n",
      "   macro avg       0.95      0.94      0.94     32100\n",
      "weighted avg       0.95      0.94      0.94     32100\n",
      "\n",
      "epoch [3/100], loss:52793.7344\n",
      "epoch [3/100], loss:52625.8906\n",
      "epoch [3/100], loss:52153.5391\n",
      "epoch [3/100], loss:52702.8789\n",
      "epoch [3/100], loss:52059.4297\n",
      "epoch [3/100], loss:51563.0859\n",
      "epoch [3/100], loss:50695.5742\n",
      "epoch [3/100], loss:49950.0156\n",
      "epoch [3/100], loss:50189.2891\n",
      "epoch [3/100], loss:50444.0938\n",
      "epoch [3/100], loss:50622.0156\n",
      "epoch [3/100], loss:42214.4844\n",
      "validation loss is 1290261.625000\n",
      "Test Accuracy 0.937259 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.95      0.93      0.94     16050\n",
      "        real       0.93      0.95      0.94     16050\n",
      "\n",
      "    accuracy                           0.94     32100\n",
      "   macro avg       0.94      0.94      0.94     32100\n",
      "weighted avg       0.94      0.94      0.94     32100\n",
      "\n",
      "epoch [4/100], loss:49868.5078\n",
      "epoch [4/100], loss:48738.1562\n",
      "epoch [4/100], loss:49191.0703\n",
      "epoch [4/100], loss:48668.1484\n",
      "epoch [4/100], loss:47980.0391\n",
      "epoch [4/100], loss:47880.5234\n",
      "epoch [4/100], loss:47624.3555\n",
      "epoch [4/100], loss:47258.2148\n",
      "epoch [4/100], loss:47827.3438\n",
      "epoch [4/100], loss:47020.9023\n",
      "epoch [4/100], loss:46598.8438\n",
      "epoch [4/100], loss:38994.1719\n",
      "validation loss is 1242056.500000\n",
      "Test Accuracy 0.943645 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.94      0.94      0.94     16050\n",
      "        real       0.94      0.94      0.94     16050\n",
      "\n",
      "    accuracy                           0.94     32100\n",
      "   macro avg       0.94      0.94      0.94     32100\n",
      "weighted avg       0.94      0.94      0.94     32100\n",
      "\n",
      "epoch [5/100], loss:46917.1406\n",
      "epoch [5/100], loss:46661.1445\n",
      "epoch [5/100], loss:45635.9297\n",
      "epoch [5/100], loss:45387.8047\n",
      "epoch [5/100], loss:45061.4453\n",
      "epoch [5/100], loss:45633.0078\n",
      "epoch [5/100], loss:45078.8711\n",
      "epoch [5/100], loss:43840.4688\n",
      "epoch [5/100], loss:44463.6172\n",
      "epoch [5/100], loss:44617.5117\n",
      "epoch [5/100], loss:43871.5977\n",
      "epoch [5/100], loss:37394.7578\n",
      "validation loss is 1141595.875000\n",
      "Test Accuracy 0.937944 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.95      0.93      0.94     16050\n",
      "        real       0.93      0.95      0.94     16050\n",
      "\n",
      "    accuracy                           0.94     32100\n",
      "   macro avg       0.94      0.94      0.94     32100\n",
      "weighted avg       0.94      0.94      0.94     32100\n",
      "\n",
      "epoch [6/100], loss:43591.9453\n",
      "epoch [6/100], loss:43796.7031\n",
      "epoch [6/100], loss:44320.6406\n",
      "epoch [6/100], loss:42127.0781\n",
      "epoch [6/100], loss:43220.9375\n",
      "epoch [6/100], loss:41604.6484\n",
      "epoch [6/100], loss:42545.8984\n",
      "epoch [6/100], loss:41330.1953\n",
      "epoch [6/100], loss:42470.3242\n",
      "epoch [6/100], loss:41497.8008\n",
      "epoch [6/100], loss:41519.9062\n",
      "epoch [6/100], loss:35573.8594\n",
      "validation loss is 1155272.000000\n",
      "Test Accuracy 0.934486 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.89      0.99      0.94     16050\n",
      "        real       0.98      0.88      0.93     16050\n",
      "\n",
      "    accuracy                           0.93     32100\n",
      "   macro avg       0.94      0.93      0.93     32100\n",
      "weighted avg       0.94      0.93      0.93     32100\n",
      "\n",
      "epoch [7/100], loss:40750.3008\n",
      "epoch [7/100], loss:40242.0469\n",
      "epoch [7/100], loss:39256.0312\n",
      "epoch [7/100], loss:39259.5312\n",
      "epoch [7/100], loss:39733.1406\n",
      "epoch [7/100], loss:39265.9531\n",
      "epoch [7/100], loss:39792.6953\n",
      "epoch [7/100], loss:39318.5312\n",
      "epoch [7/100], loss:40228.0312\n",
      "epoch [7/100], loss:39891.1641\n",
      "epoch [7/100], loss:38604.3594\n",
      "epoch [7/100], loss:31694.4355\n",
      "validation loss is 939695.687500\n",
      "Test Accuracy 0.966822 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.98      0.95      0.97     16050\n",
      "        real       0.95      0.98      0.97     16050\n",
      "\n",
      "    accuracy                           0.97     32100\n",
      "   macro avg       0.97      0.97      0.97     32100\n",
      "weighted avg       0.97      0.97      0.97     32100\n",
      "\n",
      "epoch [8/100], loss:38942.7969\n",
      "epoch [8/100], loss:37615.2031\n",
      "epoch [8/100], loss:36736.9141\n",
      "epoch [8/100], loss:39363.6133\n",
      "epoch [8/100], loss:37942.5156\n",
      "epoch [8/100], loss:37408.4531\n",
      "epoch [8/100], loss:37095.2383\n",
      "epoch [8/100], loss:37098.0547\n",
      "epoch [8/100], loss:36304.7422\n",
      "epoch [8/100], loss:36011.5352\n",
      "epoch [8/100], loss:37257.4883\n",
      "epoch [8/100], loss:30842.0117\n",
      "validation loss is 1114105.875000\n",
      "Test Accuracy 0.929595 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.89      0.98      0.93     16050\n",
      "        real       0.98      0.88      0.93     16050\n",
      "\n",
      "    accuracy                           0.93     32100\n",
      "   macro avg       0.93      0.93      0.93     32100\n",
      "weighted avg       0.93      0.93      0.93     32100\n",
      "\n",
      "epoch [9/100], loss:36596.5781\n",
      "epoch [9/100], loss:36108.6328\n",
      "epoch [9/100], loss:36193.4883\n",
      "epoch [9/100], loss:34863.1172\n",
      "epoch [9/100], loss:36321.6719\n",
      "epoch [9/100], loss:36147.5547\n",
      "epoch [9/100], loss:35551.8516\n",
      "epoch [9/100], loss:34538.9062\n",
      "epoch [9/100], loss:34874.1055\n",
      "epoch [9/100], loss:36295.5312\n",
      "epoch [9/100], loss:35082.6562\n",
      "epoch [9/100], loss:29315.8164\n",
      "validation loss is 844308.937500\n",
      "Test Accuracy 0.961215 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.97      0.96      0.96     16050\n",
      "        real       0.96      0.97      0.96     16050\n",
      "\n",
      "    accuracy                           0.96     32100\n",
      "   macro avg       0.96      0.96      0.96     32100\n",
      "weighted avg       0.96      0.96      0.96     32100\n",
      "\n",
      "epoch [10/100], loss:35416.1797\n",
      "epoch [10/100], loss:34068.7930\n",
      "epoch [10/100], loss:33881.0000\n",
      "epoch [10/100], loss:33434.9375\n",
      "epoch [10/100], loss:33054.5078\n",
      "epoch [10/100], loss:33711.2891\n",
      "epoch [10/100], loss:33163.9102\n",
      "epoch [10/100], loss:34022.5078\n",
      "epoch [10/100], loss:33846.2266\n",
      "epoch [10/100], loss:33912.8828\n",
      "epoch [10/100], loss:34704.3047\n",
      "epoch [10/100], loss:29024.1367\n",
      "validation loss is 852501.000000\n",
      "Test Accuracy 0.961495 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.97      0.95      0.96     16050\n",
      "        real       0.95      0.97      0.96     16050\n",
      "\n",
      "    accuracy                           0.96     32100\n",
      "   macro avg       0.96      0.96      0.96     32100\n",
      "weighted avg       0.96      0.96      0.96     32100\n",
      "\n",
      "epoch [11/100], loss:32497.0879\n",
      "epoch [11/100], loss:35353.6055\n",
      "epoch [11/100], loss:34447.6875\n",
      "epoch [11/100], loss:34988.0508\n",
      "epoch [11/100], loss:33999.6484\n",
      "epoch [11/100], loss:32762.7266\n",
      "epoch [11/100], loss:32140.1523\n",
      "epoch [11/100], loss:33262.0781\n",
      "epoch [11/100], loss:32834.3906\n",
      "epoch [11/100], loss:34543.5039\n",
      "epoch [11/100], loss:33766.5898\n",
      "epoch [11/100], loss:27363.0391\n",
      "validation loss is 763910.312500\n",
      "Test Accuracy 0.977757 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.98      0.98      0.98     16050\n",
      "        real       0.98      0.98      0.98     16050\n",
      "\n",
      "    accuracy                           0.98     32100\n",
      "   macro avg       0.98      0.98      0.98     32100\n",
      "weighted avg       0.98      0.98      0.98     32100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [12/100], loss:33703.2656\n",
      "epoch [12/100], loss:32019.4961\n",
      "epoch [12/100], loss:31715.5938\n",
      "epoch [12/100], loss:32852.7773\n",
      "epoch [12/100], loss:32187.8320\n",
      "epoch [12/100], loss:32683.9648\n",
      "epoch [12/100], loss:32986.4180\n",
      "epoch [12/100], loss:32915.7422\n",
      "epoch [12/100], loss:32007.4863\n",
      "epoch [12/100], loss:31931.0957\n",
      "epoch [12/100], loss:31771.5078\n",
      "epoch [12/100], loss:26864.7148\n",
      "validation loss is 874245.437500\n",
      "Test Accuracy 0.957632 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.94      0.98      0.96     16050\n",
      "        real       0.98      0.93      0.96     16050\n",
      "\n",
      "    accuracy                           0.96     32100\n",
      "   macro avg       0.96      0.96      0.96     32100\n",
      "weighted avg       0.96      0.96      0.96     32100\n",
      "\n",
      "epoch [13/100], loss:32252.5312\n",
      "epoch [13/100], loss:31690.6699\n",
      "epoch [13/100], loss:32193.3262\n",
      "epoch [13/100], loss:31742.2383\n",
      "epoch [13/100], loss:32487.1797\n",
      "epoch [13/100], loss:32104.4277\n",
      "epoch [13/100], loss:31964.1055\n",
      "epoch [13/100], loss:31235.4980\n",
      "epoch [13/100], loss:33647.3164\n",
      "epoch [13/100], loss:33578.6875\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torchvision.utils as vutils\n",
    "from sklearn.metrics import classification_report\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "target_names = ['fake','real']\n",
    "loss_val =0\n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for _, (x,label) in enumerate(train_dataloader):\n",
    "        init = x\n",
    "        init= init.to(device)\n",
    "        x = x.view(x.size(),-1)\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output,act_data = model(x,label)\n",
    "        rec_loss = criterion1(output, init)\n",
    "        act_loss = act_loss_func(act_data, label)\n",
    "        loss = act_loss+0.1*rec_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (_+1) % 10 == 0:\n",
    "            print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "        \n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "\n",
    "#     vutils.save_image(x,'deepfake_%d_real_samples.png' % epoch,normalize=True)\n",
    "#     vutils.save_image(output,'deepfake_%d_generated_samples.png' % epoch,normalize=True)\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    pred= []\n",
    "    labels= []\n",
    "    correct =0\n",
    "    total =0\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for _, (x,label) in enumerate(validation_dataloader):\n",
    "            init = x\n",
    "            init= init.to(device)\n",
    "            x = x.view(x.size(),-1)\n",
    "            x = x.to(device)\n",
    "            a= label.shape[0]\n",
    "            temp = torch.rand([a])\n",
    "            output,act_data = model(x,temp)\n",
    "            outputs  = act_loss_test(act_data)\n",
    "            \n",
    "            rec_loss = criterion1(output, init)\n",
    "            act_loss = act_loss_func(act_data, label)\n",
    "\n",
    "            loss += act_loss+0.1*rec_loss  \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            pred += predicted.tolist()\n",
    "            labels += label.tolist()\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(\"validation loss is %f\" % loss)\n",
    "        temp =correct/len(validation_dataset)\n",
    "        print('Test Accuracy %f %%' % temp)\n",
    "        print(classification_report(labels, pred, target_names=target_names))\n",
    "        \n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best=True, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL= Autoencoder()\n",
    "MODEL.load_state_dict(torch.load(\"stylegan90epoch_.pth\"))\n",
    "MODEL.cuda()\n",
    "MODEL.eval()\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "target_data_1 =torchvision.datasets.ImageFolder(root=r\"C:\\Users\\jonathan\\Desktop\\prj\\gan_detection\\PGGAN_128\\fine_tune\",\n",
    "                                                transform = transforms.Compose([transforms.Resize((128 ,128 )),transforms.ToTensor()]))\n",
    "target_data_2 =torchvision.datasets.ImageFolder(root=r\"C:\\Users\\jonathan\\Desktop\\prj\\gan_detection\\StyleGAN2_256\",\n",
    "                                                transform = transforms.Compose([transforms.Resize((128 ,128 )),transforms.ToTensor()]))\n",
    "\n",
    "\n",
    "zeroshot_data_1 = torch.utils.data.DataLoader(dataset=target_data_1,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "zeroshot_data_2 = torch.utils.data.DataLoader(dataset=target_data_2,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct= 0\n",
    "    loss = 0\n",
    "    pred= []\n",
    "    labels = []\n",
    "    for _, (x,label) in enumerate(zeroshot_data_1):\n",
    "        init = x\n",
    "        init= init.to(device)\n",
    "        x = x.view(x.size(),-1)\n",
    "        x = x.to(device)\n",
    "        a= label.shape[0]\n",
    "        temp = torch.rand([a])\n",
    "        output,act_data = MODEL(x,temp)\n",
    "        outputs  = act_loss_test(act_data)\n",
    "        \n",
    "        rec_loss = criterion1(output, init)\n",
    "        act_loss = act_loss_func(act_data, label)\n",
    "\n",
    "        loss += act_loss+0.1*rec_loss  \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred += predicted.tolist()\n",
    "        labels += label.tolist()\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # print(\"PGGAN_128 loss is %f\" % loss)\n",
    "    temp =correct/len(zeroshot_data_1)\n",
    "    print('Test Accuracy %f %%' % temp)\n",
    "    print(classification_report(labels, pred, target_names=target_names))\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct= 0\n",
    "    loss = 0\n",
    "    pred= []\n",
    "    labels = []\n",
    "    for _, (x,label) in enumerate(zeroshot_data_2):\n",
    "        init = x\n",
    "        init= init.to(device)\n",
    "        x = x.view(x.size(),-1)\n",
    "        x = x.to(device)\n",
    "        a= label.shape[0]\n",
    "        temp = torch.rand([a])\n",
    "        output,act_data = MODEL(x,temp)\n",
    "        outputs  = act_loss_test(act_data)\n",
    "        \n",
    "        rec_loss = criterion1(output, init)\n",
    "        act_loss = act_loss_func(act_data, label)\n",
    "\n",
    "        loss += act_loss+0.1*rec_loss  \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        pred += predicted.tolist()\n",
    "        labels += label.tolist()\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(\"StyleGAN2 loss is %f\" % loss)\n",
    "    temp =correct/len(target_data_2)\n",
    "    print('Test Accuracy %f %%' % temp)\n",
    "    print(classification_report(labels, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
