{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN2_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained =  './log/pggan/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 200\n",
    "test_batch = 200\n",
    "lr = 0.1\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/pggan/128/b0/to_style2/1000shot/self' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.01\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'style2/1000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/pggan/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.5, 'drop_connect_rate':0.5})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)\n",
    "        \n",
    "        outputs = student_model(inputs)\n",
    "        loss_main = criterion(outputs, targets)\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + sp_alpha*loss_sp + sp_beta*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.100000\n",
      "Train | 20/20 | Loss:0.9417 | MainLoss:0.8962 | Alpha:0.0216 | SPLoss:1.4114 | CLSLoss:1.4380 | top1:49.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8531 | MainLoss:0.6929 | SPLoss:1.4647 | CLSLoss:1.3707 | top1:50.0000 | AUROC:0.5740\n",
      "Test | 161/20 | Loss:0.8421 | MainLoss:0.6819 | SPLoss:1.4647 | CLSLoss:1.3707 | top1:59.2804 | AUROC:0.9999\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.130000\n",
      "Train | 20/20 | Loss:0.7409 | MainLoss:0.6937 | Alpha:0.0246 | SPLoss:1.3767 | CLSLoss:1.3337 | top1:50.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8336 | MainLoss:0.6929 | SPLoss:1.2782 | CLSLoss:1.2943 | top1:50.0000 | AUROC:0.5751\n",
      "Test | 161/20 | Loss:0.8178 | MainLoss:0.6770 | SPLoss:1.2782 | CLSLoss:1.2943 | top1:73.8380 | AUROC:1.0000\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.160000\n",
      "Train | 20/20 | Loss:0.7342 | MainLoss:0.6945 | Alpha:0.0229 | SPLoss:1.1910 | CLSLoss:1.2506 | top1:50.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8146 | MainLoss:0.6931 | SPLoss:1.0953 | CLSLoss:1.2019 | top1:50.0000 | AUROC:0.5767\n",
      "Test | 161/20 | Loss:0.7854 | MainLoss:0.6638 | SPLoss:1.0953 | CLSLoss:1.2019 | top1:50.0000 | AUROC:1.0000\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.190000\n",
      "Train | 20/20 | Loss:0.7285 | MainLoss:0.6938 | Alpha:0.0231 | SPLoss:1.0003 | CLSLoss:1.1556 | top1:50.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7932 | MainLoss:0.6919 | SPLoss:0.9026 | CLSLoss:1.1046 | top1:51.2821 | AUROC:0.5790\n",
      "Test | 161/20 | Loss:0.7498 | MainLoss:0.6485 | SPLoss:0.9026 | CLSLoss:1.1046 | top1:85.8941 | AUROC:1.0000\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.220000\n",
      "Train | 20/20 | Loss:0.7228 | MainLoss:0.6924 | Alpha:0.0246 | SPLoss:0.7999 | CLSLoss:1.0565 | top1:51.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7703 | MainLoss:0.6905 | SPLoss:0.6979 | CLSLoss:1.0044 | top1:53.2692 | AUROC:0.5825\n",
      "Test | 161/20 | Loss:0.6768 | MainLoss:0.5969 | SPLoss:0.6979 | CLSLoss:1.0044 | top1:99.8474 | AUROC:1.0000\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.250000\n",
      "Train | 20/20 | Loss:0.7140 | MainLoss:0.6902 | Alpha:0.0229 | SPLoss:0.6181 | CLSLoss:0.9578 | top1:53.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7458 | MainLoss:0.6841 | SPLoss:0.5256 | CLSLoss:0.9099 | top1:57.4359 | AUROC:0.6015\n",
      "Test | 161/20 | Loss:0.5171 | MainLoss:0.4554 | SPLoss:0.5256 | CLSLoss:0.9099 | top1:99.2586 | AUROC:1.0000\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.280000\n",
      "Train | 20/20 | Loss:0.7071 | MainLoss:0.6865 | Alpha:0.0242 | SPLoss:0.4938 | CLSLoss:0.8535 | top1:55.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7213 | MainLoss:0.6642 | SPLoss:0.4916 | CLSLoss:0.7895 | top1:62.3205 | AUROC:0.6711\n",
      "Test | 161/20 | Loss:0.3992 | MainLoss:0.3421 | SPLoss:0.4916 | CLSLoss:0.7895 | top1:98.6199 | AUROC:1.0000\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.310000\n",
      "Train | 20/20 | Loss:0.7140 | MainLoss:0.6860 | Alpha:0.0232 | SPLoss:0.8966 | CLSLoss:0.6742 | top1:57.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8120 | MainLoss:0.6874 | SPLoss:1.1920 | CLSLoss:0.5392 | top1:52.5513 | AUROC:0.6888\n",
      "Test | 161/20 | Loss:0.7518 | MainLoss:0.6272 | SPLoss:1.1920 | CLSLoss:0.5392 | top1:98.0343 | AUROC:0.9994\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.340000\n",
      "Train | 20/20 | Loss:0.7183 | MainLoss:0.6870 | Alpha:0.0233 | SPLoss:1.1163 | CLSLoss:0.5129 | top1:57.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8438 | MainLoss:0.6875 | SPLoss:1.5283 | CLSLoss:0.3522 | top1:50.0000 | AUROC:0.7442\n",
      "Test | 161/20 | Loss:0.7988 | MainLoss:0.6425 | SPLoss:1.5283 | CLSLoss:0.3522 | top1:81.5389 | AUROC:0.9998\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.370000\n",
      "Train | 20/20 | Loss:0.6668 | MainLoss:0.6322 | Alpha:0.0218 | SPLoss:1.4145 | CLSLoss:0.3923 | top1:66.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:79.7756 | MainLoss:0.5617 | SPLoss:792.0952 | CLSLoss:0.4443 | top1:71.4487 | AUROC:0.8487\n",
      "Test | 161/20 | Loss:79.5575 | MainLoss:0.3435 | SPLoss:792.0968 | CLSLoss:0.4443 | top1:85.2274 | AUROC:0.9838\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.400000\n",
      "Train | 20/20 | Loss:6.2916 | MainLoss:0.6004 | Alpha:0.3667 | SPLoss:15.5202 | CLSLoss:0.3227 | top1:70.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6622 | MainLoss:0.5458 | SPLoss:1.1408 | CLSLoss:0.2314 | top1:78.0897 | AUROC:0.8672\n",
      "Test | 161/20 | Loss:0.6697 | MainLoss:0.5533 | SPLoss:1.1408 | CLSLoss:0.2314 | top1:74.1433 | AUROC:0.8773\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.400000\n",
      "Train | 20/20 | Loss:46.2522 | MainLoss:0.5549 | Alpha:0.3677 | SPLoss:124.0331 | CLSLoss:0.3046 | top1:73.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:7.2910 | MainLoss:0.4967 | SPLoss:67.9064 | CLSLoss:0.3692 | top1:76.2051 | AUROC:0.8806\n",
      "Test | 161/20 | Loss:7.4406 | MainLoss:0.6463 | SPLoss:67.9063 | CLSLoss:0.3692 | top1:64.4642 | AUROC:0.8451\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.399999\n",
      "Train | 20/20 | Loss:7.2550 | MainLoss:0.5796 | Alpha:0.3666 | SPLoss:18.0327 | CLSLoss:0.2999 | top1:72.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:1.6261 | MainLoss:0.4456 | SPLoss:11.7698 | CLSLoss:0.3524 | top1:80.8846 | AUROC:0.8964\n",
      "Test | 161/20 | Loss:1.6238 | MainLoss:0.4433 | SPLoss:11.7698 | CLSLoss:0.3524 | top1:79.6542 | AUROC:0.9023\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.399996\n",
      "Train | 20/20 | Loss:3.5724 | MainLoss:0.5766 | Alpha:0.3654 | SPLoss:8.1612 | CLSLoss:0.2972 | top1:72.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8467 | MainLoss:0.6771 | SPLoss:1.6596 | CLSLoss:0.3617 | top1:62.3718 | AUROC:0.8841\n",
      "Test | 161/20 | Loss:0.5068 | MainLoss:0.3372 | SPLoss:1.6596 | CLSLoss:0.3617 | top1:89.1184 | AUROC:0.9677\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.399991\n",
      "Train | 20/20 | Loss:7.3076 | MainLoss:0.6225 | Alpha:0.3673 | SPLoss:18.4332 | CLSLoss:0.2828 | top1:69.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:5.4726 | MainLoss:0.5592 | SPLoss:49.1069 | CLSLoss:0.2627 | top1:73.6282 | AUROC:0.8415\n",
      "Test | 161/20 | Loss:5.4705 | MainLoss:0.5571 | SPLoss:49.1069 | CLSLoss:0.2627 | top1:74.5888 | AUROC:0.8251\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.399984\n",
      "Train | 20/20 | Loss:16.3666 | MainLoss:0.5672 | Alpha:0.3660 | SPLoss:43.2989 | CLSLoss:0.3156 | top1:71.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.8625 | MainLoss:0.5413 | SPLoss:3.1698 | CLSLoss:0.4225 | top1:73.3333 | AUROC:0.8877\n",
      "Test | 161/20 | Loss:0.7094 | MainLoss:0.3882 | SPLoss:3.1698 | CLSLoss:0.4225 | top1:83.9502 | AUROC:0.9173\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.399975\n",
      "Train | 20/20 | Loss:7.3176 | MainLoss:0.5898 | Alpha:0.3678 | SPLoss:18.0779 | CLSLoss:0.3418 | top1:70.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:13.6380 | MainLoss:0.6043 | SPLoss:130.3031 | CLSLoss:0.3371 | top1:67.5256 | AUROC:0.8264\n",
      "Test | 161/20 | Loss:13.7864 | MainLoss:0.7528 | SPLoss:130.3028 | CLSLoss:0.3371 | top1:56.7726 | AUROC:0.7327\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.399964\n",
      "Train | 20/20 | Loss:20.6038 | MainLoss:0.5634 | Alpha:0.3670 | SPLoss:54.4093 | CLSLoss:0.3538 | top1:71.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:4.0174 | MainLoss:0.4749 | SPLoss:35.3883 | CLSLoss:0.3716 | top1:79.0513 | AUROC:0.8886\n",
      "Test | 161/20 | Loss:4.0480 | MainLoss:0.5055 | SPLoss:35.3884 | CLSLoss:0.3716 | top1:75.6386 | AUROC:0.8597\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.399952\n",
      "Train | 20/20 | Loss:9.4599 | MainLoss:0.6002 | Alpha:0.3679 | SPLoss:23.9848 | CLSLoss:0.3353 | top1:71.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:2.7554 | MainLoss:0.7791 | SPLoss:19.7274 | CLSLoss:0.3610 | top1:56.9872 | AUROC:0.7963\n",
      "Test | 161/20 | Loss:2.6842 | MainLoss:0.7078 | SPLoss:19.7274 | CLSLoss:0.3610 | top1:59.3832 | AUROC:0.8187\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.399937\n",
      "Train | 20/20 | Loss:23.0372 | MainLoss:0.6390 | Alpha:0.3683 | SPLoss:60.8160 | CLSLoss:0.2734 | top1:65.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:6.8443 | MainLoss:0.6392 | SPLoss:62.0283 | CLSLoss:0.2192 | top1:61.5513 | AUROC:0.7963\n",
      "Test | 161/20 | Loss:6.8161 | MainLoss:0.6111 | SPLoss:62.0283 | CLSLoss:0.2192 | top1:66.3801 | AUROC:0.8482\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.399920\n",
      "Train | 20/20 | Loss:2.4374 | MainLoss:0.5583 | Alpha:0.3472 | SPLoss:5.4070 | CLSLoss:0.3617 | top1:71.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6519 | MainLoss:0.4829 | SPLoss:1.6486 | CLSLoss:0.4131 | top1:78.1026 | AUROC:0.9099\n",
      "Test | 161/20 | Loss:0.6638 | MainLoss:0.4948 | SPLoss:1.6486 | CLSLoss:0.4131 | top1:79.9502 | AUROC:0.8791\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.399901\n",
      "Train | 20/20 | Loss:0.9563 | MainLoss:0.4914 | Alpha:0.3469 | SPLoss:1.3260 | CLSLoss:0.5454 | top1:75.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:1.4266 | MainLoss:0.5710 | SPLoss:8.5031 | CLSLoss:0.5236 | top1:65.9615 | AUROC:0.8975\n",
      "Test | 161/20 | Loss:1.4184 | MainLoss:0.5628 | SPLoss:8.5031 | CLSLoss:0.5236 | top1:73.1215 | AUROC:0.8371\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.399881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:3.7409 | MainLoss:0.5088 | Alpha:0.3471 | SPLoss:9.2718 | CLSLoss:0.5813 | top1:76.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:1.0694 | MainLoss:0.5015 | SPLoss:5.6299 | CLSLoss:0.4957 | top1:77.9487 | AUROC:0.9221\n",
      "Test | 161/20 | Loss:1.1426 | MainLoss:0.5746 | SPLoss:5.6299 | CLSLoss:0.4957 | top1:72.4019 | AUROC:0.8003\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.399858\n",
      "Train | 20/20 | Loss:2.2691 | MainLoss:0.4983 | Alpha:0.3465 | SPLoss:5.1120 | CLSLoss:0.5678 | top1:76.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:4.1588 | MainLoss:0.3750 | SPLoss:37.7695 | CLSLoss:0.6854 | top1:83.4103 | AUROC:0.9179\n",
      "Test | 161/20 | Loss:4.4252 | MainLoss:0.6413 | SPLoss:37.7696 | CLSLoss:0.6854 | top1:68.3489 | AUROC:0.8099\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.399833\n",
      "Train | 20/20 | Loss:21.6828 | MainLoss:0.5665 | Alpha:0.3466 | SPLoss:60.4746 | CLSLoss:0.5442 | top1:71.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:1.7766 | MainLoss:0.4255 | SPLoss:13.4602 | CLSLoss:0.5131 | top1:83.2308 | AUROC:0.9066\n",
      "Test | 161/20 | Loss:2.0645 | MainLoss:0.7134 | SPLoss:13.4603 | CLSLoss:0.5131 | top1:57.7570 | AUROC:0.6950\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.399807\n",
      "Train | 20/20 | Loss:3.6095 | MainLoss:0.5211 | Alpha:0.3471 | SPLoss:8.8828 | CLSLoss:0.5391 | top1:74.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:1.2496 | MainLoss:0.6239 | SPLoss:6.2076 | CLSLoss:0.5011 | top1:62.1026 | AUROC:0.8820\n",
      "Test | 161/20 | Loss:1.5436 | MainLoss:0.9178 | SPLoss:6.2076 | CLSLoss:0.5011 | top1:50.4268 | AUROC:0.5224\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.399778\n",
      "Train | 20/20 | Loss:1.6185 | MainLoss:0.5115 | Alpha:0.3460 | SPLoss:3.1814 | CLSLoss:0.5671 | top1:75.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7757 | MainLoss:0.4423 | SPLoss:3.2805 | CLSLoss:0.5359 | top1:81.2692 | AUROC:0.9169\n",
      "Test | 161/20 | Loss:0.9798 | MainLoss:0.6464 | SPLoss:3.2805 | CLSLoss:0.5359 | top1:63.1090 | AUROC:0.6829\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.399747\n",
      "Train | 20/20 | Loss:3.2072 | MainLoss:0.4866 | Alpha:0.3466 | SPLoss:7.8155 | CLSLoss:0.6162 | top1:77.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:6.6013 | MainLoss:0.3735 | SPLoss:62.2092 | CLSLoss:0.6871 | top1:83.6410 | AUROC:0.9272\n",
      "Test | 161/20 | Loss:6.8427 | MainLoss:0.6149 | SPLoss:62.2093 | CLSLoss:0.6871 | top1:68.1589 | AUROC:0.7647\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.399715\n",
      "Train | 20/20 | Loss:6.9568 | MainLoss:0.5598 | Alpha:0.3467 | SPLoss:18.3935 | CLSLoss:0.5101 | top1:73.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:1.8219 | MainLoss:0.5287 | SPLoss:12.8789 | CLSLoss:0.5295 | top1:72.7051 | AUROC:0.9043\n",
      "Test | 161/20 | Loss:2.1044 | MainLoss:0.8112 | SPLoss:12.8789 | CLSLoss:0.5295 | top1:55.0156 | AUROC:0.7111\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.399680\n",
      "Train | 20/20 | Loss:3.4078 | MainLoss:0.5330 | Alpha:0.3474 | SPLoss:8.2390 | CLSLoss:0.5381 | top1:73.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7409 | MainLoss:0.6150 | SPLoss:1.2076 | CLSLoss:0.5057 | top1:62.6026 | AUROC:0.9013\n",
      "Test | 161/20 | Loss:0.7375 | MainLoss:0.6117 | SPLoss:1.2076 | CLSLoss:0.5057 | top1:65.2897 | AUROC:0.7631\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.399644\n",
      "Train | 20/20 | Loss:0.8493 | MainLoss:0.4636 | Alpha:0.3549 | SPLoss:1.0673 | CLSLoss:0.6414 | top1:78.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5312 | MainLoss:0.3976 | SPLoss:1.2707 | CLSLoss:0.6587 | top1:84.3205 | AUROC:0.9213\n",
      "Test | 161/20 | Loss:0.9164 | MainLoss:0.7828 | SPLoss:1.2707 | CLSLoss:0.6587 | top1:54.3489 | AUROC:0.5595\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.399605\n",
      "Train | 20/20 | Loss:0.9036 | MainLoss:0.4070 | Alpha:0.3530 | SPLoss:1.3891 | CLSLoss:0.7776 | top1:81.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4433 | MainLoss:0.3501 | SPLoss:0.8526 | CLSLoss:0.7876 | top1:87.4103 | AUROC:0.9446\n",
      "Test | 161/20 | Loss:0.8843 | MainLoss:0.7912 | SPLoss:0.8526 | CLSLoss:0.7876 | top1:55.9346 | AUROC:0.6156\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.399565\n",
      "Train | 20/20 | Loss:0.9307 | MainLoss:0.4051 | Alpha:0.3535 | SPLoss:1.4719 | CLSLoss:0.8616 | top1:81.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5219 | MainLoss:0.4384 | SPLoss:0.7423 | CLSLoss:0.9319 | top1:81.1667 | AUROC:0.9417\n",
      "Test | 161/20 | Loss:0.8168 | MainLoss:0.7332 | SPLoss:0.7424 | CLSLoss:0.9319 | top1:64.3583 | AUROC:0.7031\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.399523\n",
      "Train | 20/20 | Loss:0.9225 | MainLoss:0.4077 | Alpha:0.3542 | SPLoss:1.4284 | CLSLoss:0.8952 | top1:82.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4353 | MainLoss:0.3149 | SPLoss:1.1066 | CLSLoss:0.9770 | top1:86.6154 | AUROC:0.9507\n",
      "Test | 161/20 | Loss:0.9095 | MainLoss:0.7891 | SPLoss:1.1066 | CLSLoss:0.9770 | top1:61.8162 | AUROC:0.6819\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.399478\n",
      "Train | 20/20 | Loss:0.7762 | MainLoss:0.3995 | Alpha:0.3558 | SPLoss:1.0327 | CLSLoss:0.9717 | top1:81.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4104 | MainLoss:0.3416 | SPLoss:0.5827 | CLSLoss:1.0463 | top1:85.1410 | AUROC:0.9491\n",
      "Test | 161/20 | Loss:0.8827 | MainLoss:0.8140 | SPLoss:0.5827 | CLSLoss:1.0463 | top1:62.9657 | AUROC:0.6984\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.399432\n",
      "Train | 20/20 | Loss:0.9248 | MainLoss:0.4389 | Alpha:0.3540 | SPLoss:1.3525 | CLSLoss:0.9295 | top1:79.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.7343 | MainLoss:0.5845 | SPLoss:1.4116 | CLSLoss:0.8697 | top1:62.8333 | AUROC:0.9300\n",
      "Test | 161/20 | Loss:1.1478 | MainLoss:0.9980 | SPLoss:1.4116 | CLSLoss:0.8697 | top1:50.1558 | AUROC:0.5684\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.399383\n",
      "Train | 20/20 | Loss:0.7108 | MainLoss:0.3958 | Alpha:0.3565 | SPLoss:0.8571 | CLSLoss:0.9600 | top1:81.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5572 | MainLoss:0.4535 | SPLoss:0.9399 | CLSLoss:0.9686 | top1:78.1282 | AUROC:0.9503\n",
      "Test | 161/20 | Loss:0.8829 | MainLoss:0.7792 | SPLoss:0.9399 | CLSLoss:0.9686 | top1:56.7072 | AUROC:0.5996\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.399333\n",
      "Train | 20/20 | Loss:0.7604 | MainLoss:0.3963 | Alpha:0.3546 | SPLoss:1.0022 | CLSLoss:0.9773 | top1:82.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5794 | MainLoss:0.4718 | SPLoss:0.9751 | CLSLoss:1.0125 | top1:78.9487 | AUROC:0.9389\n",
      "Test | 161/20 | Loss:0.8632 | MainLoss:0.7556 | SPLoss:0.9751 | CLSLoss:1.0125 | top1:62.2835 | AUROC:0.6676\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.399281\n",
      "Train | 20/20 | Loss:0.8108 | MainLoss:0.4169 | Alpha:0.3542 | SPLoss:1.0853 | CLSLoss:0.9546 | top1:81.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.6204 | MainLoss:0.3241 | SPLoss:2.8623 | CLSLoss:1.0126 | top1:86.5513 | AUROC:0.9389\n",
      "Test | 161/20 | Loss:1.2821 | MainLoss:0.9858 | SPLoss:2.8623 | CLSLoss:1.0126 | top1:53.8660 | AUROC:0.5742\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.399227\n",
      "Train | 20/20 | Loss:1.1500 | MainLoss:0.4189 | Alpha:0.3546 | SPLoss:2.0313 | CLSLoss:0.9890 | top1:80.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4504 | MainLoss:0.3173 | SPLoss:1.2303 | CLSLoss:1.0077 | top1:86.8077 | AUROC:0.9467\n",
      "Test | 161/20 | Loss:1.1753 | MainLoss:1.0422 | SPLoss:1.2303 | CLSLoss:1.0077 | top1:53.5701 | AUROC:0.5833\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.399171\n",
      "Train | 20/20 | Loss:0.5374 | MainLoss:0.3300 | Alpha:0.4125 | SPLoss:0.4792 | CLSLoss:1.0226 | top1:86.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3966 | MainLoss:0.3321 | SPLoss:0.5427 | CLSLoss:1.0284 | top1:85.6795 | AUROC:0.9686\n",
      "Test | 161/20 | Loss:0.9490 | MainLoss:0.8845 | SPLoss:0.5427 | CLSLoss:1.0284 | top1:57.2991 | AUROC:0.6070\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.399112\n",
      "Train | 20/20 | Loss:1.1300 | MainLoss:0.3666 | Alpha:0.4127 | SPLoss:1.8163 | CLSLoss:0.9581 | top1:84.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.5065 | MainLoss:0.3145 | SPLoss:1.8323 | CLSLoss:0.8770 | top1:86.9359 | AUROC:0.9647\n",
      "Test | 161/20 | Loss:1.0224 | MainLoss:0.8304 | SPLoss:1.8323 | CLSLoss:0.8770 | top1:55.9439 | AUROC:0.5854\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.399052\n",
      "Train | 20/20 | Loss:1.6322 | MainLoss:0.3373 | Alpha:0.4137 | SPLoss:3.1003 | CLSLoss:0.8912 | top1:84.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4239 | MainLoss:0.3004 | SPLoss:1.1421 | CLSLoss:0.9268 | top1:87.5000 | AUROC:0.9500\n",
      "Test | 161/20 | Loss:1.1237 | MainLoss:1.0002 | SPLoss:1.1421 | CLSLoss:0.9268 | top1:58.1589 | AUROC:0.6606\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.398990\n",
      "Train | 20/20 | Loss:0.6179 | MainLoss:0.3138 | Alpha:0.4139 | SPLoss:0.7118 | CLSLoss:0.9467 | top1:87.1053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3531 | MainLoss:0.2814 | SPLoss:0.6255 | CLSLoss:0.9149 | top1:89.1923 | AUROC:0.9563\n",
      "Test | 161/20 | Loss:1.0531 | MainLoss:0.9814 | SPLoss:0.6255 | CLSLoss:0.9149 | top1:55.4050 | AUROC:0.6293\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.398926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.7242 | MainLoss:0.3407 | Alpha:0.4135 | SPLoss:0.9104 | CLSLoss:0.8963 | top1:85.1053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3748 | MainLoss:0.3081 | SPLoss:0.5725 | CLSLoss:0.9447 | top1:87.5256 | AUROC:0.9528\n",
      "Test | 161/20 | Loss:1.4650 | MainLoss:1.3983 | SPLoss:0.5725 | CLSLoss:0.9447 | top1:53.5109 | AUROC:0.5991\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.398860\n",
      "Train | 20/20 | Loss:0.6214 | MainLoss:0.3713 | Alpha:0.4144 | SPLoss:0.5821 | CLSLoss:0.8712 | top1:83.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4172 | MainLoss:0.3545 | SPLoss:0.5460 | CLSLoss:0.8101 | top1:84.8846 | AUROC:0.9549\n",
      "Test | 161/20 | Loss:1.0875 | MainLoss:1.0248 | SPLoss:0.5460 | CLSLoss:0.8101 | top1:50.8505 | AUROC:0.5117\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.398792\n",
      "Train | 20/20 | Loss:0.5533 | MainLoss:0.3532 | Alpha:0.4160 | SPLoss:0.4610 | CLSLoss:0.8273 | top1:84.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3118 | MainLoss:0.2425 | SPLoss:0.6058 | CLSLoss:0.8723 | top1:90.2949 | AUROC:0.9675\n",
      "Test | 161/20 | Loss:1.1503 | MainLoss:1.0810 | SPLoss:0.6058 | CLSLoss:0.8723 | top1:52.9346 | AUROC:0.5590\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.398722\n",
      "Train | 20/20 | Loss:0.8072 | MainLoss:0.3228 | Alpha:0.4155 | SPLoss:1.1487 | CLSLoss:0.8760 | top1:86.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3359 | MainLoss:0.2758 | SPLoss:0.5096 | CLSLoss:0.9135 | top1:88.4359 | AUROC:0.9591\n",
      "Test | 161/20 | Loss:1.1727 | MainLoss:1.1126 | SPLoss:0.5096 | CLSLoss:0.9135 | top1:53.8287 | AUROC:0.6268\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.398650\n",
      "Train | 20/20 | Loss:0.9786 | MainLoss:0.3588 | Alpha:0.4131 | SPLoss:1.4819 | CLSLoss:0.8696 | top1:83.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.4262 | MainLoss:0.3398 | SPLoss:0.7784 | CLSLoss:0.8544 | top1:86.0385 | AUROC:0.9660\n",
      "Test | 161/20 | Loss:0.8485 | MainLoss:0.7621 | SPLoss:0.7784 | CLSLoss:0.8544 | top1:61.9221 | AUROC:0.6792\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.398577\n",
      "Train | 20/20 | Loss:0.9227 | MainLoss:0.3607 | Alpha:0.4123 | SPLoss:1.3313 | CLSLoss:0.8353 | top1:84.3421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3152 | MainLoss:0.2674 | SPLoss:0.3951 | CLSLoss:0.8204 | top1:89.1667 | AUROC:0.9612\n",
      "Test | 161/20 | Loss:0.9677 | MainLoss:0.9200 | SPLoss:0.3951 | CLSLoss:0.8204 | top1:56.1464 | AUROC:0.5971\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.398501\n",
      "Train | 20/20 | Loss:0.5491 | MainLoss:0.3126 | Alpha:0.4362 | SPLoss:0.5236 | CLSLoss:0.8362 | top1:87.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.3204 | MainLoss:0.2651 | SPLoss:0.4716 | CLSLoss:0.8122 | top1:89.7179 | AUROC:0.9655\n",
      "Test | 161/20 | Loss:1.0862 | MainLoss:1.0309 | SPLoss:0.4716 | CLSLoss:0.8122 | top1:51.1184 | AUROC:0.5035\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.039842\n",
      "Train | 20/20 | Loss:0.4039 | MainLoss:0.2451 | Alpha:0.4337 | SPLoss:0.3470 | CLSLoss:0.8365 | top1:90.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2469 | MainLoss:0.2133 | SPLoss:0.2506 | CLSLoss:0.8606 | top1:91.6795 | AUROC:0.9752\n",
      "Test | 161/20 | Loss:1.1661 | MainLoss:1.1324 | SPLoss:0.2506 | CLSLoss:0.8606 | top1:53.4081 | AUROC:0.5548\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.039834\n",
      "Train | 20/20 | Loss:0.3090 | MainLoss:0.2114 | Alpha:0.4355 | SPLoss:0.2040 | CLSLoss:0.8759 | top1:91.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2301 | MainLoss:0.2048 | SPLoss:0.1644 | CLSLoss:0.8882 | top1:92.0256 | AUROC:0.9775\n",
      "Test | 161/20 | Loss:1.1798 | MainLoss:1.1545 | SPLoss:0.1644 | CLSLoss:0.8882 | top1:54.6199 | AUROC:0.5854\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.039826\n",
      "Train | 20/20 | Loss:0.2767 | MainLoss:0.2044 | Alpha:0.4344 | SPLoss:0.1459 | CLSLoss:0.8983 | top1:92.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2233 | MainLoss:0.2015 | SPLoss:0.1271 | CLSLoss:0.9099 | top1:92.0128 | AUROC:0.9778\n",
      "Test | 161/20 | Loss:1.1828 | MainLoss:1.1610 | SPLoss:0.1271 | CLSLoss:0.9099 | top1:55.0031 | AUROC:0.6005\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.039818\n",
      "Train | 20/20 | Loss:0.2497 | MainLoss:0.1877 | Alpha:0.4368 | SPLoss:0.1208 | CLSLoss:0.9225 | top1:92.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2259 | MainLoss:0.2051 | SPLoss:0.1140 | CLSLoss:0.9373 | top1:91.8333 | AUROC:0.9776\n",
      "Test | 161/20 | Loss:1.1973 | MainLoss:1.1766 | SPLoss:0.1140 | CLSLoss:0.9373 | top1:56.0249 | AUROC:0.6196\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.039809\n",
      "Train | 20/20 | Loss:0.2588 | MainLoss:0.2004 | Alpha:0.4347 | SPLoss:0.1125 | CLSLoss:0.9420 | top1:92.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2281 | MainLoss:0.2076 | SPLoss:0.1103 | CLSLoss:0.9479 | top1:91.6410 | AUROC:0.9779\n",
      "Test | 161/20 | Loss:1.1744 | MainLoss:1.1539 | SPLoss:0.1103 | CLSLoss:0.9479 | top1:55.9564 | AUROC:0.6161\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.039800\n",
      "Train | 20/20 | Loss:0.2548 | MainLoss:0.1967 | Alpha:0.4346 | SPLoss:0.1117 | CLSLoss:0.9578 | top1:91.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2239 | MainLoss:0.2032 | SPLoss:0.1108 | CLSLoss:0.9645 | top1:91.8846 | AUROC:0.9776\n",
      "Test | 161/20 | Loss:1.1887 | MainLoss:1.1680 | SPLoss:0.1108 | CLSLoss:0.9645 | top1:56.1122 | AUROC:0.6101\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.039792\n",
      "Train | 20/20 | Loss:0.2481 | MainLoss:0.1906 | Alpha:0.4365 | SPLoss:0.1093 | CLSLoss:0.9751 | top1:92.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2240 | MainLoss:0.2034 | SPLoss:0.1082 | CLSLoss:0.9766 | top1:91.7692 | AUROC:0.9780\n",
      "Test | 161/20 | Loss:1.1456 | MainLoss:1.1251 | SPLoss:0.1082 | CLSLoss:0.9766 | top1:56.6978 | AUROC:0.6330\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.039782\n",
      "Train | 20/20 | Loss:0.2461 | MainLoss:0.1897 | Alpha:0.4360 | SPLoss:0.1068 | CLSLoss:0.9839 | top1:92.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2215 | MainLoss:0.2008 | SPLoss:0.1071 | CLSLoss:0.9942 | top1:91.7949 | AUROC:0.9778\n",
      "Test | 161/20 | Loss:1.2107 | MainLoss:1.1900 | SPLoss:0.1071 | CLSLoss:0.9942 | top1:55.8723 | AUROC:0.6195\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.039773\n",
      "Train | 20/20 | Loss:0.2484 | MainLoss:0.1914 | Alpha:0.4355 | SPLoss:0.1079 | CLSLoss:0.9955 | top1:92.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2176 | MainLoss:0.1972 | SPLoss:0.1033 | CLSLoss:1.0049 | top1:92.3333 | AUROC:0.9784\n",
      "Test | 161/20 | Loss:1.2192 | MainLoss:1.1988 | SPLoss:0.1033 | CLSLoss:1.0049 | top1:55.4548 | AUROC:0.6142\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.039763\n",
      "Train | 20/20 | Loss:0.2063 | MainLoss:0.1907 | Alpha:0.4556 | SPLoss:0.0123 | CLSLoss:1.0067 | top1:92.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1930 | MainLoss:0.1805 | SPLoss:0.0235 | CLSLoss:1.0121 | top1:92.8077 | AUROC:0.9814\n",
      "Test | 161/20 | Loss:1.2845 | MainLoss:1.2720 | SPLoss:0.0235 | CLSLoss:1.0121 | top1:55.2399 | AUROC:0.6192\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.039754\n",
      "Train | 20/20 | Loss:0.1838 | MainLoss:0.1583 | Alpha:0.4611 | SPLoss:0.0332 | CLSLoss:1.0188 | top1:93.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1950 | MainLoss:0.1807 | SPLoss:0.0410 | CLSLoss:1.0251 | top1:92.5897 | AUROC:0.9825\n",
      "Test | 161/20 | Loss:1.3724 | MainLoss:1.3581 | SPLoss:0.0410 | CLSLoss:1.0251 | top1:55.2461 | AUROC:0.6233\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.039744\n",
      "Train | 20/20 | Loss:0.1939 | MainLoss:0.1616 | Alpha:0.4586 | SPLoss:0.0481 | CLSLoss:1.0258 | top1:93.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1937 | MainLoss:0.1781 | SPLoss:0.0533 | CLSLoss:1.0275 | top1:92.7949 | AUROC:0.9839\n",
      "Test | 161/20 | Loss:1.3372 | MainLoss:1.3216 | SPLoss:0.0533 | CLSLoss:1.0275 | top1:55.5576 | AUROC:0.6231\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.039734\n",
      "Train | 20/20 | Loss:0.1941 | MainLoss:0.1576 | Alpha:0.4583 | SPLoss:0.0571 | CLSLoss:1.0300 | top1:93.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1861 | MainLoss:0.1699 | SPLoss:0.0588 | CLSLoss:1.0350 | top1:93.3974 | AUROC:0.9843\n",
      "Test | 161/20 | Loss:1.3960 | MainLoss:1.3798 | SPLoss:0.0588 | CLSLoss:1.0350 | top1:55.0374 | AUROC:0.6243\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.039723\n",
      "Train | 20/20 | Loss:0.1902 | MainLoss:0.1520 | Alpha:0.4583 | SPLoss:0.0606 | CLSLoss:1.0371 | top1:94.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.2119 | MainLoss:0.1950 | SPLoss:0.0649 | CLSLoss:1.0382 | top1:92.3333 | AUROC:0.9830\n",
      "Test | 161/20 | Loss:1.2792 | MainLoss:1.2623 | SPLoss:0.0649 | CLSLoss:1.0382 | top1:56.3209 | AUROC:0.6394\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.039713\n",
      "Train | 20/20 | Loss:0.1999 | MainLoss:0.1585 | Alpha:0.4567 | SPLoss:0.0679 | CLSLoss:1.0398 | top1:93.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1861 | MainLoss:0.1686 | SPLoss:0.0703 | CLSLoss:1.0413 | top1:93.5256 | AUROC:0.9835\n",
      "Test | 161/20 | Loss:1.3820 | MainLoss:1.3646 | SPLoss:0.0703 | CLSLoss:1.0413 | top1:55.2679 | AUROC:0.6398\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.039702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.1919 | MainLoss:0.1483 | Alpha:0.4576 | SPLoss:0.0725 | CLSLoss:1.0436 | top1:94.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1860 | MainLoss:0.1683 | SPLoss:0.0720 | CLSLoss:1.0469 | top1:93.4487 | AUROC:0.9839\n",
      "Test | 161/20 | Loss:1.4311 | MainLoss:1.4135 | SPLoss:0.0720 | CLSLoss:1.0469 | top1:54.9190 | AUROC:0.6270\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.039691\n",
      "Train | 20/20 | Loss:0.1859 | MainLoss:0.1420 | Alpha:0.4596 | SPLoss:0.0727 | CLSLoss:1.0500 | top1:94.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1989 | MainLoss:0.1810 | SPLoss:0.0744 | CLSLoss:1.0528 | top1:92.9615 | AUROC:0.9836\n",
      "Test | 161/20 | Loss:1.4170 | MainLoss:1.3990 | SPLoss:0.0744 | CLSLoss:1.0528 | top1:55.5296 | AUROC:0.6247\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.039680\n",
      "Train | 20/20 | Loss:0.1897 | MainLoss:0.1449 | Alpha:0.4596 | SPLoss:0.0745 | CLSLoss:1.0539 | top1:94.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1871 | MainLoss:0.1690 | SPLoss:0.0759 | CLSLoss:1.0553 | top1:93.4615 | AUROC:0.9838\n",
      "Test | 161/20 | Loss:1.4226 | MainLoss:1.4044 | SPLoss:0.0759 | CLSLoss:1.0553 | top1:55.4019 | AUROC:0.6257\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.039669\n",
      "Train | 20/20 | Loss:0.2002 | MainLoss:0.1561 | Alpha:0.4578 | SPLoss:0.0734 | CLSLoss:1.0557 | top1:94.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1772 | MainLoss:0.1594 | SPLoss:0.0728 | CLSLoss:1.0516 | top1:93.8718 | AUROC:0.9846\n",
      "Test | 161/20 | Loss:1.4277 | MainLoss:1.4099 | SPLoss:0.0728 | CLSLoss:1.0516 | top1:54.5109 | AUROC:0.6237\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.039657\n",
      "Train | 20/20 | Loss:0.1652 | MainLoss:0.1492 | Alpha:0.4666 | SPLoss:0.0118 | CLSLoss:1.0538 | top1:94.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1753 | MainLoss:0.1625 | SPLoss:0.0229 | CLSLoss:1.0535 | top1:93.7179 | AUROC:0.9855\n",
      "Test | 161/20 | Loss:1.4402 | MainLoss:1.4274 | SPLoss:0.0229 | CLSLoss:1.0535 | top1:54.9346 | AUROC:0.6285\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.039646\n",
      "Train | 20/20 | Loss:0.1602 | MainLoss:0.1359 | Alpha:0.4664 | SPLoss:0.0295 | CLSLoss:1.0566 | top1:94.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1733 | MainLoss:0.1591 | SPLoss:0.0363 | CLSLoss:1.0579 | top1:93.8205 | AUROC:0.9865\n",
      "Test | 161/20 | Loss:1.4271 | MainLoss:1.4129 | SPLoss:0.0363 | CLSLoss:1.0579 | top1:55.8162 | AUROC:0.6483\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.039634\n",
      "Train | 20/20 | Loss:0.1563 | MainLoss:0.1260 | Alpha:0.4686 | SPLoss:0.0421 | CLSLoss:1.0606 | top1:95.3947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1752 | MainLoss:0.1597 | SPLoss:0.0490 | CLSLoss:1.0613 | top1:93.7051 | AUROC:0.9872\n",
      "Test | 161/20 | Loss:1.4891 | MainLoss:1.4736 | SPLoss:0.0490 | CLSLoss:1.0613 | top1:55.8505 | AUROC:0.6483\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.039622\n",
      "Train | 20/20 | Loss:0.1529 | MainLoss:0.1190 | Alpha:0.4703 | SPLoss:0.0495 | CLSLoss:1.0654 | top1:95.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1658 | MainLoss:0.1501 | SPLoss:0.0506 | CLSLoss:1.0631 | top1:94.3333 | AUROC:0.9867\n",
      "Test | 161/20 | Loss:1.5678 | MainLoss:1.5521 | SPLoss:0.0506 | CLSLoss:1.0631 | top1:55.2835 | AUROC:0.6507\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.039610\n",
      "Train | 20/20 | Loss:0.1582 | MainLoss:0.1226 | Alpha:0.4677 | SPLoss:0.0534 | CLSLoss:1.0619 | top1:95.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1799 | MainLoss:0.1639 | SPLoss:0.0541 | CLSLoss:1.0625 | top1:93.6795 | AUROC:0.9870\n",
      "Test | 161/20 | Loss:1.4863 | MainLoss:1.4702 | SPLoss:0.0541 | CLSLoss:1.0625 | top1:56.0935 | AUROC:0.6433\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.039597\n",
      "Train | 20/20 | Loss:0.1636 | MainLoss:0.1265 | Alpha:0.4661 | SPLoss:0.0568 | CLSLoss:1.0638 | top1:94.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1757 | MainLoss:0.1591 | SPLoss:0.0595 | CLSLoss:1.0640 | top1:93.8846 | AUROC:0.9862\n",
      "Test | 161/20 | Loss:1.5283 | MainLoss:1.5117 | SPLoss:0.0595 | CLSLoss:1.0640 | top1:55.4424 | AUROC:0.6363\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.039584\n",
      "Train | 20/20 | Loss:0.1679 | MainLoss:0.1290 | Alpha:0.4663 | SPLoss:0.0606 | CLSLoss:1.0614 | top1:95.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1724 | MainLoss:0.1559 | SPLoss:0.0583 | CLSLoss:1.0634 | top1:94.0000 | AUROC:0.9871\n",
      "Test | 161/20 | Loss:1.5520 | MainLoss:1.5355 | SPLoss:0.0583 | CLSLoss:1.0634 | top1:55.7445 | AUROC:0.6526\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.039572\n",
      "Train | 20/20 | Loss:0.1573 | MainLoss:0.1183 | Alpha:0.4683 | SPLoss:0.0606 | CLSLoss:1.0644 | top1:95.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1657 | MainLoss:0.1490 | SPLoss:0.0612 | CLSLoss:1.0606 | top1:94.2051 | AUROC:0.9871\n",
      "Test | 161/20 | Loss:1.5539 | MainLoss:1.5371 | SPLoss:0.0612 | CLSLoss:1.0606 | top1:55.0779 | AUROC:0.6461\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.039559\n",
      "Train | 20/20 | Loss:0.1495 | MainLoss:0.1113 | Alpha:0.4707 | SPLoss:0.0585 | CLSLoss:1.0642 | top1:96.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1751 | MainLoss:0.1588 | SPLoss:0.0563 | CLSLoss:1.0689 | top1:94.0385 | AUROC:0.9865\n",
      "Test | 161/20 | Loss:1.6005 | MainLoss:1.5842 | SPLoss:0.0563 | CLSLoss:1.0689 | top1:55.2025 | AUROC:0.6342\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.039545\n",
      "Train | 20/20 | Loss:0.1529 | MainLoss:0.1149 | Alpha:0.4707 | SPLoss:0.0581 | CLSLoss:1.0655 | top1:95.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1738 | MainLoss:0.1576 | SPLoss:0.0550 | CLSLoss:1.0680 | top1:93.9744 | AUROC:0.9869\n",
      "Test | 161/20 | Loss:1.5839 | MainLoss:1.5677 | SPLoss:0.0550 | CLSLoss:1.0680 | top1:55.3178 | AUROC:0.6348\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.039532\n",
      "Train | 20/20 | Loss:0.1303 | MainLoss:0.1139 | Alpha:0.4768 | SPLoss:0.0122 | CLSLoss:1.0683 | top1:95.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1705 | MainLoss:0.1574 | SPLoss:0.0256 | CLSLoss:1.0630 | top1:94.1282 | AUROC:0.9881\n",
      "Test | 161/20 | Loss:1.5738 | MainLoss:1.5606 | SPLoss:0.0256 | CLSLoss:1.0630 | top1:55.5483 | AUROC:0.6398\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.039518\n",
      "Train | 20/20 | Loss:0.1354 | MainLoss:0.1101 | Alpha:0.4757 | SPLoss:0.0310 | CLSLoss:1.0623 | top1:95.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1586 | MainLoss:0.1444 | SPLoss:0.0354 | CLSLoss:1.0596 | top1:94.5000 | AUROC:0.9880\n",
      "Test | 161/20 | Loss:1.6394 | MainLoss:1.6252 | SPLoss:0.0354 | CLSLoss:1.0596 | top1:55.2991 | AUROC:0.6601\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.039505\n",
      "Train | 20/20 | Loss:0.1237 | MainLoss:0.0951 | Alpha:0.4770 | SPLoss:0.0378 | CLSLoss:1.0628 | top1:96.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1698 | MainLoss:0.1552 | SPLoss:0.0398 | CLSLoss:1.0625 | top1:94.1538 | AUROC:0.9880\n",
      "Test | 161/20 | Loss:1.6763 | MainLoss:1.6617 | SPLoss:0.0398 | CLSLoss:1.0625 | top1:55.7695 | AUROC:0.6538\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.039491\n",
      "Train | 20/20 | Loss:0.1296 | MainLoss:0.0983 | Alpha:0.4773 | SPLoss:0.0432 | CLSLoss:1.0643 | top1:96.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1584 | MainLoss:0.1431 | SPLoss:0.0471 | CLSLoss:1.0585 | top1:94.5769 | AUROC:0.9885\n",
      "Test | 161/20 | Loss:1.7052 | MainLoss:1.6899 | SPLoss:0.0471 | CLSLoss:1.0585 | top1:55.0685 | AUROC:0.6499\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.039476\n",
      "Train | 20/20 | Loss:0.1288 | MainLoss:0.0945 | Alpha:0.4770 | SPLoss:0.0495 | CLSLoss:1.0598 | top1:96.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1688 | MainLoss:0.1531 | SPLoss:0.0512 | CLSLoss:1.0606 | top1:94.3718 | AUROC:0.9881\n",
      "Test | 161/20 | Loss:1.6966 | MainLoss:1.6808 | SPLoss:0.0512 | CLSLoss:1.0606 | top1:55.9533 | AUROC:0.6546\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.039462\n",
      "Train | 20/20 | Loss:0.1336 | MainLoss:0.0982 | Alpha:0.4767 | SPLoss:0.0521 | CLSLoss:1.0583 | top1:96.7105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1656 | MainLoss:0.1500 | SPLoss:0.0507 | CLSLoss:1.0566 | top1:94.4615 | AUROC:0.9886\n",
      "Test | 161/20 | Loss:1.7305 | MainLoss:1.7149 | SPLoss:0.0507 | CLSLoss:1.0566 | top1:55.4455 | AUROC:0.6551\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.039447\n",
      "Train | 20/20 | Loss:0.1376 | MainLoss:0.1029 | Alpha:0.4755 | SPLoss:0.0509 | CLSLoss:1.0561 | top1:96.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1678 | MainLoss:0.1523 | SPLoss:0.0498 | CLSLoss:1.0495 | top1:94.2564 | AUROC:0.9892\n",
      "Test | 161/20 | Loss:1.6697 | MainLoss:1.6542 | SPLoss:0.0498 | CLSLoss:1.0495 | top1:55.1620 | AUROC:0.6396\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.039433\n",
      "Train | 20/20 | Loss:0.1411 | MainLoss:0.1066 | Alpha:0.4747 | SPLoss:0.0506 | CLSLoss:1.0452 | top1:96.3421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1657 | MainLoss:0.1504 | SPLoss:0.0485 | CLSLoss:1.0450 | top1:94.3333 | AUROC:0.9885\n",
      "Test | 161/20 | Loss:1.6726 | MainLoss:1.6573 | SPLoss:0.0485 | CLSLoss:1.0450 | top1:55.3146 | AUROC:0.6453\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.039418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.1325 | MainLoss:0.0982 | Alpha:0.4756 | SPLoss:0.0502 | CLSLoss:1.0445 | top1:96.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1616 | MainLoss:0.1460 | SPLoss:0.0522 | CLSLoss:1.0428 | top1:94.5641 | AUROC:0.9887\n",
      "Test | 161/20 | Loss:1.7129 | MainLoss:1.6972 | SPLoss:0.0522 | CLSLoss:1.0428 | top1:54.8100 | AUROC:0.6372\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.039403\n",
      "Train | 20/20 | Loss:0.1369 | MainLoss:0.1001 | Alpha:0.4764 | SPLoss:0.0554 | CLSLoss:1.0438 | top1:96.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1584 | MainLoss:0.1423 | SPLoss:0.0569 | CLSLoss:1.0386 | top1:94.6923 | AUROC:0.9882\n",
      "Test | 161/20 | Loss:1.7360 | MainLoss:1.7199 | SPLoss:0.0569 | CLSLoss:1.0386 | top1:54.2835 | AUROC:0.6278\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.039387\n",
      "Train | 20/20 | Loss:0.1060 | MainLoss:0.0897 | Alpha:0.4814 | SPLoss:0.0124 | CLSLoss:1.0410 | top1:97.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1524 | MainLoss:0.1395 | SPLoss:0.0238 | CLSLoss:1.0447 | top1:94.9231 | AUROC:0.9896\n",
      "Test | 161/20 | Loss:1.8305 | MainLoss:1.8177 | SPLoss:0.0238 | CLSLoss:1.0447 | top1:54.2928 | AUROC:0.6302\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.039372\n",
      "Train | 20/20 | Loss:0.1173 | MainLoss:0.0924 | Alpha:0.4791 | SPLoss:0.0303 | CLSLoss:1.0407 | top1:96.9474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1649 | MainLoss:0.1512 | SPLoss:0.0331 | CLSLoss:1.0407 | top1:94.5385 | AUROC:0.9896\n",
      "Test | 161/20 | Loss:1.7466 | MainLoss:1.7328 | SPLoss:0.0331 | CLSLoss:1.0407 | top1:55.4361 | AUROC:0.6429\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.039356\n",
      "Train | 20/20 | Loss:0.1156 | MainLoss:0.0859 | Alpha:0.4803 | SPLoss:0.0402 | CLSLoss:1.0414 | top1:97.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1514 | MainLoss:0.1338 | SPLoss:0.0723 | CLSLoss:1.0356 | top1:95.1538 | AUROC:0.9894\n",
      "Test | 161/20 | Loss:1.8944 | MainLoss:1.8768 | SPLoss:0.0723 | CLSLoss:1.0356 | top1:53.9128 | AUROC:0.6275\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.039340\n",
      "Train | 20/20 | Loss:0.1298 | MainLoss:0.0891 | Alpha:0.4807 | SPLoss:0.0631 | CLSLoss:1.0324 | top1:97.1842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1498 | MainLoss:0.1339 | SPLoss:0.0564 | CLSLoss:1.0304 | top1:95.0000 | AUROC:0.9902\n",
      "Test | 161/20 | Loss:1.8127 | MainLoss:1.7968 | SPLoss:0.0564 | CLSLoss:1.0304 | top1:54.5389 | AUROC:0.6415\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.039324\n",
      "Train | 20/20 | Loss:0.1254 | MainLoss:0.0888 | Alpha:0.4801 | SPLoss:0.0549 | CLSLoss:1.0283 | top1:96.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1534 | MainLoss:0.1377 | SPLoss:0.0544 | CLSLoss:1.0270 | top1:94.9231 | AUROC:0.9897\n",
      "Test | 161/20 | Loss:1.7365 | MainLoss:1.7208 | SPLoss:0.0544 | CLSLoss:1.0270 | top1:55.4798 | AUROC:0.6699\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.039308\n",
      "Train | 20/20 | Loss:0.1260 | MainLoss:0.0892 | Alpha:0.4793 | SPLoss:0.0555 | CLSLoss:1.0250 | top1:96.6842 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1689 | MainLoss:0.1533 | SPLoss:0.0527 | CLSLoss:1.0260 | top1:94.4359 | AUROC:0.9896\n",
      "Test | 161/20 | Loss:1.7353 | MainLoss:1.7197 | SPLoss:0.0527 | CLSLoss:1.0260 | top1:55.9439 | AUROC:0.6611\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.039291\n",
      "Train | 20/20 | Loss:0.1294 | MainLoss:0.0941 | Alpha:0.4788 | SPLoss:0.0522 | CLSLoss:1.0244 | top1:96.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1475 | MainLoss:0.1319 | SPLoss:0.0538 | CLSLoss:1.0176 | top1:95.1538 | AUROC:0.9906\n",
      "Test | 161/20 | Loss:1.7248 | MainLoss:1.7093 | SPLoss:0.0538 | CLSLoss:1.0176 | top1:54.7259 | AUROC:0.6462\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.039274\n",
      "Train | 20/20 | Loss:0.1182 | MainLoss:0.0839 | Alpha:0.4797 | SPLoss:0.0504 | CLSLoss:1.0177 | top1:97.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1640 | MainLoss:0.1492 | SPLoss:0.0466 | CLSLoss:1.0182 | top1:94.5641 | AUROC:0.9905\n",
      "Test | 161/20 | Loss:1.7105 | MainLoss:1.6956 | SPLoss:0.0466 | CLSLoss:1.0182 | top1:55.6012 | AUROC:0.6436\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.039258\n",
      "Train | 20/20 | Loss:0.1152 | MainLoss:0.0832 | Alpha:0.4810 | SPLoss:0.0454 | CLSLoss:1.0175 | top1:97.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1580 | MainLoss:0.1433 | SPLoss:0.0449 | CLSLoss:1.0155 | top1:94.8718 | AUROC:0.9897\n",
      "Test | 161/20 | Loss:1.7555 | MainLoss:1.7408 | SPLoss:0.0449 | CLSLoss:1.0155 | top1:55.1713 | AUROC:0.6467\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.039241\n",
      "Train | 20/20 | Loss:0.1217 | MainLoss:0.0889 | Alpha:0.4794 | SPLoss:0.0473 | CLSLoss:1.0125 | top1:96.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1637 | MainLoss:0.1489 | SPLoss:0.0470 | CLSLoss:1.0133 | top1:94.7564 | AUROC:0.9897\n",
      "Test | 161/20 | Loss:1.7394 | MainLoss:1.7246 | SPLoss:0.0470 | CLSLoss:1.0133 | top1:55.5296 | AUROC:0.6491\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.039223\n",
      "Train | 20/20 | Loss:0.1018 | MainLoss:0.0848 | Alpha:0.4838 | SPLoss:0.0142 | CLSLoss:1.0128 | top1:96.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1629 | MainLoss:0.1501 | SPLoss:0.0274 | CLSLoss:1.0094 | top1:94.6667 | AUROC:0.9904\n",
      "Test | 161/20 | Loss:1.6812 | MainLoss:1.6684 | SPLoss:0.0274 | CLSLoss:1.0094 | top1:56.2523 | AUROC:0.6723\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.039206\n",
      "Train | 20/20 | Loss:0.0941 | MainLoss:0.0685 | Alpha:0.4856 | SPLoss:0.0318 | CLSLoss:1.0113 | top1:97.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1596 | MainLoss:0.1462 | SPLoss:0.0331 | CLSLoss:1.0124 | top1:94.9487 | AUROC:0.9900\n",
      "Test | 161/20 | Loss:1.8627 | MainLoss:1.8492 | SPLoss:0.0331 | CLSLoss:1.0124 | top1:55.2212 | AUROC:0.6621\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.039188\n",
      "Train | 20/20 | Loss:0.1092 | MainLoss:0.0807 | Alpha:0.4831 | SPLoss:0.0380 | CLSLoss:1.0102 | top1:97.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1523 | MainLoss:0.1381 | SPLoss:0.0421 | CLSLoss:1.0049 | top1:95.2949 | AUROC:0.9900\n",
      "Test | 161/20 | Loss:1.8491 | MainLoss:1.8349 | SPLoss:0.0421 | CLSLoss:1.0049 | top1:54.7508 | AUROC:0.6636\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.039170\n",
      "Train | 20/20 | Loss:0.1069 | MainLoss:0.0763 | Alpha:0.4838 | SPLoss:0.0426 | CLSLoss:1.0054 | top1:97.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1460 | MainLoss:0.1316 | SPLoss:0.0443 | CLSLoss:0.9998 | top1:95.3462 | AUROC:0.9908\n",
      "Test | 161/20 | Loss:1.8853 | MainLoss:1.8708 | SPLoss:0.0443 | CLSLoss:0.9998 | top1:54.5857 | AUROC:0.6569\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.039152\n",
      "Train | 20/20 | Loss:0.1044 | MainLoss:0.0731 | Alpha:0.4845 | SPLoss:0.0440 | CLSLoss:0.9987 | top1:97.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1624 | MainLoss:0.1483 | SPLoss:0.0414 | CLSLoss:0.9965 | top1:95.0385 | AUROC:0.9905\n",
      "Test | 161/20 | Loss:1.7903 | MainLoss:1.7762 | SPLoss:0.0414 | CLSLoss:0.9965 | top1:55.7913 | AUROC:0.6637\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.039134\n",
      "Train | 20/20 | Loss:0.1062 | MainLoss:0.0763 | Alpha:0.4830 | SPLoss:0.0414 | CLSLoss:0.9947 | top1:97.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1434 | MainLoss:0.1294 | SPLoss:0.0410 | CLSLoss:0.9908 | top1:95.4487 | AUROC:0.9910\n",
      "Test | 161/20 | Loss:1.9464 | MainLoss:1.9324 | SPLoss:0.0410 | CLSLoss:0.9908 | top1:54.2960 | AUROC:0.6521\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.039116\n",
      "Train | 20/20 | Loss:0.1151 | MainLoss:0.0811 | Alpha:0.4838 | SPLoss:0.0498 | CLSLoss:0.9895 | top1:96.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1556 | MainLoss:0.1404 | SPLoss:0.0528 | CLSLoss:0.9876 | top1:95.0128 | AUROC:0.9908\n",
      "Test | 161/20 | Loss:1.8011 | MainLoss:1.7860 | SPLoss:0.0528 | CLSLoss:0.9876 | top1:55.4922 | AUROC:0.6606\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.039097\n",
      "Train | 20/20 | Loss:0.0995 | MainLoss:0.0662 | Alpha:0.4862 | SPLoss:0.0482 | CLSLoss:0.9872 | top1:97.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1620 | MainLoss:0.1477 | SPLoss:0.0442 | CLSLoss:0.9885 | top1:94.9231 | AUROC:0.9907\n",
      "Test | 161/20 | Loss:1.8307 | MainLoss:1.8164 | SPLoss:0.0442 | CLSLoss:0.9885 | top1:55.8567 | AUROC:0.6530\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.039079\n",
      "Train | 20/20 | Loss:0.1057 | MainLoss:0.0741 | Alpha:0.4838 | SPLoss:0.0448 | CLSLoss:0.9856 | top1:97.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1572 | MainLoss:0.1429 | SPLoss:0.0443 | CLSLoss:0.9843 | top1:95.0641 | AUROC:0.9901\n",
      "Test | 161/20 | Loss:1.7908 | MainLoss:1.7766 | SPLoss:0.0443 | CLSLoss:0.9843 | top1:55.8785 | AUROC:0.6593\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.039060\n",
      "Train | 20/20 | Loss:0.1009 | MainLoss:0.0692 | Alpha:0.4852 | SPLoss:0.0450 | CLSLoss:0.9861 | top1:97.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1581 | MainLoss:0.1438 | SPLoss:0.0442 | CLSLoss:0.9874 | top1:95.1026 | AUROC:0.9905\n",
      "Test | 161/20 | Loss:1.8472 | MainLoss:1.8329 | SPLoss:0.0442 | CLSLoss:0.9874 | top1:55.5140 | AUROC:0.6586\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.039040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0884 | MainLoss:0.0728 | Alpha:0.4865 | SPLoss:0.0119 | CLSLoss:0.9863 | top1:97.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1446 | MainLoss:0.1320 | SPLoss:0.0275 | CLSLoss:0.9844 | top1:95.4231 | AUROC:0.9911\n",
      "Test | 161/20 | Loss:1.9612 | MainLoss:1.9486 | SPLoss:0.0275 | CLSLoss:0.9844 | top1:54.5919 | AUROC:0.6520\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.039021\n",
      "Train | 20/20 | Loss:0.0961 | MainLoss:0.0709 | Alpha:0.4858 | SPLoss:0.0316 | CLSLoss:0.9802 | top1:97.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1609 | MainLoss:0.1479 | SPLoss:0.0315 | CLSLoss:0.9807 | top1:95.1026 | AUROC:0.9914\n",
      "Test | 161/20 | Loss:1.8993 | MainLoss:1.8863 | SPLoss:0.0315 | CLSLoss:0.9807 | top1:55.3115 | AUROC:0.6535\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.039002\n",
      "Train | 20/20 | Loss:0.0957 | MainLoss:0.0687 | Alpha:0.4862 | SPLoss:0.0354 | CLSLoss:0.9790 | top1:97.4737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1425 | MainLoss:0.1286 | SPLoss:0.0413 | CLSLoss:0.9775 | top1:95.5769 | AUROC:0.9913\n",
      "Test | 161/20 | Loss:2.0250 | MainLoss:2.0111 | SPLoss:0.0413 | CLSLoss:0.9775 | top1:54.2212 | AUROC:0.6434\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.038982\n",
      "Train | 20/20 | Loss:0.0983 | MainLoss:0.0700 | Alpha:0.4870 | SPLoss:0.0380 | CLSLoss:0.9741 | top1:97.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1522 | MainLoss:0.1388 | SPLoss:0.0369 | CLSLoss:0.9695 | top1:95.2179 | AUROC:0.9913\n",
      "Test | 161/20 | Loss:1.9039 | MainLoss:1.8905 | SPLoss:0.0369 | CLSLoss:0.9695 | top1:54.7414 | AUROC:0.6384\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.038962\n",
      "Train | 20/20 | Loss:0.0992 | MainLoss:0.0711 | Alpha:0.4862 | SPLoss:0.0379 | CLSLoss:0.9657 | top1:97.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1537 | MainLoss:0.1404 | SPLoss:0.0360 | CLSLoss:0.9643 | top1:95.3077 | AUROC:0.9916\n",
      "Test | 161/20 | Loss:1.8762 | MainLoss:1.8630 | SPLoss:0.0360 | CLSLoss:0.9643 | top1:55.4704 | AUROC:0.6484\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.038942\n",
      "Train | 20/20 | Loss:0.1029 | MainLoss:0.0758 | Alpha:0.4844 | SPLoss:0.0361 | CLSLoss:0.9607 | top1:97.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1395 | MainLoss:0.1259 | SPLoss:0.0406 | CLSLoss:0.9549 | top1:95.5769 | AUROC:0.9919\n",
      "Test | 161/20 | Loss:1.9493 | MainLoss:1.9357 | SPLoss:0.0406 | CLSLoss:0.9549 | top1:54.4517 | AUROC:0.6503\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.038922\n",
      "Train | 20/20 | Loss:0.1066 | MainLoss:0.0766 | Alpha:0.4841 | SPLoss:0.0424 | CLSLoss:0.9522 | top1:97.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1535 | MainLoss:0.1398 | SPLoss:0.0422 | CLSLoss:0.9496 | top1:95.1923 | AUROC:0.9913\n",
      "Test | 161/20 | Loss:1.8220 | MainLoss:1.8083 | SPLoss:0.0422 | CLSLoss:0.9496 | top1:55.4798 | AUROC:0.6575\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.038901\n",
      "Train | 20/20 | Loss:0.1027 | MainLoss:0.0720 | Alpha:0.4855 | SPLoss:0.0436 | CLSLoss:0.9494 | top1:97.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1500 | MainLoss:0.1363 | SPLoss:0.0423 | CLSLoss:0.9479 | top1:95.2949 | AUROC:0.9910\n",
      "Test | 161/20 | Loss:1.9565 | MainLoss:1.9428 | SPLoss:0.0423 | CLSLoss:0.9479 | top1:54.5919 | AUROC:0.6481\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.038881\n",
      "Train | 20/20 | Loss:0.1073 | MainLoss:0.0766 | Alpha:0.4835 | SPLoss:0.0439 | CLSLoss:0.9452 | top1:97.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1523 | MainLoss:0.1385 | SPLoss:0.0440 | CLSLoss:0.9406 | top1:95.3077 | AUROC:0.9911\n",
      "Test | 161/20 | Loss:1.9010 | MainLoss:1.8871 | SPLoss:0.0440 | CLSLoss:0.9406 | top1:54.8193 | AUROC:0.6485\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.038860\n",
      "Train | 20/20 | Loss:0.1005 | MainLoss:0.0691 | Alpha:0.4844 | SPLoss:0.0453 | CLSLoss:0.9403 | top1:97.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1574 | MainLoss:0.1429 | SPLoss:0.0507 | CLSLoss:0.9410 | top1:95.2436 | AUROC:0.9908\n",
      "Test | 161/20 | Loss:1.9759 | MainLoss:1.9614 | SPLoss:0.0507 | CLSLoss:0.9410 | top1:54.3583 | AUROC:0.6357\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.038839\n",
      "Train | 20/20 | Loss:0.0778 | MainLoss:0.0627 | Alpha:0.4879 | SPLoss:0.0118 | CLSLoss:0.9395 | top1:97.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1603 | MainLoss:0.1488 | SPLoss:0.0218 | CLSLoss:0.9366 | top1:94.8846 | AUROC:0.9912\n",
      "Test | 161/20 | Loss:1.9485 | MainLoss:1.9369 | SPLoss:0.0218 | CLSLoss:0.9366 | top1:54.7726 | AUROC:0.6336\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.038818\n",
      "Train | 20/20 | Loss:0.0798 | MainLoss:0.0576 | Alpha:0.4884 | SPLoss:0.0264 | CLSLoss:0.9364 | top1:98.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1435 | MainLoss:0.1315 | SPLoss:0.0272 | CLSLoss:0.9359 | top1:95.4615 | AUROC:0.9918\n",
      "Test | 161/20 | Loss:2.0866 | MainLoss:2.0745 | SPLoss:0.0272 | CLSLoss:0.9359 | top1:54.0717 | AUROC:0.6379\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.038796\n",
      "Train | 20/20 | Loss:0.0807 | MainLoss:0.0563 | Alpha:0.4882 | SPLoss:0.0308 | CLSLoss:0.9346 | top1:97.8947 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1541 | MainLoss:0.1415 | SPLoss:0.0316 | CLSLoss:0.9358 | top1:95.3974 | AUROC:0.9921\n",
      "Test | 161/20 | Loss:2.0934 | MainLoss:2.0809 | SPLoss:0.0316 | CLSLoss:0.9358 | top1:53.8629 | AUROC:0.6087\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.038775\n",
      "Train | 20/20 | Loss:0.0851 | MainLoss:0.0589 | Alpha:0.4882 | SPLoss:0.0346 | CLSLoss:0.9336 | top1:98.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1585 | MainLoss:0.1457 | SPLoss:0.0345 | CLSLoss:0.9303 | top1:95.2692 | AUROC:0.9921\n",
      "Test | 161/20 | Loss:1.9874 | MainLoss:1.9746 | SPLoss:0.0345 | CLSLoss:0.9303 | top1:54.4143 | AUROC:0.6316\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.038753\n",
      "Train | 20/20 | Loss:0.1018 | MainLoss:0.0739 | Alpha:0.4857 | SPLoss:0.0383 | CLSLoss:0.9249 | top1:97.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1488 | MainLoss:0.1355 | SPLoss:0.0410 | CLSLoss:0.9227 | top1:95.3974 | AUROC:0.9916\n",
      "Test | 161/20 | Loss:2.0018 | MainLoss:1.9885 | SPLoss:0.0410 | CLSLoss:0.9226 | top1:54.0717 | AUROC:0.6358\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.038731\n",
      "Train | 20/20 | Loss:0.0977 | MainLoss:0.0677 | Alpha:0.4859 | SPLoss:0.0430 | CLSLoss:0.9203 | top1:97.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1673 | MainLoss:0.1537 | SPLoss:0.0441 | CLSLoss:0.9184 | top1:94.8974 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.8594 | MainLoss:1.8459 | SPLoss:0.0441 | CLSLoss:0.9184 | top1:55.2087 | AUROC:0.6395\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.038709\n",
      "Train | 20/20 | Loss:0.0922 | MainLoss:0.0631 | Alpha:0.4867 | SPLoss:0.0411 | CLSLoss:0.9158 | top1:97.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1452 | MainLoss:0.1323 | SPLoss:0.0378 | CLSLoss:0.9159 | top1:95.5256 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:2.0133 | MainLoss:2.0004 | SPLoss:0.0378 | CLSLoss:0.9159 | top1:54.3271 | AUROC:0.6408\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.038687\n",
      "Train | 20/20 | Loss:0.0849 | MainLoss:0.0557 | Alpha:0.4871 | SPLoss:0.0411 | CLSLoss:0.9155 | top1:97.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1590 | MainLoss:0.1458 | SPLoss:0.0399 | CLSLoss:0.9155 | top1:95.1026 | AUROC:0.9916\n",
      "Test | 161/20 | Loss:2.0128 | MainLoss:1.9997 | SPLoss:0.0399 | CLSLoss:0.9155 | top1:54.6885 | AUROC:0.6436\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.038664\n",
      "Train | 20/20 | Loss:0.0869 | MainLoss:0.0586 | Alpha:0.4883 | SPLoss:0.0392 | CLSLoss:0.9141 | top1:98.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1565 | MainLoss:0.1438 | SPLoss:0.0357 | CLSLoss:0.9132 | top1:95.2308 | AUROC:0.9919\n",
      "Test | 161/20 | Loss:2.0259 | MainLoss:2.0132 | SPLoss:0.0357 | CLSLoss:0.9132 | top1:54.6075 | AUROC:0.6394\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.038641\n",
      "Train | 20/20 | Loss:0.0976 | MainLoss:0.0687 | Alpha:0.4857 | SPLoss:0.0408 | CLSLoss:0.9083 | top1:97.5526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1434 | MainLoss:0.1304 | SPLoss:0.0396 | CLSLoss:0.9064 | top1:95.5385 | AUROC:0.9917\n",
      "Test | 161/20 | Loss:2.0389 | MainLoss:2.0259 | SPLoss:0.0396 | CLSLoss:0.9064 | top1:54.1090 | AUROC:0.6391\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.038619\n",
      "Train | 20/20 | Loss:0.0820 | MainLoss:0.0659 | Alpha:0.4884 | SPLoss:0.0144 | CLSLoss:0.9035 | top1:97.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1465 | MainLoss:0.1347 | SPLoss:0.0279 | CLSLoss:0.9020 | top1:95.4231 | AUROC:0.9920\n",
      "Test | 161/20 | Loss:1.9353 | MainLoss:1.9235 | SPLoss:0.0279 | CLSLoss:0.9020 | top1:54.9346 | AUROC:0.6596\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.038596\n",
      "Train | 20/20 | Loss:0.0897 | MainLoss:0.0662 | Alpha:0.4866 | SPLoss:0.0298 | CLSLoss:0.8994 | top1:97.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1333 | MainLoss:0.1211 | SPLoss:0.0325 | CLSLoss:0.8975 | top1:95.7821 | AUROC:0.9921\n",
      "Test | 161/20 | Loss:2.0985 | MainLoss:2.0862 | SPLoss:0.0325 | CLSLoss:0.8975 | top1:53.4766 | AUROC:0.6374\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.038572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0771 | MainLoss:0.0522 | Alpha:0.4897 | SPLoss:0.0324 | CLSLoss:0.8983 | top1:98.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1433 | MainLoss:0.1314 | SPLoss:0.0297 | CLSLoss:0.9002 | top1:95.6026 | AUROC:0.9921\n",
      "Test | 161/20 | Loss:2.1361 | MainLoss:2.1241 | SPLoss:0.0297 | CLSLoss:0.9002 | top1:54.1028 | AUROC:0.6441\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.038549\n",
      "Train | 20/20 | Loss:0.0690 | MainLoss:0.0428 | Alpha:0.4921 | SPLoss:0.0350 | CLSLoss:0.9011 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1543 | MainLoss:0.1414 | SPLoss:0.0381 | CLSLoss:0.9055 | top1:95.3718 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:2.0211 | MainLoss:2.0082 | SPLoss:0.0381 | CLSLoss:0.9055 | top1:55.5826 | AUROC:0.6575\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.038525\n",
      "Train | 20/20 | Loss:0.0739 | MainLoss:0.0475 | Alpha:0.4906 | SPLoss:0.0355 | CLSLoss:0.9050 | top1:98.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1593 | MainLoss:0.1468 | SPLoss:0.0344 | CLSLoss:0.9045 | top1:95.2821 | AUROC:0.9924\n",
      "Test | 161/20 | Loss:2.0565 | MainLoss:2.0441 | SPLoss:0.0344 | CLSLoss:0.9045 | top1:55.0717 | AUROC:0.6538\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.038502\n",
      "Train | 20/20 | Loss:0.1000 | MainLoss:0.0721 | Alpha:0.4849 | SPLoss:0.0390 | CLSLoss:0.8981 | top1:97.6579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1358 | MainLoss:0.1229 | SPLoss:0.0397 | CLSLoss:0.8869 | top1:95.6410 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:2.0316 | MainLoss:2.0188 | SPLoss:0.0397 | CLSLoss:0.8869 | top1:53.7445 | AUROC:0.6453\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.038478\n",
      "Train | 20/20 | Loss:0.0878 | MainLoss:0.0606 | Alpha:0.4892 | SPLoss:0.0374 | CLSLoss:0.8858 | top1:98.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1381 | MainLoss:0.1257 | SPLoss:0.0357 | CLSLoss:0.8840 | top1:95.6795 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:2.0193 | MainLoss:2.0069 | SPLoss:0.0357 | CLSLoss:0.8840 | top1:54.1277 | AUROC:0.6413\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.038453\n",
      "Train | 20/20 | Loss:0.0814 | MainLoss:0.0547 | Alpha:0.4884 | SPLoss:0.0365 | CLSLoss:0.8827 | top1:98.0789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1382 | MainLoss:0.1260 | SPLoss:0.0337 | CLSLoss:0.8821 | top1:96.0128 | AUROC:0.9924\n",
      "Test | 161/20 | Loss:2.0856 | MainLoss:2.0734 | SPLoss:0.0337 | CLSLoss:0.8821 | top1:53.8505 | AUROC:0.6381\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.038429\n",
      "Train | 20/20 | Loss:0.0810 | MainLoss:0.0562 | Alpha:0.4891 | SPLoss:0.0326 | CLSLoss:0.8818 | top1:98.1053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1443 | MainLoss:0.1321 | SPLoss:0.0342 | CLSLoss:0.8796 | top1:95.6154 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:2.0118 | MainLoss:1.9996 | SPLoss:0.0342 | CLSLoss:0.8796 | top1:54.8474 | AUROC:0.6608\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.038405\n",
      "Train | 20/20 | Loss:0.0732 | MainLoss:0.0479 | Alpha:0.4894 | SPLoss:0.0337 | CLSLoss:0.8793 | top1:98.2368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1439 | MainLoss:0.1320 | SPLoss:0.0299 | CLSLoss:0.8821 | top1:95.6795 | AUROC:0.9923\n",
      "Test | 161/20 | Loss:2.1134 | MainLoss:2.1016 | SPLoss:0.0299 | CLSLoss:0.8821 | top1:54.2555 | AUROC:0.6522\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.038380\n",
      "Train | 20/20 | Loss:0.0708 | MainLoss:0.0552 | Alpha:0.4901 | SPLoss:0.0139 | CLSLoss:0.8807 | top1:98.0526 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1425 | MainLoss:0.1315 | SPLoss:0.0220 | CLSLoss:0.8774 | top1:95.5000 | AUROC:0.9921\n",
      "Test | 161/20 | Loss:2.0838 | MainLoss:2.0728 | SPLoss:0.0220 | CLSLoss:0.8774 | top1:54.2804 | AUROC:0.6510\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.038355\n",
      "Train | 20/20 | Loss:0.0741 | MainLoss:0.0530 | Alpha:0.4891 | SPLoss:0.0252 | CLSLoss:0.8760 | top1:98.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1677 | MainLoss:0.1556 | SPLoss:0.0330 | CLSLoss:0.8760 | top1:94.9231 | AUROC:0.9922\n",
      "Test | 161/20 | Loss:1.9769 | MainLoss:1.9649 | SPLoss:0.0330 | CLSLoss:0.8760 | top1:55.7944 | AUROC:0.6645\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.038330\n",
      "Train | 20/20 | Loss:0.0677 | MainLoss:0.0434 | Alpha:0.4925 | SPLoss:0.0317 | CLSLoss:0.8741 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1567 | MainLoss:0.1448 | SPLoss:0.0314 | CLSLoss:0.8755 | top1:95.3718 | AUROC:0.9925\n",
      "Test | 161/20 | Loss:2.1114 | MainLoss:2.0996 | SPLoss:0.0314 | CLSLoss:0.8755 | top1:54.6978 | AUROC:0.6504\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.038305\n",
      "Train | 20/20 | Loss:0.0686 | MainLoss:0.0447 | Alpha:0.4922 | SPLoss:0.0309 | CLSLoss:0.8746 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1415 | MainLoss:0.1296 | SPLoss:0.0312 | CLSLoss:0.8725 | top1:95.7564 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:2.1932 | MainLoss:2.1814 | SPLoss:0.0312 | CLSLoss:0.8725 | top1:53.8474 | AUROC:0.6351\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.038279\n",
      "Train | 20/20 | Loss:0.0831 | MainLoss:0.0586 | Alpha:0.4875 | SPLoss:0.0325 | CLSLoss:0.8655 | top1:97.9211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1371 | MainLoss:0.1252 | SPLoss:0.0324 | CLSLoss:0.8642 | top1:95.8333 | AUROC:0.9930\n",
      "Test | 161/20 | Loss:2.1806 | MainLoss:2.1687 | SPLoss:0.0324 | CLSLoss:0.8642 | top1:53.9470 | AUROC:0.6542\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.038254\n",
      "Train | 20/20 | Loss:0.0786 | MainLoss:0.0540 | Alpha:0.4896 | SPLoss:0.0326 | CLSLoss:0.8587 | top1:98.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1438 | MainLoss:0.1321 | SPLoss:0.0311 | CLSLoss:0.8586 | top1:95.6923 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:2.1133 | MainLoss:2.1016 | SPLoss:0.0311 | CLSLoss:0.8586 | top1:54.4206 | AUROC:0.6524\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.038228\n",
      "Train | 20/20 | Loss:0.0776 | MainLoss:0.0531 | Alpha:0.4893 | SPLoss:0.0326 | CLSLoss:0.8574 | top1:98.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1386 | MainLoss:0.1265 | SPLoss:0.0363 | CLSLoss:0.8518 | top1:95.7179 | AUROC:0.9931\n",
      "Test | 161/20 | Loss:2.1103 | MainLoss:2.0982 | SPLoss:0.0363 | CLSLoss:0.8518 | top1:53.7882 | AUROC:0.6415\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.038202\n",
      "Train | 20/20 | Loss:0.0770 | MainLoss:0.0510 | Alpha:0.4914 | SPLoss:0.0356 | CLSLoss:0.8504 | top1:98.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1600 | MainLoss:0.1478 | SPLoss:0.0370 | CLSLoss:0.8499 | top1:95.1410 | AUROC:0.9924\n",
      "Test | 161/20 | Loss:1.9906 | MainLoss:1.9784 | SPLoss:0.0370 | CLSLoss:0.8499 | top1:55.5545 | AUROC:0.6625\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.038176\n",
      "Train | 20/20 | Loss:0.0783 | MainLoss:0.0521 | Alpha:0.4892 | SPLoss:0.0362 | CLSLoss:0.8498 | top1:98.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1333 | MainLoss:0.1213 | SPLoss:0.0361 | CLSLoss:0.8454 | top1:95.8462 | AUROC:0.9929\n",
      "Test | 161/20 | Loss:2.2089 | MainLoss:2.1969 | SPLoss:0.0361 | CLSLoss:0.8454 | top1:53.5452 | AUROC:0.6427\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.038150\n",
      "Train | 20/20 | Loss:0.0713 | MainLoss:0.0456 | Alpha:0.4915 | SPLoss:0.0352 | CLSLoss:0.8455 | top1:98.5263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1276 | MainLoss:0.1156 | SPLoss:0.0355 | CLSLoss:0.8434 | top1:96.0000 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:2.3014 | MainLoss:2.2895 | SPLoss:0.0355 | CLSLoss:0.8434 | top1:52.8910 | AUROC:0.6378\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.038123\n",
      "Train | 20/20 | Loss:0.0690 | MainLoss:0.0542 | Alpha:0.4894 | SPLoss:0.0129 | CLSLoss:0.8405 | top1:98.1316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1275 | MainLoss:0.1167 | SPLoss:0.0238 | CLSLoss:0.8395 | top1:96.0641 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:2.1468 | MainLoss:2.1360 | SPLoss:0.0238 | CLSLoss:0.8395 | top1:53.9688 | AUROC:0.6567\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.038097\n",
      "Train | 20/20 | Loss:0.0712 | MainLoss:0.0498 | Alpha:0.4896 | SPLoss:0.0266 | CLSLoss:0.8383 | top1:98.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1317 | MainLoss:0.1207 | SPLoss:0.0271 | CLSLoss:0.8363 | top1:95.9487 | AUROC:0.9930\n",
      "Test | 161/20 | Loss:2.2012 | MainLoss:2.1902 | SPLoss:0.0271 | CLSLoss:0.8363 | top1:53.3894 | AUROC:0.6422\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.038070\n",
      "Train | 20/20 | Loss:0.0737 | MainLoss:0.0504 | Alpha:0.4896 | SPLoss:0.0305 | CLSLoss:0.8350 | top1:98.2895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1327 | MainLoss:0.1213 | SPLoss:0.0307 | CLSLoss:0.8345 | top1:96.0385 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:2.1952 | MainLoss:2.1838 | SPLoss:0.0307 | CLSLoss:0.8345 | top1:53.8349 | AUROC:0.6650\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.038043\n",
      "Train | 20/20 | Loss:0.0848 | MainLoss:0.0594 | Alpha:0.4879 | SPLoss:0.0351 | CLSLoss:0.8314 | top1:98.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1310 | MainLoss:0.1190 | SPLoss:0.0373 | CLSLoss:0.8251 | top1:96.0385 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:2.0825 | MainLoss:2.0705 | SPLoss:0.0373 | CLSLoss:0.8251 | top1:53.6760 | AUROC:0.6405\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.038015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0657 | MainLoss:0.0402 | Alpha:0.4913 | SPLoss:0.0351 | CLSLoss:0.8267 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1413 | MainLoss:0.1294 | SPLoss:0.0358 | CLSLoss:0.8268 | top1:95.8205 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:2.1678 | MainLoss:2.1560 | SPLoss:0.0358 | CLSLoss:0.8268 | top1:53.8193 | AUROC:0.6293\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.037988\n",
      "Train | 20/20 | Loss:0.0695 | MainLoss:0.0448 | Alpha:0.4910 | SPLoss:0.0337 | CLSLoss:0.8258 | top1:98.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1345 | MainLoss:0.1224 | SPLoss:0.0380 | CLSLoss:0.8230 | top1:95.9359 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:2.1789 | MainLoss:2.1669 | SPLoss:0.0380 | CLSLoss:0.8230 | top1:53.5296 | AUROC:0.6331\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.037961\n",
      "Train | 20/20 | Loss:0.0696 | MainLoss:0.0426 | Alpha:0.4907 | SPLoss:0.0384 | CLSLoss:0.8243 | top1:98.5789 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1300 | MainLoss:0.1181 | SPLoss:0.0371 | CLSLoss:0.8245 | top1:96.0769 | AUROC:0.9933\n",
      "Test | 161/20 | Loss:2.2382 | MainLoss:2.2263 | SPLoss:0.0371 | CLSLoss:0.8245 | top1:53.4081 | AUROC:0.6466\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.037933\n",
      "Train | 20/20 | Loss:0.0695 | MainLoss:0.0443 | Alpha:0.4905 | SPLoss:0.0346 | CLSLoss:0.8232 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1327 | MainLoss:0.1213 | SPLoss:0.0321 | CLSLoss:0.8221 | top1:95.9359 | AUROC:0.9932\n",
      "Test | 161/20 | Loss:2.1913 | MainLoss:2.1799 | SPLoss:0.0321 | CLSLoss:0.8221 | top1:54.0156 | AUROC:0.6609\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.037905\n",
      "Train | 20/20 | Loss:0.0638 | MainLoss:0.0407 | Alpha:0.4924 | SPLoss:0.0302 | CLSLoss:0.8214 | top1:98.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1361 | MainLoss:0.1252 | SPLoss:0.0271 | CLSLoss:0.8207 | top1:95.8462 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:2.2379 | MainLoss:2.2269 | SPLoss:0.0271 | CLSLoss:0.8207 | top1:53.5358 | AUROC:0.6387\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.037877\n",
      "Train | 20/20 | Loss:0.0620 | MainLoss:0.0403 | Alpha:0.4922 | SPLoss:0.0274 | CLSLoss:0.8201 | top1:98.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1395 | MainLoss:0.1284 | SPLoss:0.0286 | CLSLoss:0.8192 | top1:95.7564 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:2.2176 | MainLoss:2.2066 | SPLoss:0.0286 | CLSLoss:0.8192 | top1:53.6573 | AUROC:0.6350\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.037849\n",
      "Train | 20/20 | Loss:0.0633 | MainLoss:0.0496 | Alpha:0.4919 | SPLoss:0.0112 | CLSLoss:0.8177 | top1:98.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1422 | MainLoss:0.1321 | SPLoss:0.0203 | CLSLoss:0.8155 | top1:95.5897 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:2.1783 | MainLoss:2.1681 | SPLoss:0.0203 | CLSLoss:0.8155 | top1:54.0935 | AUROC:0.6420\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.037820\n",
      "Train | 20/20 | Loss:0.0745 | MainLoss:0.0537 | Alpha:0.4902 | SPLoss:0.0259 | CLSLoss:0.8119 | top1:97.9737 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1361 | MainLoss:0.1252 | SPLoss:0.0278 | CLSLoss:0.8096 | top1:95.9231 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:2.1870 | MainLoss:2.1762 | SPLoss:0.0278 | CLSLoss:0.8096 | top1:53.6262 | AUROC:0.6416\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.037792\n",
      "Train | 20/20 | Loss:0.0665 | MainLoss:0.0440 | Alpha:0.4923 | SPLoss:0.0292 | CLSLoss:0.8096 | top1:98.2105 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1296 | MainLoss:0.1183 | SPLoss:0.0321 | CLSLoss:0.8077 | top1:96.0641 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:2.1996 | MainLoss:2.1883 | SPLoss:0.0321 | CLSLoss:0.8077 | top1:53.4704 | AUROC:0.6420\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.037763\n",
      "Train | 20/20 | Loss:0.0686 | MainLoss:0.0447 | Alpha:0.4919 | SPLoss:0.0322 | CLSLoss:0.8059 | top1:98.7632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1246 | MainLoss:0.1134 | SPLoss:0.0312 | CLSLoss:0.8054 | top1:96.2179 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:2.3311 | MainLoss:2.3199 | SPLoss:0.0312 | CLSLoss:0.8054 | top1:52.6636 | AUROC:0.6161\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.037734\n",
      "Train | 20/20 | Loss:0.0677 | MainLoss:0.0446 | Alpha:0.4924 | SPLoss:0.0306 | CLSLoss:0.8046 | top1:98.6316 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1393 | MainLoss:0.1280 | SPLoss:0.0329 | CLSLoss:0.8025 | top1:95.6795 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:2.1737 | MainLoss:2.1624 | SPLoss:0.0329 | CLSLoss:0.8025 | top1:53.8785 | AUROC:0.6364\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.037705\n",
      "Train | 20/20 | Loss:0.0674 | MainLoss:0.0421 | Alpha:0.4919 | SPLoss:0.0351 | CLSLoss:0.8015 | top1:98.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1419 | MainLoss:0.1302 | SPLoss:0.0375 | CLSLoss:0.8017 | top1:95.7821 | AUROC:0.9937\n",
      "Test | 161/20 | Loss:2.1123 | MainLoss:2.1006 | SPLoss:0.0375 | CLSLoss:0.8017 | top1:53.7570 | AUROC:0.6421\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.037675\n",
      "Train | 20/20 | Loss:0.0755 | MainLoss:0.0496 | Alpha:0.4927 | SPLoss:0.0365 | CLSLoss:0.7996 | top1:98.3158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1348 | MainLoss:0.1234 | SPLoss:0.0342 | CLSLoss:0.7969 | top1:95.8205 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:2.1941 | MainLoss:2.1827 | SPLoss:0.0342 | CLSLoss:0.7969 | top1:53.7664 | AUROC:0.6429\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.037646\n",
      "Train | 20/20 | Loss:0.0733 | MainLoss:0.0484 | Alpha:0.4902 | SPLoss:0.0346 | CLSLoss:0.7954 | top1:98.2632 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1402 | MainLoss:0.1290 | SPLoss:0.0328 | CLSLoss:0.7927 | top1:95.4744 | AUROC:0.9935\n",
      "Test | 161/20 | Loss:2.1221 | MainLoss:2.1109 | SPLoss:0.0328 | CLSLoss:0.7927 | top1:54.2368 | AUROC:0.6417\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.037616\n",
      "Train | 20/20 | Loss:0.0800 | MainLoss:0.0555 | Alpha:0.4881 | SPLoss:0.0339 | CLSLoss:0.7896 | top1:98.1579 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1163 | MainLoss:0.1044 | SPLoss:0.0405 | CLSLoss:0.7856 | top1:96.3205 | AUROC:0.9942\n",
      "Test | 161/20 | Loss:2.2663 | MainLoss:2.2544 | SPLoss:0.0405 | CLSLoss:0.7856 | top1:52.8692 | AUROC:0.6410\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.037586\n",
      "Train | 20/20 | Loss:0.0723 | MainLoss:0.0463 | Alpha:0.4903 | SPLoss:0.0369 | CLSLoss:0.7854 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1373 | MainLoss:0.1258 | SPLoss:0.0367 | CLSLoss:0.7853 | top1:95.8205 | AUROC:0.9936\n",
      "Test | 161/20 | Loss:2.2133 | MainLoss:2.2018 | SPLoss:0.0367 | CLSLoss:0.7853 | top1:53.5047 | AUROC:0.6302\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.037556\n",
      "Train | 20/20 | Loss:0.0578 | MainLoss:0.0460 | Alpha:0.4893 | SPLoss:0.0079 | CLSLoss:0.7843 | top1:98.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1283 | MainLoss:0.1191 | SPLoss:0.0141 | CLSLoss:0.7824 | top1:96.0000 | AUROC:0.9941\n",
      "Test | 161/20 | Loss:2.2617 | MainLoss:2.2524 | SPLoss:0.0141 | CLSLoss:0.7824 | top1:53.2648 | AUROC:0.6297\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.037526\n",
      "Train | 20/20 | Loss:0.0617 | MainLoss:0.0445 | Alpha:0.4916 | SPLoss:0.0191 | CLSLoss:0.7815 | top1:98.3684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1186 | MainLoss:0.1082 | SPLoss:0.0259 | CLSLoss:0.7785 | top1:96.4231 | AUROC:0.9943\n",
      "Test | 161/20 | Loss:2.2610 | MainLoss:2.2507 | SPLoss:0.0259 | CLSLoss:0.7785 | top1:52.9502 | AUROC:0.6288\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.037496\n",
      "Train | 20/20 | Loss:0.0635 | MainLoss:0.0423 | Alpha:0.4920 | SPLoss:0.0272 | CLSLoss:0.7778 | top1:98.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1225 | MainLoss:0.1121 | SPLoss:0.0259 | CLSLoss:0.7782 | top1:96.3590 | AUROC:0.9941\n",
      "Test | 161/20 | Loss:2.2471 | MainLoss:2.2367 | SPLoss:0.0259 | CLSLoss:0.7782 | top1:53.3583 | AUROC:0.6397\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.037465\n",
      "Train | 20/20 | Loss:0.0579 | MainLoss:0.0370 | Alpha:0.4927 | SPLoss:0.0266 | CLSLoss:0.7786 | top1:98.8158 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1240 | MainLoss:0.1135 | SPLoss:0.0268 | CLSLoss:0.7779 | top1:96.2564 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:2.3405 | MainLoss:2.3300 | SPLoss:0.0268 | CLSLoss:0.7779 | top1:52.8972 | AUROC:0.6219\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.037435\n",
      "Train | 20/20 | Loss:0.0731 | MainLoss:0.0502 | Alpha:0.4905 | SPLoss:0.0311 | CLSLoss:0.7724 | top1:98.5000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1363 | MainLoss:0.1258 | SPLoss:0.0277 | CLSLoss:0.7715 | top1:95.9231 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:2.1753 | MainLoss:2.1648 | SPLoss:0.0277 | CLSLoss:0.7715 | top1:54.0685 | AUROC:0.6462\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.037404\n",
      "Train | 20/20 | Loss:0.0587 | MainLoss:0.0368 | Alpha:0.4934 | SPLoss:0.0286 | CLSLoss:0.7712 | top1:98.8421 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1251 | MainLoss:0.1148 | SPLoss:0.0263 | CLSLoss:0.7708 | top1:96.2179 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:2.3484 | MainLoss:2.3380 | SPLoss:0.0263 | CLSLoss:0.7708 | top1:52.8162 | AUROC:0.6210\n",
      "\n",
      "Epoch: [177 | 1000] LR: 0.037373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 20/20 | Loss:0.0532 | MainLoss:0.0333 | Alpha:0.4927 | SPLoss:0.0247 | CLSLoss:0.7703 | top1:99.0263 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1366 | MainLoss:0.1264 | SPLoss:0.0240 | CLSLoss:0.7709 | top1:95.8846 | AUROC:0.9937\n",
      "Test | 161/20 | Loss:2.2754 | MainLoss:2.2653 | SPLoss:0.0240 | CLSLoss:0.7709 | top1:53.6012 | AUROC:0.6368\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.037341\n",
      "Train | 20/20 | Loss:0.0714 | MainLoss:0.0498 | Alpha:0.4907 | SPLoss:0.0284 | CLSLoss:0.7680 | top1:98.0000 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1209 | MainLoss:0.1096 | SPLoss:0.0369 | CLSLoss:0.7628 | top1:96.1923 | AUROC:0.9939\n",
      "Test | 161/20 | Loss:2.3151 | MainLoss:2.3038 | SPLoss:0.0369 | CLSLoss:0.7628 | top1:52.9470 | AUROC:0.6381\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.037310\n",
      "Train | 20/20 | Loss:0.0692 | MainLoss:0.0456 | Alpha:0.4924 | SPLoss:0.0323 | CLSLoss:0.7646 | top1:98.4211 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1255 | MainLoss:0.1142 | SPLoss:0.0374 | CLSLoss:0.7586 | top1:96.2051 | AUROC:0.9934\n",
      "Test | 161/20 | Loss:2.1846 | MainLoss:2.1733 | SPLoss:0.0374 | CLSLoss:0.7586 | top1:53.6293 | AUROC:0.6528\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.037278\n",
      "Train | 20/20 | Loss:0.0684 | MainLoss:0.0442 | Alpha:0.4912 | SPLoss:0.0337 | CLSLoss:0.7599 | top1:98.6053 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1207 | MainLoss:0.1096 | SPLoss:0.0358 | CLSLoss:0.7578 | top1:96.3974 | AUROC:0.9937\n",
      "Test | 161/20 | Loss:2.2299 | MainLoss:2.2188 | SPLoss:0.0358 | CLSLoss:0.7578 | top1:53.4081 | AUROC:0.6543\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.037247\n",
      "Train | 20/20 | Loss:0.0490 | MainLoss:0.0378 | Alpha:0.4929 | SPLoss:0.0074 | CLSLoss:0.7593 | top1:98.8684 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1498 | MainLoss:0.1406 | SPLoss:0.0159 | CLSLoss:0.7593 | top1:95.5641 | AUROC:0.9937\n",
      "Test | 161/20 | Loss:2.1439 | MainLoss:2.1348 | SPLoss:0.0159 | CLSLoss:0.7593 | top1:54.5452 | AUROC:0.6476\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.037215\n",
      "Train | 20/20 | Loss:0.0587 | MainLoss:0.0414 | Alpha:0.4924 | SPLoss:0.0197 | CLSLoss:0.7560 | top1:98.7368 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1319 | MainLoss:0.1223 | SPLoss:0.0203 | CLSLoss:0.7572 | top1:96.1923 | AUROC:0.9938\n",
      "Test | 161/20 | Loss:2.2823 | MainLoss:2.2727 | SPLoss:0.0203 | CLSLoss:0.7572 | top1:53.5327 | AUROC:0.6404\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.037183\n",
      "Train | 20/20 | Loss:0.0553 | MainLoss:0.0374 | Alpha:0.4929 | SPLoss:0.0210 | CLSLoss:0.7554 | top1:98.7895 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1292 | MainLoss:0.1195 | SPLoss:0.0212 | CLSLoss:0.7558 | top1:96.1667 | AUROC:0.9942\n",
      "Test | 161/20 | Loss:2.2826 | MainLoss:2.2730 | SPLoss:0.0212 | CLSLoss:0.7558 | top1:53.6480 | AUROC:0.6364\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.037151\n",
      "Train | 20/20 | Loss:0.0684 | MainLoss:0.0469 | Alpha:0.4912 | SPLoss:0.0284 | CLSLoss:0.7537 | top1:98.4474 | AUROC:0.0000\n",
      "Test | 39/20 | Loss:0.1355 | MainLoss:0.1247 | SPLoss:0.0334 | CLSLoss:0.7521 | top1:95.9231 | AUROC:0.9940\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a69c5a1ee38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_target_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msource_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_auroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_source_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-887471094425>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(val_loader, model, criterion, epoch, use_cuda)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mloss_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_ratio\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depthwise_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gd/EfficientNet/model_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mweight_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mweight_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "\n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
