{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 1: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/media/data2/dataset/GAN_ImageData/StarGAN_128/'\n",
    "target_dir = '/media/data2/dataset/GAN_ImageData/PGGAN_128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = './log/star/128/b0/aug/checkpoint.pth.tar'\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 1000\n",
    "start_epoch = 0\n",
    "train_batch = 500\n",
    "test_batch = 500\n",
    "lr = 0.04\n",
    "schedule = [50, 250, 500, 750]\n",
    "momentum = 0.1\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/star/128/b0/to_pggan/2000shot/cm' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# Image\n",
    "size = (128, 128)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "# sp\n",
    "sp_alpha = 0.1\n",
    "sp_beta = 0.1\n",
    "fc_name = '_fc.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(source_dir, 'pggan/2000_shot_only')\n",
    "val_target_dir = os.path.join(target_dir, 'validation')\n",
    "val_source_dir = os.path.join(source_dir, 'validation')\n",
    "\n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_target_loader = DataLoader(datasets.ImageFolder(val_target_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_source_loader = DataLoader(datasets.ImageFolder(val_source_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using pre-trained model './log/star/128/b0/aug/checkpoint.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "teacher_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "student_model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.4, 'drop_connect_rate':0.4})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    teacher_model.load_state_dict(torch.load(pretrained)['state_dict'])\n",
    "    student_model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "teacher_model.to('cuda')\n",
    "student_model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in teacher_model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(student_model.parameters())\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=4, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    student_model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Source Loss', 'Train Acc.', 'Valid Acc.', 'Source ACC.', 'Train AUROC', 'Valid AUROC', 'Source AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_weights = {}\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    teacher_model_weights[name] = param.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cls(model):\n",
    "    l2_cls = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(fc_name):\n",
    "            l2_cls += 0.5 * torch.norm(param) ** 2\n",
    "    return l2_cls\n",
    "\n",
    "def reg_l2sp(model):\n",
    "    sp_loss = torch.tensor(0.).cuda()\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(fc_name):\n",
    "            sp_loss += 0.5 * torch.norm(param - teacher_model_weights[name]) ** 2\n",
    "    return sp_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "    alpha = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(inputs)\n",
    "            teacher_loss = criterion(teacher_outputs, targets)\n",
    "            sp_alpha = 0\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            sp_alpha += sigmoid(-teacher_loss)    \n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            target_a = targets\n",
    "            target_b = targets[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            lam = 1 - ((bbx2 - bbx1)*(bby2 - bby1) / (inputs.size()[-1]*inputs.size()[-2]))\n",
    "            \n",
    "            outputs = student_model(inputs)\n",
    "            loss_main = (criterion(outputs, target_a) * lam) + (criterion(outputs, target_b) * (1. - lam))\n",
    "            \n",
    "        else:\n",
    "            outputs = student_model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "\n",
    "        # compute output\n",
    "        loss_cls = 0\n",
    "        loss_sp = 0\n",
    "        loss_cls = reg_cls(student_model)\n",
    "        loss_sp = reg_l2sp(student_model)\n",
    "        loss =  loss_main + sp_alpha*loss_sp + sp_alpha*loss_cls\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        cls_losses.update(loss_cls, inputs.size(0))\n",
    "        sp_losses.update(loss_sp, inputs.size(0))\n",
    "        main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "        alpha.update(sp_alpha, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('Train | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | Alpha:{alp:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, alp=alpha.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    cls_losses = AverageMeter()\n",
    "    sp_losses = AverageMeter()\n",
    "    main_losses = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss_main = criterion(outputs, targets)\n",
    "            loss_cls = 0\n",
    "            loss_sp = 0\n",
    "            loss_cls = reg_cls(model)\n",
    "            loss_sp = reg_l2sp(model)\n",
    "            loss = loss_main + 0*loss_sp + 0*loss_cls\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "            cls_losses.update(loss_cls, inputs.size(0))\n",
    "            sp_losses.update(loss_sp, inputs.size(0))\n",
    "            main_losses.update(loss_main.tolist(), inputs.size(0))\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('Test | {batch}/{size} | Loss:{loss:.4f} | MainLoss:{main:.4f} | SPLoss:{sp:.4f} | CLSLoss:{cls:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, main=main_losses.avg, sp=sp_losses.avg, cls=cls_losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 1000] LR: 0.040000\n",
      "Train | 8/8 | Loss:1.3597 | MainLoss:1.1163 | Alpha:0.1131 | SPLoss:0.2850 | CLSLoss:1.8654 | top1:56.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.5964 | MainLoss:0.5964 | SPLoss:0.3534 | CLSLoss:1.7399 | top1:74.9813 | AUROC:0.8383\n",
      "Test | 62/8 | Loss:0.4479 | MainLoss:0.4479 | SPLoss:0.3534 | CLSLoss:1.7399 | top1:81.3696 | AUROC:0.9905\n",
      "\n",
      "Epoch: [2 | 1000] LR: 0.052000\n",
      "Train | 8/8 | Loss:0.8516 | MainLoss:0.6186 | Alpha:0.1163 | SPLoss:0.3364 | CLSLoss:1.6639 | top1:69.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.4874 | MainLoss:0.4874 | SPLoss:0.3193 | CLSLoss:1.5820 | top1:84.8442 | AUROC:0.9327\n",
      "Test | 62/8 | Loss:0.4210 | MainLoss:0.4210 | SPLoss:0.3193 | CLSLoss:1.5820 | top1:80.0000 | AUROC:0.9900\n",
      "\n",
      "Epoch: [3 | 1000] LR: 0.064000\n",
      "Train | 8/8 | Loss:0.7697 | MainLoss:0.5584 | Alpha:0.1162 | SPLoss:0.3122 | CLSLoss:1.5049 | top1:72.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.3382 | MainLoss:0.3382 | SPLoss:0.3137 | CLSLoss:1.4156 | top1:91.3115 | AUROC:0.9767\n",
      "Test | 62/8 | Loss:0.4074 | MainLoss:0.4074 | SPLoss:0.3137 | CLSLoss:1.4156 | top1:78.7189 | AUROC:0.9886\n",
      "\n",
      "Epoch: [4 | 1000] LR: 0.076000\n",
      "Train | 8/8 | Loss:0.7278 | MainLoss:0.5473 | Alpha:0.1094 | SPLoss:0.3181 | CLSLoss:1.3307 | top1:70.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.3409 | MainLoss:0.3409 | SPLoss:0.3170 | CLSLoss:1.2264 | top1:89.3863 | AUROC:0.9890\n",
      "Test | 62/8 | Loss:0.3412 | MainLoss:0.3412 | SPLoss:0.3170 | CLSLoss:1.2264 | top1:86.1501 | AUROC:0.9894\n",
      "\n",
      "Epoch: [5 | 1000] LR: 0.088000\n",
      "Train | 8/8 | Loss:0.6838 | MainLoss:0.5155 | Alpha:0.1147 | SPLoss:0.3187 | CLSLoss:1.1472 | top1:76.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2596 | MainLoss:0.2596 | SPLoss:0.3170 | CLSLoss:1.0430 | top1:95.3925 | AUROC:0.9933\n",
      "Test | 62/8 | Loss:0.3971 | MainLoss:0.3971 | SPLoss:0.3170 | CLSLoss:1.0430 | top1:78.3781 | AUROC:0.9896\n",
      "\n",
      "Epoch: [6 | 1000] LR: 0.100000\n",
      "Train | 8/8 | Loss:0.6620 | MainLoss:0.5129 | Alpha:0.1176 | SPLoss:0.3094 | CLSLoss:0.9595 | top1:73.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2440 | MainLoss:0.2440 | SPLoss:0.3059 | CLSLoss:0.8598 | top1:94.9003 | AUROC:0.9947\n",
      "Test | 62/8 | Loss:0.3609 | MainLoss:0.3609 | SPLoss:0.3059 | CLSLoss:0.8598 | top1:81.6678 | AUROC:0.9909\n",
      "\n",
      "Epoch: [7 | 1000] LR: 0.112000\n",
      "Train | 8/8 | Loss:0.6700 | MainLoss:0.5483 | Alpha:0.1121 | SPLoss:0.3022 | CLSLoss:0.7821 | top1:69.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2734 | MainLoss:0.2734 | SPLoss:0.2943 | CLSLoss:0.6740 | top1:96.4766 | AUROC:0.9949\n",
      "Test | 62/8 | Loss:0.4203 | MainLoss:0.4203 | SPLoss:0.2943 | CLSLoss:0.6740 | top1:76.3762 | AUROC:0.9916\n",
      "\n",
      "Epoch: [8 | 1000] LR: 0.124000\n",
      "Train | 8/8 | Loss:0.6093 | MainLoss:0.5044 | Alpha:0.1127 | SPLoss:0.3044 | CLSLoss:0.6269 | top1:78.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2376 | MainLoss:0.2376 | SPLoss:0.3212 | CLSLoss:0.5403 | top1:97.2648 | AUROC:0.9969\n",
      "Test | 62/8 | Loss:0.4201 | MainLoss:0.4201 | SPLoss:0.3212 | CLSLoss:0.5403 | top1:76.4515 | AUROC:0.9908\n",
      "\n",
      "Epoch: [9 | 1000] LR: 0.136000\n",
      "Train | 8/8 | Loss:0.6405 | MainLoss:0.5493 | Alpha:0.1146 | SPLoss:0.3090 | CLSLoss:0.4889 | top1:69.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2762 | MainLoss:0.2762 | SPLoss:0.2944 | CLSLoss:0.4065 | top1:96.5857 | AUROC:0.9957\n",
      "Test | 62/8 | Loss:0.4111 | MainLoss:0.4111 | SPLoss:0.2944 | CLSLoss:0.4065 | top1:78.0505 | AUROC:0.9922\n",
      "\n",
      "Epoch: [10 | 1000] LR: 0.148000\n",
      "Train | 8/8 | Loss:0.5481 | MainLoss:0.4695 | Alpha:0.1170 | SPLoss:0.2907 | CLSLoss:0.3816 | top1:78.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2444 | MainLoss:0.2444 | SPLoss:0.2984 | CLSLoss:0.3590 | top1:91.2991 | AUROC:0.9948\n",
      "Test | 62/8 | Loss:0.2525 | MainLoss:0.2525 | SPLoss:0.2984 | CLSLoss:0.3590 | top1:89.6167 | AUROC:0.9928\n",
      "\n",
      "Epoch: [11 | 1000] LR: 0.160000\n",
      "Train | 8/8 | Loss:0.5395 | MainLoss:0.4676 | Alpha:0.1133 | SPLoss:0.3240 | CLSLoss:0.3120 | top1:78.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2834 | MainLoss:0.2834 | SPLoss:0.3175 | CLSLoss:0.2892 | top1:88.2087 | AUROC:0.9957\n",
      "Test | 62/8 | Loss:0.2331 | MainLoss:0.2331 | SPLoss:0.3175 | CLSLoss:0.2892 | top1:91.6841 | AUROC:0.9939\n",
      "\n",
      "Epoch: [12 | 1000] LR: 0.160000\n",
      "Train | 8/8 | Loss:0.5734 | MainLoss:0.5100 | Alpha:0.1098 | SPLoss:0.3267 | CLSLoss:0.2507 | top1:77.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2499 | MainLoss:0.2499 | SPLoss:0.3156 | CLSLoss:0.1961 | top1:95.1277 | AUROC:0.9969\n",
      "Test | 62/8 | Loss:0.3538 | MainLoss:0.3538 | SPLoss:0.3156 | CLSLoss:0.1961 | top1:83.5944 | AUROC:0.9936\n",
      "\n",
      "Epoch: [13 | 1000] LR: 0.160000\n",
      "Train | 8/8 | Loss:0.5426 | MainLoss:0.4843 | Alpha:0.1155 | SPLoss:0.3153 | CLSLoss:0.1896 | top1:76.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2283 | MainLoss:0.2283 | SPLoss:0.3244 | CLSLoss:0.1907 | top1:94.2243 | AUROC:0.9961\n",
      "Test | 62/8 | Loss:0.3033 | MainLoss:0.3033 | SPLoss:0.3244 | CLSLoss:0.1907 | top1:86.1828 | AUROC:0.9935\n",
      "\n",
      "Epoch: [14 | 1000] LR: 0.159998\n",
      "Train | 8/8 | Loss:0.5714 | MainLoss:0.5173 | Alpha:0.1108 | SPLoss:0.3235 | CLSLoss:0.1649 | top1:73.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2519 | MainLoss:0.2519 | SPLoss:0.3246 | CLSLoss:0.1565 | top1:93.7009 | AUROC:0.9970\n",
      "Test | 62/8 | Loss:0.3073 | MainLoss:0.3073 | SPLoss:0.3246 | CLSLoss:0.1565 | top1:87.5786 | AUROC:0.9935\n",
      "\n",
      "Epoch: [15 | 1000] LR: 0.159996\n",
      "Train | 8/8 | Loss:0.5226 | MainLoss:0.4666 | Alpha:0.1124 | SPLoss:0.3325 | CLSLoss:0.1646 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2006 | MainLoss:0.2006 | SPLoss:0.3432 | CLSLoss:0.1539 | top1:97.1215 | AUROC:0.9976\n",
      "Test | 62/8 | Loss:0.3741 | MainLoss:0.3741 | SPLoss:0.3432 | CLSLoss:0.1539 | top1:80.2916 | AUROC:0.9937\n",
      "\n",
      "Epoch: [16 | 1000] LR: 0.159994\n",
      "Train | 8/8 | Loss:0.5634 | MainLoss:0.5097 | Alpha:0.1105 | SPLoss:0.3382 | CLSLoss:0.1473 | top1:76.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2093 | MainLoss:0.2093 | SPLoss:0.3460 | CLSLoss:0.1350 | top1:97.2586 | AUROC:0.9981\n",
      "Test | 62/8 | Loss:0.3952 | MainLoss:0.3952 | SPLoss:0.3460 | CLSLoss:0.1350 | top1:78.8467 | AUROC:0.9942\n",
      "\n",
      "Epoch: [17 | 1000] LR: 0.159990\n",
      "Train | 8/8 | Loss:0.4967 | MainLoss:0.4421 | Alpha:0.1114 | SPLoss:0.3446 | CLSLoss:0.1459 | top1:81.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1450 | MainLoss:0.1450 | SPLoss:0.3829 | CLSLoss:0.1609 | top1:98.1963 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.4794 | MainLoss:0.4794 | SPLoss:0.3829 | CLSLoss:0.1609 | top1:71.4122 | AUROC:0.9935\n",
      "\n",
      "Epoch: [18 | 1000] LR: 0.159986\n",
      "Train | 8/8 | Loss:0.5517 | MainLoss:0.4926 | Alpha:0.1122 | SPLoss:0.3821 | CLSLoss:0.1434 | top1:76.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2292 | MainLoss:0.2292 | SPLoss:0.3744 | CLSLoss:0.1232 | top1:96.7695 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3870 | MainLoss:0.3870 | SPLoss:0.3744 | CLSLoss:0.1232 | top1:81.9954 | AUROC:0.9940\n",
      "\n",
      "Epoch: [19 | 1000] LR: 0.159981\n",
      "Train | 8/8 | Loss:0.5531 | MainLoss:0.5001 | Alpha:0.1132 | SPLoss:0.3488 | CLSLoss:0.1206 | top1:74.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1778 | MainLoss:0.1778 | SPLoss:0.3638 | CLSLoss:0.1379 | top1:97.0654 | AUROC:0.9976\n",
      "Test | 62/8 | Loss:0.3682 | MainLoss:0.3682 | SPLoss:0.3638 | CLSLoss:0.1379 | top1:81.1861 | AUROC:0.9936\n",
      "\n",
      "Epoch: [20 | 1000] LR: 0.159975\n",
      "Train | 8/8 | Loss:0.5573 | MainLoss:0.5058 | Alpha:0.1122 | SPLoss:0.3398 | CLSLoss:0.1188 | top1:74.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2587 | MainLoss:0.2587 | SPLoss:0.3421 | CLSLoss:0.1343 | top1:90.4860 | AUROC:0.9967\n",
      "Test | 62/8 | Loss:0.2581 | MainLoss:0.2581 | SPLoss:0.3421 | CLSLoss:0.1343 | top1:91.1173 | AUROC:0.9948\n",
      "\n",
      "Epoch: [21 | 1000] LR: 0.159968\n",
      "Train | 8/8 | Loss:0.5152 | MainLoss:0.4619 | Alpha:0.1135 | SPLoss:0.3387 | CLSLoss:0.1308 | top1:77.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2227 | MainLoss:0.2227 | SPLoss:0.3500 | CLSLoss:0.1325 | top1:93.7539 | AUROC:0.9968\n",
      "Test | 62/8 | Loss:0.2970 | MainLoss:0.2970 | SPLoss:0.3500 | CLSLoss:0.1325 | top1:87.5885 | AUROC:0.9951\n",
      "\n",
      "Epoch: [22 | 1000] LR: 0.159961\n",
      "Train | 8/8 | Loss:0.5630 | MainLoss:0.5107 | Alpha:0.1144 | SPLoss:0.3448 | CLSLoss:0.1123 | top1:74.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2024 | MainLoss:0.2024 | SPLoss:0.3558 | CLSLoss:0.1263 | top1:95.3676 | AUROC:0.9970\n",
      "Test | 62/8 | Loss:0.3193 | MainLoss:0.3193 | SPLoss:0.3558 | CLSLoss:0.1263 | top1:85.5832 | AUROC:0.9948\n",
      "\n",
      "Epoch: [23 | 1000] LR: 0.159952\n",
      "Train | 8/8 | Loss:0.5570 | MainLoss:0.5050 | Alpha:0.1109 | SPLoss:0.3556 | CLSLoss:0.1136 | top1:72.3250 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.2567 | MainLoss:0.2567 | SPLoss:0.3550 | CLSLoss:0.1051 | top1:93.4704 | AUROC:0.9979\n",
      "Test | 62/8 | Loss:0.3346 | MainLoss:0.3346 | SPLoss:0.3550 | CLSLoss:0.1051 | top1:86.3270 | AUROC:0.9949\n",
      "\n",
      "Epoch: [24 | 1000] LR: 0.159943\n",
      "Train | 8/8 | Loss:0.5331 | MainLoss:0.4824 | Alpha:0.1100 | SPLoss:0.3545 | CLSLoss:0.1058 | top1:77.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1642 | MainLoss:0.1642 | SPLoss:0.3798 | CLSLoss:0.1233 | top1:97.7726 | AUROC:0.9976\n",
      "Test | 62/8 | Loss:0.4622 | MainLoss:0.4622 | SPLoss:0.3798 | CLSLoss:0.1233 | top1:73.9613 | AUROC:0.9937\n",
      "\n",
      "Epoch: [25 | 1000] LR: 0.159933\n",
      "Train | 8/8 | Loss:0.5644 | MainLoss:0.5121 | Alpha:0.1144 | SPLoss:0.3512 | CLSLoss:0.1056 | top1:74.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2038 | MainLoss:0.2038 | SPLoss:0.3553 | CLSLoss:0.1226 | top1:95.1059 | AUROC:0.9969\n",
      "Test | 62/8 | Loss:0.3337 | MainLoss:0.3337 | SPLoss:0.3553 | CLSLoss:0.1226 | top1:84.6101 | AUROC:0.9949\n",
      "\n",
      "Epoch: [26 | 1000] LR: 0.159923\n",
      "Train | 8/8 | Loss:0.5568 | MainLoss:0.5052 | Alpha:0.1135 | SPLoss:0.3480 | CLSLoss:0.1069 | top1:77.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1779 | MainLoss:0.1779 | SPLoss:0.3804 | CLSLoss:0.1181 | top1:97.5545 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.4149 | MainLoss:0.4149 | SPLoss:0.3804 | CLSLoss:0.1181 | top1:77.1592 | AUROC:0.9941\n",
      "\n",
      "Epoch: [27 | 1000] LR: 0.159911\n",
      "Train | 8/8 | Loss:0.5545 | MainLoss:0.5005 | Alpha:0.1137 | SPLoss:0.3634 | CLSLoss:0.1118 | top1:77.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2291 | MainLoss:0.2291 | SPLoss:0.3584 | CLSLoss:0.1072 | top1:94.5296 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.3194 | MainLoss:0.3194 | SPLoss:0.3584 | CLSLoss:0.1072 | top1:87.5131 | AUROC:0.9954\n",
      "\n",
      "Epoch: [28 | 1000] LR: 0.159899\n",
      "Train | 8/8 | Loss:0.5125 | MainLoss:0.4582 | Alpha:0.1139 | SPLoss:0.3611 | CLSLoss:0.1151 | top1:78.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1734 | MainLoss:0.1734 | SPLoss:0.3690 | CLSLoss:0.1192 | top1:97.3925 | AUROC:0.9976\n",
      "Test | 62/8 | Loss:0.4030 | MainLoss:0.4030 | SPLoss:0.3690 | CLSLoss:0.1192 | top1:78.5911 | AUROC:0.9953\n",
      "\n",
      "Epoch: [29 | 1000] LR: 0.159886\n",
      "Train | 8/8 | Loss:0.5832 | MainLoss:0.5310 | Alpha:0.1147 | SPLoss:0.3541 | CLSLoss:0.1023 | top1:74.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2718 | MainLoss:0.2718 | SPLoss:0.3540 | CLSLoss:0.0875 | top1:93.8816 | AUROC:0.9974\n",
      "Test | 62/8 | Loss:0.3332 | MainLoss:0.3332 | SPLoss:0.3540 | CLSLoss:0.0875 | top1:88.4207 | AUROC:0.9946\n",
      "\n",
      "Epoch: [30 | 1000] LR: 0.159872\n",
      "Train | 8/8 | Loss:0.5489 | MainLoss:0.4971 | Alpha:0.1103 | SPLoss:0.3670 | CLSLoss:0.1022 | top1:76.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2909 | MainLoss:0.2909 | SPLoss:0.3554 | CLSLoss:0.0857 | top1:92.1464 | AUROC:0.9976\n",
      "Test | 62/8 | Loss:0.3379 | MainLoss:0.3379 | SPLoss:0.3554 | CLSLoss:0.0857 | top1:89.2497 | AUROC:0.9944\n",
      "\n",
      "Epoch: [31 | 1000] LR: 0.159858\n",
      "Train | 8/8 | Loss:0.6031 | MainLoss:0.5541 | Alpha:0.1133 | SPLoss:0.3519 | CLSLoss:0.0788 | top1:72.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2681 | MainLoss:0.2681 | SPLoss:0.3292 | CLSLoss:0.0924 | top1:92.3520 | AUROC:0.9961\n",
      "Test | 62/8 | Loss:0.3075 | MainLoss:0.3075 | SPLoss:0.3292 | CLSLoss:0.0924 | top1:89.2005 | AUROC:0.9947\n",
      "\n",
      "Epoch: [32 | 1000] LR: 0.159842\n",
      "Train | 8/8 | Loss:0.5552 | MainLoss:0.5063 | Alpha:0.1150 | SPLoss:0.3305 | CLSLoss:0.0946 | top1:72.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2437 | MainLoss:0.2437 | SPLoss:0.3436 | CLSLoss:0.0927 | top1:95.1090 | AUROC:0.9971\n",
      "Test | 62/8 | Loss:0.3449 | MainLoss:0.3449 | SPLoss:0.3436 | CLSLoss:0.0927 | top1:85.1147 | AUROC:0.9942\n",
      "\n",
      "Epoch: [33 | 1000] LR: 0.159826\n",
      "Train | 8/8 | Loss:0.5111 | MainLoss:0.4551 | Alpha:0.1172 | SPLoss:0.3655 | CLSLoss:0.1106 | top1:79.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1911 | MainLoss:0.1911 | SPLoss:0.4090 | CLSLoss:0.1193 | top1:97.0966 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3735 | MainLoss:0.3735 | SPLoss:0.4090 | CLSLoss:0.1193 | top1:81.5498 | AUROC:0.9917\n",
      "\n",
      "Epoch: [34 | 1000] LR: 0.159809\n",
      "Train | 8/8 | Loss:0.5252 | MainLoss:0.4683 | Alpha:0.1141 | SPLoss:0.3843 | CLSLoss:0.1132 | top1:75.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1788 | MainLoss:0.1788 | SPLoss:0.3816 | CLSLoss:0.1322 | top1:95.9564 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.3345 | MainLoss:0.3345 | SPLoss:0.3816 | CLSLoss:0.1322 | top1:84.5610 | AUROC:0.9934\n",
      "\n",
      "Epoch: [35 | 1000] LR: 0.159791\n",
      "Train | 8/8 | Loss:0.4802 | MainLoss:0.4222 | Alpha:0.1136 | SPLoss:0.3791 | CLSLoss:0.1316 | top1:82.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1444 | MainLoss:0.1444 | SPLoss:0.4011 | CLSLoss:0.1396 | top1:97.7882 | AUROC:0.9978\n",
      "Test | 62/8 | Loss:0.4344 | MainLoss:0.4344 | SPLoss:0.4011 | CLSLoss:0.1396 | top1:76.4548 | AUROC:0.9926\n",
      "\n",
      "Epoch: [36 | 1000] LR: 0.159773\n",
      "Train | 8/8 | Loss:0.5762 | MainLoss:0.5175 | Alpha:0.1137 | SPLoss:0.3928 | CLSLoss:0.1241 | top1:74.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2530 | MainLoss:0.2530 | SPLoss:0.3649 | CLSLoss:0.0899 | top1:97.2305 | AUROC:0.9979\n",
      "Test | 62/8 | Loss:0.4134 | MainLoss:0.4134 | SPLoss:0.3649 | CLSLoss:0.0899 | top1:79.8132 | AUROC:0.9944\n",
      "\n",
      "Epoch: [37 | 1000] LR: 0.159753\n",
      "Train | 8/8 | Loss:0.5459 | MainLoss:0.4907 | Alpha:0.1113 | SPLoss:0.3834 | CLSLoss:0.1108 | top1:75.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2333 | MainLoss:0.2333 | SPLoss:0.4175 | CLSLoss:0.0887 | top1:98.4766 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.4559 | MainLoss:0.4559 | SPLoss:0.4175 | CLSLoss:0.0887 | top1:74.3676 | AUROC:0.9920\n",
      "\n",
      "Epoch: [38 | 1000] LR: 0.159733\n",
      "Train | 8/8 | Loss:0.5155 | MainLoss:0.4575 | Alpha:0.1132 | SPLoss:0.4071 | CLSLoss:0.1057 | top1:78.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1747 | MainLoss:0.1747 | SPLoss:0.4022 | CLSLoss:0.1193 | top1:97.3271 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.3746 | MainLoss:0.3746 | SPLoss:0.4022 | CLSLoss:0.1193 | top1:80.8224 | AUROC:0.9940\n",
      "\n",
      "Epoch: [39 | 1000] LR: 0.159712\n",
      "Train | 8/8 | Loss:0.5279 | MainLoss:0.4688 | Alpha:0.1156 | SPLoss:0.3885 | CLSLoss:0.1238 | top1:77.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2808 | MainLoss:0.2808 | SPLoss:0.3547 | CLSLoss:0.0934 | top1:91.7913 | AUROC:0.9971\n",
      "Test | 62/8 | Loss:0.3100 | MainLoss:0.3100 | SPLoss:0.3547 | CLSLoss:0.0934 | top1:90.0950 | AUROC:0.9948\n",
      "\n",
      "Epoch: [40 | 1000] LR: 0.159691\n",
      "Train | 8/8 | Loss:0.5293 | MainLoss:0.4742 | Alpha:0.1158 | SPLoss:0.3684 | CLSLoss:0.1069 | top1:79.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2094 | MainLoss:0.2094 | SPLoss:0.3734 | CLSLoss:0.1194 | top1:94.7508 | AUROC:0.9976\n",
      "Test | 62/8 | Loss:0.3117 | MainLoss:0.3117 | SPLoss:0.3734 | CLSLoss:0.1194 | top1:87.1429 | AUROC:0.9958\n",
      "\n",
      "Epoch: [41 | 1000] LR: 0.159668\n",
      "Train | 8/8 | Loss:0.5506 | MainLoss:0.4936 | Alpha:0.1149 | SPLoss:0.3788 | CLSLoss:0.1186 | top1:79.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2869 | MainLoss:0.2869 | SPLoss:0.3763 | CLSLoss:0.0808 | top1:94.4548 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3893 | MainLoss:0.3893 | SPLoss:0.3763 | CLSLoss:0.0808 | top1:86.1435 | AUROC:0.9948\n",
      "\n",
      "Epoch: [42 | 1000] LR: 0.159645\n",
      "Train | 8/8 | Loss:0.5087 | MainLoss:0.4520 | Alpha:0.1153 | SPLoss:0.3886 | CLSLoss:0.1023 | top1:76.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2315 | MainLoss:0.2315 | SPLoss:0.3808 | CLSLoss:0.1297 | top1:91.9782 | AUROC:0.9970\n",
      "Test | 62/8 | Loss:0.2778 | MainLoss:0.2778 | SPLoss:0.3808 | CLSLoss:0.1297 | top1:89.1219 | AUROC:0.9942\n",
      "\n",
      "Epoch: [43 | 1000] LR: 0.159621\n",
      "Train | 8/8 | Loss:0.5837 | MainLoss:0.5285 | Alpha:0.1118 | SPLoss:0.3906 | CLSLoss:0.1036 | top1:76.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2482 | MainLoss:0.2482 | SPLoss:0.3734 | CLSLoss:0.0957 | top1:94.4579 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.3509 | MainLoss:0.3509 | SPLoss:0.3734 | CLSLoss:0.0957 | top1:86.2779 | AUROC:0.9945\n",
      "\n",
      "Epoch: [44 | 1000] LR: 0.159596\n",
      "Train | 8/8 | Loss:0.4846 | MainLoss:0.4265 | Alpha:0.1163 | SPLoss:0.3852 | CLSLoss:0.1144 | top1:81.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1851 | MainLoss:0.1851 | SPLoss:0.3999 | CLSLoss:0.1328 | top1:95.6075 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.3276 | MainLoss:0.3276 | SPLoss:0.3999 | CLSLoss:0.1328 | top1:84.9672 | AUROC:0.9938\n",
      "\n",
      "Epoch: [45 | 1000] LR: 0.159570\n",
      "Train | 8/8 | Loss:0.5822 | MainLoss:0.5271 | Alpha:0.1138 | SPLoss:0.3727 | CLSLoss:0.1107 | top1:76.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2179 | MainLoss:0.2179 | SPLoss:0.3921 | CLSLoss:0.0915 | top1:97.5732 | AUROC:0.9977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.5071 | MainLoss:0.5071 | SPLoss:0.3921 | CLSLoss:0.0915 | top1:69.0400 | AUROC:0.9942\n",
      "\n",
      "Epoch: [46 | 1000] LR: 0.159544\n",
      "Train | 8/8 | Loss:0.5665 | MainLoss:0.5111 | Alpha:0.1158 | SPLoss:0.3788 | CLSLoss:0.0998 | top1:74.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2776 | MainLoss:0.2776 | SPLoss:0.3489 | CLSLoss:0.0816 | top1:93.8505 | AUROC:0.9973\n",
      "Test | 62/8 | Loss:0.3471 | MainLoss:0.3471 | SPLoss:0.3489 | CLSLoss:0.0816 | top1:88.2765 | AUROC:0.9957\n",
      "\n",
      "Epoch: [47 | 1000] LR: 0.159517\n",
      "Train | 8/8 | Loss:0.5177 | MainLoss:0.4661 | Alpha:0.1103 | SPLoss:0.3635 | CLSLoss:0.1043 | top1:77.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1666 | MainLoss:0.1666 | SPLoss:0.3947 | CLSLoss:0.1220 | top1:97.7882 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.4256 | MainLoss:0.4256 | SPLoss:0.3947 | CLSLoss:0.1220 | top1:76.4744 | AUROC:0.9950\n",
      "\n",
      "Epoch: [48 | 1000] LR: 0.159489\n",
      "Train | 8/8 | Loss:0.5420 | MainLoss:0.4855 | Alpha:0.1108 | SPLoss:0.3898 | CLSLoss:0.1197 | top1:77.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2915 | MainLoss:0.2915 | SPLoss:0.3778 | CLSLoss:0.1000 | top1:90.4642 | AUROC:0.9980\n",
      "Test | 62/8 | Loss:0.3065 | MainLoss:0.3065 | SPLoss:0.3778 | CLSLoss:0.1000 | top1:92.2149 | AUROC:0.9951\n",
      "\n",
      "Epoch: [49 | 1000] LR: 0.159460\n",
      "Train | 8/8 | Loss:0.5826 | MainLoss:0.5269 | Alpha:0.1154 | SPLoss:0.3807 | CLSLoss:0.1012 | top1:75.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2627 | MainLoss:0.2627 | SPLoss:0.3565 | CLSLoss:0.0842 | top1:96.0187 | AUROC:0.9975\n",
      "Test | 62/8 | Loss:0.3886 | MainLoss:0.3886 | SPLoss:0.3565 | CLSLoss:0.0842 | top1:83.2667 | AUROC:0.9954\n",
      "\n",
      "Epoch: [50 | 1000] LR: 0.159431\n",
      "Train | 8/8 | Loss:0.5504 | MainLoss:0.4993 | Alpha:0.1118 | SPLoss:0.3639 | CLSLoss:0.0927 | top1:76.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1966 | MainLoss:0.1966 | SPLoss:0.3831 | CLSLoss:0.1087 | top1:97.0280 | AUROC:0.9983\n",
      "Test | 62/8 | Loss:0.3678 | MainLoss:0.3678 | SPLoss:0.3831 | CLSLoss:0.1087 | top1:82.1887 | AUROC:0.9944\n",
      "\n",
      "Epoch: [51 | 1000] LR: 0.159400\n",
      "Train | 8/8 | Loss:0.5687 | MainLoss:0.5373 | Alpha:0.4050 | SPLoss:0.0027 | CLSLoss:0.0749 | top1:73.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2610 | MainLoss:0.2610 | SPLoss:0.0087 | CLSLoss:0.0528 | top1:97.9595 | AUROC:0.9983\n",
      "Test | 62/8 | Loss:0.4583 | MainLoss:0.4583 | SPLoss:0.0087 | CLSLoss:0.0528 | top1:76.3303 | AUROC:0.9926\n",
      "\n",
      "Epoch: [52 | 1000] LR: 0.015937\n",
      "Train | 8/8 | Loss:0.5102 | MainLoss:0.4846 | Alpha:0.4044 | SPLoss:0.0080 | CLSLoss:0.0553 | top1:76.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2501 | MainLoss:0.2501 | SPLoss:0.0073 | CLSLoss:0.0565 | top1:97.2399 | AUROC:0.9982\n",
      "Test | 62/8 | Loss:0.4095 | MainLoss:0.4095 | SPLoss:0.0073 | CLSLoss:0.0565 | top1:81.6547 | AUROC:0.9926\n",
      "\n",
      "Epoch: [53 | 1000] LR: 0.015934\n",
      "Train | 8/8 | Loss:0.5768 | MainLoss:0.5514 | Alpha:0.4057 | SPLoss:0.0071 | CLSLoss:0.0555 | top1:64.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2609 | MainLoss:0.2609 | SPLoss:0.0068 | CLSLoss:0.0525 | top1:96.9564 | AUROC:0.9983\n",
      "Test | 62/8 | Loss:0.4049 | MainLoss:0.4049 | SPLoss:0.0068 | CLSLoss:0.0525 | top1:82.9325 | AUROC:0.9929\n",
      "\n",
      "Epoch: [54 | 1000] LR: 0.015930\n",
      "Train | 8/8 | Loss:0.5599 | MainLoss:0.5363 | Alpha:0.4049 | SPLoss:0.0066 | CLSLoss:0.0516 | top1:72.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2616 | MainLoss:0.2616 | SPLoss:0.0064 | CLSLoss:0.0519 | top1:96.8162 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.4009 | MainLoss:0.4009 | SPLoss:0.0064 | CLSLoss:0.0519 | top1:83.5059 | AUROC:0.9929\n",
      "\n",
      "Epoch: [55 | 1000] LR: 0.015927\n",
      "Train | 8/8 | Loss:0.5192 | MainLoss:0.4943 | Alpha:0.4047 | SPLoss:0.0067 | CLSLoss:0.0549 | top1:77.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2552 | MainLoss:0.2552 | SPLoss:0.0066 | CLSLoss:0.0545 | top1:96.6231 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3911 | MainLoss:0.3911 | SPLoss:0.0066 | CLSLoss:0.0545 | top1:84.2202 | AUROC:0.9930\n",
      "\n",
      "Epoch: [56 | 1000] LR: 0.015924\n",
      "Train | 8/8 | Loss:0.4863 | MainLoss:0.4606 | Alpha:0.4053 | SPLoss:0.0069 | CLSLoss:0.0565 | top1:79.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2394 | MainLoss:0.2394 | SPLoss:0.0073 | CLSLoss:0.0603 | top1:96.5545 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.3802 | MainLoss:0.3802 | SPLoss:0.0073 | CLSLoss:0.0603 | top1:84.3938 | AUROC:0.9930\n",
      "\n",
      "Epoch: [57 | 1000] LR: 0.015920\n",
      "Train | 8/8 | Loss:0.5146 | MainLoss:0.4878 | Alpha:0.4048 | SPLoss:0.0072 | CLSLoss:0.0590 | top1:74.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2366 | MainLoss:0.2366 | SPLoss:0.0076 | CLSLoss:0.0618 | top1:96.4206 | AUROC:0.9984\n",
      "Test | 62/8 | Loss:0.3731 | MainLoss:0.3731 | SPLoss:0.0076 | CLSLoss:0.0618 | top1:84.9017 | AUROC:0.9929\n",
      "\n",
      "Epoch: [58 | 1000] LR: 0.015917\n",
      "Train | 8/8 | Loss:0.5187 | MainLoss:0.4905 | Alpha:0.4050 | SPLoss:0.0076 | CLSLoss:0.0618 | top1:78.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2308 | MainLoss:0.2308 | SPLoss:0.0078 | CLSLoss:0.0619 | top1:96.7352 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3800 | MainLoss:0.3800 | SPLoss:0.0078 | CLSLoss:0.0619 | top1:84.1088 | AUROC:0.9928\n",
      "\n",
      "Epoch: [59 | 1000] LR: 0.015913\n",
      "Train | 8/8 | Loss:0.5053 | MainLoss:0.4764 | Alpha:0.4054 | SPLoss:0.0081 | CLSLoss:0.0633 | top1:73.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:0.0082 | CLSLoss:0.0621 | top1:97.0280 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3874 | MainLoss:0.3874 | SPLoss:0.0082 | CLSLoss:0.0621 | top1:83.2536 | AUROC:0.9929\n",
      "\n",
      "Epoch: [60 | 1000] LR: 0.015909\n",
      "Train | 8/8 | Loss:0.5280 | MainLoss:0.5001 | Alpha:0.4047 | SPLoss:0.0081 | CLSLoss:0.0609 | top1:72.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2376 | MainLoss:0.2376 | SPLoss:0.0081 | CLSLoss:0.0593 | top1:96.5047 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3759 | MainLoss:0.3759 | SPLoss:0.0081 | CLSLoss:0.0593 | top1:84.8296 | AUROC:0.9932\n",
      "\n",
      "Epoch: [61 | 1000] LR: 0.015905\n",
      "Train | 8/8 | Loss:0.4970 | MainLoss:0.4692 | Alpha:0.4046 | SPLoss:0.0083 | CLSLoss:0.0605 | top1:81.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2359 | MainLoss:0.2359 | SPLoss:0.0087 | CLSLoss:0.0616 | top1:96.1153 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3646 | MainLoss:0.3646 | SPLoss:0.0087 | CLSLoss:0.0616 | top1:85.7143 | AUROC:0.9931\n",
      "\n",
      "Epoch: [62 | 1000] LR: 0.015902\n",
      "Train | 8/8 | Loss:0.5178 | MainLoss:0.4903 | Alpha:0.4052 | SPLoss:0.0085 | CLSLoss:0.0594 | top1:74.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2304 | MainLoss:0.2304 | SPLoss:0.0088 | CLSLoss:0.0615 | top1:96.5327 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3730 | MainLoss:0.3730 | SPLoss:0.0088 | CLSLoss:0.0615 | top1:84.7936 | AUROC:0.9932\n",
      "\n",
      "Epoch: [63 | 1000] LR: 0.015898\n",
      "Train | 8/8 | Loss:0.5521 | MainLoss:0.5237 | Alpha:0.4058 | SPLoss:0.0089 | CLSLoss:0.0612 | top1:75.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2340 | MainLoss:0.2340 | SPLoss:0.0087 | CLSLoss:0.0581 | top1:96.8411 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3825 | MainLoss:0.3825 | SPLoss:0.0087 | CLSLoss:0.0581 | top1:84.3512 | AUROC:0.9932\n",
      "\n",
      "Epoch: [64 | 1000] LR: 0.015893\n",
      "Train | 8/8 | Loss:0.4596 | MainLoss:0.4310 | Alpha:0.4051 | SPLoss:0.0093 | CLSLoss:0.0614 | top1:83.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2269 | MainLoss:0.2269 | SPLoss:0.0097 | CLSLoss:0.0633 | top1:96.4019 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3646 | MainLoss:0.3646 | SPLoss:0.0097 | CLSLoss:0.0633 | top1:85.4817 | AUROC:0.9932\n",
      "\n",
      "Epoch: [65 | 1000] LR: 0.015889\n",
      "Train | 8/8 | Loss:0.5121 | MainLoss:0.4827 | Alpha:0.4038 | SPLoss:0.0098 | CLSLoss:0.0631 | top1:80.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2265 | MainLoss:0.2265 | SPLoss:0.0099 | CLSLoss:0.0627 | top1:96.4922 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3662 | MainLoss:0.3662 | SPLoss:0.0099 | CLSLoss:0.0627 | top1:85.3604 | AUROC:0.9931\n",
      "\n",
      "Epoch: [66 | 1000] LR: 0.015885\n",
      "Train | 8/8 | Loss:0.4851 | MainLoss:0.4543 | Alpha:0.4044 | SPLoss:0.0105 | CLSLoss:0.0654 | top1:80.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2184 | MainLoss:0.2184 | SPLoss:0.0105 | CLSLoss:0.0645 | top1:96.8069 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3710 | MainLoss:0.3710 | SPLoss:0.0105 | CLSLoss:0.0645 | top1:84.6429 | AUROC:0.9932\n",
      "\n",
      "Epoch: [67 | 1000] LR: 0.015881\n",
      "Train | 8/8 | Loss:0.4575 | MainLoss:0.4262 | Alpha:0.4060 | SPLoss:0.0109 | CLSLoss:0.0660 | top1:80.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2143 | MainLoss:0.2143 | SPLoss:0.0115 | CLSLoss:0.0685 | top1:96.4393 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3566 | MainLoss:0.3566 | SPLoss:0.0115 | CLSLoss:0.0685 | top1:85.6389 | AUROC:0.9931\n",
      "\n",
      "Epoch: [68 | 1000] LR: 0.015877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.4976 | MainLoss:0.4665 | Alpha:0.4058 | SPLoss:0.0112 | CLSLoss:0.0655 | top1:73.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2205 | MainLoss:0.2205 | SPLoss:0.0117 | CLSLoss:0.0675 | top1:96.0997 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3502 | MainLoss:0.3502 | SPLoss:0.0117 | CLSLoss:0.0675 | top1:86.3139 | AUROC:0.9931\n",
      "\n",
      "Epoch: [69 | 1000] LR: 0.015872\n",
      "Train | 8/8 | Loss:0.5357 | MainLoss:0.5050 | Alpha:0.4042 | SPLoss:0.0114 | CLSLoss:0.0644 | top1:75.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2233 | MainLoss:0.2233 | SPLoss:0.0115 | CLSLoss:0.0631 | top1:96.5047 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3652 | MainLoss:0.3652 | SPLoss:0.0115 | CLSLoss:0.0631 | top1:85.4522 | AUROC:0.9933\n",
      "\n",
      "Epoch: [70 | 1000] LR: 0.015868\n",
      "Train | 8/8 | Loss:0.5242 | MainLoss:0.4945 | Alpha:0.4043 | SPLoss:0.0114 | CLSLoss:0.0620 | top1:78.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2260 | MainLoss:0.2260 | SPLoss:0.0113 | CLSLoss:0.0605 | top1:96.6916 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3714 | MainLoss:0.3714 | SPLoss:0.0113 | CLSLoss:0.0605 | top1:85.1245 | AUROC:0.9934\n",
      "\n",
      "Epoch: [71 | 1000] LR: 0.015863\n",
      "Train | 8/8 | Loss:0.4796 | MainLoss:0.4497 | Alpha:0.4054 | SPLoss:0.0117 | CLSLoss:0.0621 | top1:81.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:0.0121 | CLSLoss:0.0635 | top1:96.4424 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3616 | MainLoss:0.3616 | SPLoss:0.0121 | CLSLoss:0.0635 | top1:85.7110 | AUROC:0.9932\n",
      "\n",
      "Epoch: [72 | 1000] LR: 0.015858\n",
      "Train | 8/8 | Loss:0.5391 | MainLoss:0.5094 | Alpha:0.4058 | SPLoss:0.0117 | CLSLoss:0.0614 | top1:77.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2239 | MainLoss:0.2239 | SPLoss:0.0117 | CLSLoss:0.0607 | top1:96.7819 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3730 | MainLoss:0.3730 | SPLoss:0.0117 | CLSLoss:0.0607 | top1:84.9214 | AUROC:0.9934\n",
      "\n",
      "Epoch: [73 | 1000] LR: 0.015854\n",
      "Train | 8/8 | Loss:0.5289 | MainLoss:0.5003 | Alpha:0.4042 | SPLoss:0.0116 | CLSLoss:0.0592 | top1:74.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2248 | MainLoss:0.2248 | SPLoss:0.0116 | CLSLoss:0.0591 | top1:96.9907 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3803 | MainLoss:0.3803 | SPLoss:0.0116 | CLSLoss:0.0591 | top1:84.3611 | AUROC:0.9933\n",
      "\n",
      "Epoch: [74 | 1000] LR: 0.015849\n",
      "Train | 8/8 | Loss:0.4868 | MainLoss:0.4573 | Alpha:0.4049 | SPLoss:0.0121 | CLSLoss:0.0608 | top1:78.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2243 | MainLoss:0.2243 | SPLoss:0.0123 | CLSLoss:0.0609 | top1:96.6542 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3676 | MainLoss:0.3676 | SPLoss:0.0123 | CLSLoss:0.0609 | top1:85.4063 | AUROC:0.9932\n",
      "\n",
      "Epoch: [75 | 1000] LR: 0.015844\n",
      "Train | 8/8 | Loss:0.4353 | MainLoss:0.4040 | Alpha:0.4056 | SPLoss:0.0130 | CLSLoss:0.0640 | top1:83.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2085 | MainLoss:0.2085 | SPLoss:0.0138 | CLSLoss:0.0680 | top1:96.6542 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3579 | MainLoss:0.3579 | SPLoss:0.0138 | CLSLoss:0.0680 | top1:85.3899 | AUROC:0.9932\n",
      "\n",
      "Epoch: [76 | 1000] LR: 0.015839\n",
      "Train | 8/8 | Loss:0.5233 | MainLoss:0.4917 | Alpha:0.4051 | SPLoss:0.0132 | CLSLoss:0.0647 | top1:73.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2169 | MainLoss:0.2169 | SPLoss:0.0132 | CLSLoss:0.0637 | top1:96.6729 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3629 | MainLoss:0.3629 | SPLoss:0.0132 | CLSLoss:0.0637 | top1:85.4194 | AUROC:0.9932\n",
      "\n",
      "Epoch: [77 | 1000] LR: 0.015834\n",
      "Train | 8/8 | Loss:0.5075 | MainLoss:0.4771 | Alpha:0.4059 | SPLoss:0.0130 | CLSLoss:0.0618 | top1:76.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2158 | MainLoss:0.2158 | SPLoss:0.0134 | CLSLoss:0.0633 | top1:96.7477 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3649 | MainLoss:0.3649 | SPLoss:0.0134 | CLSLoss:0.0633 | top1:85.1737 | AUROC:0.9933\n",
      "\n",
      "Epoch: [78 | 1000] LR: 0.015829\n",
      "Train | 8/8 | Loss:0.5444 | MainLoss:0.5132 | Alpha:0.4057 | SPLoss:0.0135 | CLSLoss:0.0633 | top1:75.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2286 | MainLoss:0.2286 | SPLoss:0.0127 | CLSLoss:0.0579 | top1:96.6698 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3702 | MainLoss:0.3702 | SPLoss:0.0127 | CLSLoss:0.0579 | top1:85.3637 | AUROC:0.9933\n",
      "\n",
      "Epoch: [79 | 1000] LR: 0.015823\n",
      "Train | 8/8 | Loss:0.4849 | MainLoss:0.4558 | Alpha:0.4049 | SPLoss:0.0129 | CLSLoss:0.0589 | top1:80.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:0.0133 | CLSLoss:0.0610 | top1:96.5639 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3636 | MainLoss:0.3636 | SPLoss:0.0133 | CLSLoss:0.0610 | top1:85.5832 | AUROC:0.9933\n",
      "\n",
      "Epoch: [80 | 1000] LR: 0.015818\n",
      "Train | 8/8 | Loss:0.4845 | MainLoss:0.4537 | Alpha:0.4057 | SPLoss:0.0137 | CLSLoss:0.0622 | top1:78.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2225 | MainLoss:0.2225 | SPLoss:0.0140 | CLSLoss:0.0623 | top1:96.2243 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3526 | MainLoss:0.3526 | SPLoss:0.0140 | CLSLoss:0.0623 | top1:86.4155 | AUROC:0.9933\n",
      "\n",
      "Epoch: [81 | 1000] LR: 0.015813\n",
      "Train | 8/8 | Loss:0.5163 | MainLoss:0.4863 | Alpha:0.4047 | SPLoss:0.0136 | CLSLoss:0.0605 | top1:74.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2254 | MainLoss:0.2254 | SPLoss:0.0137 | CLSLoss:0.0604 | top1:96.3364 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3588 | MainLoss:0.3588 | SPLoss:0.0137 | CLSLoss:0.0604 | top1:86.0780 | AUROC:0.9934\n",
      "\n",
      "Epoch: [82 | 1000] LR: 0.015807\n",
      "Train | 8/8 | Loss:0.4806 | MainLoss:0.4501 | Alpha:0.4044 | SPLoss:0.0140 | CLSLoss:0.0615 | top1:80.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2197 | MainLoss:0.2197 | SPLoss:0.0143 | CLSLoss:0.0627 | top1:96.3271 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3546 | MainLoss:0.3546 | SPLoss:0.0143 | CLSLoss:0.0627 | top1:86.0223 | AUROC:0.9933\n",
      "\n",
      "Epoch: [83 | 1000] LR: 0.015802\n",
      "Train | 8/8 | Loss:0.5342 | MainLoss:0.5040 | Alpha:0.4057 | SPLoss:0.0140 | CLSLoss:0.0604 | top1:75.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2241 | MainLoss:0.2241 | SPLoss:0.0140 | CLSLoss:0.0599 | top1:96.4922 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3611 | MainLoss:0.3611 | SPLoss:0.0140 | CLSLoss:0.0599 | top1:85.7536 | AUROC:0.9933\n",
      "\n",
      "Epoch: [84 | 1000] LR: 0.015796\n",
      "Train | 8/8 | Loss:0.4996 | MainLoss:0.4695 | Alpha:0.4044 | SPLoss:0.0142 | CLSLoss:0.0603 | top1:75.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2268 | MainLoss:0.2268 | SPLoss:0.0140 | CLSLoss:0.0590 | top1:96.3614 | AUROC:0.9985\n",
      "Test | 62/8 | Loss:0.3602 | MainLoss:0.3602 | SPLoss:0.0140 | CLSLoss:0.0590 | top1:85.9797 | AUROC:0.9934\n",
      "\n",
      "Epoch: [85 | 1000] LR: 0.015791\n",
      "Train | 8/8 | Loss:0.5082 | MainLoss:0.4791 | Alpha:0.4052 | SPLoss:0.0139 | CLSLoss:0.0578 | top1:77.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:0.0145 | CLSLoss:0.0597 | top1:96.3832 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3586 | MainLoss:0.3586 | SPLoss:0.0145 | CLSLoss:0.0597 | top1:85.9469 | AUROC:0.9934\n",
      "\n",
      "Epoch: [86 | 1000] LR: 0.015785\n",
      "Train | 8/8 | Loss:0.5294 | MainLoss:0.4996 | Alpha:0.4054 | SPLoss:0.0143 | CLSLoss:0.0591 | top1:74.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:0.0137 | CLSLoss:0.0568 | top1:96.9657 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3780 | MainLoss:0.3780 | SPLoss:0.0137 | CLSLoss:0.0568 | top1:84.4004 | AUROC:0.9934\n",
      "\n",
      "Epoch: [87 | 1000] LR: 0.015779\n",
      "Train | 8/8 | Loss:0.5413 | MainLoss:0.5127 | Alpha:0.4035 | SPLoss:0.0139 | CLSLoss:0.0569 | top1:75.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2308 | MainLoss:0.2308 | SPLoss:0.0135 | CLSLoss:0.0544 | top1:96.9346 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3802 | MainLoss:0.3802 | SPLoss:0.0135 | CLSLoss:0.0544 | top1:84.5216 | AUROC:0.9933\n",
      "\n",
      "Epoch: [88 | 1000] LR: 0.015773\n",
      "Train | 8/8 | Loss:0.5271 | MainLoss:0.4996 | Alpha:0.4043 | SPLoss:0.0136 | CLSLoss:0.0544 | top1:70.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2303 | MainLoss:0.2303 | SPLoss:0.0134 | CLSLoss:0.0533 | top1:97.1526 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3867 | MainLoss:0.3867 | SPLoss:0.0134 | CLSLoss:0.0533 | top1:84.0727 | AUROC:0.9935\n",
      "\n",
      "Epoch: [89 | 1000] LR: 0.015767\n",
      "Train | 8/8 | Loss:0.5261 | MainLoss:0.4988 | Alpha:0.4041 | SPLoss:0.0136 | CLSLoss:0.0540 | top1:76.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2326 | MainLoss:0.2326 | SPLoss:0.0136 | CLSLoss:0.0533 | top1:96.8567 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3791 | MainLoss:0.3791 | SPLoss:0.0136 | CLSLoss:0.0533 | top1:84.7936 | AUROC:0.9933\n",
      "\n",
      "Epoch: [90 | 1000] LR: 0.015761\n",
      "Train | 8/8 | Loss:0.4803 | MainLoss:0.4520 | Alpha:0.4051 | SPLoss:0.0144 | CLSLoss:0.0557 | top1:81.7750 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.2219 | MainLoss:0.2219 | SPLoss:0.0145 | CLSLoss:0.0569 | top1:96.8474 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3710 | MainLoss:0.3710 | SPLoss:0.0145 | CLSLoss:0.0569 | top1:84.8460 | AUROC:0.9935\n",
      "\n",
      "Epoch: [91 | 1000] LR: 0.015755\n",
      "Train | 8/8 | Loss:0.4651 | MainLoss:0.4355 | Alpha:0.4044 | SPLoss:0.0148 | CLSLoss:0.0583 | top1:79.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2246 | MainLoss:0.2246 | SPLoss:0.0154 | CLSLoss:0.0610 | top1:96.6885 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3636 | MainLoss:0.3636 | SPLoss:0.0154 | CLSLoss:0.0610 | top1:85.3506 | AUROC:0.9935\n",
      "\n",
      "Epoch: [92 | 1000] LR: 0.015749\n",
      "Train | 8/8 | Loss:0.5767 | MainLoss:0.5469 | Alpha:0.4053 | SPLoss:0.0148 | CLSLoss:0.0585 | top1:74.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2267 | MainLoss:0.2267 | SPLoss:0.0141 | CLSLoss:0.0558 | top1:97.0031 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3797 | MainLoss:0.3797 | SPLoss:0.0141 | CLSLoss:0.0558 | top1:84.2792 | AUROC:0.9936\n",
      "\n",
      "Epoch: [93 | 1000] LR: 0.015742\n",
      "Train | 8/8 | Loss:0.5074 | MainLoss:0.4791 | Alpha:0.4049 | SPLoss:0.0141 | CLSLoss:0.0557 | top1:81.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2199 | MainLoss:0.2199 | SPLoss:0.0144 | CLSLoss:0.0569 | top1:97.1963 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3834 | MainLoss:0.3834 | SPLoss:0.0144 | CLSLoss:0.0569 | top1:83.7811 | AUROC:0.9935\n",
      "\n",
      "Epoch: [94 | 1000] LR: 0.015736\n",
      "Train | 8/8 | Loss:0.5482 | MainLoss:0.5197 | Alpha:0.4048 | SPLoss:0.0142 | CLSLoss:0.0563 | top1:73.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2387 | MainLoss:0.2387 | SPLoss:0.0135 | CLSLoss:0.0530 | top1:96.7664 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3813 | MainLoss:0.3813 | SPLoss:0.0135 | CLSLoss:0.0530 | top1:84.7936 | AUROC:0.9936\n",
      "\n",
      "Epoch: [95 | 1000] LR: 0.015730\n",
      "Train | 8/8 | Loss:0.5282 | MainLoss:0.5015 | Alpha:0.4060 | SPLoss:0.0133 | CLSLoss:0.0523 | top1:75.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2316 | MainLoss:0.2316 | SPLoss:0.0137 | CLSLoss:0.0538 | top1:96.8505 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3815 | MainLoss:0.3815 | SPLoss:0.0137 | CLSLoss:0.0538 | top1:84.5216 | AUROC:0.9935\n",
      "\n",
      "Epoch: [96 | 1000] LR: 0.015723\n",
      "Train | 8/8 | Loss:0.5183 | MainLoss:0.4903 | Alpha:0.4047 | SPLoss:0.0140 | CLSLoss:0.0551 | top1:76.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2298 | MainLoss:0.2298 | SPLoss:0.0139 | CLSLoss:0.0539 | top1:96.8723 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3819 | MainLoss:0.3819 | SPLoss:0.0139 | CLSLoss:0.0539 | top1:84.4266 | AUROC:0.9936\n",
      "\n",
      "Epoch: [97 | 1000] LR: 0.015716\n",
      "Train | 8/8 | Loss:0.4789 | MainLoss:0.4512 | Alpha:0.4058 | SPLoss:0.0141 | CLSLoss:0.0542 | top1:79.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2243 | MainLoss:0.2243 | SPLoss:0.0150 | CLSLoss:0.0580 | top1:96.3707 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3612 | MainLoss:0.3612 | SPLoss:0.0150 | CLSLoss:0.0580 | top1:85.8191 | AUROC:0.9935\n",
      "\n",
      "Epoch: [98 | 1000] LR: 0.015710\n",
      "Train | 8/8 | Loss:0.5177 | MainLoss:0.4885 | Alpha:0.4057 | SPLoss:0.0148 | CLSLoss:0.0572 | top1:78.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2234 | MainLoss:0.2234 | SPLoss:0.0150 | CLSLoss:0.0568 | top1:96.5857 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3647 | MainLoss:0.3647 | SPLoss:0.0150 | CLSLoss:0.0568 | top1:85.4620 | AUROC:0.9935\n",
      "\n",
      "Epoch: [99 | 1000] LR: 0.015703\n",
      "Train | 8/8 | Loss:0.4585 | MainLoss:0.4286 | Alpha:0.4045 | SPLoss:0.0154 | CLSLoss:0.0586 | top1:82.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2162 | MainLoss:0.2162 | SPLoss:0.0162 | CLSLoss:0.0619 | top1:96.3022 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3502 | MainLoss:0.3502 | SPLoss:0.0162 | CLSLoss:0.0619 | top1:86.1370 | AUROC:0.9933\n",
      "\n",
      "Epoch: [100 | 1000] LR: 0.015696\n",
      "Train | 8/8 | Loss:0.4836 | MainLoss:0.4524 | Alpha:0.4046 | SPLoss:0.0159 | CLSLoss:0.0613 | top1:75.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2155 | MainLoss:0.2155 | SPLoss:0.0162 | CLSLoss:0.0622 | top1:96.3271 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3493 | MainLoss:0.3493 | SPLoss:0.0162 | CLSLoss:0.0622 | top1:86.1828 | AUROC:0.9933\n",
      "\n",
      "Epoch: [101 | 1000] LR: 0.015689\n",
      "Train | 8/8 | Loss:0.4977 | MainLoss:0.4725 | Alpha:0.4054 | SPLoss:0.0000 | CLSLoss:0.0620 | top1:78.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2147 | MainLoss:0.2147 | SPLoss:0.0001 | CLSLoss:0.0601 | top1:96.5794 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3583 | MainLoss:0.3583 | SPLoss:0.0001 | CLSLoss:0.0601 | top1:85.5963 | AUROC:0.9934\n",
      "\n",
      "Epoch: [102 | 1000] LR: 0.015682\n",
      "Train | 8/8 | Loss:0.4499 | MainLoss:0.4245 | Alpha:0.4062 | SPLoss:0.0002 | CLSLoss:0.0623 | top1:82.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2107 | MainLoss:0.2107 | SPLoss:0.0004 | CLSLoss:0.0631 | top1:96.2087 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3423 | MainLoss:0.3423 | SPLoss:0.0004 | CLSLoss:0.0631 | top1:86.4974 | AUROC:0.9933\n",
      "\n",
      "Epoch: [103 | 1000] LR: 0.015675\n",
      "Train | 8/8 | Loss:0.5297 | MainLoss:0.5044 | Alpha:0.4057 | SPLoss:0.0006 | CLSLoss:0.0619 | top1:75.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2211 | MainLoss:0.2211 | SPLoss:0.0009 | CLSLoss:0.0573 | top1:97.1869 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3810 | MainLoss:0.3810 | SPLoss:0.0009 | CLSLoss:0.0573 | top1:83.9613 | AUROC:0.9933\n",
      "\n",
      "Epoch: [104 | 1000] LR: 0.015668\n",
      "Train | 8/8 | Loss:0.4704 | MainLoss:0.4456 | Alpha:0.4064 | SPLoss:0.0009 | CLSLoss:0.0600 | top1:80.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2192 | MainLoss:0.2192 | SPLoss:0.0009 | CLSLoss:0.0583 | top1:96.2710 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3512 | MainLoss:0.3512 | SPLoss:0.0009 | CLSLoss:0.0583 | top1:86.4482 | AUROC:0.9935\n",
      "\n",
      "Epoch: [105 | 1000] LR: 0.015661\n",
      "Train | 8/8 | Loss:0.4491 | MainLoss:0.4241 | Alpha:0.4067 | SPLoss:0.0011 | CLSLoss:0.0603 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2158 | MainLoss:0.2158 | SPLoss:0.0013 | CLSLoss:0.0608 | top1:95.8723 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3346 | MainLoss:0.3346 | SPLoss:0.0013 | CLSLoss:0.0608 | top1:87.3919 | AUROC:0.9933\n",
      "\n",
      "Epoch: [106 | 1000] LR: 0.015654\n",
      "Train | 8/8 | Loss:0.5316 | MainLoss:0.5074 | Alpha:0.4048 | SPLoss:0.0014 | CLSLoss:0.0584 | top1:74.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2159 | MainLoss:0.2159 | SPLoss:0.0016 | CLSLoss:0.0554 | top1:96.8380 | AUROC:0.9986\n",
      "Test | 62/8 | Loss:0.3690 | MainLoss:0.3690 | SPLoss:0.0016 | CLSLoss:0.0554 | top1:84.9705 | AUROC:0.9933\n",
      "\n",
      "Epoch: [107 | 1000] LR: 0.015646\n",
      "Train | 8/8 | Loss:0.5275 | MainLoss:0.5057 | Alpha:0.4064 | SPLoss:0.0017 | CLSLoss:0.0520 | top1:73.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2238 | MainLoss:0.2238 | SPLoss:0.0017 | CLSLoss:0.0521 | top1:96.6760 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3688 | MainLoss:0.3688 | SPLoss:0.0017 | CLSLoss:0.0521 | top1:85.4522 | AUROC:0.9933\n",
      "\n",
      "Epoch: [108 | 1000] LR: 0.015639\n",
      "Train | 8/8 | Loss:0.5008 | MainLoss:0.4793 | Alpha:0.4053 | SPLoss:0.0018 | CLSLoss:0.0512 | top1:77.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2152 | MainLoss:0.2152 | SPLoss:0.0020 | CLSLoss:0.0526 | top1:97.0779 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3752 | MainLoss:0.3752 | SPLoss:0.0020 | CLSLoss:0.0526 | top1:84.5511 | AUROC:0.9931\n",
      "\n",
      "Epoch: [109 | 1000] LR: 0.015631\n",
      "Train | 8/8 | Loss:0.5449 | MainLoss:0.5232 | Alpha:0.4069 | SPLoss:0.0021 | CLSLoss:0.0512 | top1:73.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2265 | MainLoss:0.2265 | SPLoss:0.0021 | CLSLoss:0.0483 | top1:97.0592 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3806 | MainLoss:0.3806 | SPLoss:0.0021 | CLSLoss:0.0483 | top1:84.6658 | AUROC:0.9933\n",
      "\n",
      "Epoch: [110 | 1000] LR: 0.015624\n",
      "Train | 8/8 | Loss:0.5339 | MainLoss:0.5141 | Alpha:0.4057 | SPLoss:0.0022 | CLSLoss:0.0466 | top1:75.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2274 | MainLoss:0.2274 | SPLoss:0.0024 | CLSLoss:0.0482 | top1:96.8567 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3746 | MainLoss:0.3746 | SPLoss:0.0024 | CLSLoss:0.0482 | top1:85.4030 | AUROC:0.9934\n",
      "\n",
      "Epoch: [111 | 1000] LR: 0.015616\n",
      "Train | 8/8 | Loss:0.4483 | MainLoss:0.4270 | Alpha:0.4060 | SPLoss:0.0026 | CLSLoss:0.0500 | top1:80.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2131 | MainLoss:0.2131 | SPLoss:0.0031 | CLSLoss:0.0543 | top1:96.3053 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3444 | MainLoss:0.3444 | SPLoss:0.0031 | CLSLoss:0.0543 | top1:86.9332 | AUROC:0.9931\n",
      "\n",
      "Epoch: [112 | 1000] LR: 0.015608\n",
      "Train | 8/8 | Loss:0.4749 | MainLoss:0.4512 | Alpha:0.4059 | SPLoss:0.0034 | CLSLoss:0.0550 | top1:81.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2094 | MainLoss:0.2094 | SPLoss:0.0036 | CLSLoss:0.0548 | top1:96.4766 | AUROC:0.9988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.3527 | MainLoss:0.3527 | SPLoss:0.0036 | CLSLoss:0.0548 | top1:86.2090 | AUROC:0.9930\n",
      "\n",
      "Epoch: [113 | 1000] LR: 0.015601\n",
      "Train | 8/8 | Loss:0.5242 | MainLoss:0.5010 | Alpha:0.4056 | SPLoss:0.0037 | CLSLoss:0.0534 | top1:75.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2193 | MainLoss:0.2193 | SPLoss:0.0037 | CLSLoss:0.0509 | top1:96.5701 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3623 | MainLoss:0.3623 | SPLoss:0.0037 | CLSLoss:0.0509 | top1:85.9797 | AUROC:0.9930\n",
      "\n",
      "Epoch: [114 | 1000] LR: 0.015593\n",
      "Train | 8/8 | Loss:0.5248 | MainLoss:0.5032 | Alpha:0.4057 | SPLoss:0.0037 | CLSLoss:0.0494 | top1:74.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2231 | MainLoss:0.2231 | SPLoss:0.0037 | CLSLoss:0.0478 | top1:96.8910 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3751 | MainLoss:0.3751 | SPLoss:0.0037 | CLSLoss:0.0478 | top1:85.1311 | AUROC:0.9930\n",
      "\n",
      "Epoch: [115 | 1000] LR: 0.015585\n",
      "Train | 8/8 | Loss:0.5463 | MainLoss:0.5263 | Alpha:0.4063 | SPLoss:0.0037 | CLSLoss:0.0455 | top1:72.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2326 | MainLoss:0.2326 | SPLoss:0.0037 | CLSLoss:0.0454 | top1:96.6012 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3731 | MainLoss:0.3731 | SPLoss:0.0037 | CLSLoss:0.0454 | top1:85.8847 | AUROC:0.9932\n",
      "\n",
      "Epoch: [116 | 1000] LR: 0.015577\n",
      "Train | 8/8 | Loss:0.4779 | MainLoss:0.4570 | Alpha:0.4057 | SPLoss:0.0040 | CLSLoss:0.0475 | top1:79.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2275 | MainLoss:0.2275 | SPLoss:0.0042 | CLSLoss:0.0485 | top1:96.1308 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3522 | MainLoss:0.3522 | SPLoss:0.0042 | CLSLoss:0.0485 | top1:87.1953 | AUROC:0.9931\n",
      "\n",
      "Epoch: [117 | 1000] LR: 0.015569\n",
      "Train | 8/8 | Loss:0.5067 | MainLoss:0.4854 | Alpha:0.4055 | SPLoss:0.0043 | CLSLoss:0.0482 | top1:76.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2175 | MainLoss:0.2175 | SPLoss:0.0046 | CLSLoss:0.0489 | top1:96.8193 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3711 | MainLoss:0.3711 | SPLoss:0.0046 | CLSLoss:0.0489 | top1:85.3604 | AUROC:0.9931\n",
      "\n",
      "Epoch: [118 | 1000] LR: 0.015561\n",
      "Train | 8/8 | Loss:0.4997 | MainLoss:0.4780 | Alpha:0.4062 | SPLoss:0.0047 | CLSLoss:0.0487 | top1:78.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2169 | MainLoss:0.2169 | SPLoss:0.0049 | CLSLoss:0.0488 | top1:96.9034 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3754 | MainLoss:0.3754 | SPLoss:0.0049 | CLSLoss:0.0488 | top1:84.9935 | AUROC:0.9931\n",
      "\n",
      "Epoch: [119 | 1000] LR: 0.015552\n",
      "Train | 8/8 | Loss:0.5005 | MainLoss:0.4785 | Alpha:0.4053 | SPLoss:0.0050 | CLSLoss:0.0493 | top1:78.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2232 | MainLoss:0.2232 | SPLoss:0.0051 | CLSLoss:0.0483 | top1:96.5919 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3676 | MainLoss:0.3676 | SPLoss:0.0051 | CLSLoss:0.0483 | top1:85.8290 | AUROC:0.9931\n",
      "\n",
      "Epoch: [120 | 1000] LR: 0.015544\n",
      "Train | 8/8 | Loss:0.5136 | MainLoss:0.4926 | Alpha:0.4066 | SPLoss:0.0051 | CLSLoss:0.0465 | top1:74.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2237 | MainLoss:0.2237 | SPLoss:0.0053 | CLSLoss:0.0487 | top1:96.4081 | AUROC:0.9987\n",
      "Test | 62/8 | Loss:0.3609 | MainLoss:0.3609 | SPLoss:0.0053 | CLSLoss:0.0487 | top1:86.3762 | AUROC:0.9931\n",
      "\n",
      "Epoch: [121 | 1000] LR: 0.015536\n",
      "Train | 8/8 | Loss:0.5318 | MainLoss:0.5102 | Alpha:0.4055 | SPLoss:0.0054 | CLSLoss:0.0479 | top1:76.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2193 | MainLoss:0.2193 | SPLoss:0.0055 | CLSLoss:0.0466 | top1:97.2897 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3897 | MainLoss:0.3897 | SPLoss:0.0055 | CLSLoss:0.0466 | top1:83.8172 | AUROC:0.9932\n",
      "\n",
      "Epoch: [122 | 1000] LR: 0.015527\n",
      "Train | 8/8 | Loss:0.4879 | MainLoss:0.4664 | Alpha:0.4055 | SPLoss:0.0056 | CLSLoss:0.0475 | top1:79.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2173 | MainLoss:0.2173 | SPLoss:0.0057 | CLSLoss:0.0483 | top1:96.7632 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3685 | MainLoss:0.3685 | SPLoss:0.0057 | CLSLoss:0.0483 | top1:85.5603 | AUROC:0.9932\n",
      "\n",
      "Epoch: [123 | 1000] LR: 0.015518\n",
      "Train | 8/8 | Loss:0.4553 | MainLoss:0.4321 | Alpha:0.4063 | SPLoss:0.0062 | CLSLoss:0.0509 | top1:83.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2160 | MainLoss:0.2160 | SPLoss:0.0065 | CLSLoss:0.0520 | top1:95.8474 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3359 | MainLoss:0.3359 | SPLoss:0.0065 | CLSLoss:0.0520 | top1:87.7261 | AUROC:0.9932\n",
      "\n",
      "Epoch: [124 | 1000] LR: 0.015510\n",
      "Train | 8/8 | Loss:0.4982 | MainLoss:0.4748 | Alpha:0.4055 | SPLoss:0.0065 | CLSLoss:0.0511 | top1:79.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2034 | MainLoss:0.2034 | SPLoss:0.0069 | CLSLoss:0.0519 | top1:96.8754 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3659 | MainLoss:0.3659 | SPLoss:0.0069 | CLSLoss:0.0519 | top1:85.3473 | AUROC:0.9931\n",
      "\n",
      "Epoch: [125 | 1000] LR: 0.015501\n",
      "Train | 8/8 | Loss:0.5274 | MainLoss:0.5041 | Alpha:0.4070 | SPLoss:0.0068 | CLSLoss:0.0505 | top1:71.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2286 | MainLoss:0.2286 | SPLoss:0.0065 | CLSLoss:0.0472 | top1:96.1589 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3550 | MainLoss:0.3550 | SPLoss:0.0065 | CLSLoss:0.0472 | top1:87.0872 | AUROC:0.9933\n",
      "\n",
      "Epoch: [126 | 1000] LR: 0.015492\n",
      "Train | 8/8 | Loss:0.4630 | MainLoss:0.4399 | Alpha:0.4049 | SPLoss:0.0070 | CLSLoss:0.0501 | top1:80.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2154 | MainLoss:0.2154 | SPLoss:0.0070 | CLSLoss:0.0497 | top1:96.4486 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3560 | MainLoss:0.3560 | SPLoss:0.0070 | CLSLoss:0.0497 | top1:86.4482 | AUROC:0.9933\n",
      "\n",
      "Epoch: [127 | 1000] LR: 0.015484\n",
      "Train | 8/8 | Loss:0.4982 | MainLoss:0.4754 | Alpha:0.4058 | SPLoss:0.0070 | CLSLoss:0.0492 | top1:77.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2225 | MainLoss:0.2225 | SPLoss:0.0070 | CLSLoss:0.0488 | top1:96.1963 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3499 | MainLoss:0.3499 | SPLoss:0.0070 | CLSLoss:0.0488 | top1:87.1461 | AUROC:0.9934\n",
      "\n",
      "Epoch: [128 | 1000] LR: 0.015475\n",
      "Train | 8/8 | Loss:0.4762 | MainLoss:0.4538 | Alpha:0.4055 | SPLoss:0.0071 | CLSLoss:0.0483 | top1:79.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2119 | MainLoss:0.2119 | SPLoss:0.0074 | CLSLoss:0.0504 | top1:96.4081 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3493 | MainLoss:0.3493 | SPLoss:0.0074 | CLSLoss:0.0504 | top1:86.6645 | AUROC:0.9934\n",
      "\n",
      "Epoch: [129 | 1000] LR: 0.015466\n",
      "Train | 8/8 | Loss:0.4508 | MainLoss:0.4267 | Alpha:0.4057 | SPLoss:0.0077 | CLSLoss:0.0517 | top1:82.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2047 | MainLoss:0.2047 | SPLoss:0.0081 | CLSLoss:0.0537 | top1:96.2274 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3365 | MainLoss:0.3365 | SPLoss:0.0081 | CLSLoss:0.0537 | top1:87.2838 | AUROC:0.9932\n",
      "\n",
      "Epoch: [130 | 1000] LR: 0.015457\n",
      "Train | 8/8 | Loss:0.4729 | MainLoss:0.4480 | Alpha:0.4067 | SPLoss:0.0080 | CLSLoss:0.0532 | top1:80.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2039 | MainLoss:0.2039 | SPLoss:0.0082 | CLSLoss:0.0540 | top1:96.3925 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3441 | MainLoss:0.3441 | SPLoss:0.0082 | CLSLoss:0.0540 | top1:86.7955 | AUROC:0.9933\n",
      "\n",
      "Epoch: [131 | 1000] LR: 0.015447\n",
      "Train | 8/8 | Loss:0.5072 | MainLoss:0.4833 | Alpha:0.4058 | SPLoss:0.0078 | CLSLoss:0.0512 | top1:74.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2177 | MainLoss:0.2177 | SPLoss:0.0079 | CLSLoss:0.0507 | top1:96.1651 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3448 | MainLoss:0.3448 | SPLoss:0.0079 | CLSLoss:0.0507 | top1:87.3755 | AUROC:0.9933\n",
      "\n",
      "Epoch: [132 | 1000] LR: 0.015438\n",
      "Train | 8/8 | Loss:0.4603 | MainLoss:0.4359 | Alpha:0.4053 | SPLoss:0.0081 | CLSLoss:0.0519 | top1:80.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2180 | MainLoss:0.2180 | SPLoss:0.0084 | CLSLoss:0.0530 | top1:95.4548 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3228 | MainLoss:0.3228 | SPLoss:0.0084 | CLSLoss:0.0530 | top1:88.6959 | AUROC:0.9933\n",
      "\n",
      "Epoch: [133 | 1000] LR: 0.015429\n",
      "Train | 8/8 | Loss:0.4537 | MainLoss:0.4289 | Alpha:0.4049 | SPLoss:0.0084 | CLSLoss:0.0528 | top1:81.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1993 | MainLoss:0.1993 | SPLoss:0.0092 | CLSLoss:0.0565 | top1:96.0467 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3285 | MainLoss:0.3285 | SPLoss:0.0092 | CLSLoss:0.0565 | top1:87.7228 | AUROC:0.9932\n",
      "\n",
      "Epoch: [134 | 1000] LR: 0.015420\n",
      "Train | 8/8 | Loss:0.4449 | MainLoss:0.4186 | Alpha:0.4059 | SPLoss:0.0090 | CLSLoss:0.0558 | top1:83.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1973 | MainLoss:0.1973 | SPLoss:0.0095 | CLSLoss:0.0582 | top1:95.8131 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3190 | MainLoss:0.3190 | SPLoss:0.0095 | CLSLoss:0.0582 | top1:88.1782 | AUROC:0.9931\n",
      "\n",
      "Epoch: [135 | 1000] LR: 0.015410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.4910 | MainLoss:0.4645 | Alpha:0.4062 | SPLoss:0.0091 | CLSLoss:0.0563 | top1:78.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2141 | MainLoss:0.2141 | SPLoss:0.0090 | CLSLoss:0.0552 | top1:95.7352 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3263 | MainLoss:0.3263 | SPLoss:0.0090 | CLSLoss:0.0552 | top1:88.2339 | AUROC:0.9930\n",
      "\n",
      "Epoch: [136 | 1000] LR: 0.015401\n",
      "Train | 8/8 | Loss:0.5742 | MainLoss:0.5502 | Alpha:0.4048 | SPLoss:0.0085 | CLSLoss:0.0508 | top1:73.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2262 | MainLoss:0.2262 | SPLoss:0.0083 | CLSLoss:0.0460 | top1:97.1184 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3816 | MainLoss:0.3816 | SPLoss:0.0083 | CLSLoss:0.0460 | top1:84.9148 | AUROC:0.9932\n",
      "\n",
      "Epoch: [137 | 1000] LR: 0.015391\n",
      "Train | 8/8 | Loss:0.5580 | MainLoss:0.5366 | Alpha:0.4056 | SPLoss:0.0083 | CLSLoss:0.0445 | top1:67.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2472 | MainLoss:0.2472 | SPLoss:0.0081 | CLSLoss:0.0419 | top1:96.4704 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3741 | MainLoss:0.3741 | SPLoss:0.0081 | CLSLoss:0.0419 | top1:86.6284 | AUROC:0.9934\n",
      "\n",
      "Epoch: [138 | 1000] LR: 0.015381\n",
      "Train | 8/8 | Loss:0.4533 | MainLoss:0.4314 | Alpha:0.4062 | SPLoss:0.0083 | CLSLoss:0.0455 | top1:82.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2149 | MainLoss:0.2149 | SPLoss:0.0086 | CLSLoss:0.0500 | top1:96.4735 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3551 | MainLoss:0.3551 | SPLoss:0.0086 | CLSLoss:0.0500 | top1:86.6973 | AUROC:0.9932\n",
      "\n",
      "Epoch: [139 | 1000] LR: 0.015372\n",
      "Train | 8/8 | Loss:0.5010 | MainLoss:0.4778 | Alpha:0.4061 | SPLoss:0.0085 | CLSLoss:0.0488 | top1:75.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2099 | MainLoss:0.2099 | SPLoss:0.0089 | CLSLoss:0.0504 | top1:96.4829 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3518 | MainLoss:0.3518 | SPLoss:0.0089 | CLSLoss:0.0504 | top1:86.8644 | AUROC:0.9931\n",
      "\n",
      "Epoch: [140 | 1000] LR: 0.015362\n",
      "Train | 8/8 | Loss:0.4567 | MainLoss:0.4318 | Alpha:0.4045 | SPLoss:0.0092 | CLSLoss:0.0523 | top1:82.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2047 | MainLoss:0.2047 | SPLoss:0.0093 | CLSLoss:0.0528 | top1:96.3209 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3424 | MainLoss:0.3424 | SPLoss:0.0093 | CLSLoss:0.0528 | top1:87.2477 | AUROC:0.9931\n",
      "\n",
      "Epoch: [141 | 1000] LR: 0.015352\n",
      "Train | 8/8 | Loss:0.5199 | MainLoss:0.4963 | Alpha:0.4055 | SPLoss:0.0089 | CLSLoss:0.0493 | top1:74.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2118 | MainLoss:0.2118 | SPLoss:0.0088 | CLSLoss:0.0499 | top1:96.7072 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3583 | MainLoss:0.3583 | SPLoss:0.0088 | CLSLoss:0.0499 | top1:86.4613 | AUROC:0.9932\n",
      "\n",
      "Epoch: [142 | 1000] LR: 0.015342\n",
      "Train | 8/8 | Loss:0.4394 | MainLoss:0.4151 | Alpha:0.4061 | SPLoss:0.0090 | CLSLoss:0.0507 | top1:81.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2007 | MainLoss:0.2007 | SPLoss:0.0097 | CLSLoss:0.0549 | top1:96.1495 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3320 | MainLoss:0.3320 | SPLoss:0.0097 | CLSLoss:0.0549 | top1:87.7326 | AUROC:0.9931\n",
      "\n",
      "Epoch: [143 | 1000] LR: 0.015332\n",
      "Train | 8/8 | Loss:0.5027 | MainLoss:0.4775 | Alpha:0.4045 | SPLoss:0.0093 | CLSLoss:0.0529 | top1:80.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2026 | MainLoss:0.2026 | SPLoss:0.0093 | CLSLoss:0.0525 | top1:96.6729 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3525 | MainLoss:0.3525 | SPLoss:0.0093 | CLSLoss:0.0525 | top1:86.3893 | AUROC:0.9932\n",
      "\n",
      "Epoch: [144 | 1000] LR: 0.015322\n",
      "Train | 8/8 | Loss:0.4827 | MainLoss:0.4581 | Alpha:0.4054 | SPLoss:0.0092 | CLSLoss:0.0515 | top1:80.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2028 | MainLoss:0.2028 | SPLoss:0.0094 | CLSLoss:0.0526 | top1:96.6449 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3510 | MainLoss:0.3510 | SPLoss:0.0094 | CLSLoss:0.0526 | top1:86.3827 | AUROC:0.9931\n",
      "\n",
      "Epoch: [145 | 1000] LR: 0.015312\n",
      "Train | 8/8 | Loss:0.5118 | MainLoss:0.4866 | Alpha:0.4051 | SPLoss:0.0095 | CLSLoss:0.0529 | top1:74.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2182 | MainLoss:0.2182 | SPLoss:0.0089 | CLSLoss:0.0487 | top1:96.6978 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3625 | MainLoss:0.3625 | SPLoss:0.0089 | CLSLoss:0.0487 | top1:86.2582 | AUROC:0.9930\n",
      "\n",
      "Epoch: [146 | 1000] LR: 0.015302\n",
      "Train | 8/8 | Loss:0.4785 | MainLoss:0.4550 | Alpha:0.4054 | SPLoss:0.0090 | CLSLoss:0.0489 | top1:80.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2122 | MainLoss:0.2122 | SPLoss:0.0093 | CLSLoss:0.0511 | top1:96.4081 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3484 | MainLoss:0.3484 | SPLoss:0.0093 | CLSLoss:0.0511 | top1:87.0118 | AUROC:0.9931\n",
      "\n",
      "Epoch: [147 | 1000] LR: 0.015291\n",
      "Train | 8/8 | Loss:0.4205 | MainLoss:0.3947 | Alpha:0.4053 | SPLoss:0.0099 | CLSLoss:0.0539 | top1:83.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2000 | MainLoss:0.2000 | SPLoss:0.0105 | CLSLoss:0.0568 | top1:95.9065 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3222 | MainLoss:0.3222 | SPLoss:0.0105 | CLSLoss:0.0568 | top1:88.0898 | AUROC:0.9930\n",
      "\n",
      "Epoch: [148 | 1000] LR: 0.015281\n",
      "Train | 8/8 | Loss:0.4490 | MainLoss:0.4212 | Alpha:0.4048 | SPLoss:0.0107 | CLSLoss:0.0580 | top1:82.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2207 | MainLoss:0.2207 | SPLoss:0.0112 | CLSLoss:0.0569 | top1:97.0966 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3721 | MainLoss:0.3721 | SPLoss:0.0112 | CLSLoss:0.0569 | top1:85.2654 | AUROC:0.9931\n",
      "\n",
      "Epoch: [149 | 1000] LR: 0.015270\n",
      "Train | 8/8 | Loss:0.4111 | MainLoss:0.3825 | Alpha:0.4061 | SPLoss:0.0110 | CLSLoss:0.0595 | top1:84.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1897 | MainLoss:0.1897 | SPLoss:0.0113 | CLSLoss:0.0625 | top1:96.1308 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3241 | MainLoss:0.3241 | SPLoss:0.0113 | CLSLoss:0.0625 | top1:87.6409 | AUROC:0.9931\n",
      "\n",
      "Epoch: [150 | 1000] LR: 0.015260\n",
      "Train | 8/8 | Loss:0.5709 | MainLoss:0.5438 | Alpha:0.4056 | SPLoss:0.0105 | CLSLoss:0.0561 | top1:71.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2117 | MainLoss:0.2117 | SPLoss:0.0100 | CLSLoss:0.0512 | top1:96.9221 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3674 | MainLoss:0.3674 | SPLoss:0.0100 | CLSLoss:0.0512 | top1:85.5439 | AUROC:0.9934\n",
      "\n",
      "Epoch: [151 | 1000] LR: 0.015249\n",
      "Train | 8/8 | Loss:0.4916 | MainLoss:0.4707 | Alpha:0.4071 | SPLoss:0.0001 | CLSLoss:0.0512 | top1:78.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2134 | MainLoss:0.2134 | SPLoss:0.0001 | CLSLoss:0.0510 | top1:96.5265 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3548 | MainLoss:0.3548 | SPLoss:0.0001 | CLSLoss:0.0510 | top1:86.6973 | AUROC:0.9935\n",
      "\n",
      "Epoch: [152 | 1000] LR: 0.015239\n",
      "Train | 8/8 | Loss:0.5279 | MainLoss:0.5079 | Alpha:0.4065 | SPLoss:0.0002 | CLSLoss:0.0489 | top1:73.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:0.0002 | CLSLoss:0.0470 | top1:96.6480 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3651 | MainLoss:0.3651 | SPLoss:0.0002 | CLSLoss:0.0470 | top1:86.3532 | AUROC:0.9935\n",
      "\n",
      "Epoch: [153 | 1000] LR: 0.015228\n",
      "Train | 8/8 | Loss:0.4479 | MainLoss:0.4277 | Alpha:0.4061 | SPLoss:0.0004 | CLSLoss:0.0494 | top1:83.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2143 | MainLoss:0.2143 | SPLoss:0.0006 | CLSLoss:0.0517 | top1:95.8318 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3325 | MainLoss:0.3325 | SPLoss:0.0006 | CLSLoss:0.0517 | top1:88.2929 | AUROC:0.9933\n",
      "\n",
      "Epoch: [154 | 1000] LR: 0.015217\n",
      "Train | 8/8 | Loss:0.4563 | MainLoss:0.4348 | Alpha:0.4056 | SPLoss:0.0007 | CLSLoss:0.0523 | top1:80.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2013 | MainLoss:0.2013 | SPLoss:0.0009 | CLSLoss:0.0543 | top1:96.1807 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3366 | MainLoss:0.3366 | SPLoss:0.0009 | CLSLoss:0.0543 | top1:87.5098 | AUROC:0.9933\n",
      "\n",
      "Epoch: [155 | 1000] LR: 0.015206\n",
      "Train | 8/8 | Loss:0.4991 | MainLoss:0.4772 | Alpha:0.4063 | SPLoss:0.0009 | CLSLoss:0.0530 | top1:79.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2067 | MainLoss:0.2067 | SPLoss:0.0008 | CLSLoss:0.0507 | top1:96.7103 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3565 | MainLoss:0.3565 | SPLoss:0.0008 | CLSLoss:0.0507 | top1:86.3925 | AUROC:0.9934\n",
      "\n",
      "Epoch: [156 | 1000] LR: 0.015195\n",
      "Train | 8/8 | Loss:0.4662 | MainLoss:0.4452 | Alpha:0.4076 | SPLoss:0.0010 | CLSLoss:0.0506 | top1:80.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2076 | MainLoss:0.2076 | SPLoss:0.0013 | CLSLoss:0.0528 | top1:95.8723 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3320 | MainLoss:0.3320 | SPLoss:0.0013 | CLSLoss:0.0528 | top1:88.1848 | AUROC:0.9934\n",
      "\n",
      "Epoch: [157 | 1000] LR: 0.015184\n",
      "Train | 8/8 | Loss:0.5095 | MainLoss:0.4875 | Alpha:0.4065 | SPLoss:0.0015 | CLSLoss:0.0527 | top1:73.8750 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.2192 | MainLoss:0.2192 | SPLoss:0.0014 | CLSLoss:0.0486 | top1:96.0405 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3475 | MainLoss:0.3475 | SPLoss:0.0014 | CLSLoss:0.0486 | top1:87.5459 | AUROC:0.9934\n",
      "\n",
      "Epoch: [158 | 1000] LR: 0.015173\n",
      "Train | 8/8 | Loss:0.4783 | MainLoss:0.4579 | Alpha:0.4067 | SPLoss:0.0015 | CLSLoss:0.0487 | top1:80.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2113 | MainLoss:0.2113 | SPLoss:0.0017 | CLSLoss:0.0498 | top1:96.1308 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3447 | MainLoss:0.3447 | SPLoss:0.0017 | CLSLoss:0.0498 | top1:87.5819 | AUROC:0.9933\n",
      "\n",
      "Epoch: [159 | 1000] LR: 0.015162\n",
      "Train | 8/8 | Loss:0.4487 | MainLoss:0.4269 | Alpha:0.4065 | SPLoss:0.0020 | CLSLoss:0.0517 | top1:83.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2087 | MainLoss:0.2087 | SPLoss:0.0022 | CLSLoss:0.0520 | top1:95.7508 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3296 | MainLoss:0.3296 | SPLoss:0.0022 | CLSLoss:0.0520 | top1:88.3814 | AUROC:0.9933\n",
      "\n",
      "Epoch: [160 | 1000] LR: 0.015151\n",
      "Train | 8/8 | Loss:0.4979 | MainLoss:0.4761 | Alpha:0.4062 | SPLoss:0.0022 | CLSLoss:0.0514 | top1:77.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2089 | MainLoss:0.2089 | SPLoss:0.0021 | CLSLoss:0.0488 | top1:96.6480 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3574 | MainLoss:0.3574 | SPLoss:0.0021 | CLSLoss:0.0488 | top1:86.5531 | AUROC:0.9933\n",
      "\n",
      "Epoch: [161 | 1000] LR: 0.015139\n",
      "Train | 8/8 | Loss:0.4839 | MainLoss:0.4628 | Alpha:0.4069 | SPLoss:0.0023 | CLSLoss:0.0495 | top1:79.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1999 | MainLoss:0.1999 | SPLoss:0.0024 | CLSLoss:0.0491 | top1:97.1153 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3692 | MainLoss:0.3692 | SPLoss:0.0024 | CLSLoss:0.0491 | top1:85.4325 | AUROC:0.9931\n",
      "\n",
      "Epoch: [162 | 1000] LR: 0.015128\n",
      "Train | 8/8 | Loss:0.5015 | MainLoss:0.4806 | Alpha:0.4057 | SPLoss:0.0025 | CLSLoss:0.0490 | top1:78.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2077 | MainLoss:0.2077 | SPLoss:0.0025 | CLSLoss:0.0472 | top1:97.0561 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3715 | MainLoss:0.3715 | SPLoss:0.0025 | CLSLoss:0.0472 | top1:85.5210 | AUROC:0.9931\n",
      "\n",
      "Epoch: [163 | 1000] LR: 0.015117\n",
      "Train | 8/8 | Loss:0.4503 | MainLoss:0.4291 | Alpha:0.4058 | SPLoss:0.0028 | CLSLoss:0.0492 | top1:80.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2039 | MainLoss:0.2039 | SPLoss:0.0030 | CLSLoss:0.0501 | top1:96.3022 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3447 | MainLoss:0.3447 | SPLoss:0.0030 | CLSLoss:0.0501 | top1:87.3722 | AUROC:0.9930\n",
      "\n",
      "Epoch: [164 | 1000] LR: 0.015105\n",
      "Train | 8/8 | Loss:0.4460 | MainLoss:0.4242 | Alpha:0.4070 | SPLoss:0.0033 | CLSLoss:0.0504 | top1:80.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1995 | MainLoss:0.1995 | SPLoss:0.0037 | CLSLoss:0.0527 | top1:95.8006 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3251 | MainLoss:0.3251 | SPLoss:0.0037 | CLSLoss:0.0527 | top1:88.4011 | AUROC:0.9931\n",
      "\n",
      "Epoch: [165 | 1000] LR: 0.015094\n",
      "Train | 8/8 | Loss:0.4802 | MainLoss:0.4580 | Alpha:0.4061 | SPLoss:0.0036 | CLSLoss:0.0511 | top1:78.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1906 | MainLoss:0.1906 | SPLoss:0.0037 | CLSLoss:0.0511 | top1:97.0997 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3616 | MainLoss:0.3616 | SPLoss:0.0037 | CLSLoss:0.0511 | top1:85.5996 | AUROC:0.9931\n",
      "\n",
      "Epoch: [166 | 1000] LR: 0.015082\n",
      "Train | 8/8 | Loss:0.4810 | MainLoss:0.4596 | Alpha:0.4058 | SPLoss:0.0036 | CLSLoss:0.0491 | top1:79.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1971 | MainLoss:0.1971 | SPLoss:0.0039 | CLSLoss:0.0510 | top1:96.6199 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3464 | MainLoss:0.3464 | SPLoss:0.0039 | CLSLoss:0.0510 | top1:86.8906 | AUROC:0.9931\n",
      "\n",
      "Epoch: [167 | 1000] LR: 0.015070\n",
      "Train | 8/8 | Loss:0.4447 | MainLoss:0.4219 | Alpha:0.4055 | SPLoss:0.0042 | CLSLoss:0.0520 | top1:76.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2042 | MainLoss:0.2042 | SPLoss:0.0044 | CLSLoss:0.0522 | top1:95.7103 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3212 | MainLoss:0.3212 | SPLoss:0.0044 | CLSLoss:0.0522 | top1:88.6501 | AUROC:0.9932\n",
      "\n",
      "Epoch: [168 | 1000] LR: 0.015058\n",
      "Train | 8/8 | Loss:0.4783 | MainLoss:0.4553 | Alpha:0.4064 | SPLoss:0.0045 | CLSLoss:0.0520 | top1:79.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1999 | MainLoss:0.1999 | SPLoss:0.0045 | CLSLoss:0.0510 | top1:96.4393 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3427 | MainLoss:0.3427 | SPLoss:0.0045 | CLSLoss:0.0510 | top1:87.2608 | AUROC:0.9932\n",
      "\n",
      "Epoch: [169 | 1000] LR: 0.015046\n",
      "Train | 8/8 | Loss:0.4448 | MainLoss:0.4220 | Alpha:0.4066 | SPLoss:0.0046 | CLSLoss:0.0513 | top1:82.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1964 | MainLoss:0.1964 | SPLoss:0.0051 | CLSLoss:0.0534 | top1:96.0062 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3265 | MainLoss:0.3265 | SPLoss:0.0051 | CLSLoss:0.0534 | top1:88.2045 | AUROC:0.9934\n",
      "\n",
      "Epoch: [170 | 1000] LR: 0.015035\n",
      "Train | 8/8 | Loss:0.4931 | MainLoss:0.4698 | Alpha:0.4071 | SPLoss:0.0051 | CLSLoss:0.0520 | top1:77.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2063 | MainLoss:0.2063 | SPLoss:0.0051 | CLSLoss:0.0503 | top1:96.0062 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3335 | MainLoss:0.3335 | SPLoss:0.0051 | CLSLoss:0.0503 | top1:88.1782 | AUROC:0.9935\n",
      "\n",
      "Epoch: [171 | 1000] LR: 0.015023\n",
      "Train | 8/8 | Loss:0.4328 | MainLoss:0.4097 | Alpha:0.4055 | SPLoss:0.0054 | CLSLoss:0.0516 | top1:82.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1967 | MainLoss:0.1967 | SPLoss:0.0057 | CLSLoss:0.0536 | top1:95.8847 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3224 | MainLoss:0.3224 | SPLoss:0.0057 | CLSLoss:0.0536 | top1:88.5059 | AUROC:0.9935\n",
      "\n",
      "Epoch: [172 | 1000] LR: 0.015010\n",
      "Train | 8/8 | Loss:0.4789 | MainLoss:0.4545 | Alpha:0.4064 | SPLoss:0.0059 | CLSLoss:0.0542 | top1:79.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2096 | MainLoss:0.2096 | SPLoss:0.0056 | CLSLoss:0.0505 | top1:96.0654 | AUROC:0.9988\n",
      "Test | 62/8 | Loss:0.3340 | MainLoss:0.3340 | SPLoss:0.0056 | CLSLoss:0.0505 | top1:88.1029 | AUROC:0.9934\n",
      "\n",
      "Epoch: [173 | 1000] LR: 0.014998\n",
      "Train | 8/8 | Loss:0.4924 | MainLoss:0.4701 | Alpha:0.4058 | SPLoss:0.0055 | CLSLoss:0.0496 | top1:78.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2051 | MainLoss:0.2051 | SPLoss:0.0057 | CLSLoss:0.0498 | top1:96.5545 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3465 | MainLoss:0.3465 | SPLoss:0.0057 | CLSLoss:0.0498 | top1:87.1953 | AUROC:0.9935\n",
      "\n",
      "Epoch: [174 | 1000] LR: 0.014986\n",
      "Train | 8/8 | Loss:0.4575 | MainLoss:0.4347 | Alpha:0.4066 | SPLoss:0.0059 | CLSLoss:0.0504 | top1:79.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1929 | MainLoss:0.1929 | SPLoss:0.0061 | CLSLoss:0.0517 | top1:96.7072 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3454 | MainLoss:0.3454 | SPLoss:0.0061 | CLSLoss:0.0517 | top1:86.9397 | AUROC:0.9934\n",
      "\n",
      "Epoch: [175 | 1000] LR: 0.014974\n",
      "Train | 8/8 | Loss:0.5014 | MainLoss:0.4790 | Alpha:0.4061 | SPLoss:0.0059 | CLSLoss:0.0492 | top1:77.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2053 | MainLoss:0.2053 | SPLoss:0.0060 | CLSLoss:0.0495 | top1:96.2710 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3364 | MainLoss:0.3364 | SPLoss:0.0060 | CLSLoss:0.0495 | top1:87.9849 | AUROC:0.9935\n",
      "\n",
      "Epoch: [176 | 1000] LR: 0.014961\n",
      "Train | 8/8 | Loss:0.4256 | MainLoss:0.4023 | Alpha:0.4063 | SPLoss:0.0062 | CLSLoss:0.0512 | top1:84.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1992 | MainLoss:0.1992 | SPLoss:0.0068 | CLSLoss:0.0542 | top1:95.4579 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3077 | MainLoss:0.3077 | SPLoss:0.0068 | CLSLoss:0.0542 | top1:89.5347 | AUROC:0.9934\n",
      "\n",
      "Epoch: [177 | 1000] LR: 0.014949\n",
      "Train | 8/8 | Loss:0.5085 | MainLoss:0.4836 | Alpha:0.4071 | SPLoss:0.0069 | CLSLoss:0.0541 | top1:79.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2028 | MainLoss:0.2028 | SPLoss:0.0063 | CLSLoss:0.0486 | top1:96.9097 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3551 | MainLoss:0.3551 | SPLoss:0.0063 | CLSLoss:0.0486 | top1:86.7398 | AUROC:0.9935\n",
      "\n",
      "Epoch: [178 | 1000] LR: 0.014937\n",
      "Train | 8/8 | Loss:0.5076 | MainLoss:0.4853 | Alpha:0.4060 | SPLoss:0.0064 | CLSLoss:0.0487 | top1:76.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2108 | MainLoss:0.2108 | SPLoss:0.0062 | CLSLoss:0.0457 | top1:96.9065 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3601 | MainLoss:0.3601 | SPLoss:0.0062 | CLSLoss:0.0457 | top1:86.7759 | AUROC:0.9936\n",
      "\n",
      "Epoch: [179 | 1000] LR: 0.014924\n",
      "Train | 8/8 | Loss:0.5055 | MainLoss:0.4841 | Alpha:0.4059 | SPLoss:0.0062 | CLSLoss:0.0463 | top1:76.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2141 | MainLoss:0.2141 | SPLoss:0.0063 | CLSLoss:0.0455 | top1:96.7508 | AUROC:0.9990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.3562 | MainLoss:0.3562 | SPLoss:0.0063 | CLSLoss:0.0455 | top1:87.2051 | AUROC:0.9935\n",
      "\n",
      "Epoch: [180 | 1000] LR: 0.014911\n",
      "Train | 8/8 | Loss:0.5058 | MainLoss:0.4851 | Alpha:0.4064 | SPLoss:0.0062 | CLSLoss:0.0448 | top1:77.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2151 | MainLoss:0.2151 | SPLoss:0.0062 | CLSLoss:0.0444 | top1:96.7819 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3582 | MainLoss:0.3582 | SPLoss:0.0062 | CLSLoss:0.0444 | top1:87.1461 | AUROC:0.9934\n",
      "\n",
      "Epoch: [181 | 1000] LR: 0.014899\n",
      "Train | 8/8 | Loss:0.4589 | MainLoss:0.4376 | Alpha:0.4062 | SPLoss:0.0064 | CLSLoss:0.0461 | top1:82.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2021 | MainLoss:0.2021 | SPLoss:0.0065 | CLSLoss:0.0479 | top1:96.5608 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3461 | MainLoss:0.3461 | SPLoss:0.0065 | CLSLoss:0.0479 | top1:87.6180 | AUROC:0.9934\n",
      "\n",
      "Epoch: [182 | 1000] LR: 0.014886\n",
      "Train | 8/8 | Loss:0.4767 | MainLoss:0.4536 | Alpha:0.4052 | SPLoss:0.0068 | CLSLoss:0.0500 | top1:79.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2143 | MainLoss:0.2143 | SPLoss:0.0066 | CLSLoss:0.0472 | top1:95.9439 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3336 | MainLoss:0.3336 | SPLoss:0.0066 | CLSLoss:0.0472 | top1:88.7942 | AUROC:0.9935\n",
      "\n",
      "Epoch: [183 | 1000] LR: 0.014873\n",
      "Train | 8/8 | Loss:0.4484 | MainLoss:0.4260 | Alpha:0.4070 | SPLoss:0.0067 | CLSLoss:0.0483 | top1:77.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2025 | MainLoss:0.2025 | SPLoss:0.0069 | CLSLoss:0.0500 | top1:96.0499 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3290 | MainLoss:0.3290 | SPLoss:0.0069 | CLSLoss:0.0500 | top1:88.6271 | AUROC:0.9933\n",
      "\n",
      "Epoch: [184 | 1000] LR: 0.014860\n",
      "Train | 8/8 | Loss:0.4410 | MainLoss:0.4178 | Alpha:0.4066 | SPLoss:0.0070 | CLSLoss:0.0502 | top1:84.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1937 | MainLoss:0.1937 | SPLoss:0.0075 | CLSLoss:0.0527 | top1:95.8380 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3173 | MainLoss:0.3173 | SPLoss:0.0075 | CLSLoss:0.0527 | top1:88.9744 | AUROC:0.9932\n",
      "\n",
      "Epoch: [185 | 1000] LR: 0.014847\n",
      "Train | 8/8 | Loss:0.4865 | MainLoss:0.4624 | Alpha:0.4061 | SPLoss:0.0076 | CLSLoss:0.0517 | top1:79.6000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1966 | MainLoss:0.1966 | SPLoss:0.0073 | CLSLoss:0.0501 | top1:96.5919 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3440 | MainLoss:0.3440 | SPLoss:0.0073 | CLSLoss:0.0501 | top1:87.5066 | AUROC:0.9932\n",
      "\n",
      "Epoch: [186 | 1000] LR: 0.014834\n",
      "Train | 8/8 | Loss:0.5112 | MainLoss:0.4884 | Alpha:0.4064 | SPLoss:0.0072 | CLSLoss:0.0489 | top1:73.5500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2155 | MainLoss:0.2155 | SPLoss:0.0070 | CLSLoss:0.0458 | top1:96.2025 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3432 | MainLoss:0.3432 | SPLoss:0.0070 | CLSLoss:0.0458 | top1:88.3028 | AUROC:0.9934\n",
      "\n",
      "Epoch: [187 | 1000] LR: 0.014821\n",
      "Train | 8/8 | Loss:0.5036 | MainLoss:0.4815 | Alpha:0.4062 | SPLoss:0.0071 | CLSLoss:0.0474 | top1:78.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2136 | MainLoss:0.2136 | SPLoss:0.0068 | CLSLoss:0.0439 | top1:97.1464 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3709 | MainLoss:0.3709 | SPLoss:0.0068 | CLSLoss:0.0439 | top1:86.1370 | AUROC:0.9936\n",
      "\n",
      "Epoch: [188 | 1000] LR: 0.014808\n",
      "Train | 8/8 | Loss:0.5292 | MainLoss:0.5087 | Alpha:0.4062 | SPLoss:0.0068 | CLSLoss:0.0436 | top1:74.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2220 | MainLoss:0.2220 | SPLoss:0.0068 | CLSLoss:0.0422 | top1:96.9159 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3673 | MainLoss:0.3673 | SPLoss:0.0068 | CLSLoss:0.0422 | top1:86.8709 | AUROC:0.9936\n",
      "\n",
      "Epoch: [189 | 1000] LR: 0.014795\n",
      "Train | 8/8 | Loss:0.4580 | MainLoss:0.4370 | Alpha:0.4063 | SPLoss:0.0069 | CLSLoss:0.0447 | top1:77.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2146 | MainLoss:0.2146 | SPLoss:0.0071 | CLSLoss:0.0465 | top1:96.1090 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3396 | MainLoss:0.3396 | SPLoss:0.0071 | CLSLoss:0.0465 | top1:88.3978 | AUROC:0.9934\n",
      "\n",
      "Epoch: [190 | 1000] LR: 0.014781\n",
      "Train | 8/8 | Loss:0.5642 | MainLoss:0.5435 | Alpha:0.4051 | SPLoss:0.0069 | CLSLoss:0.0441 | top1:74.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2189 | MainLoss:0.2189 | SPLoss:0.0067 | CLSLoss:0.0411 | top1:97.4112 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3854 | MainLoss:0.3854 | SPLoss:0.0067 | CLSLoss:0.0411 | top1:85.1638 | AUROC:0.9934\n",
      "\n",
      "Epoch: [191 | 1000] LR: 0.014768\n",
      "Train | 8/8 | Loss:0.5560 | MainLoss:0.5367 | Alpha:0.4074 | SPLoss:0.0066 | CLSLoss:0.0408 | top1:72.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2412 | MainLoss:0.2412 | SPLoss:0.0065 | CLSLoss:0.0384 | top1:96.4517 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3697 | MainLoss:0.3697 | SPLoss:0.0065 | CLSLoss:0.0384 | top1:87.6376 | AUROC:0.9933\n",
      "\n",
      "Epoch: [192 | 1000] LR: 0.014755\n",
      "Train | 8/8 | Loss:0.5087 | MainLoss:0.4902 | Alpha:0.4065 | SPLoss:0.0064 | CLSLoss:0.0391 | top1:80.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2191 | MainLoss:0.2191 | SPLoss:0.0065 | CLSLoss:0.0413 | top1:96.9844 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3705 | MainLoss:0.3705 | SPLoss:0.0065 | CLSLoss:0.0413 | top1:86.5498 | AUROC:0.9932\n",
      "\n",
      "Epoch: [193 | 1000] LR: 0.014741\n",
      "Train | 8/8 | Loss:0.5121 | MainLoss:0.4924 | Alpha:0.4058 | SPLoss:0.0066 | CLSLoss:0.0419 | top1:73.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2270 | MainLoss:0.2270 | SPLoss:0.0065 | CLSLoss:0.0405 | top1:96.5639 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3624 | MainLoss:0.3624 | SPLoss:0.0065 | CLSLoss:0.0405 | top1:87.4476 | AUROC:0.9933\n",
      "\n",
      "Epoch: [194 | 1000] LR: 0.014728\n",
      "Train | 8/8 | Loss:0.4357 | MainLoss:0.4157 | Alpha:0.4051 | SPLoss:0.0067 | CLSLoss:0.0426 | top1:82.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2088 | MainLoss:0.2088 | SPLoss:0.0073 | CLSLoss:0.0479 | top1:95.6698 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3266 | MainLoss:0.3266 | SPLoss:0.0073 | CLSLoss:0.0479 | top1:89.0957 | AUROC:0.9932\n",
      "\n",
      "Epoch: [195 | 1000] LR: 0.014714\n",
      "Train | 8/8 | Loss:0.4607 | MainLoss:0.4371 | Alpha:0.4062 | SPLoss:0.0077 | CLSLoss:0.0504 | top1:80.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2066 | MainLoss:0.2066 | SPLoss:0.0074 | CLSLoss:0.0482 | top1:96.0374 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3332 | MainLoss:0.3332 | SPLoss:0.0074 | CLSLoss:0.0482 | top1:88.4207 | AUROC:0.9931\n",
      "\n",
      "Epoch: [196 | 1000] LR: 0.014700\n",
      "Train | 8/8 | Loss:0.4648 | MainLoss:0.4423 | Alpha:0.4070 | SPLoss:0.0074 | CLSLoss:0.0479 | top1:80.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1973 | MainLoss:0.1973 | SPLoss:0.0077 | CLSLoss:0.0501 | top1:96.1340 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3307 | MainLoss:0.3307 | SPLoss:0.0077 | CLSLoss:0.0501 | top1:88.3421 | AUROC:0.9931\n",
      "\n",
      "Epoch: [197 | 1000] LR: 0.014686\n",
      "Train | 8/8 | Loss:0.4611 | MainLoss:0.4370 | Alpha:0.4064 | SPLoss:0.0080 | CLSLoss:0.0514 | top1:82.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2075 | MainLoss:0.2075 | SPLoss:0.0079 | CLSLoss:0.0503 | top1:95.4704 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3144 | MainLoss:0.3144 | SPLoss:0.0079 | CLSLoss:0.0503 | top1:89.6298 | AUROC:0.9931\n",
      "\n",
      "Epoch: [198 | 1000] LR: 0.014673\n",
      "Train | 8/8 | Loss:0.5389 | MainLoss:0.5168 | Alpha:0.4058 | SPLoss:0.0076 | CLSLoss:0.0469 | top1:74.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2170 | MainLoss:0.2170 | SPLoss:0.0072 | CLSLoss:0.0440 | top1:96.5358 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3539 | MainLoss:0.3539 | SPLoss:0.0072 | CLSLoss:0.0440 | top1:87.5754 | AUROC:0.9933\n",
      "\n",
      "Epoch: [199 | 1000] LR: 0.014659\n",
      "Train | 8/8 | Loss:0.4990 | MainLoss:0.4776 | Alpha:0.4062 | SPLoss:0.0073 | CLSLoss:0.0455 | top1:78.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2264 | MainLoss:0.2264 | SPLoss:0.0072 | CLSLoss:0.0436 | top1:95.9003 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3431 | MainLoss:0.3431 | SPLoss:0.0072 | CLSLoss:0.0436 | top1:88.7844 | AUROC:0.9934\n",
      "\n",
      "Epoch: [200 | 1000] LR: 0.014645\n",
      "Train | 8/8 | Loss:0.4740 | MainLoss:0.4530 | Alpha:0.4066 | SPLoss:0.0072 | CLSLoss:0.0443 | top1:80.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2149 | MainLoss:0.2149 | SPLoss:0.0074 | CLSLoss:0.0466 | top1:95.7788 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3316 | MainLoss:0.3316 | SPLoss:0.0074 | CLSLoss:0.0466 | top1:88.9482 | AUROC:0.9934\n",
      "\n",
      "Epoch: [201 | 1000] LR: 0.014631\n",
      "Train | 8/8 | Loss:0.4418 | MainLoss:0.4213 | Alpha:0.4087 | SPLoss:0.0001 | CLSLoss:0.0500 | top1:81.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2025 | MainLoss:0.2025 | SPLoss:0.0001 | CLSLoss:0.0491 | top1:95.9875 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3303 | MainLoss:0.3303 | SPLoss:0.0001 | CLSLoss:0.0491 | top1:88.6730 | AUROC:0.9933\n",
      "\n",
      "Epoch: [202 | 1000] LR: 0.014617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.4494 | MainLoss:0.4284 | Alpha:0.4074 | SPLoss:0.0003 | CLSLoss:0.0513 | top1:81.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1997 | MainLoss:0.1997 | SPLoss:0.0003 | CLSLoss:0.0495 | top1:96.4642 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3400 | MainLoss:0.3400 | SPLoss:0.0003 | CLSLoss:0.0495 | top1:87.6507 | AUROC:0.9932\n",
      "\n",
      "Epoch: [203 | 1000] LR: 0.014602\n",
      "Train | 8/8 | Loss:0.4384 | MainLoss:0.4172 | Alpha:0.4079 | SPLoss:0.0004 | CLSLoss:0.0514 | top1:80.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2013 | MainLoss:0.2013 | SPLoss:0.0005 | CLSLoss:0.0518 | top1:95.7009 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3176 | MainLoss:0.3176 | SPLoss:0.0005 | CLSLoss:0.0518 | top1:89.0269 | AUROC:0.9934\n",
      "\n",
      "Epoch: [204 | 1000] LR: 0.014588\n",
      "Train | 8/8 | Loss:0.5201 | MainLoss:0.4994 | Alpha:0.4089 | SPLoss:0.0006 | CLSLoss:0.0500 | top1:74.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2131 | MainLoss:0.2131 | SPLoss:0.0007 | CLSLoss:0.0469 | top1:96.2056 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3395 | MainLoss:0.3395 | SPLoss:0.0007 | CLSLoss:0.0469 | top1:88.1979 | AUROC:0.9934\n",
      "\n",
      "Epoch: [205 | 1000] LR: 0.014574\n",
      "Train | 8/8 | Loss:0.4976 | MainLoss:0.4782 | Alpha:0.4070 | SPLoss:0.0008 | CLSLoss:0.0470 | top1:79.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2116 | MainLoss:0.2116 | SPLoss:0.0009 | CLSLoss:0.0462 | top1:96.4206 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3443 | MainLoss:0.3443 | SPLoss:0.0009 | CLSLoss:0.0462 | top1:87.7654 | AUROC:0.9933\n",
      "\n",
      "Epoch: [206 | 1000] LR: 0.014560\n",
      "Train | 8/8 | Loss:0.4224 | MainLoss:0.4019 | Alpha:0.4097 | SPLoss:0.0010 | CLSLoss:0.0490 | top1:85.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2033 | MainLoss:0.2033 | SPLoss:0.0012 | CLSLoss:0.0510 | top1:95.2960 | AUROC:0.9989\n",
      "Test | 62/8 | Loss:0.3071 | MainLoss:0.3071 | SPLoss:0.0012 | CLSLoss:0.0510 | top1:89.7936 | AUROC:0.9931\n",
      "\n",
      "Epoch: [207 | 1000] LR: 0.014545\n",
      "Train | 8/8 | Loss:0.4935 | MainLoss:0.4725 | Alpha:0.4084 | SPLoss:0.0013 | CLSLoss:0.0502 | top1:78.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1976 | MainLoss:0.1976 | SPLoss:0.0013 | CLSLoss:0.0473 | top1:96.8567 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3529 | MainLoss:0.3529 | SPLoss:0.0013 | CLSLoss:0.0473 | top1:86.7726 | AUROC:0.9932\n",
      "\n",
      "Epoch: [208 | 1000] LR: 0.014531\n",
      "Train | 8/8 | Loss:0.5081 | MainLoss:0.4886 | Alpha:0.4072 | SPLoss:0.0013 | CLSLoss:0.0463 | top1:76.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2048 | MainLoss:0.2048 | SPLoss:0.0015 | CLSLoss:0.0444 | top1:97.1807 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3656 | MainLoss:0.3656 | SPLoss:0.0015 | CLSLoss:0.0444 | top1:86.0878 | AUROC:0.9931\n",
      "\n",
      "Epoch: [209 | 1000] LR: 0.014516\n",
      "Train | 8/8 | Loss:0.4523 | MainLoss:0.4330 | Alpha:0.4095 | SPLoss:0.0015 | CLSLoss:0.0456 | top1:80.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2025 | MainLoss:0.2025 | SPLoss:0.0016 | CLSLoss:0.0475 | top1:96.1464 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3327 | MainLoss:0.3327 | SPLoss:0.0016 | CLSLoss:0.0475 | top1:88.3650 | AUROC:0.9931\n",
      "\n",
      "Epoch: [210 | 1000] LR: 0.014502\n",
      "Train | 8/8 | Loss:0.4872 | MainLoss:0.4669 | Alpha:0.4089 | SPLoss:0.0017 | CLSLoss:0.0480 | top1:79.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2103 | MainLoss:0.2103 | SPLoss:0.0017 | CLSLoss:0.0460 | top1:95.9346 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3307 | MainLoss:0.3307 | SPLoss:0.0017 | CLSLoss:0.0460 | top1:88.7713 | AUROC:0.9931\n",
      "\n",
      "Epoch: [211 | 1000] LR: 0.014487\n",
      "Train | 8/8 | Loss:0.4214 | MainLoss:0.4004 | Alpha:0.4090 | SPLoss:0.0020 | CLSLoss:0.0495 | top1:82.5000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1982 | MainLoss:0.1982 | SPLoss:0.0021 | CLSLoss:0.0505 | top1:95.7508 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3194 | MainLoss:0.3194 | SPLoss:0.0021 | CLSLoss:0.0505 | top1:88.9908 | AUROC:0.9931\n",
      "\n",
      "Epoch: [212 | 1000] LR: 0.014472\n",
      "Train | 8/8 | Loss:0.5246 | MainLoss:0.5044 | Alpha:0.4084 | SPLoss:0.0022 | CLSLoss:0.0474 | top1:75.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2082 | MainLoss:0.2082 | SPLoss:0.0023 | CLSLoss:0.0450 | top1:96.6854 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3524 | MainLoss:0.3524 | SPLoss:0.0023 | CLSLoss:0.0450 | top1:87.2412 | AUROC:0.9932\n",
      "\n",
      "Epoch: [213 | 1000] LR: 0.014457\n",
      "Train | 8/8 | Loss:0.4900 | MainLoss:0.4707 | Alpha:0.4090 | SPLoss:0.0023 | CLSLoss:0.0450 | top1:80.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2109 | MainLoss:0.2109 | SPLoss:0.0024 | CLSLoss:0.0455 | top1:96.1464 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3373 | MainLoss:0.3373 | SPLoss:0.0024 | CLSLoss:0.0455 | top1:88.4928 | AUROC:0.9934\n",
      "\n",
      "Epoch: [214 | 1000] LR: 0.014442\n",
      "Train | 8/8 | Loss:0.4216 | MainLoss:0.4008 | Alpha:0.4076 | SPLoss:0.0026 | CLSLoss:0.0484 | top1:83.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1977 | MainLoss:0.1977 | SPLoss:0.0028 | CLSLoss:0.0507 | top1:95.6978 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3149 | MainLoss:0.3149 | SPLoss:0.0028 | CLSLoss:0.0507 | top1:89.2890 | AUROC:0.9932\n",
      "\n",
      "Epoch: [215 | 1000] LR: 0.014428\n",
      "Train | 8/8 | Loss:0.5709 | MainLoss:0.5511 | Alpha:0.4075 | SPLoss:0.0027 | CLSLoss:0.0458 | top1:67.5750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2275 | MainLoss:0.2275 | SPLoss:0.0027 | CLSLoss:0.0408 | top1:96.2181 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3490 | MainLoss:0.3490 | SPLoss:0.0027 | CLSLoss:0.0408 | top1:88.3093 | AUROC:0.9932\n",
      "\n",
      "Epoch: [216 | 1000] LR: 0.014413\n",
      "Train | 8/8 | Loss:0.4239 | MainLoss:0.4046 | Alpha:0.4080 | SPLoss:0.0028 | CLSLoss:0.0445 | top1:84.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1944 | MainLoss:0.1944 | SPLoss:0.0031 | CLSLoss:0.0483 | top1:96.2305 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3295 | MainLoss:0.3295 | SPLoss:0.0031 | CLSLoss:0.0483 | top1:88.3814 | AUROC:0.9931\n",
      "\n",
      "Epoch: [217 | 1000] LR: 0.014397\n",
      "Train | 8/8 | Loss:0.4926 | MainLoss:0.4713 | Alpha:0.4081 | SPLoss:0.0033 | CLSLoss:0.0489 | top1:77.0750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2009 | MainLoss:0.2009 | SPLoss:0.0033 | CLSLoss:0.0452 | top1:97.2710 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3629 | MainLoss:0.3629 | SPLoss:0.0033 | CLSLoss:0.0452 | top1:86.2451 | AUROC:0.9931\n",
      "\n",
      "Epoch: [218 | 1000] LR: 0.014382\n",
      "Train | 8/8 | Loss:0.4667 | MainLoss:0.4470 | Alpha:0.4068 | SPLoss:0.0032 | CLSLoss:0.0453 | top1:84.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1981 | MainLoss:0.1981 | SPLoss:0.0033 | CLSLoss:0.0469 | top1:96.5576 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3395 | MainLoss:0.3395 | SPLoss:0.0033 | CLSLoss:0.0469 | top1:87.8735 | AUROC:0.9929\n",
      "\n",
      "Epoch: [219 | 1000] LR: 0.014367\n",
      "Train | 8/8 | Loss:0.4404 | MainLoss:0.4192 | Alpha:0.4083 | SPLoss:0.0034 | CLSLoss:0.0484 | top1:83.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1997 | MainLoss:0.1997 | SPLoss:0.0036 | CLSLoss:0.0495 | top1:95.8754 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3167 | MainLoss:0.3167 | SPLoss:0.0036 | CLSLoss:0.0495 | top1:89.2661 | AUROC:0.9930\n",
      "\n",
      "Epoch: [220 | 1000] LR: 0.014352\n",
      "Train | 8/8 | Loss:0.5092 | MainLoss:0.4889 | Alpha:0.4083 | SPLoss:0.0036 | CLSLoss:0.0462 | top1:76.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2076 | MainLoss:0.2076 | SPLoss:0.0036 | CLSLoss:0.0460 | top1:96.3458 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3366 | MainLoss:0.3366 | SPLoss:0.0036 | CLSLoss:0.0460 | top1:88.3617 | AUROC:0.9931\n",
      "\n",
      "Epoch: [221 | 1000] LR: 0.014337\n",
      "Train | 8/8 | Loss:0.4974 | MainLoss:0.4771 | Alpha:0.4080 | SPLoss:0.0036 | CLSLoss:0.0462 | top1:77.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2098 | MainLoss:0.2098 | SPLoss:0.0037 | CLSLoss:0.0452 | top1:96.2773 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3366 | MainLoss:0.3366 | SPLoss:0.0037 | CLSLoss:0.0452 | top1:88.5223 | AUROC:0.9931\n",
      "\n",
      "Epoch: [222 | 1000] LR: 0.014321\n",
      "Train | 8/8 | Loss:0.5635 | MainLoss:0.5443 | Alpha:0.4086 | SPLoss:0.0037 | CLSLoss:0.0433 | top1:74.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2259 | MainLoss:0.2259 | SPLoss:0.0037 | CLSLoss:0.0397 | top1:96.5732 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3578 | MainLoss:0.3578 | SPLoss:0.0037 | CLSLoss:0.0397 | top1:87.7752 | AUROC:0.9931\n",
      "\n",
      "Epoch: [223 | 1000] LR: 0.014306\n",
      "Train | 8/8 | Loss:0.4914 | MainLoss:0.4729 | Alpha:0.4079 | SPLoss:0.0037 | CLSLoss:0.0415 | top1:76.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2260 | MainLoss:0.2260 | SPLoss:0.0037 | CLSLoss:0.0411 | top1:95.9813 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3414 | MainLoss:0.3414 | SPLoss:0.0037 | CLSLoss:0.0411 | top1:89.0072 | AUROC:0.9932\n",
      "\n",
      "Epoch: [224 | 1000] LR: 0.014290\n",
      "Train | 8/8 | Loss:0.4772 | MainLoss:0.4587 | Alpha:0.4082 | SPLoss:0.0038 | CLSLoss:0.0414 | top1:78.6500 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.2100 | MainLoss:0.2100 | SPLoss:0.0039 | CLSLoss:0.0440 | top1:96.0561 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3325 | MainLoss:0.3325 | SPLoss:0.0039 | CLSLoss:0.0440 | top1:89.0531 | AUROC:0.9930\n",
      "\n",
      "Epoch: [225 | 1000] LR: 0.014275\n",
      "Train | 8/8 | Loss:0.4860 | MainLoss:0.4660 | Alpha:0.4083 | SPLoss:0.0041 | CLSLoss:0.0450 | top1:77.1750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2156 | MainLoss:0.2156 | SPLoss:0.0041 | CLSLoss:0.0440 | top1:95.9221 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3310 | MainLoss:0.3310 | SPLoss:0.0041 | CLSLoss:0.0440 | top1:89.1809 | AUROC:0.9930\n",
      "\n",
      "Epoch: [226 | 1000] LR: 0.014259\n",
      "Train | 8/8 | Loss:0.5256 | MainLoss:0.5066 | Alpha:0.4087 | SPLoss:0.0042 | CLSLoss:0.0423 | top1:73.9500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2164 | MainLoss:0.2164 | SPLoss:0.0042 | CLSLoss:0.0420 | top1:96.3583 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3469 | MainLoss:0.3469 | SPLoss:0.0042 | CLSLoss:0.0420 | top1:88.2667 | AUROC:0.9930\n",
      "\n",
      "Epoch: [227 | 1000] LR: 0.014243\n",
      "Train | 8/8 | Loss:0.4870 | MainLoss:0.4683 | Alpha:0.4078 | SPLoss:0.0042 | CLSLoss:0.0415 | top1:80.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2080 | MainLoss:0.2080 | SPLoss:0.0043 | CLSLoss:0.0436 | top1:96.3240 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3429 | MainLoss:0.3429 | SPLoss:0.0043 | CLSLoss:0.0436 | top1:88.3355 | AUROC:0.9929\n",
      "\n",
      "Epoch: [228 | 1000] LR: 0.014228\n",
      "Train | 8/8 | Loss:0.4382 | MainLoss:0.4177 | Alpha:0.4080 | SPLoss:0.0044 | CLSLoss:0.0460 | top1:83.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2003 | MainLoss:0.2003 | SPLoss:0.0045 | CLSLoss:0.0475 | top1:95.9408 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3227 | MainLoss:0.3227 | SPLoss:0.0045 | CLSLoss:0.0475 | top1:89.2038 | AUROC:0.9928\n",
      "\n",
      "Epoch: [229 | 1000] LR: 0.014212\n",
      "Train | 8/8 | Loss:0.4234 | MainLoss:0.4019 | Alpha:0.4076 | SPLoss:0.0046 | CLSLoss:0.0482 | top1:82.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1929 | MainLoss:0.1929 | SPLoss:0.0049 | CLSLoss:0.0519 | top1:95.5545 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3064 | MainLoss:0.3064 | SPLoss:0.0049 | CLSLoss:0.0519 | top1:89.7706 | AUROC:0.9927\n",
      "\n",
      "Epoch: [230 | 1000] LR: 0.014196\n",
      "Train | 8/8 | Loss:0.5061 | MainLoss:0.4834 | Alpha:0.4080 | SPLoss:0.0049 | CLSLoss:0.0508 | top1:78.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1989 | MainLoss:0.1989 | SPLoss:0.0047 | CLSLoss:0.0465 | top1:96.7539 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3464 | MainLoss:0.3464 | SPLoss:0.0047 | CLSLoss:0.0465 | top1:87.4607 | AUROC:0.9930\n",
      "\n",
      "Epoch: [231 | 1000] LR: 0.014180\n",
      "Train | 8/8 | Loss:0.4360 | MainLoss:0.4143 | Alpha:0.4083 | SPLoss:0.0048 | CLSLoss:0.0484 | top1:84.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1954 | MainLoss:0.1954 | SPLoss:0.0049 | CLSLoss:0.0499 | top1:96.0218 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3204 | MainLoss:0.3204 | SPLoss:0.0049 | CLSLoss:0.0499 | top1:88.9876 | AUROC:0.9929\n",
      "\n",
      "Epoch: [232 | 1000] LR: 0.014164\n",
      "Train | 8/8 | Loss:0.4649 | MainLoss:0.4428 | Alpha:0.4077 | SPLoss:0.0050 | CLSLoss:0.0493 | top1:81.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1964 | MainLoss:0.1964 | SPLoss:0.0051 | CLSLoss:0.0496 | top1:96.0654 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3236 | MainLoss:0.3236 | SPLoss:0.0051 | CLSLoss:0.0496 | top1:88.8139 | AUROC:0.9929\n",
      "\n",
      "Epoch: [233 | 1000] LR: 0.014148\n",
      "Train | 8/8 | Loss:0.4702 | MainLoss:0.4483 | Alpha:0.4078 | SPLoss:0.0051 | CLSLoss:0.0486 | top1:78.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1950 | MainLoss:0.1950 | SPLoss:0.0052 | CLSLoss:0.0488 | top1:96.2741 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3302 | MainLoss:0.3302 | SPLoss:0.0052 | CLSLoss:0.0488 | top1:88.4994 | AUROC:0.9929\n",
      "\n",
      "Epoch: [234 | 1000] LR: 0.014132\n",
      "Train | 8/8 | Loss:0.4538 | MainLoss:0.4321 | Alpha:0.4085 | SPLoss:0.0052 | CLSLoss:0.0479 | top1:81.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1959 | MainLoss:0.1959 | SPLoss:0.0054 | CLSLoss:0.0497 | top1:95.8287 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3165 | MainLoss:0.3165 | SPLoss:0.0054 | CLSLoss:0.0497 | top1:89.3119 | AUROC:0.9926\n",
      "\n",
      "Epoch: [235 | 1000] LR: 0.014116\n",
      "Train | 8/8 | Loss:0.5274 | MainLoss:0.5057 | Alpha:0.4082 | SPLoss:0.0054 | CLSLoss:0.0477 | top1:73.9000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2071 | MainLoss:0.2071 | SPLoss:0.0053 | CLSLoss:0.0442 | top1:96.6324 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3471 | MainLoss:0.3471 | SPLoss:0.0053 | CLSLoss:0.0442 | top1:87.6573 | AUROC:0.9926\n",
      "\n",
      "Epoch: [236 | 1000] LR: 0.014100\n",
      "Train | 8/8 | Loss:0.4623 | MainLoss:0.4416 | Alpha:0.4087 | SPLoss:0.0054 | CLSLoss:0.0455 | top1:79.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2098 | MainLoss:0.2098 | SPLoss:0.0054 | CLSLoss:0.0467 | top1:95.7446 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3225 | MainLoss:0.3225 | SPLoss:0.0054 | CLSLoss:0.0467 | top1:89.3971 | AUROC:0.9925\n",
      "\n",
      "Epoch: [237 | 1000] LR: 0.014083\n",
      "Train | 8/8 | Loss:0.4613 | MainLoss:0.4395 | Alpha:0.4081 | SPLoss:0.0056 | CLSLoss:0.0480 | top1:80.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1990 | MainLoss:0.1990 | SPLoss:0.0056 | CLSLoss:0.0475 | top1:96.1402 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3278 | MainLoss:0.3278 | SPLoss:0.0056 | CLSLoss:0.0475 | top1:88.7385 | AUROC:0.9928\n",
      "\n",
      "Epoch: [238 | 1000] LR: 0.014067\n",
      "Train | 8/8 | Loss:0.4746 | MainLoss:0.4531 | Alpha:0.4084 | SPLoss:0.0056 | CLSLoss:0.0470 | top1:80.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2082 | MainLoss:0.2082 | SPLoss:0.0055 | CLSLoss:0.0464 | top1:95.7072 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3197 | MainLoss:0.3197 | SPLoss:0.0055 | CLSLoss:0.0464 | top1:89.5642 | AUROC:0.9927\n",
      "\n",
      "Epoch: [239 | 1000] LR: 0.014050\n",
      "Train | 8/8 | Loss:0.5189 | MainLoss:0.4988 | Alpha:0.4074 | SPLoss:0.0055 | CLSLoss:0.0439 | top1:77.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2032 | MainLoss:0.2032 | SPLoss:0.0056 | CLSLoss:0.0435 | top1:96.8785 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3524 | MainLoss:0.3524 | SPLoss:0.0056 | CLSLoss:0.0435 | top1:87.3558 | AUROC:0.9928\n",
      "\n",
      "Epoch: [240 | 1000] LR: 0.014034\n",
      "Train | 8/8 | Loss:0.4809 | MainLoss:0.4609 | Alpha:0.4090 | SPLoss:0.0056 | CLSLoss:0.0435 | top1:75.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2069 | MainLoss:0.2069 | SPLoss:0.0056 | CLSLoss:0.0444 | top1:96.0405 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3312 | MainLoss:0.3312 | SPLoss:0.0056 | CLSLoss:0.0444 | top1:88.9024 | AUROC:0.9926\n",
      "\n",
      "Epoch: [241 | 1000] LR: 0.014017\n",
      "Train | 8/8 | Loss:0.4939 | MainLoss:0.4741 | Alpha:0.4073 | SPLoss:0.0056 | CLSLoss:0.0431 | top1:80.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2002 | MainLoss:0.2002 | SPLoss:0.0057 | CLSLoss:0.0444 | top1:96.8006 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3466 | MainLoss:0.3466 | SPLoss:0.0057 | CLSLoss:0.0444 | top1:87.5688 | AUROC:0.9926\n",
      "\n",
      "Epoch: [242 | 1000] LR: 0.014001\n",
      "Train | 8/8 | Loss:0.4881 | MainLoss:0.4673 | Alpha:0.4086 | SPLoss:0.0058 | CLSLoss:0.0452 | top1:77.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1985 | MainLoss:0.1985 | SPLoss:0.0059 | CLSLoss:0.0438 | top1:97.2118 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3593 | MainLoss:0.3593 | SPLoss:0.0059 | CLSLoss:0.0438 | top1:86.4646 | AUROC:0.9927\n",
      "\n",
      "Epoch: [243 | 1000] LR: 0.013984\n",
      "Train | 8/8 | Loss:0.4748 | MainLoss:0.4539 | Alpha:0.4090 | SPLoss:0.0059 | CLSLoss:0.0452 | top1:82.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1991 | MainLoss:0.1991 | SPLoss:0.0059 | CLSLoss:0.0443 | top1:97.0717 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3519 | MainLoss:0.3519 | SPLoss:0.0059 | CLSLoss:0.0443 | top1:87.0446 | AUROC:0.9929\n",
      "\n",
      "Epoch: [244 | 1000] LR: 0.013968\n",
      "Train | 8/8 | Loss:0.5170 | MainLoss:0.4973 | Alpha:0.4090 | SPLoss:0.0058 | CLSLoss:0.0424 | top1:74.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2172 | MainLoss:0.2172 | SPLoss:0.0057 | CLSLoss:0.0417 | top1:96.1994 | AUROC:0.9990\n",
      "Test | 62/8 | Loss:0.3381 | MainLoss:0.3381 | SPLoss:0.0057 | CLSLoss:0.0417 | top1:88.7910 | AUROC:0.9928\n",
      "\n",
      "Epoch: [245 | 1000] LR: 0.013951\n",
      "Train | 8/8 | Loss:0.4786 | MainLoss:0.4590 | Alpha:0.4086 | SPLoss:0.0057 | CLSLoss:0.0424 | top1:79.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2062 | MainLoss:0.2062 | SPLoss:0.0058 | CLSLoss:0.0444 | top1:96.2181 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3325 | MainLoss:0.3325 | SPLoss:0.0058 | CLSLoss:0.0444 | top1:88.7877 | AUROC:0.9927\n",
      "\n",
      "Epoch: [246 | 1000] LR: 0.013934\n",
      "Train | 8/8 | Loss:0.5103 | MainLoss:0.4901 | Alpha:0.4093 | SPLoss:0.0058 | CLSLoss:0.0436 | top1:76.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2078 | MainLoss:0.2078 | SPLoss:0.0058 | CLSLoss:0.0432 | top1:96.4393 | AUROC:0.9991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.3402 | MainLoss:0.3402 | SPLoss:0.0058 | CLSLoss:0.0432 | top1:88.4142 | AUROC:0.9928\n",
      "\n",
      "Epoch: [247 | 1000] LR: 0.013917\n",
      "Train | 8/8 | Loss:0.4419 | MainLoss:0.4211 | Alpha:0.4091 | SPLoss:0.0059 | CLSLoss:0.0449 | top1:81.1000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2028 | MainLoss:0.2028 | SPLoss:0.0060 | CLSLoss:0.0475 | top1:95.7072 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3140 | MainLoss:0.3140 | SPLoss:0.0060 | CLSLoss:0.0475 | top1:89.8231 | AUROC:0.9926\n",
      "\n",
      "Epoch: [248 | 1000] LR: 0.013900\n",
      "Train | 8/8 | Loss:0.4762 | MainLoss:0.4542 | Alpha:0.4084 | SPLoss:0.0061 | CLSLoss:0.0478 | top1:80.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1975 | MainLoss:0.1975 | SPLoss:0.0061 | CLSLoss:0.0474 | top1:96.4424 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3329 | MainLoss:0.3329 | SPLoss:0.0061 | CLSLoss:0.0474 | top1:88.4830 | AUROC:0.9929\n",
      "\n",
      "Epoch: [249 | 1000] LR: 0.013883\n",
      "Train | 8/8 | Loss:0.4672 | MainLoss:0.4452 | Alpha:0.4080 | SPLoss:0.0062 | CLSLoss:0.0478 | top1:78.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2032 | MainLoss:0.2032 | SPLoss:0.0062 | CLSLoss:0.0480 | top1:95.7632 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3159 | MainLoss:0.3159 | SPLoss:0.0062 | CLSLoss:0.0480 | top1:89.7477 | AUROC:0.9929\n",
      "\n",
      "Epoch: [250 | 1000] LR: 0.013866\n",
      "Train | 8/8 | Loss:0.4655 | MainLoss:0.4432 | Alpha:0.4089 | SPLoss:0.0062 | CLSLoss:0.0483 | top1:79.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2011 | MainLoss:0.2011 | SPLoss:0.0063 | CLSLoss:0.0474 | top1:95.9813 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3213 | MainLoss:0.3213 | SPLoss:0.0063 | CLSLoss:0.0474 | top1:89.3512 | AUROC:0.9929\n",
      "\n",
      "Epoch: [251 | 1000] LR: 0.013849\n",
      "Train | 8/8 | Loss:0.4712 | MainLoss:0.4519 | Alpha:0.4117 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:81.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1971 | MainLoss:0.1971 | SPLoss:0.0001 | CLSLoss:0.0479 | top1:96.2150 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3235 | MainLoss:0.3235 | SPLoss:0.0001 | CLSLoss:0.0479 | top1:89.0695 | AUROC:0.9930\n",
      "\n",
      "Epoch: [252 | 1000] LR: 0.001383\n",
      "Train | 8/8 | Loss:0.5976 | MainLoss:0.5781 | Alpha:0.4109 | SPLoss:0.0001 | CLSLoss:0.0474 | top1:72.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2013 | MainLoss:0.2013 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:96.2835 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3279 | MainLoss:0.3279 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:88.9646 | AUROC:0.9930\n",
      "\n",
      "Epoch: [253 | 1000] LR: 0.001381\n",
      "Train | 8/8 | Loss:0.4729 | MainLoss:0.4537 | Alpha:0.4102 | SPLoss:0.0001 | CLSLoss:0.0467 | top1:76.7750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2015 | MainLoss:0.2015 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:96.2555 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3272 | MainLoss:0.3272 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:89.0236 | AUROC:0.9930\n",
      "\n",
      "Epoch: [254 | 1000] LR: 0.001380\n",
      "Train | 8/8 | Loss:0.4624 | MainLoss:0.4431 | Alpha:0.4108 | SPLoss:0.0001 | CLSLoss:0.0467 | top1:82.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2004 | MainLoss:0.2004 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:96.2773 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3272 | MainLoss:0.3272 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:88.9843 | AUROC:0.9929\n",
      "\n",
      "Epoch: [255 | 1000] LR: 0.001378\n",
      "Train | 8/8 | Loss:0.4637 | MainLoss:0.4445 | Alpha:0.4097 | SPLoss:0.0001 | CLSLoss:0.0467 | top1:72.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2009 | MainLoss:0.2009 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:96.2056 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3258 | MainLoss:0.3258 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:89.1022 | AUROC:0.9929\n",
      "\n",
      "Epoch: [256 | 1000] LR: 0.001376\n",
      "Train | 8/8 | Loss:0.4638 | MainLoss:0.4447 | Alpha:0.4106 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:75.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2004 | MainLoss:0.2004 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:96.2274 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3258 | MainLoss:0.3258 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:89.0695 | AUROC:0.9928\n",
      "\n",
      "Epoch: [257 | 1000] LR: 0.001375\n",
      "Train | 8/8 | Loss:0.5100 | MainLoss:0.4909 | Alpha:0.4094 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:72.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2013 | MainLoss:0.2013 | SPLoss:0.0001 | CLSLoss:0.0465 | top1:96.2461 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3267 | MainLoss:0.3267 | SPLoss:0.0001 | CLSLoss:0.0465 | top1:89.0596 | AUROC:0.9930\n",
      "\n",
      "Epoch: [258 | 1000] LR: 0.001373\n",
      "Train | 8/8 | Loss:0.5110 | MainLoss:0.4921 | Alpha:0.4097 | SPLoss:0.0001 | CLSLoss:0.0462 | top1:78.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2019 | MainLoss:0.2019 | SPLoss:0.0001 | CLSLoss:0.0462 | top1:96.2804 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3279 | MainLoss:0.3279 | SPLoss:0.0001 | CLSLoss:0.0462 | top1:88.9941 | AUROC:0.9928\n",
      "\n",
      "Epoch: [259 | 1000] LR: 0.001371\n",
      "Train | 8/8 | Loss:0.4181 | MainLoss:0.3989 | Alpha:0.4097 | SPLoss:0.0001 | CLSLoss:0.0465 | top1:84.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2016 | MainLoss:0.2016 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:96.1371 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3232 | MainLoss:0.3232 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:89.2693 | AUROC:0.9929\n",
      "\n",
      "Epoch: [260 | 1000] LR: 0.001369\n",
      "Train | 8/8 | Loss:0.4738 | MainLoss:0.4545 | Alpha:0.4109 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:79.7000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2009 | MainLoss:0.2009 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:96.1620 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3237 | MainLoss:0.3237 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:89.2104 | AUROC:0.9929\n",
      "\n",
      "Epoch: [261 | 1000] LR: 0.001367\n",
      "Train | 8/8 | Loss:0.4906 | MainLoss:0.4714 | Alpha:0.4103 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:78.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2016 | MainLoss:0.2016 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:96.1495 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3236 | MainLoss:0.3236 | SPLoss:0.0001 | CLSLoss:0.0466 | top1:89.2431 | AUROC:0.9929\n",
      "\n",
      "Epoch: [262 | 1000] LR: 0.001366\n",
      "Train | 8/8 | Loss:0.4475 | MainLoss:0.4282 | Alpha:0.4107 | SPLoss:0.0001 | CLSLoss:0.0467 | top1:83.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2003 | MainLoss:0.2003 | SPLoss:0.0001 | CLSLoss:0.0469 | top1:96.1433 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3226 | MainLoss:0.3226 | SPLoss:0.0001 | CLSLoss:0.0469 | top1:89.2595 | AUROC:0.9929\n",
      "\n",
      "Epoch: [263 | 1000] LR: 0.001364\n",
      "Train | 8/8 | Loss:0.4668 | MainLoss:0.4475 | Alpha:0.4118 | SPLoss:0.0001 | CLSLoss:0.0468 | top1:76.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2006 | MainLoss:0.2006 | SPLoss:0.0001 | CLSLoss:0.0469 | top1:96.0966 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3212 | MainLoss:0.3212 | SPLoss:0.0001 | CLSLoss:0.0469 | top1:89.3840 | AUROC:0.9929\n",
      "\n",
      "Epoch: [264 | 1000] LR: 0.001362\n",
      "Train | 8/8 | Loss:0.4074 | MainLoss:0.3879 | Alpha:0.4108 | SPLoss:0.0001 | CLSLoss:0.0472 | top1:84.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1998 | MainLoss:0.1998 | SPLoss:0.0001 | CLSLoss:0.0475 | top1:95.9907 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3174 | MainLoss:0.3174 | SPLoss:0.0001 | CLSLoss:0.0475 | top1:89.5741 | AUROC:0.9928\n",
      "\n",
      "Epoch: [265 | 1000] LR: 0.001360\n",
      "Train | 8/8 | Loss:0.4437 | MainLoss:0.4241 | Alpha:0.4101 | SPLoss:0.0001 | CLSLoss:0.0477 | top1:80.4750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1982 | MainLoss:0.1982 | SPLoss:0.0002 | CLSLoss:0.0478 | top1:96.0218 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3176 | MainLoss:0.3176 | SPLoss:0.0002 | CLSLoss:0.0478 | top1:89.5282 | AUROC:0.9928\n",
      "\n",
      "Epoch: [266 | 1000] LR: 0.001359\n",
      "Train | 8/8 | Loss:0.4690 | MainLoss:0.4494 | Alpha:0.4098 | SPLoss:0.0002 | CLSLoss:0.0477 | top1:81.2500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1970 | MainLoss:0.1970 | SPLoss:0.0002 | CLSLoss:0.0477 | top1:96.1371 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3201 | MainLoss:0.3201 | SPLoss:0.0002 | CLSLoss:0.0477 | top1:89.2792 | AUROC:0.9928\n",
      "\n",
      "Epoch: [267 | 1000] LR: 0.001357\n",
      "Train | 8/8 | Loss:0.4925 | MainLoss:0.4730 | Alpha:0.4109 | SPLoss:0.0002 | CLSLoss:0.0474 | top1:78.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1988 | MainLoss:0.1988 | SPLoss:0.0002 | CLSLoss:0.0474 | top1:96.0903 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3195 | MainLoss:0.3195 | SPLoss:0.0002 | CLSLoss:0.0474 | top1:89.4266 | AUROC:0.9928\n",
      "\n",
      "Epoch: [268 | 1000] LR: 0.001355\n",
      "Train | 8/8 | Loss:0.4975 | MainLoss:0.4781 | Alpha:0.4107 | SPLoss:0.0002 | CLSLoss:0.0472 | top1:80.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1975 | MainLoss:0.1975 | SPLoss:0.0002 | CLSLoss:0.0472 | top1:96.2243 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3237 | MainLoss:0.3237 | SPLoss:0.0002 | CLSLoss:0.0472 | top1:89.1022 | AUROC:0.9929\n",
      "\n",
      "Epoch: [269 | 1000] LR: 0.001353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | 8/8 | Loss:0.5426 | MainLoss:0.5233 | Alpha:0.4110 | SPLoss:0.0002 | CLSLoss:0.0470 | top1:76.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1972 | MainLoss:0.1972 | SPLoss:0.0002 | CLSLoss:0.0465 | top1:96.4704 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3305 | MainLoss:0.3305 | SPLoss:0.0002 | CLSLoss:0.0465 | top1:88.6730 | AUROC:0.9927\n",
      "\n",
      "Epoch: [270 | 1000] LR: 0.001351\n",
      "Train | 8/8 | Loss:0.3888 | MainLoss:0.3695 | Alpha:0.4120 | SPLoss:0.0002 | CLSLoss:0.0468 | top1:85.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1954 | MainLoss:0.1954 | SPLoss:0.0002 | CLSLoss:0.0472 | top1:96.3770 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3262 | MainLoss:0.3262 | SPLoss:0.0002 | CLSLoss:0.0472 | top1:88.8630 | AUROC:0.9928\n",
      "\n",
      "Epoch: [271 | 1000] LR: 0.001349\n",
      "Train | 8/8 | Loss:0.4656 | MainLoss:0.4460 | Alpha:0.4109 | SPLoss:0.0002 | CLSLoss:0.0473 | top1:83.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1954 | MainLoss:0.1954 | SPLoss:0.0002 | CLSLoss:0.0473 | top1:96.3209 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3251 | MainLoss:0.3251 | SPLoss:0.0002 | CLSLoss:0.0473 | top1:88.9450 | AUROC:0.9928\n",
      "\n",
      "Epoch: [272 | 1000] LR: 0.001348\n",
      "Train | 8/8 | Loss:0.5425 | MainLoss:0.5231 | Alpha:0.4120 | SPLoss:0.0002 | CLSLoss:0.0469 | top1:72.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1974 | MainLoss:0.1974 | SPLoss:0.0002 | CLSLoss:0.0466 | top1:96.3832 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3279 | MainLoss:0.3279 | SPLoss:0.0002 | CLSLoss:0.0466 | top1:88.8467 | AUROC:0.9928\n",
      "\n",
      "Epoch: [273 | 1000] LR: 0.001346\n",
      "Train | 8/8 | Loss:0.4540 | MainLoss:0.4349 | Alpha:0.4096 | SPLoss:0.0002 | CLSLoss:0.0466 | top1:79.8000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1969 | MainLoss:0.1969 | SPLoss:0.0002 | CLSLoss:0.0467 | top1:96.3925 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3276 | MainLoss:0.3276 | SPLoss:0.0002 | CLSLoss:0.0467 | top1:88.8401 | AUROC:0.9928\n",
      "\n",
      "Epoch: [274 | 1000] LR: 0.001344\n",
      "Train | 8/8 | Loss:0.4475 | MainLoss:0.4282 | Alpha:0.4099 | SPLoss:0.0002 | CLSLoss:0.0468 | top1:81.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1968 | MainLoss:0.1968 | SPLoss:0.0002 | CLSLoss:0.0469 | top1:96.3115 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3254 | MainLoss:0.3254 | SPLoss:0.0002 | CLSLoss:0.0469 | top1:88.9712 | AUROC:0.9927\n",
      "\n",
      "Epoch: [275 | 1000] LR: 0.001342\n",
      "Train | 8/8 | Loss:0.4519 | MainLoss:0.4325 | Alpha:0.4099 | SPLoss:0.0002 | CLSLoss:0.0470 | top1:80.2250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1963 | MainLoss:0.1963 | SPLoss:0.0002 | CLSLoss:0.0471 | top1:96.3084 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3246 | MainLoss:0.3246 | SPLoss:0.0002 | CLSLoss:0.0471 | top1:88.9974 | AUROC:0.9927\n",
      "\n",
      "Epoch: [276 | 1000] LR: 0.001340\n",
      "Train | 8/8 | Loss:0.5226 | MainLoss:0.5032 | Alpha:0.4102 | SPLoss:0.0002 | CLSLoss:0.0471 | top1:73.4000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1969 | MainLoss:0.1969 | SPLoss:0.0002 | CLSLoss:0.0465 | top1:96.4361 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3285 | MainLoss:0.3285 | SPLoss:0.0002 | CLSLoss:0.0465 | top1:88.7844 | AUROC:0.9928\n",
      "\n",
      "Epoch: [277 | 1000] LR: 0.001338\n",
      "Train | 8/8 | Loss:0.4558 | MainLoss:0.4366 | Alpha:0.4112 | SPLoss:0.0002 | CLSLoss:0.0465 | top1:81.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1973 | MainLoss:0.1973 | SPLoss:0.0002 | CLSLoss:0.0467 | top1:96.3614 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3267 | MainLoss:0.3267 | SPLoss:0.0002 | CLSLoss:0.0467 | top1:88.8991 | AUROC:0.9927\n",
      "\n",
      "Epoch: [278 | 1000] LR: 0.001337\n",
      "Train | 8/8 | Loss:0.5114 | MainLoss:0.4922 | Alpha:0.4099 | SPLoss:0.0003 | CLSLoss:0.0465 | top1:73.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2006 | MainLoss:0.2006 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:96.2586 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3257 | MainLoss:0.3257 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:89.0695 | AUROC:0.9928\n",
      "\n",
      "Epoch: [279 | 1000] LR: 0.001335\n",
      "Train | 8/8 | Loss:0.4776 | MainLoss:0.4585 | Alpha:0.4104 | SPLoss:0.0003 | CLSLoss:0.0462 | top1:81.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2005 | MainLoss:0.2005 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:96.2617 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3257 | MainLoss:0.3257 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:89.0662 | AUROC:0.9928\n",
      "\n",
      "Epoch: [280 | 1000] LR: 0.001333\n",
      "Train | 8/8 | Loss:0.4740 | MainLoss:0.4550 | Alpha:0.4105 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:78.8250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1996 | MainLoss:0.1996 | SPLoss:0.0003 | CLSLoss:0.0462 | top1:96.3084 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3268 | MainLoss:0.3268 | SPLoss:0.0003 | CLSLoss:0.0462 | top1:88.9613 | AUROC:0.9927\n",
      "\n",
      "Epoch: [281 | 1000] LR: 0.001331\n",
      "Train | 8/8 | Loss:0.5210 | MainLoss:0.5020 | Alpha:0.4113 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:75.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2019 | MainLoss:0.2019 | SPLoss:0.0003 | CLSLoss:0.0458 | top1:96.2243 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3260 | MainLoss:0.3260 | SPLoss:0.0003 | CLSLoss:0.0458 | top1:89.1284 | AUROC:0.9927\n",
      "\n",
      "Epoch: [282 | 1000] LR: 0.001329\n",
      "Train | 8/8 | Loss:0.4436 | MainLoss:0.4246 | Alpha:0.4106 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:82.3750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2009 | MainLoss:0.2009 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:96.2181 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3248 | MainLoss:0.3248 | SPLoss:0.0003 | CLSLoss:0.0461 | top1:89.1547 | AUROC:0.9928\n",
      "\n",
      "Epoch: [283 | 1000] LR: 0.001327\n",
      "Train | 8/8 | Loss:0.3899 | MainLoss:0.3707 | Alpha:0.4107 | SPLoss:0.0003 | CLSLoss:0.0464 | top1:85.2750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1984 | MainLoss:0.1984 | SPLoss:0.0003 | CLSLoss:0.0469 | top1:96.1682 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3215 | MainLoss:0.3215 | SPLoss:0.0003 | CLSLoss:0.0469 | top1:89.2693 | AUROC:0.9927\n",
      "\n",
      "Epoch: [284 | 1000] LR: 0.001325\n",
      "Train | 8/8 | Loss:0.5382 | MainLoss:0.5189 | Alpha:0.4109 | SPLoss:0.0003 | CLSLoss:0.0467 | top1:74.3500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1990 | MainLoss:0.1990 | SPLoss:0.0003 | CLSLoss:0.0462 | top1:96.2991 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3263 | MainLoss:0.3263 | SPLoss:0.0003 | CLSLoss:0.0462 | top1:89.0072 | AUROC:0.9927\n",
      "\n",
      "Epoch: [285 | 1000] LR: 0.001323\n",
      "Train | 8/8 | Loss:0.4858 | MainLoss:0.4668 | Alpha:0.4113 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:77.6750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1997 | MainLoss:0.1997 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:96.3022 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3270 | MainLoss:0.3270 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:89.0039 | AUROC:0.9928\n",
      "\n",
      "Epoch: [286 | 1000] LR: 0.001321\n",
      "Train | 8/8 | Loss:0.4760 | MainLoss:0.4570 | Alpha:0.4107 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:78.7250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2011 | MainLoss:0.2011 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:96.1994 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3246 | MainLoss:0.3246 | SPLoss:0.0003 | CLSLoss:0.0460 | top1:89.2038 | AUROC:0.9928\n",
      "\n",
      "Epoch: [287 | 1000] LR: 0.001320\n",
      "Train | 8/8 | Loss:0.4522 | MainLoss:0.4332 | Alpha:0.4102 | SPLoss:0.0003 | CLSLoss:0.0462 | top1:82.4500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2003 | MainLoss:0.2003 | SPLoss:0.0003 | CLSLoss:0.0463 | top1:96.1932 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3238 | MainLoss:0.3238 | SPLoss:0.0003 | CLSLoss:0.0463 | top1:89.2169 | AUROC:0.9927\n",
      "\n",
      "Epoch: [288 | 1000] LR: 0.001318\n",
      "Train | 8/8 | Loss:0.5087 | MainLoss:0.4895 | Alpha:0.4109 | SPLoss:0.0003 | CLSLoss:0.0463 | top1:77.3000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2009 | MainLoss:0.2009 | SPLoss:0.0003 | CLSLoss:0.0459 | top1:96.2150 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3255 | MainLoss:0.3255 | SPLoss:0.0003 | CLSLoss:0.0459 | top1:89.1383 | AUROC:0.9928\n",
      "\n",
      "Epoch: [289 | 1000] LR: 0.001316\n",
      "Train | 8/8 | Loss:0.4795 | MainLoss:0.4606 | Alpha:0.4095 | SPLoss:0.0003 | CLSLoss:0.0458 | top1:74.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2009 | MainLoss:0.2009 | SPLoss:0.0004 | CLSLoss:0.0458 | top1:96.2555 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3267 | MainLoss:0.3267 | SPLoss:0.0004 | CLSLoss:0.0458 | top1:89.0662 | AUROC:0.9928\n",
      "\n",
      "Epoch: [290 | 1000] LR: 0.001314\n",
      "Train | 8/8 | Loss:0.4621 | MainLoss:0.4432 | Alpha:0.4102 | SPLoss:0.0004 | CLSLoss:0.0457 | top1:81.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2005 | MainLoss:0.2005 | SPLoss:0.0004 | CLSLoss:0.0459 | top1:96.2368 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3260 | MainLoss:0.3260 | SPLoss:0.0004 | CLSLoss:0.0459 | top1:89.1088 | AUROC:0.9927\n",
      "\n",
      "Epoch: [291 | 1000] LR: 0.001312\n",
      "Train | 8/8 | Loss:0.4570 | MainLoss:0.4379 | Alpha:0.4108 | SPLoss:0.0004 | CLSLoss:0.0460 | top1:78.8750 | AUROC:0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 65/8 | Loss:0.1990 | MainLoss:0.1990 | SPLoss:0.0004 | CLSLoss:0.0461 | top1:96.3022 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3267 | MainLoss:0.3267 | SPLoss:0.0004 | CLSLoss:0.0461 | top1:89.0138 | AUROC:0.9928\n",
      "\n",
      "Epoch: [292 | 1000] LR: 0.001310\n",
      "Train | 8/8 | Loss:0.5229 | MainLoss:0.5040 | Alpha:0.4102 | SPLoss:0.0004 | CLSLoss:0.0458 | top1:73.1250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2003 | MainLoss:0.2003 | SPLoss:0.0004 | CLSLoss:0.0457 | top1:96.3115 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3281 | MainLoss:0.3281 | SPLoss:0.0004 | CLSLoss:0.0457 | top1:88.9581 | AUROC:0.9928\n",
      "\n",
      "Epoch: [293 | 1000] LR: 0.001308\n",
      "Train | 8/8 | Loss:0.5055 | MainLoss:0.4866 | Alpha:0.4111 | SPLoss:0.0004 | CLSLoss:0.0456 | top1:77.5250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2017 | MainLoss:0.2017 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:96.2928 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3282 | MainLoss:0.3282 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:89.0170 | AUROC:0.9927\n",
      "\n",
      "Epoch: [294 | 1000] LR: 0.001306\n",
      "Train | 8/8 | Loss:0.4786 | MainLoss:0.4598 | Alpha:0.4109 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:79.0250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2017 | MainLoss:0.2017 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:96.2461 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3273 | MainLoss:0.3273 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:89.0793 | AUROC:0.9928\n",
      "\n",
      "Epoch: [295 | 1000] LR: 0.001304\n",
      "Train | 8/8 | Loss:0.4933 | MainLoss:0.4745 | Alpha:0.4111 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:78.9250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2019 | MainLoss:0.2019 | SPLoss:0.0004 | CLSLoss:0.0452 | top1:96.2773 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3281 | MainLoss:0.3281 | SPLoss:0.0004 | CLSLoss:0.0452 | top1:89.0433 | AUROC:0.9928\n",
      "\n",
      "Epoch: [296 | 1000] LR: 0.001302\n",
      "Train | 8/8 | Loss:0.5001 | MainLoss:0.4815 | Alpha:0.4101 | SPLoss:0.0004 | CLSLoss:0.0450 | top1:78.4250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2022 | MainLoss:0.2022 | SPLoss:0.0004 | CLSLoss:0.0450 | top1:96.3084 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3292 | MainLoss:0.3292 | SPLoss:0.0004 | CLSLoss:0.0450 | top1:88.9744 | AUROC:0.9927\n",
      "\n",
      "Epoch: [297 | 1000] LR: 0.001300\n",
      "Train | 8/8 | Loss:0.5009 | MainLoss:0.4824 | Alpha:0.4093 | SPLoss:0.0004 | CLSLoss:0.0448 | top1:77.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2026 | MainLoss:0.2026 | SPLoss:0.0004 | CLSLoss:0.0449 | top1:96.3240 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3302 | MainLoss:0.3302 | SPLoss:0.0004 | CLSLoss:0.0449 | top1:88.9384 | AUROC:0.9928\n",
      "\n",
      "Epoch: [298 | 1000] LR: 0.001298\n",
      "Train | 8/8 | Loss:0.4615 | MainLoss:0.4429 | Alpha:0.4102 | SPLoss:0.0004 | CLSLoss:0.0449 | top1:83.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2018 | MainLoss:0.2018 | SPLoss:0.0004 | CLSLoss:0.0450 | top1:96.3240 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3298 | MainLoss:0.3298 | SPLoss:0.0004 | CLSLoss:0.0450 | top1:88.9351 | AUROC:0.9928\n",
      "\n",
      "Epoch: [299 | 1000] LR: 0.001296\n",
      "Train | 8/8 | Loss:0.4342 | MainLoss:0.4155 | Alpha:0.4103 | SPLoss:0.0004 | CLSLoss:0.0452 | top1:83.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.2011 | MainLoss:0.2011 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:96.2368 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3270 | MainLoss:0.3270 | SPLoss:0.0004 | CLSLoss:0.0454 | top1:89.0891 | AUROC:0.9928\n",
      "\n",
      "Epoch: [300 | 1000] LR: 0.001294\n",
      "Train | 8/8 | Loss:0.4438 | MainLoss:0.4249 | Alpha:0.4110 | SPLoss:0.0005 | CLSLoss:0.0456 | top1:82.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1991 | MainLoss:0.1991 | SPLoss:0.0005 | CLSLoss:0.0458 | top1:96.3084 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3270 | MainLoss:0.3270 | SPLoss:0.0005 | CLSLoss:0.0458 | top1:89.0105 | AUROC:0.9927\n",
      "\n",
      "Epoch: [301 | 1000] LR: 0.001292\n",
      "Train | 8/8 | Loss:0.4403 | MainLoss:0.4215 | Alpha:0.4101 | SPLoss:0.0000 | CLSLoss:0.0459 | top1:81.0500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1990 | MainLoss:0.1990 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:96.2274 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3245 | MainLoss:0.3245 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:89.1645 | AUROC:0.9927\n",
      "\n",
      "Epoch: [302 | 1000] LR: 0.001290\n",
      "Train | 8/8 | Loss:0.4355 | MainLoss:0.4165 | Alpha:0.4112 | SPLoss:0.0000 | CLSLoss:0.0462 | top1:80.8750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1985 | MainLoss:0.1985 | SPLoss:0.0000 | CLSLoss:0.0463 | top1:96.1963 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3228 | MainLoss:0.3228 | SPLoss:0.0000 | CLSLoss:0.0463 | top1:89.2562 | AUROC:0.9927\n",
      "\n",
      "Epoch: [303 | 1000] LR: 0.001288\n",
      "Train | 8/8 | Loss:0.4213 | MainLoss:0.4021 | Alpha:0.4110 | SPLoss:0.0000 | CLSLoss:0.0465 | top1:84.6250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1976 | MainLoss:0.1976 | SPLoss:0.0000 | CLSLoss:0.0467 | top1:96.1433 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3205 | MainLoss:0.3205 | SPLoss:0.0000 | CLSLoss:0.0467 | top1:89.3578 | AUROC:0.9926\n",
      "\n",
      "Epoch: [304 | 1000] LR: 0.001286\n",
      "Train | 8/8 | Loss:0.4482 | MainLoss:0.4290 | Alpha:0.4108 | SPLoss:0.0000 | CLSLoss:0.0469 | top1:76.9750 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1973 | MainLoss:0.1973 | SPLoss:0.0000 | CLSLoss:0.0468 | top1:96.1340 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3199 | MainLoss:0.3199 | SPLoss:0.0000 | CLSLoss:0.0468 | top1:89.3807 | AUROC:0.9926\n",
      "\n",
      "Epoch: [305 | 1000] LR: 0.001284\n",
      "Train | 8/8 | Loss:0.5559 | MainLoss:0.5369 | Alpha:0.4106 | SPLoss:0.0000 | CLSLoss:0.0463 | top1:74.3250 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1990 | MainLoss:0.1990 | SPLoss:0.0000 | CLSLoss:0.0459 | top1:96.2461 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3250 | MainLoss:0.3250 | SPLoss:0.0000 | CLSLoss:0.0459 | top1:89.1252 | AUROC:0.9927\n",
      "\n",
      "Epoch: [306 | 1000] LR: 0.001282\n",
      "Train | 8/8 | Loss:0.4579 | MainLoss:0.4390 | Alpha:0.4106 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:80.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1991 | MainLoss:0.1991 | SPLoss:0.0000 | CLSLoss:0.0460 | top1:96.2181 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3239 | MainLoss:0.3239 | SPLoss:0.0000 | CLSLoss:0.0460 | top1:89.2104 | AUROC:0.9927\n",
      "\n",
      "Epoch: [307 | 1000] LR: 0.001280\n",
      "Train | 8/8 | Loss:0.4582 | MainLoss:0.4393 | Alpha:0.4102 | SPLoss:0.0000 | CLSLoss:0.0460 | top1:81.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1990 | MainLoss:0.1990 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:96.1994 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3232 | MainLoss:0.3232 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:89.2431 | AUROC:0.9927\n",
      "\n",
      "Epoch: [308 | 1000] LR: 0.001278\n",
      "Train | 8/8 | Loss:0.3998 | MainLoss:0.3807 | Alpha:0.4109 | SPLoss:0.0000 | CLSLoss:0.0464 | top1:84.8500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1975 | MainLoss:0.1975 | SPLoss:0.0000 | CLSLoss:0.0467 | top1:96.1308 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3200 | MainLoss:0.3200 | SPLoss:0.0000 | CLSLoss:0.0467 | top1:89.4070 | AUROC:0.9927\n",
      "\n",
      "Epoch: [309 | 1000] LR: 0.001276\n",
      "Train | 8/8 | Loss:0.5214 | MainLoss:0.5023 | Alpha:0.4107 | SPLoss:0.0000 | CLSLoss:0.0467 | top1:76.2000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1988 | MainLoss:0.1988 | SPLoss:0.0000 | CLSLoss:0.0462 | top1:96.2056 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3229 | MainLoss:0.3229 | SPLoss:0.0000 | CLSLoss:0.0462 | top1:89.2398 | AUROC:0.9927\n",
      "\n",
      "Epoch: [310 | 1000] LR: 0.001274\n",
      "Train | 8/8 | Loss:0.4927 | MainLoss:0.4737 | Alpha:0.4114 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:80.6500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1987 | MainLoss:0.1987 | SPLoss:0.0000 | CLSLoss:0.0460 | top1:96.2710 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3251 | MainLoss:0.3251 | SPLoss:0.0000 | CLSLoss:0.0460 | top1:89.0924 | AUROC:0.9927\n",
      "\n",
      "Epoch: [311 | 1000] LR: 0.001272\n",
      "Train | 8/8 | Loss:0.4384 | MainLoss:0.4194 | Alpha:0.4115 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:81.1500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1973 | MainLoss:0.1973 | SPLoss:0.0000 | CLSLoss:0.0463 | top1:96.2804 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3248 | MainLoss:0.3248 | SPLoss:0.0000 | CLSLoss:0.0463 | top1:89.0433 | AUROC:0.9927\n",
      "\n",
      "Epoch: [312 | 1000] LR: 0.001270\n",
      "Train | 8/8 | Loss:0.4757 | MainLoss:0.4567 | Alpha:0.4105 | SPLoss:0.0000 | CLSLoss:0.0461 | top1:77.7500 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1978 | MainLoss:0.1978 | SPLoss:0.0000 | CLSLoss:0.0462 | top1:96.2804 | AUROC:0.9991\n",
      "Test | 62/8 | Loss:0.3247 | MainLoss:0.3247 | SPLoss:0.0000 | CLSLoss:0.0462 | top1:89.0596 | AUROC:0.9927\n",
      "\n",
      "Epoch: [313 | 1000] LR: 0.001268\n",
      "Train | 8/8 | Loss:0.5030 | MainLoss:0.4841 | Alpha:0.4105 | SPLoss:0.0000 | CLSLoss:0.0460 | top1:79.0000 | AUROC:0.0000\n",
      "Test | 65/8 | Loss:0.1974 | MainLoss:0.1974 | SPLoss:0.0000 | CLSLoss:0.0459 | top1:96.3801 | AUROC:0.9991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test | 62/8 | Loss:0.3281 | MainLoss:0.3281 | SPLoss:0.0000 | CLSLoss:0.0459 | top1:88.8532 | AUROC:0.9926\n",
      "\n",
      "Epoch: [314 | 1000] LR: 0.001266\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, teacher_model, student_model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_target_loader, student_model, criterion, epoch, use_cuda)\n",
    "    source_loss, source_acc, source_auroc = test(val_source_loader, student_model, criterion, epoch, use_cuda)\n",
    "\n",
    "    loss_list.append([train_loss, test_loss, source_loss])\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss,  source_loss, train_acc, test_acc,source_acc, train_auroc, test_auroc, source_auroc])\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : student_model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)\n",
    "    scheduler_warmup.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        teacher_model.load_state_dict(student_model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
