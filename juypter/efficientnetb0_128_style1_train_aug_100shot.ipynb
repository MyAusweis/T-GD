{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "from model_pytorch import EfficientNet\n",
    "from utils import Bar,Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageFile, ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 3: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = 3\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device %d:\" %(gpu_id), use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data2/dataset/GAN_ImageData/StyleGAN_256/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = ''\n",
    "resume = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_name = 'efficientnet-b0' # b0-b7 scale\n",
    "\n",
    "# Optimization\n",
    "num_classes = 2\n",
    "epochs = 400\n",
    "start_epoch = 0\n",
    "train_batch = 100\n",
    "test_batch = 100\n",
    "lr = 0.04\n",
    "schedule = [75, 150, 225]\n",
    "momentum = 0.9\n",
    "gamma = 0.1 # LR is multiplied by gamma on schedule\n",
    "\n",
    "# CheckPoint\n",
    "checkpoint = './log/style1/128/b0/100shot' # dir\n",
    "if not os.path.isdir(checkpoint):\n",
    "    os.makedirs(checkpoint)\n",
    "num_workers = 8\n",
    "\n",
    "# Image\n",
    "ex_size = (140, 140)\n",
    "size = (128, 128)\n",
    "\n",
    "# Seed\n",
    "manual_seed = 7\n",
    "random.seed(manual_seed)\n",
    "torch.cuda.manual_seed_all(manual_seed)\n",
    "\n",
    "# cutmix\n",
    "cm_prob = 0.5\n",
    "cm_prob_init = 0.99\n",
    "cm_prob_low = 0.01\n",
    "cm_beta = 1.0\n",
    "\n",
    "# augmentation\n",
    "blur_prob = 0.5\n",
    "blog_sig = 0.5\n",
    "jpg_prob = 0.5\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {}\n",
    "state['num_classes'] = num_classes\n",
    "state['epochs'] = epochs\n",
    "state['start_epoch'] = start_epoch\n",
    "state['train_batch'] = train_batch\n",
    "state['test_batch'] = test_batch\n",
    "state['lr'] = lr\n",
    "state['schedule'] = schedule\n",
    "state['momentum'] = momentum\n",
    "state['gamma'] = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(img):\n",
    "    img = np.array(img)\n",
    "\n",
    "    if random.random() < blur_prob:\n",
    "        sig = np.random.uniform(0.0, 3.0)\n",
    "        gaussian_blur(img, sig)\n",
    "\n",
    "    if random.random() < jpg_prob:\n",
    "        qual = np.random.uniform(30.0, 100.0)\n",
    "        img = cv2_jpg(img, qual)\n",
    "\n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def gaussian_blur(img, sigma):\n",
    "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
    "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
    "\n",
    "\n",
    "def cv2_jpg(img, compress_val):\n",
    "    img_cv2 = img[:,:,::-1]\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
    "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
    "    decimg = cv2.imdecode(encimg, 1)\n",
    "    return decimg[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(data_dir, '100_shot')\n",
    "val_dir = os.path.join(data_dir, 'validation')    \n",
    "train_aug = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: data_augment(img)),\n",
    "    transforms.Resize(ex_size),\n",
    "    transforms.RandomCrop(size),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#     transforms.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3), value=0, inplace=True),\n",
    "])\n",
    "\n",
    "val_aug = transforms.Compose([\n",
    "    transforms.Resize(ex_size),\n",
    "    transforms.CenterCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# pin_memory : cuda pin memeory use\n",
    "train_loader = DataLoader(datasets.ImageFolder(train_dir, transform=train_aug),\n",
    "                          batch_size=train_batch, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(datasets.ImageFolder(val_dir, val_aug),\n",
    "                       batch_size=test_batch, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientNet.from_name(model_name, num_classes=num_classes,\n",
    "                              override_params={'dropout_rate':0.2, 'drop_connect_rate':0.2})\n",
    "\n",
    "# Pre-trained\n",
    "if pretrained:\n",
    "    print(\"=> using pre-trained model '{}'\".format(pretrained))\n",
    "    model.load_state_dict(torch.load(pretrained)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_gn0): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 16, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 96, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 24, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 144, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 40, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 240, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 80, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 480, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 112, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 672, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn0): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_gn1): GroupNorm(8, 1152, eps=1e-05, affine=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_gn2): GroupNorm(8, 320, eps=1e-05, affine=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_gn1): GroupNorm(8, 1280, eps=1e-05, affine=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (_fc): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total params: 4.01M\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=10, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    checkpoint = os.path.dirname(resume)\n",
    "#     checkpoint = torch.load(resume)\n",
    "    resume = torch.load(resume)\n",
    "    best_acc = resume['best_acc']\n",
    "    start_epoch = resume['epoch']\n",
    "    model.load_state_dict(resume['state_dict'])\n",
    "    optimizer.load_state_dict(resume['optimizer'])\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(checkpoint, 'log.txt'))\n",
    "    logger.set_names(['Learning Rate', 'Train Loss', 'Valid Loss', 'Train Acc.', 'Valid Acc.', 'Train AUROC.', 'Valid AUROC.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    model.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_size = inputs.size(0)\n",
    "        if batch_size < train_batch:\n",
    "            continue\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "        r = np.random.rand(1)\n",
    "        if cm_beta > 0 and r < cm_prob:\n",
    "            \n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            tt= targets[rand_index]\n",
    "            boolean = targets==tt\n",
    "            rand_index = rand_index[boolean]\n",
    "            lam = np.random.beta(cm_beta, cm_beta)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[boolean, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs.data, targets.data)\n",
    "#         auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "        losses.update(loss.data.tolist(), inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "#         arc.update(auroc, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "#                      batch=batch_idx+1, size=len(train_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "                     batch=batch_idx+1, size=len(train_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(val_loader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    arc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # compute output\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, targets.data)\n",
    "            auroc = roc_auc_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy()[:,1])\n",
    "            losses.update(loss.data.tolist(), inputs.size(0))\n",
    "            top1.update(prec1[0], inputs.size(0))\n",
    "            arc.update(auroc, inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "    print('{batch}/{size} | Loss:{loss:.4f} | top1:{tp1:.4f} | AUROC:{ac:.4f}'.format(\n",
    "         batch=batch_idx+1, size=len(val_loader), loss=losses.avg, tp1=top1.avg, ac=arc.avg))\n",
    "    return (losses.avg, top1.avg, arc.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    lr_set = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    lr_list = schedule.copy()\n",
    "    lr_list.append(epoch)\n",
    "    lr_list.sort()\n",
    "    idx = lr_list.index(epoch)\n",
    "    state['lr'] *= lr_set[idx]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: [1 | 400] LR: 0.040000\n",
      "2/2 | Loss:0.7225 | top1:43.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6970 | top1:49.6923 | AUROC:0.5006\n",
      "\n",
      "Epoch: [2 | 400] LR: 0.068000\n",
      "2/2 | Loss:0.7048 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7194 | top1:50.0513 | AUROC:0.5013\n",
      "\n",
      "Epoch: [3 | 400] LR: 0.096000\n",
      "2/2 | Loss:0.8238 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:1.3549 | top1:50.0000 | AUROC:0.4991\n",
      "\n",
      "Epoch: [4 | 400] LR: 0.124000\n",
      "2/2 | Loss:2.0863 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:3.9877 | top1:50.0000 | AUROC:0.4920\n",
      "\n",
      "Epoch: [5 | 400] LR: 0.152000\n",
      "2/2 | Loss:6.9480 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:14.4638 | top1:50.0000 | AUROC:0.4966\n",
      "\n",
      "Epoch: [6 | 400] LR: 0.180000\n",
      "2/2 | Loss:20.5471 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:27.2936 | top1:50.0000 | AUROC:0.5053\n",
      "\n",
      "Epoch: [7 | 400] LR: 0.208000\n",
      "2/2 | Loss:15.2843 | top1:54.5000 | AUROC:0.0000\n",
      "78/78 | Loss:10.2318 | top1:50.0000 | AUROC:0.4975\n",
      "\n",
      "Epoch: [8 | 400] LR: 0.236000\n",
      "2/2 | Loss:19.4156 | top1:44.0000 | AUROC:0.0000\n",
      "78/78 | Loss:14.0381 | top1:50.0000 | AUROC:0.4995\n",
      "\n",
      "Epoch: [9 | 400] LR: 0.264000\n",
      "2/2 | Loss:10.0696 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:8.1482 | top1:50.0000 | AUROC:0.5079\n",
      "\n",
      "Epoch: [10 | 400] LR: 0.292000\n",
      "2/2 | Loss:4.2455 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:9.2171 | top1:50.0000 | AUROC:0.4972\n",
      "\n",
      "Epoch: [11 | 400] LR: 0.320000\n",
      "2/2 | Loss:8.8634 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.8563 | top1:50.0513 | AUROC:0.4877\n",
      "\n",
      "Epoch: [12 | 400] LR: 0.320000\n",
      "2/2 | Loss:3.5804 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:2.4250 | top1:50.0000 | AUROC:0.4959\n",
      "\n",
      "Epoch: [13 | 400] LR: 0.319995\n",
      "2/2 | Loss:4.1515 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:7.2936 | top1:50.0000 | AUROC:0.4952\n",
      "\n",
      "Epoch: [14 | 400] LR: 0.319980\n",
      "2/2 | Loss:6.0276 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:2.1818 | top1:50.0000 | AUROC:0.4975\n",
      "\n",
      "Epoch: [15 | 400] LR: 0.319956\n",
      "2/2 | Loss:3.2108 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.6252 | top1:50.0000 | AUROC:0.5116\n",
      "\n",
      "Epoch: [16 | 400] LR: 0.319921\n",
      "2/2 | Loss:2.4297 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:4.4822 | top1:50.0000 | AUROC:0.5134\n",
      "\n",
      "Epoch: [17 | 400] LR: 0.319877\n",
      "2/2 | Loss:3.0557 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:3.5754 | top1:50.0000 | AUROC:0.5117\n",
      "\n",
      "Epoch: [18 | 400] LR: 0.319822\n",
      "2/2 | Loss:4.2876 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:2.9450 | top1:50.0000 | AUROC:0.5127\n",
      "\n",
      "Epoch: [19 | 400] LR: 0.319758\n",
      "2/2 | Loss:2.2246 | top1:54.0000 | AUROC:0.0000\n",
      "78/78 | Loss:3.6395 | top1:50.0000 | AUROC:0.5114\n",
      "\n",
      "Epoch: [20 | 400] LR: 0.319684\n",
      "2/2 | Loss:3.0065 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.6474 | top1:50.0000 | AUROC:0.5097\n",
      "\n",
      "Epoch: [21 | 400] LR: 0.319600\n",
      "2/2 | Loss:2.3309 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:1.7856 | top1:50.0000 | AUROC:0.5107\n",
      "\n",
      "Epoch: [22 | 400] LR: 0.319507\n",
      "2/2 | Loss:1.9567 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:2.5838 | top1:50.0000 | AUROC:0.5106\n",
      "\n",
      "Epoch: [23 | 400] LR: 0.319403\n",
      "2/2 | Loss:1.9426 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:1.9979 | top1:50.0000 | AUROC:0.5117\n",
      "\n",
      "Epoch: [24 | 400] LR: 0.319290\n",
      "2/2 | Loss:2.2166 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.8673 | top1:50.0000 | AUROC:0.4952\n",
      "\n",
      "Epoch: [25 | 400] LR: 0.319167\n",
      "2/2 | Loss:1.7010 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:2.2891 | top1:50.0000 | AUROC:0.4961\n",
      "\n",
      "Epoch: [26 | 400] LR: 0.319034\n",
      "2/2 | Loss:1.6826 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.7321 | top1:50.0000 | AUROC:0.4874\n",
      "\n",
      "Epoch: [27 | 400] LR: 0.318891\n",
      "2/2 | Loss:1.9176 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7847 | top1:50.0000 | AUROC:0.4895\n",
      "\n",
      "Epoch: [28 | 400] LR: 0.318738\n",
      "2/2 | Loss:1.2628 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.4749 | top1:50.0000 | AUROC:0.4903\n",
      "\n",
      "Epoch: [29 | 400] LR: 0.318576\n",
      "2/2 | Loss:1.2635 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.2924 | top1:50.0000 | AUROC:0.4886\n",
      "\n",
      "Epoch: [30 | 400] LR: 0.318404\n",
      "2/2 | Loss:1.4165 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6951 | top1:50.0000 | AUROC:0.4854\n",
      "\n",
      "Epoch: [31 | 400] LR: 0.318222\n",
      "2/2 | Loss:1.1978 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.1523 | top1:50.0000 | AUROC:0.4875\n",
      "\n",
      "Epoch: [32 | 400] LR: 0.318030\n",
      "2/2 | Loss:1.0529 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:1.2461 | top1:50.0000 | AUROC:0.4904\n",
      "\n",
      "Epoch: [33 | 400] LR: 0.317829\n",
      "2/2 | Loss:1.3302 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6935 | top1:49.3205 | AUROC:0.4878\n",
      "\n",
      "Epoch: [34 | 400] LR: 0.317617\n",
      "2/2 | Loss:1.0844 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.1222 | top1:50.0000 | AUROC:0.4891\n",
      "\n",
      "Epoch: [35 | 400] LR: 0.317397\n",
      "2/2 | Loss:1.1070 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:1.1659 | top1:50.0000 | AUROC:0.4884\n",
      "\n",
      "Epoch: [36 | 400] LR: 0.317166\n",
      "2/2 | Loss:1.1076 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7260 | top1:50.0000 | AUROC:0.4873\n",
      "\n",
      "Epoch: [37 | 400] LR: 0.316926\n",
      "2/2 | Loss:1.0732 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:1.0133 | top1:50.0000 | AUROC:0.4892\n",
      "\n",
      "Epoch: [38 | 400] LR: 0.316676\n",
      "2/2 | Loss:1.1077 | top1:44.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.9920 | top1:50.0000 | AUROC:0.4879\n",
      "\n",
      "Epoch: [39 | 400] LR: 0.316416\n",
      "2/2 | Loss:1.0951 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7288 | top1:50.0000 | AUROC:0.4890\n",
      "\n",
      "Epoch: [40 | 400] LR: 0.316147\n",
      "2/2 | Loss:0.9331 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.8570 | top1:50.0000 | AUROC:0.4893\n",
      "\n",
      "Epoch: [41 | 400] LR: 0.315868\n",
      "2/2 | Loss:0.8970 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.9047 | top1:50.0000 | AUROC:0.4890\n",
      "\n",
      "Epoch: [42 | 400] LR: 0.315579\n",
      "2/2 | Loss:0.9703 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6963 | top1:50.0000 | AUROC:0.4903\n",
      "\n",
      "Epoch: [43 | 400] LR: 0.315281\n",
      "2/2 | Loss:0.8708 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7325 | top1:50.0000 | AUROC:0.4882\n",
      "\n",
      "Epoch: [44 | 400] LR: 0.314973\n",
      "2/2 | Loss:0.8293 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7791 | top1:50.0000 | AUROC:0.4898\n",
      "\n",
      "Epoch: [45 | 400] LR: 0.314656\n",
      "2/2 | Loss:0.8507 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.2949 | AUROC:0.4893\n",
      "\n",
      "Epoch: [46 | 400] LR: 0.314329\n",
      "2/2 | Loss:0.8608 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7218 | top1:50.0000 | AUROC:0.4883\n",
      "\n",
      "Epoch: [47 | 400] LR: 0.313993\n",
      "2/2 | Loss:0.8135 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7442 | top1:50.0000 | AUROC:0.4892\n",
      "\n",
      "Epoch: [48 | 400] LR: 0.313647\n",
      "2/2 | Loss:0.7746 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7023 | top1:50.0000 | AUROC:0.4899\n",
      "\n",
      "Epoch: [49 | 400] LR: 0.313292\n",
      "2/2 | Loss:0.8080 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7257 | top1:50.0000 | AUROC:0.4886\n",
      "\n",
      "Epoch: [50 | 400] LR: 0.312927\n",
      "2/2 | Loss:0.7912 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7593 | top1:50.0000 | AUROC:0.4877\n",
      "\n",
      "Epoch: [51 | 400] LR: 0.312553\n",
      "2/2 | Loss:0.8040 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6944 | top1:50.0000 | AUROC:0.4920\n",
      "\n",
      "Epoch: [52 | 400] LR: 0.312169\n",
      "2/2 | Loss:0.7378 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7049 | top1:50.0000 | AUROC:0.4905\n",
      "\n",
      "Epoch: [53 | 400] LR: 0.311776\n",
      "2/2 | Loss:0.7346 | top1:54.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7165 | top1:50.0000 | AUROC:0.4907\n",
      "\n",
      "Epoch: [54 | 400] LR: 0.311374\n",
      "2/2 | Loss:0.7902 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0000 | AUROC:0.4903\n",
      "\n",
      "Epoch: [55 | 400] LR: 0.310962\n",
      "2/2 | Loss:0.7698 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7211 | top1:50.0000 | AUROC:0.4924\n",
      "\n",
      "Epoch: [56 | 400] LR: 0.310541\n",
      "2/2 | Loss:0.7127 | top1:60.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7221 | top1:50.0000 | AUROC:0.4901\n",
      "\n",
      "Epoch: [57 | 400] LR: 0.310111\n",
      "2/2 | Loss:0.7457 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6935 | top1:50.0000 | AUROC:0.4908\n",
      "\n",
      "Epoch: [58 | 400] LR: 0.309671\n",
      "2/2 | Loss:0.7296 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7068 | top1:50.0000 | AUROC:0.4926\n",
      "\n",
      "Epoch: [59 | 400] LR: 0.309222\n",
      "2/2 | Loss:0.7405 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7011 | top1:50.0000 | AUROC:0.4920\n",
      "\n",
      "Epoch: [60 | 400] LR: 0.308764\n",
      "2/2 | Loss:0.7399 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7049 | top1:50.0000 | AUROC:0.4919\n",
      "\n",
      "Epoch: [61 | 400] LR: 0.308297\n",
      "2/2 | Loss:0.7004 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6964 | top1:50.0000 | AUROC:0.4932\n",
      "\n",
      "Epoch: [62 | 400] LR: 0.307821\n",
      "2/2 | Loss:0.7376 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7029 | top1:50.0000 | AUROC:0.4930\n",
      "\n",
      "Epoch: [63 | 400] LR: 0.307335\n",
      "2/2 | Loss:0.7131 | top1:55.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7146 | top1:50.0000 | AUROC:0.4931\n",
      "\n",
      "Epoch: [64 | 400] LR: 0.306841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 | Loss:0.7342 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6934 | top1:50.0897 | AUROC:0.4948\n",
      "\n",
      "Epoch: [65 | 400] LR: 0.306337\n",
      "2/2 | Loss:0.7071 | top1:54.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6987 | top1:50.0000 | AUROC:0.4944\n",
      "\n",
      "Epoch: [66 | 400] LR: 0.305825\n",
      "2/2 | Loss:0.7298 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6957 | top1:50.0000 | AUROC:0.4928\n",
      "\n",
      "Epoch: [67 | 400] LR: 0.305303\n",
      "2/2 | Loss:0.7246 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6941 | top1:50.0000 | AUROC:0.4925\n",
      "\n",
      "Epoch: [68 | 400] LR: 0.304772\n",
      "2/2 | Loss:0.7419 | top1:43.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.7041 | top1:50.0000 | AUROC:0.4946\n",
      "\n",
      "Epoch: [69 | 400] LR: 0.304233\n",
      "2/2 | Loss:0.7100 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6943 | top1:50.0000 | AUROC:0.4922\n",
      "\n",
      "Epoch: [70 | 400] LR: 0.303684\n",
      "2/2 | Loss:0.7100 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6956 | top1:50.0000 | AUROC:0.4915\n",
      "\n",
      "Epoch: [71 | 400] LR: 0.303127\n",
      "2/2 | Loss:0.7157 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6990 | top1:50.0000 | AUROC:0.4925\n",
      "\n",
      "Epoch: [72 | 400] LR: 0.302561\n",
      "2/2 | Loss:0.7055 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0641 | AUROC:0.4919\n",
      "\n",
      "Epoch: [73 | 400] LR: 0.301986\n",
      "2/2 | Loss:0.6931 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6981 | top1:50.0000 | AUROC:0.4924\n",
      "\n",
      "Epoch: [74 | 400] LR: 0.301403\n",
      "2/2 | Loss:0.7131 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6941 | top1:50.0641 | AUROC:0.4946\n",
      "\n",
      "Epoch: [75 | 400] LR: 0.300810\n",
      "2/2 | Loss:0.6982 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6982 | top1:50.0000 | AUROC:0.4949\n",
      "\n",
      "Epoch: [76 | 400] LR: 0.300209\n",
      "2/2 | Loss:0.6998 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6948 | top1:50.0000 | AUROC:0.4943\n",
      "\n",
      "Epoch: [77 | 400] LR: 0.029960\n",
      "2/2 | Loss:0.7032 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6943 | top1:50.0000 | AUROC:0.4922\n",
      "\n",
      "Epoch: [78 | 400] LR: 0.029898\n",
      "2/2 | Loss:0.7017 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6935 | top1:50.0000 | AUROC:0.4944\n",
      "\n",
      "Epoch: [79 | 400] LR: 0.029835\n",
      "2/2 | Loss:0.6978 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4615 | AUROC:0.4947\n",
      "\n",
      "Epoch: [80 | 400] LR: 0.029772\n",
      "2/2 | Loss:0.7080 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6935 | top1:50.1026 | AUROC:0.4924\n",
      "\n",
      "Epoch: [81 | 400] LR: 0.029707\n",
      "2/2 | Loss:0.7304 | top1:42.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6937 | top1:50.0769 | AUROC:0.4953\n",
      "\n",
      "Epoch: [82 | 400] LR: 0.029642\n",
      "2/2 | Loss:0.7036 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0641 | AUROC:0.4948\n",
      "\n",
      "Epoch: [83 | 400] LR: 0.029576\n",
      "2/2 | Loss:0.7014 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6282 | AUROC:0.4942\n",
      "\n",
      "Epoch: [84 | 400] LR: 0.029509\n",
      "2/2 | Loss:0.6799 | top1:57.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0000 | AUROC:0.4952\n",
      "\n",
      "Epoch: [85 | 400] LR: 0.029441\n",
      "2/2 | Loss:0.6892 | top1:53.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0000 | AUROC:0.4946\n",
      "\n",
      "Epoch: [86 | 400] LR: 0.029373\n",
      "2/2 | Loss:0.7055 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9103 | AUROC:0.4942\n",
      "\n",
      "Epoch: [87 | 400] LR: 0.029304\n",
      "2/2 | Loss:0.6950 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4942\n",
      "\n",
      "Epoch: [88 | 400] LR: 0.029233\n",
      "2/2 | Loss:0.6837 | top1:55.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6934 | top1:50.0769 | AUROC:0.4950\n",
      "\n",
      "Epoch: [89 | 400] LR: 0.029162\n",
      "2/2 | Loss:0.6982 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1154 | AUROC:0.4939\n",
      "\n",
      "Epoch: [90 | 400] LR: 0.029090\n",
      "2/2 | Loss:0.6938 | top1:58.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0513 | AUROC:0.4942\n",
      "\n",
      "Epoch: [91 | 400] LR: 0.029018\n",
      "2/2 | Loss:0.6990 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0513 | AUROC:0.4947\n",
      "\n",
      "Epoch: [92 | 400] LR: 0.028944\n",
      "2/2 | Loss:0.7086 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4927\n",
      "\n",
      "Epoch: [93 | 400] LR: 0.028870\n",
      "2/2 | Loss:0.7018 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0000 | AUROC:0.4941\n",
      "\n",
      "Epoch: [94 | 400] LR: 0.028795\n",
      "2/2 | Loss:0.7170 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.9744 | AUROC:0.4926\n",
      "\n",
      "Epoch: [95 | 400] LR: 0.028719\n",
      "2/2 | Loss:0.7054 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6795 | AUROC:0.4954\n",
      "\n",
      "Epoch: [96 | 400] LR: 0.028642\n",
      "2/2 | Loss:0.7116 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0769 | AUROC:0.4942\n",
      "\n",
      "Epoch: [97 | 400] LR: 0.028565\n",
      "2/2 | Loss:0.7057 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0513 | AUROC:0.4927\n",
      "\n",
      "Epoch: [98 | 400] LR: 0.028487\n",
      "2/2 | Loss:0.7199 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1923 | AUROC:0.4935\n",
      "\n",
      "Epoch: [99 | 400] LR: 0.028408\n",
      "2/2 | Loss:0.6965 | top1:56.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1538 | AUROC:0.4935\n",
      "\n",
      "Epoch: [100 | 400] LR: 0.028328\n",
      "2/2 | Loss:0.7008 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4937\n",
      "\n",
      "Epoch: [101 | 400] LR: 0.028248\n",
      "2/2 | Loss:0.7006 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1154 | AUROC:0.4945\n",
      "\n",
      "Epoch: [102 | 400] LR: 0.028166\n",
      "2/2 | Loss:0.7111 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.8846 | AUROC:0.4940\n",
      "\n",
      "Epoch: [103 | 400] LR: 0.028085\n",
      "2/2 | Loss:0.6848 | top1:58.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.9103 | AUROC:0.4937\n",
      "\n",
      "Epoch: [104 | 400] LR: 0.028002\n",
      "2/2 | Loss:0.6897 | top1:53.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0000 | AUROC:0.4954\n",
      "\n",
      "Epoch: [105 | 400] LR: 0.027918\n",
      "2/2 | Loss:0.6859 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.8462 | AUROC:0.4950\n",
      "\n",
      "Epoch: [106 | 400] LR: 0.027834\n",
      "2/2 | Loss:0.6992 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6154 | AUROC:0.4952\n",
      "\n",
      "Epoch: [107 | 400] LR: 0.027749\n",
      "2/2 | Loss:0.7040 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0000 | AUROC:0.4936\n",
      "\n",
      "Epoch: [108 | 400] LR: 0.027663\n",
      "2/2 | Loss:0.6937 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9359 | AUROC:0.4938\n",
      "\n",
      "Epoch: [109 | 400] LR: 0.027577\n",
      "2/2 | Loss:0.7044 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0000 | AUROC:0.4940\n",
      "\n",
      "Epoch: [110 | 400] LR: 0.027490\n",
      "2/2 | Loss:0.6995 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9615 | AUROC:0.4949\n",
      "\n",
      "Epoch: [111 | 400] LR: 0.027402\n",
      "2/2 | Loss:0.6871 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3077 | AUROC:0.4942\n",
      "\n",
      "Epoch: [112 | 400] LR: 0.027314\n",
      "2/2 | Loss:0.6915 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6934 | top1:50.1154 | AUROC:0.4933\n",
      "\n",
      "Epoch: [113 | 400] LR: 0.027225\n",
      "2/2 | Loss:0.7076 | top1:42.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6935 | top1:50.1154 | AUROC:0.4933\n",
      "\n",
      "Epoch: [114 | 400] LR: 0.027135\n",
      "2/2 | Loss:0.6870 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6934 | top1:50.1154 | AUROC:0.4939\n",
      "\n",
      "Epoch: [115 | 400] LR: 0.027044\n",
      "2/2 | Loss:0.7031 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1795 | AUROC:0.4947\n",
      "\n",
      "Epoch: [116 | 400] LR: 0.026953\n",
      "2/2 | Loss:0.7149 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7051 | AUROC:0.4939\n",
      "\n",
      "Epoch: [117 | 400] LR: 0.026861\n",
      "2/2 | Loss:0.7080 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0000 | AUROC:0.4944\n",
      "\n",
      "Epoch: [118 | 400] LR: 0.026768\n",
      "2/2 | Loss:0.7040 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.9103 | AUROC:0.4946\n",
      "\n",
      "Epoch: [119 | 400] LR: 0.026675\n",
      "2/2 | Loss:0.7096 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.8077 | AUROC:0.4935\n",
      "\n",
      "Epoch: [120 | 400] LR: 0.026581\n",
      "2/2 | Loss:0.7060 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:49.8590 | AUROC:0.4931\n",
      "\n",
      "Epoch: [121 | 400] LR: 0.026486\n",
      "2/2 | Loss:0.6831 | top1:55.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.4744 | AUROC:0.4925\n",
      "\n",
      "Epoch: [122 | 400] LR: 0.026391\n",
      "2/2 | Loss:0.6790 | top1:58.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7564 | AUROC:0.4951\n",
      "\n",
      "Epoch: [123 | 400] LR: 0.026295\n",
      "2/2 | Loss:0.6991 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.0897 | AUROC:0.4923\n",
      "\n",
      "Epoch: [124 | 400] LR: 0.026199\n",
      "2/2 | Loss:0.6932 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4103 | AUROC:0.4933\n",
      "\n",
      "Epoch: [125 | 400] LR: 0.026102\n",
      "2/2 | Loss:0.7022 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1923 | AUROC:0.4950\n",
      "\n",
      "Epoch: [126 | 400] LR: 0.026004\n",
      "2/2 | Loss:0.7003 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0769 | AUROC:0.4936\n",
      "\n",
      "Epoch: [127 | 400] LR: 0.025906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 | Loss:0.6911 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0513 | AUROC:0.4942\n",
      "\n",
      "Epoch: [128 | 400] LR: 0.025807\n",
      "2/2 | Loss:0.6948 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0641 | AUROC:0.4912\n",
      "\n",
      "Epoch: [129 | 400] LR: 0.025707\n",
      "2/2 | Loss:0.7086 | top1:45.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.3846 | AUROC:0.4941\n",
      "\n",
      "Epoch: [130 | 400] LR: 0.025607\n",
      "2/2 | Loss:0.7082 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1282 | AUROC:0.4932\n",
      "\n",
      "Epoch: [131 | 400] LR: 0.025506\n",
      "2/2 | Loss:0.7066 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1410 | AUROC:0.4944\n",
      "\n",
      "Epoch: [132 | 400] LR: 0.025405\n",
      "2/2 | Loss:0.6872 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.0000 | AUROC:0.4936\n",
      "\n",
      "Epoch: [133 | 400] LR: 0.025303\n",
      "2/2 | Loss:0.6968 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.2564 | AUROC:0.4952\n",
      "\n",
      "Epoch: [134 | 400] LR: 0.025200\n",
      "2/2 | Loss:0.6981 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.0769 | AUROC:0.4944\n",
      "\n",
      "Epoch: [135 | 400] LR: 0.025097\n",
      "2/2 | Loss:0.6957 | top1:54.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4744 | AUROC:0.4948\n",
      "\n",
      "Epoch: [136 | 400] LR: 0.024993\n",
      "2/2 | Loss:0.7035 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2436 | AUROC:0.4940\n",
      "\n",
      "Epoch: [137 | 400] LR: 0.024889\n",
      "2/2 | Loss:0.7027 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.2051 | AUROC:0.4932\n",
      "\n",
      "Epoch: [138 | 400] LR: 0.024784\n",
      "2/2 | Loss:0.7029 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1282 | AUROC:0.4932\n",
      "\n",
      "Epoch: [139 | 400] LR: 0.024679\n",
      "2/2 | Loss:0.6876 | top1:54.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7949 | AUROC:0.4951\n",
      "\n",
      "Epoch: [140 | 400] LR: 0.024573\n",
      "2/2 | Loss:0.7073 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1410 | AUROC:0.4940\n",
      "\n",
      "Epoch: [141 | 400] LR: 0.024467\n",
      "2/2 | Loss:0.6927 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1538 | AUROC:0.4937\n",
      "\n",
      "Epoch: [142 | 400] LR: 0.024360\n",
      "2/2 | Loss:0.6985 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9487 | AUROC:0.4940\n",
      "\n",
      "Epoch: [143 | 400] LR: 0.024253\n",
      "2/2 | Loss:0.6960 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0000 | AUROC:0.4927\n",
      "\n",
      "Epoch: [144 | 400] LR: 0.024145\n",
      "2/2 | Loss:0.7075 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0000 | AUROC:0.4935\n",
      "\n",
      "Epoch: [145 | 400] LR: 0.024036\n",
      "2/2 | Loss:0.7101 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7051 | AUROC:0.4949\n",
      "\n",
      "Epoch: [146 | 400] LR: 0.023927\n",
      "2/2 | Loss:0.7018 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.8846 | AUROC:0.4933\n",
      "\n",
      "Epoch: [147 | 400] LR: 0.023818\n",
      "2/2 | Loss:0.7106 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.0897 | AUROC:0.4949\n",
      "\n",
      "Epoch: [148 | 400] LR: 0.023708\n",
      "2/2 | Loss:0.6957 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6933 | top1:50.0385 | AUROC:0.4936\n",
      "\n",
      "Epoch: [149 | 400] LR: 0.023598\n",
      "2/2 | Loss:0.6943 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6932 | top1:50.1154 | AUROC:0.4948\n",
      "\n",
      "Epoch: [150 | 400] LR: 0.023487\n",
      "2/2 | Loss:0.6987 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2821 | AUROC:0.4930\n",
      "\n",
      "Epoch: [151 | 400] LR: 0.023376\n",
      "2/2 | Loss:0.7002 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7308 | AUROC:0.4937\n",
      "\n",
      "Epoch: [152 | 400] LR: 0.002326\n",
      "2/2 | Loss:0.6874 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7692 | AUROC:0.4930\n",
      "\n",
      "Epoch: [153 | 400] LR: 0.002315\n",
      "2/2 | Loss:0.6982 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6154 | AUROC:0.4941\n",
      "\n",
      "Epoch: [154 | 400] LR: 0.002304\n",
      "2/2 | Loss:0.7019 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6026 | AUROC:0.4939\n",
      "\n",
      "Epoch: [155 | 400] LR: 0.002293\n",
      "2/2 | Loss:0.6952 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6795 | AUROC:0.4941\n",
      "\n",
      "Epoch: [156 | 400] LR: 0.002281\n",
      "2/2 | Loss:0.7050 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.4744 | AUROC:0.4940\n",
      "\n",
      "Epoch: [157 | 400] LR: 0.002270\n",
      "2/2 | Loss:0.7066 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6538 | AUROC:0.4934\n",
      "\n",
      "Epoch: [158 | 400] LR: 0.002258\n",
      "2/2 | Loss:0.6908 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7692 | AUROC:0.4942\n",
      "\n",
      "Epoch: [159 | 400] LR: 0.002247\n",
      "2/2 | Loss:0.7069 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.5128 | AUROC:0.4918\n",
      "\n",
      "Epoch: [160 | 400] LR: 0.002235\n",
      "2/2 | Loss:0.6955 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9615 | AUROC:0.4937\n",
      "\n",
      "Epoch: [161 | 400] LR: 0.002224\n",
      "2/2 | Loss:0.7022 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9231 | AUROC:0.4944\n",
      "\n",
      "Epoch: [162 | 400] LR: 0.002212\n",
      "2/2 | Loss:0.6999 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3718 | AUROC:0.4946\n",
      "\n",
      "Epoch: [163 | 400] LR: 0.002201\n",
      "2/2 | Loss:0.7011 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2821 | AUROC:0.4945\n",
      "\n",
      "Epoch: [164 | 400] LR: 0.002189\n",
      "2/2 | Loss:0.6943 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2949 | AUROC:0.4917\n",
      "\n",
      "Epoch: [165 | 400] LR: 0.002177\n",
      "2/2 | Loss:0.6911 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4487 | AUROC:0.4931\n",
      "\n",
      "Epoch: [166 | 400] LR: 0.002166\n",
      "2/2 | Loss:0.7204 | top1:42.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4359 | AUROC:0.4924\n",
      "\n",
      "Epoch: [167 | 400] LR: 0.002154\n",
      "2/2 | Loss:0.6998 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4231 | AUROC:0.4946\n",
      "\n",
      "Epoch: [168 | 400] LR: 0.002142\n",
      "2/2 | Loss:0.6975 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4231 | AUROC:0.4930\n",
      "\n",
      "Epoch: [169 | 400] LR: 0.002130\n",
      "2/2 | Loss:0.7044 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4231 | AUROC:0.4930\n",
      "\n",
      "Epoch: [170 | 400] LR: 0.002118\n",
      "2/2 | Loss:0.6986 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2949 | AUROC:0.4950\n",
      "\n",
      "Epoch: [171 | 400] LR: 0.002106\n",
      "2/2 | Loss:0.6906 | top1:56.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2308 | AUROC:0.4949\n",
      "\n",
      "Epoch: [172 | 400] LR: 0.002094\n",
      "2/2 | Loss:0.6958 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3205 | AUROC:0.4943\n",
      "\n",
      "Epoch: [173 | 400] LR: 0.002082\n",
      "2/2 | Loss:0.6990 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3590 | AUROC:0.4942\n",
      "\n",
      "Epoch: [174 | 400] LR: 0.002070\n",
      "2/2 | Loss:0.6897 | top1:53.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2436 | AUROC:0.4953\n",
      "\n",
      "Epoch: [175 | 400] LR: 0.002058\n",
      "2/2 | Loss:0.7031 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.0513 | AUROC:0.4935\n",
      "\n",
      "Epoch: [176 | 400] LR: 0.002046\n",
      "2/2 | Loss:0.7037 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4872 | AUROC:0.4935\n",
      "\n",
      "Epoch: [177 | 400] LR: 0.002034\n",
      "2/2 | Loss:0.7053 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2179 | AUROC:0.4944\n",
      "\n",
      "Epoch: [178 | 400] LR: 0.002022\n",
      "2/2 | Loss:0.6844 | top1:55.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2564 | AUROC:0.4931\n",
      "\n",
      "Epoch: [179 | 400] LR: 0.002010\n",
      "2/2 | Loss:0.7070 | top1:44.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2821 | AUROC:0.4935\n",
      "\n",
      "Epoch: [180 | 400] LR: 0.001998\n",
      "2/2 | Loss:0.7037 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4103 | AUROC:0.4932\n",
      "\n",
      "Epoch: [181 | 400] LR: 0.001986\n",
      "2/2 | Loss:0.7045 | top1:45.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3462 | AUROC:0.4938\n",
      "\n",
      "Epoch: [182 | 400] LR: 0.001974\n",
      "2/2 | Loss:0.6956 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3846 | AUROC:0.4935\n",
      "\n",
      "Epoch: [183 | 400] LR: 0.001961\n",
      "2/2 | Loss:0.6931 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4744 | AUROC:0.4950\n",
      "\n",
      "Epoch: [184 | 400] LR: 0.001949\n",
      "2/2 | Loss:0.7039 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9103 | AUROC:0.4951\n",
      "\n",
      "Epoch: [185 | 400] LR: 0.001937\n",
      "2/2 | Loss:0.6963 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.5256 | AUROC:0.4949\n",
      "\n",
      "Epoch: [186 | 400] LR: 0.001924\n",
      "2/2 | Loss:0.6913 | top1:54.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6538 | AUROC:0.4941\n",
      "\n",
      "Epoch: [187 | 400] LR: 0.001912\n",
      "2/2 | Loss:0.7038 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.6538 | AUROC:0.4942\n",
      "\n",
      "Epoch: [188 | 400] LR: 0.001900\n",
      "2/2 | Loss:0.6940 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.7436 | AUROC:0.4935\n",
      "\n",
      "Epoch: [189 | 400] LR: 0.001887\n",
      "2/2 | Loss:0.6883 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.5513 | AUROC:0.4945\n",
      "\n",
      "Epoch: [190 | 400] LR: 0.001875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 | Loss:0.6893 | top1:57.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9103 | AUROC:0.4943\n",
      "\n",
      "Epoch: [191 | 400] LR: 0.001863\n",
      "2/2 | Loss:0.7004 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2564 | AUROC:0.4934\n",
      "\n",
      "Epoch: [192 | 400] LR: 0.001850\n",
      "2/2 | Loss:0.7014 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2564 | AUROC:0.4938\n",
      "\n",
      "Epoch: [193 | 400] LR: 0.001838\n",
      "2/2 | Loss:0.6933 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4930\n",
      "\n",
      "Epoch: [194 | 400] LR: 0.001825\n",
      "2/2 | Loss:0.6990 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3333 | AUROC:0.4926\n",
      "\n",
      "Epoch: [195 | 400] LR: 0.001813\n",
      "2/2 | Loss:0.7035 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2692 | AUROC:0.4925\n",
      "\n",
      "Epoch: [196 | 400] LR: 0.001801\n",
      "2/2 | Loss:0.6934 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9615 | AUROC:0.4951\n",
      "\n",
      "Epoch: [197 | 400] LR: 0.001788\n",
      "2/2 | Loss:0.6888 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3333 | AUROC:0.4948\n",
      "\n",
      "Epoch: [198 | 400] LR: 0.001776\n",
      "2/2 | Loss:0.7038 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1538 | AUROC:0.4954\n",
      "\n",
      "Epoch: [199 | 400] LR: 0.001763\n",
      "2/2 | Loss:0.6982 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.8205 | AUROC:0.4951\n",
      "\n",
      "Epoch: [200 | 400] LR: 0.001751\n",
      "2/2 | Loss:0.6914 | top1:57.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:49.9615 | AUROC:0.4927\n",
      "\n",
      "Epoch: [201 | 400] LR: 0.001738\n",
      "2/2 | Loss:0.6876 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.0769 | AUROC:0.4913\n",
      "\n",
      "Epoch: [202 | 400] LR: 0.001726\n",
      "2/2 | Loss:0.6969 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1795 | AUROC:0.4951\n",
      "\n",
      "Epoch: [203 | 400] LR: 0.001713\n",
      "2/2 | Loss:0.6911 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3846 | AUROC:0.4947\n",
      "\n",
      "Epoch: [204 | 400] LR: 0.001700\n",
      "2/2 | Loss:0.6934 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4744 | AUROC:0.4937\n",
      "\n",
      "Epoch: [205 | 400] LR: 0.001688\n",
      "2/2 | Loss:0.7065 | top1:44.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3462 | AUROC:0.4947\n",
      "\n",
      "Epoch: [206 | 400] LR: 0.001675\n",
      "2/2 | Loss:0.6950 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4359 | AUROC:0.4938\n",
      "\n",
      "Epoch: [207 | 400] LR: 0.001663\n",
      "2/2 | Loss:0.7084 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4744 | AUROC:0.4930\n",
      "\n",
      "Epoch: [208 | 400] LR: 0.001650\n",
      "2/2 | Loss:0.6882 | top1:57.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2949 | AUROC:0.4937\n",
      "\n",
      "Epoch: [209 | 400] LR: 0.001638\n",
      "2/2 | Loss:0.6961 | top1:52.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3462 | AUROC:0.4935\n",
      "\n",
      "Epoch: [210 | 400] LR: 0.001625\n",
      "2/2 | Loss:0.7040 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4872 | AUROC:0.4941\n",
      "\n",
      "Epoch: [211 | 400] LR: 0.001613\n",
      "2/2 | Loss:0.6952 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4103 | AUROC:0.4934\n",
      "\n",
      "Epoch: [212 | 400] LR: 0.001600\n",
      "2/2 | Loss:0.6915 | top1:57.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.5000 | AUROC:0.4934\n",
      "\n",
      "Epoch: [213 | 400] LR: 0.001587\n",
      "2/2 | Loss:0.6935 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3590 | AUROC:0.4945\n",
      "\n",
      "Epoch: [214 | 400] LR: 0.001575\n",
      "2/2 | Loss:0.7041 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4872 | AUROC:0.4940\n",
      "\n",
      "Epoch: [215 | 400] LR: 0.001562\n",
      "2/2 | Loss:0.6970 | top1:49.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3462 | AUROC:0.4953\n",
      "\n",
      "Epoch: [216 | 400] LR: 0.001550\n",
      "2/2 | Loss:0.6988 | top1:53.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2564 | AUROC:0.4922\n",
      "\n",
      "Epoch: [217 | 400] LR: 0.001537\n",
      "2/2 | Loss:0.6998 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2564 | AUROC:0.4959\n",
      "\n",
      "Epoch: [218 | 400] LR: 0.001525\n",
      "2/2 | Loss:0.6996 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2821 | AUROC:0.4954\n",
      "\n",
      "Epoch: [219 | 400] LR: 0.001512\n",
      "2/2 | Loss:0.6946 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4231 | AUROC:0.4939\n",
      "\n",
      "Epoch: [220 | 400] LR: 0.001500\n",
      "2/2 | Loss:0.7048 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.4103 | AUROC:0.4951\n",
      "\n",
      "Epoch: [221 | 400] LR: 0.001487\n",
      "2/2 | Loss:0.7092 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.3846 | AUROC:0.4950\n",
      "\n",
      "Epoch: [222 | 400] LR: 0.001474\n",
      "2/2 | Loss:0.7003 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2692 | AUROC:0.4943\n",
      "\n",
      "Epoch: [223 | 400] LR: 0.001462\n",
      "2/2 | Loss:0.7021 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1538 | AUROC:0.4939\n",
      "\n",
      "Epoch: [224 | 400] LR: 0.001449\n",
      "2/2 | Loss:0.6819 | top1:58.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1795 | AUROC:0.4934\n",
      "\n",
      "Epoch: [225 | 400] LR: 0.001437\n",
      "2/2 | Loss:0.7083 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4946\n",
      "\n",
      "Epoch: [226 | 400] LR: 0.001424\n",
      "2/2 | Loss:0.7087 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4937\n",
      "\n",
      "Epoch: [227 | 400] LR: 0.000141\n",
      "2/2 | Loss:0.6919 | top1:52.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4925\n",
      "\n",
      "Epoch: [228 | 400] LR: 0.000140\n",
      "2/2 | Loss:0.7055 | top1:45.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4936\n",
      "\n",
      "Epoch: [229 | 400] LR: 0.000139\n",
      "2/2 | Loss:0.6970 | top1:50.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4938\n",
      "\n",
      "Epoch: [230 | 400] LR: 0.000137\n",
      "2/2 | Loss:0.6937 | top1:55.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4936\n",
      "\n",
      "Epoch: [231 | 400] LR: 0.000136\n",
      "2/2 | Loss:0.7101 | top1:45.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4948\n",
      "\n",
      "Epoch: [232 | 400] LR: 0.000135\n",
      "2/2 | Loss:0.7022 | top1:46.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4943\n",
      "\n",
      "Epoch: [233 | 400] LR: 0.000134\n",
      "2/2 | Loss:0.7018 | top1:51.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4952\n",
      "\n",
      "Epoch: [234 | 400] LR: 0.000132\n",
      "2/2 | Loss:0.6956 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4942\n",
      "\n",
      "Epoch: [235 | 400] LR: 0.000131\n",
      "2/2 | Loss:0.6843 | top1:58.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4946\n",
      "\n",
      "Epoch: [236 | 400] LR: 0.000130\n",
      "2/2 | Loss:0.7000 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4938\n",
      "\n",
      "Epoch: [237 | 400] LR: 0.000129\n",
      "2/2 | Loss:0.6947 | top1:47.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4956\n",
      "\n",
      "Epoch: [238 | 400] LR: 0.000128\n",
      "2/2 | Loss:0.7053 | top1:46.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4944\n",
      "\n",
      "Epoch: [239 | 400] LR: 0.000126\n",
      "2/2 | Loss:0.6987 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4953\n",
      "\n",
      "Epoch: [240 | 400] LR: 0.000125\n",
      "2/2 | Loss:0.6988 | top1:48.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4933\n",
      "\n",
      "Epoch: [241 | 400] LR: 0.000124\n",
      "2/2 | Loss:0.6915 | top1:56.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4951\n",
      "\n",
      "Epoch: [242 | 400] LR: 0.000123\n",
      "2/2 | Loss:0.6953 | top1:49.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1795 | AUROC:0.4927\n",
      "\n",
      "Epoch: [243 | 400] LR: 0.000121\n",
      "2/2 | Loss:0.6929 | top1:54.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1795 | AUROC:0.4935\n",
      "\n",
      "Epoch: [244 | 400] LR: 0.000120\n",
      "2/2 | Loss:0.6954 | top1:50.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4913\n",
      "\n",
      "Epoch: [245 | 400] LR: 0.000119\n",
      "2/2 | Loss:0.7056 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4942\n",
      "\n",
      "Epoch: [246 | 400] LR: 0.000118\n",
      "2/2 | Loss:0.6967 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.2051 | AUROC:0.4940\n",
      "\n",
      "Epoch: [247 | 400] LR: 0.000117\n",
      "2/2 | Loss:0.6889 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4943\n",
      "\n",
      "Epoch: [248 | 400] LR: 0.000115\n",
      "2/2 | Loss:0.6975 | top1:51.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4941\n",
      "\n",
      "Epoch: [249 | 400] LR: 0.000114\n",
      "2/2 | Loss:0.7053 | top1:48.0000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1795 | AUROC:0.4949\n",
      "\n",
      "Epoch: [250 | 400] LR: 0.000113\n",
      "2/2 | Loss:0.7041 | top1:47.5000 | AUROC:0.0000\n",
      "78/78 | Loss:0.6931 | top1:50.1923 | AUROC:0.4965\n",
      "\n",
      "Epoch: [251 | 400] LR: 0.000112\n",
      "2/2 | Loss:0.6877 | top1:52.0000 | AUROC:0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    state['lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, epochs, state['lr']))\n",
    "    \n",
    "    train_loss, train_acc, train_auroc = train(train_loader, model, criterion, optimizer, epoch, use_cuda)\n",
    "    test_loss, test_acc, test_auroc = test(val_loader, model, criterion, epoch, use_cuda)\n",
    "    \n",
    "    logger.append([state['lr'], train_loss, test_loss, train_acc, test_acc, train_auroc, test_auroc])\n",
    "    scheduler_warmup.step()\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict' : model.state_dict(),\n",
    "        'acc': test_acc,\n",
    "        'best_acc': best_acc,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
